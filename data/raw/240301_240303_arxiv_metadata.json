{
    "2106.10479": {
        "title": "Practical Transferability Estimation for Image Classification Tasks",
        "authors": [
            "Yang Tan",
            "Yang Li",
            "Shao-Lun Huang"
        ],
        "comments": "This paper is not the latest version. Please refer to Transferability-Guided Cross-Domain Cross-Task Transfer Learning (IEEE TNNLS'24) for more details.this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Transferability estimation is an essential problem in transfer learning to predict how good the performance is when transferring a source model (or source task) to a target task. Recent analytical transferability metrics have been widely used for source model selection and multi-task learning. A major challenge is how to make transfereability estimation robust under the cross-domain cross-task settings. The recently proposed OTCE score solves this problem by considering both domain and task differences, with the help of transfer experiences on auxiliary tasks, which causes an efficiency overhead. In this work, we propose a practical transferability metric called JC-NCE score that dramatically improves the robustness of the task difference estimation in OTCE, thus removing the need for auxiliary tasks. Specifically, we build the joint correspondences between source and target data via solving an optimal transport problem with a ground cost considering both the sample distance and label distance, and then compute the transferability score as the negative conditional entropy of the matched labels. Extensive validations under the intra-dataset and inter-dataset transfer settings demonstrate that our JC-NCE score outperforms the auxiliary-task free version of OTCE for 7% and 12%, respectively, and is also more robust than other existing transferability metrics on average.\n    "
    },
    "2108.04567": {
        "title": "Robust and Dexterous Dual-arm Tele-Cooperation using Adaptable Impedance Control",
        "authors": [
            "Keyhan Kouhkiloui Babarahmati",
            "Mohammadreza Kasaei",
            "Carlo Tiseo",
            "Michael Mistry",
            "Sethu Vijayakumar"
        ],
        "comments": " ",
        "subjects": "Robotics (cs.RO)",
        "abstract": "In recent years, the need for robots to transition from isolated industrial tasks to shared environments, including human-robot collaboration and teleoperation, has become increasingly evident. Building on the foundation of Fractal Impedance Control (FIC) introduced in our previous work, this paper presents a novel extension to dual-arm tele-cooperation, leveraging the non-linear stiffness and passivity of FIC to adapt to diverse cooperative scenarios. Unlike traditional impedance controllers, our approach ensures stability without relying on energy tanks, as demonstrated in our prior research. In this paper, we further extend the FIC framework to bimanual operations, allowing for stable and smooth switching between different dynamic tasks without gain tuning. We also introduce a telemanipulation architecture that offers higher transparency and dexterity, addressing the challenges of signal latency and low-bandwidth communication. Through extensive experiments, we validate the robustness of our method and the results confirm the advantages of the FIC approach over traditional impedance controllers, showcasing its potential for applications in planetary exploration and other scenarios requiring dexterous telemanipulation. This paper's contributions include the seamless integration of FIC into multi-arm systems, the ability to perform robust interactions in highly variable environments, and the provision of a comprehensive comparison with competing approaches, thereby significantly enhancing the robustness and adaptability of robotic systems.\n    "
    },
    "2109.15242": {
        "title": "Transferability Estimation for Semantic Segmentation Task",
        "authors": [
            "Yang Tan",
            "Yang Li",
            "Shao-Lun Huang"
        ],
        "comments": "This paper is not the latest version. Please refer to Efficient Prediction of Model Transferability in Semantic Segmentation Tasks (ICIP'23) for more details. this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Transferability estimation is a fundamental problem in transfer learning to predict how good the performance is when transferring a source model (or source task) to a target task. With the guidance of transferability score, we can efficiently select the highly transferable source models without performing the real transfer in practice. Recent analytical transferability metrics are mainly designed for image classification problem, and currently there is no specific investigation for the transferability estimation of semantic segmentation task, which is an essential problem in autonomous driving, medical image analysis, etc. Consequently, we further extend the recent analytical transferability metric OTCE (Optimal Transport based Conditional Entropy) score to the semantic segmentation task. The challenge in applying the OTCE score is the high dimensional segmentation output, which is difficult to find the optimal coupling between so many pixels under an acceptable computation cost. Thus we propose to randomly sample N pixels for computing OTCE score and take the expectation over K repetitions as the final transferability score. Experimental evaluation on Cityscapes, BDD100K and GTA5 datasets demonstrates that the OTCE score highly correlates with the transfer performance.\n    "
    },
    "2110.11385": {
        "title": "Self-Initiated Open World Learning for Autonomous AI Agents",
        "authors": [
            "Bing Liu",
            "Eric Robertson",
            "Scott Grigsby",
            "Sahisnu Mazumder"
        ],
        "comments": "Published in AAAI 2022 Spring Symposium Series",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "As more and more AI agents are used in practice, it is time to think about how to make these agents fully autonomous so that they can learn by themselves in a self-motivated and self-supervised manner rather than being retrained periodically on the initiation of human engineers using expanded training data. As the real-world is an open environment with unknowns or novelties, detecting novelties or unknowns, characterizing them, accommodating or adapting to them, gathering ground-truth training data, and incrementally learning the unknowns/novelties are critical to making the agent more and more knowledgeable and powerful over time. The key challenge is how to automate the process so that it is carried out on the agent's own initiative and through its own interactions with humans and the environment. Since an AI agent usually has a performance task, characterizing each novelty becomes critical and necessary so that the agent can formulate an appropriate response to adapt its behavior to accommodate the novelty and to learn from it to improve the agent's adaptation capability and task performance. The process goes continually without termination. This paper proposes a theoretic framework for this learning paradigm to promote the research of building Self-initiated Open world Learning (SOL) agents. An example SOL agent is also described.\n    "
    },
    "2112.01799": {
        "title": "Global Context with Discrete Diffusion in Vector Quantised Modelling for Image Generation",
        "authors": [
            "Minghui Hu",
            "Yujie Wang",
            "Tat-Jen Cham",
            "Jianfei Yang",
            "P.N.Suganthan"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The integration of Vector Quantised Variational AutoEncoder (VQ-VAE) with autoregressive models as generation part has yielded high-quality results on image generation. However, the autoregressive models will strictly follow the progressive scanning order during the sampling phase. This leads the existing VQ series models to hardly escape the trap of lacking global information. Denoising Diffusion Probabilistic Models (DDPM) in the continuous domain have shown a capability to capture the global context, while generating high-quality images. In the discrete state space, some works have demonstrated the potential to perform text generation and low resolution image generation. We show that with the help of a content-rich discrete visual codebook from VQ-VAE, the discrete diffusion model can also generate high fidelity images with global context, which compensates for the deficiency of the classical autoregressive model along pixel space. Meanwhile, the integration of the discrete VAE with the diffusion model resolves the drawback of conventional autoregressive models being oversized, and the diffusion model which demands excessive time in the sampling process when generating images. It is found that the quality of the generated images is heavily dependent on the discrete visual codebook. Extensive experiments demonstrate that the proposed Vector Quantised Discrete Diffusion Model (VQ-DDM) is able to achieve comparable performance to top-tier methods with low complexity. It also demonstrates outstanding advantages over other vectors quantised with autoregressive models in terms of image inpainting tasks without additional training.\n    "
    },
    "2203.08964": {
        "title": "Point-Unet: A Context-aware Point-based Neural Network for Volumetric Segmentation",
        "authors": [
            "Ngoc-Vuong Ho",
            "Tan Nguyen",
            "Gia-Han Diep",
            "Ngan Le",
            "Binh-Son Hua"
        ],
        "comments": "Accepted in MICCAI 2021",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Medical image analysis using deep learning has recently been prevalent, showing great performance for various downstream tasks including medical image segmentation and its sibling, volumetric image segmentation. Particularly, a typical volumetric segmentation network strongly relies on a voxel grid representation which treats volumetric data as a stack of individual voxel `slices', which allows learning to segment a voxel grid to be as straightforward as extending existing image-based segmentation networks to the 3D domain. However, using a voxel grid representation requires a large memory footprint, expensive test-time and limiting the scalability of the solutions. In this paper, we propose Point-Unet, a novel method that incorporates the efficiency of deep learning with 3D point clouds into volumetric segmentation. Our key idea is to first predict the regions of interest in the volume by learning an attentional probability map, which is then used for sampling the volume into a sparse point cloud that is subsequently segmented using a point-based neural network. We have conducted the experiments on the medical volumetric segmentation task with both a small-scale dataset Pancreas and large-scale datasets BraTS18, BraTS19, and BraTS20 challenges. A comprehensive benchmark on different metrics has shown that our context-aware Point-Unet robustly outperforms the SOTA voxel-based networks at both accuracies, memory usage during training, and time consumption during testing. Our code is available at this https URL.\n    "
    },
    "2204.04377": {
        "title": "Robotic Surgery Remote Mentoring via AR with 3D Scene Streaming and Hand Interaction",
        "authors": [
            "Yonghao Long",
            "Chengkun Li",
            "Qi Dou"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "With the growing popularity of robotic surgery, education becomes increasingly important and urgently needed for the sake of patient safety. However, experienced surgeons have limited accessibility due to their busy clinical schedule or working in a distant city, thus can hardly provide sufficient education resources for novices. Remote mentoring, as an effective way, can help solve this problem, but traditional methods are limited to plain text, audio, or 2D video, which are not intuitive nor vivid. Augmented reality (AR), a thriving technique being widely used for various education scenarios, is promising to offer new possibilities of visual experience and interactive teaching. In this paper, we propose a novel AR-based robotic surgery remote mentoring system with efficient 3D scene visualization and natural 3D hand interaction. Using a head-mounted display (i.e., HoloLens), the mentor can remotely monitor the procedure streamed from the trainee's operation side. The mentor can also provide feedback directly with hand gestures, which is in-turn transmitted to the trainee and viewed in surgical console as guidance. We comprehensively validate the system on both real surgery stereo videos and ex-vivo scenarios of common robotic training tasks (i.e., peg-transfer and suturing). Promising results are demonstrated regarding the fidelity of streamed scene visualization, the accuracy of feedback with hand interaction, and the low-latency of each component in the entire remote mentoring system. This work showcases the feasibility of leveraging AR technology for reliable, flexible and low-cost solutions to robotic surgical education, and holds great potential for clinical applications.\n    "
    },
    "2206.03010": {
        "title": "MS-RNN: A Flexible Multi-Scale Framework for Spatiotemporal Predictive Learning",
        "authors": [
            "Zhifeng Ma",
            "Hao Zhang",
            "Jie Liu"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Spatiotemporal predictive learning, which predicts future frames through historical prior knowledge with the aid of deep learning, is widely used in many fields. Previous work essentially improves the model performance by widening or deepening the network, but it also brings surging memory overhead, which seriously hinders the development and application of this technology. In order to improve the performance without increasing memory consumption, we focus on scale, which is another dimension to improve model performance but with low memory requirement. The effectiveness has been widely demonstrated in many CNN-based tasks such as image classification and semantic segmentation, but it has not been fully explored in recent RNN models. In this paper, learning from the benefit of multi-scale, we propose a general framework named Multi-Scale RNN (MS-RNN) to boost recent RNN models for spatiotemporal predictive learning. We verify the MS-RNN framework by thorough theoretical analyses and exhaustive experiments, where the theory focuses on memory reduction and performance improvement while the experiments employ eight RNN models (ConvLSTM, TrajGRU, PredRNN, PredRNN++, MIM, MotionRNN, PredRNN-V2, and PrecipLSTM) and four datasets (Moving MNIST, TaxiBJ, KTH, and Germany). The results show the efficiency that RNN models incorporating our framework have much lower memory cost but better performance than before. Our code is released at \\url{this https URL}.\n    "
    },
    "2207.05510": {
        "title": "Transferability-Guided Cross-Domain Cross-Task Transfer Learning",
        "authors": [
            "Yang Tan",
            "Enming Zhang",
            "Yang Li",
            "Shao-Lun Huang",
            "Xiao-Ping Zhang"
        ],
        "comments": "This work is accepted by IEEE TNNLS. Please see the official version this https URL may be transferred without notice, after which this version may no longer be accessible",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We propose two novel transferability metrics F-OTCE (Fast Optimal Transport based Conditional Entropy) and JC-OTCE (Joint Correspondence OTCE) to evaluate how much the source model (task) can benefit the learning of the target task and to learn more transferable representations for cross-domain cross-task transfer learning. Unlike the existing metric that requires evaluating the empirical transferability on auxiliary tasks, our metrics are auxiliary-free such that they can be computed much more efficiently. Specifically, F-OTCE estimates transferability by first solving an Optimal Transport (OT) problem between source and target distributions, and then uses the optimal coupling to compute the Negative Conditional Entropy between source and target labels. It can also serve as a loss function to maximize the transferability of the source model before finetuning on the target task. Meanwhile, JC-OTCE improves the transferability robustness of F-OTCE by including label distances in the OT problem, though it may incur additional computation cost. Extensive experiments demonstrate that F-OTCE and JC-OTCE outperform state-of-the-art auxiliary-free metrics by 18.85% and 28.88%, respectively in correlation coefficient with the ground-truth transfer accuracy. By eliminating the training cost of auxiliary tasks, the two metrics reduces the total computation time of the previous method from 43 minutes to 9.32s and 10.78s, respectively, for a pair of tasks. When used as a loss function, F-OTCE shows consistent improvements on the transfer accuracy of the source model in few-shot classification experiments, with up to 4.41% accuracy gain.\n    "
    },
    "2209.00945": {
        "title": "IMG2IMU: Translating Knowledge from Large-Scale Images to IMU Sensing Applications",
        "authors": [
            "Hyungjun Yoon",
            "Hyeongheon Cha",
            "Hoang C. Nguyen",
            "Taesik Gong",
            "Sung-Ju Lee"
        ],
        "comments": "12 pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Pre-training representations acquired via self-supervised learning could achieve high accuracy on even tasks with small training data. Unlike in vision and natural language processing domains, pre-training for IMU-based applications is challenging, as there are few public datasets with sufficient size and diversity to learn generalizable representations. To overcome this problem, we propose IMG2IMU that adapts pre-trained representation from large-scale images to diverse IMU sensing tasks. We convert the sensor data into visually interpretable spectrograms for the model to utilize the knowledge gained from vision. We further present a sensor-aware pre-training method for images that enables models to acquire particularly impactful knowledge for IMU sensing applications. This involves using contrastive learning on our augmentation set customized for the properties of sensor data. Our evaluation with four different IMU sensing tasks shows that IMG2IMU outperforms the baselines pre-trained on sensor data by an average of 9.6%p F1-score, illustrating that vision knowledge can be usefully incorporated into IMU sensing applications where only limited training data is available.\n    "
    },
    "2210.12090": {
        "title": "AutoPrognosis 2.0: Democratizing Diagnostic and Prognostic Modeling in Healthcare with Automated Machine Learning",
        "authors": [
            "Fergus Imrie",
            "Bogdan Cebere",
            "Eoin F. McKinney",
            "Mihaela van der Schaar"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Diagnostic and prognostic models are increasingly important in medicine and inform many clinical decisions. Recently, machine learning approaches have shown improvement over conventional modeling techniques by better capturing complex interactions between patient covariates in a data-driven manner. However, the use of machine learning introduces a number of technical and practical challenges that have thus far restricted widespread adoption of such techniques in clinical settings. To address these challenges and empower healthcare professionals, we present a machine learning framework, AutoPrognosis 2.0, to develop diagnostic and prognostic models. AutoPrognosis leverages state-of-the-art advances in automated machine learning to develop optimized machine learning pipelines, incorporates model explainability tools, and enables deployment of clinical demonstrators, without requiring significant technical expertise. Our framework eliminates the major technical obstacles to predictive modeling with machine learning that currently impede clinical adoption. To demonstrate AutoPrognosis 2.0, we provide an illustrative application where we construct a prognostic risk score for diabetes using the UK Biobank, a prospective study of 502,467 individuals. The models produced by our automated framework achieve greater discrimination for diabetes than expert clinical risk scores. Our risk score has been implemented as a web-based decision support tool and can be publicly accessed by patients and clinicians worldwide. In addition, AutoPrognosis 2.0 is provided as an open-source python package. By open-sourcing our framework as a tool for the community, clinicians and other medical practitioners will be able to readily develop new risk scores, personalized diagnostics, and prognostics using modern machine learning techniques.\n    "
    },
    "2211.10307": {
        "title": "SeaTurtleID: A novel long-span dataset highlighting the importance of timestamps in wildlife re-identification",
        "authors": [
            "Kostas Papafitsoros",
            "Luk\u00e1\u0161 Adam",
            "Vojt\u011bch \u010cerm\u00e1k",
            "Luk\u00e1\u0161 Picek"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This paper introduces SeaTurtleID, the first public large-scale, long-span dataset with sea turtle photographs captured in the wild. The dataset is suitable for benchmarking re-identification methods and evaluating several other computer vision tasks. The dataset consists of 7774 high-resolution photographs of 400 unique individuals collected within 12 years in 1081 encounters. Each photograph is accompanied by rich metadata, e.g., identity label, head segmentation mask, and encounter timestamp. The 12-year span of the dataset makes it the longest-spanned public wild animal dataset with timestamps. By exploiting this unique property, we show that timestamps are necessary for an unbiased evaluation of animal re-identification methods because they allow time-aware splits of the dataset into reference and query sets. We show that time-unaware (random) splits can lead to performance overestimation of more than 100% compared to the time-aware splits for both feature- and CNN-based re-identification methods. We also argue that time-aware splits correspond to more realistic re-identification pipelines than the time-unaware ones. We recommend that animal re-identification methods should only be tested on datasets with timestamps using time-aware splits, and we encourage dataset curators to include such information in the associated metadata.\n    "
    },
    "2212.10529": {
        "title": "Evaluating Psychological Safety of Large Language Models",
        "authors": [
            "Xingxuan Li",
            "Yutong Li",
            "Lin Qiu",
            "Shafiq Joty",
            "Lidong Bing"
        ],
        "comments": "Preprint. Under review",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "In this work, we designed unbiased prompts to systematically evaluate the psychological safety of large language models (LLMs). First, we tested five different LLMs by using two personality tests: Short Dark Triad (SD-3) and Big Five Inventory (BFI). All models scored higher than the human average on SD-3, suggesting a relatively darker personality pattern. Despite being instruction fine-tuned with safety metrics to reduce toxicity, InstructGPT, GPT-3.5, and GPT-4 still showed dark personality patterns; these models scored higher than self-supervised GPT-3 on the Machiavellianism and narcissism traits on SD-3. Then, we evaluated the LLMs in the GPT series by using well-being tests to study the impact of fine-tuning with more training data. We observed a continuous increase in the well-being scores of GPT models. Following these observations, we showed that fine-tuning Llama-2-chat-7B with responses from BFI using direct preference optimization could effectively reduce the psychological toxicity of the model. Based on the findings, we recommended the application of systematic and comprehensive psychological metrics to further evaluate and improve the safety of LLMs.\n    "
    },
    "2302.02237": {
        "title": "Conformalized Semi-supervised Random Forest for Classification and Abnormality Detection",
        "authors": [
            "Yujin Han",
            "Mingwenchan Xu",
            "Leying Guan"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The Random Forests classifier, a widely utilized off-the-shelf classification tool, assumes training and test samples come from the same distribution as other standard classifiers. However, in safety-critical scenarios like medical diagnosis and network attack detection, discrepancies between the training and test sets, including the potential presence of novel outlier samples not appearing during training, can pose significant challenges. To address this problem, we introduce the Conformalized Semi-Supervised Random Forest (CSForest), which couples the conformalization technique Jackknife+aB with semi-supervised tree ensembles to construct a set-valued prediction $C(x)$. Instead of optimizing over the training distribution, CSForest employs unlabeled test samples to enhance accuracy and flag unseen outliers by generating an empty set. Theoretically, we establish CSForest to cover true labels for previously observed inlier classes under arbitrarily label-shift in the test data. We compare CSForest with state-of-the-art methods using synthetic examples and various real-world datasets, under different types of distribution changes in the test domain. Our results highlight CSForest's effective prediction of inliers and its ability to detect outlier samples unique to the test data. In addition, CSForest shows persistently good performance as the sizes of the training and test sets vary. Codes of CSForest are available at this https URL.\n    "
    },
    "2302.04831": {
        "title": "Cooperative Open-ended Learning Framework for Zero-shot Coordination",
        "authors": [
            "Yang Li",
            "Shao Zhang",
            "Jichen Sun",
            "Yali Du",
            "Ying Wen",
            "Xinbing Wang",
            "Wei Pan"
        ],
        "comments": "15 pages with 9 pages main body",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Zero-shot coordination in cooperative artificial intelligence (AI) remains a significant challenge, which means effectively coordinating with a wide range of unseen partners. Previous algorithms have attempted to address this challenge by optimizing fixed objectives within a population to improve strategy or behaviour diversity. However, these approaches can result in a loss of learning and an inability to cooperate with certain strategies within the population, known as cooperative incompatibility. To address this issue, we propose the Cooperative Open-ended LEarning (COLE) framework, which constructs open-ended objectives in cooperative games with two players from the perspective of graph theory to assess and identify the cooperative ability of each strategy. We further specify the framework and propose a practical algorithm that leverages knowledge from game theory and graph theory. Furthermore, an analysis of the learning process of the algorithm shows that it can efficiently overcome cooperative incompatibility. The experimental results in the Overcooked game environment demonstrate that our method outperforms current state-of-the-art methods when coordinating with different-level partners. Our demo is available at this https URL.\n    "
    },
    "2302.07457": {
        "title": "When Demonstrations Meet Generative World Models: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning",
        "authors": [
            "Siliang Zeng",
            "Chenliang Li",
            "Alfredo Garcia",
            "Mingyi Hong"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Offline inverse reinforcement learning (Offline IRL) aims to recover the structure of rewards and environment dynamics that underlie observed actions in a fixed, finite set of demonstrations from an expert agent. Accurate models of expertise in executing a task has applications in safety-sensitive applications such as clinical decision making and autonomous driving. However, the structure of an expert's preferences implicit in observed actions is closely linked to the expert's model of the environment dynamics (i.e. the ``world'' model). Thus, inaccurate models of the world obtained from finite data with limited coverage could compound inaccuracy in estimated rewards. To address this issue, we propose a bi-level optimization formulation of the estimation task wherein the upper level is likelihood maximization based upon a conservative model of the expert's policy (lower level). The policy model is conservative in that it maximizes reward subject to a penalty that is increasing in the uncertainty of the estimated model of the world. We propose a new algorithmic framework to solve the bi-level optimization problem formulation and provide statistical and computational guarantees of performance for the associated optimal reward estimator. Finally, we demonstrate that the proposed algorithm outperforms the state-of-the-art offline IRL and imitation learning benchmarks by a large margin, over the continuous control tasks in MuJoCo and different datasets in the D4RL benchmark.\n    "
    },
    "2303.17355": {
        "title": "Acoustic Soft Tactile Skin (AST Skin)",
        "authors": [
            "Vishnu Rajendran S",
            "Willow Mandil",
            "Simon Parsons",
            "Amir Ghalamzan E"
        ],
        "comments": "IEEE International Conference on Robotics and Automation (ICRA) 2024 (accepted)",
        "subjects": "Robotics (cs.RO)",
        "abstract": "This paper presents a novel soft tactile skin (STS) technology operating with sound waves. In this innovative approach, the sound waves generated by a speaker travel in channels embedded in a soft membrane and get modulated due to a deformation of the channel when pressed by an external force and received by a microphone at the end of the channel. The sensor leverages regression and classification methods for estimating the normal force and its contact location. Our sensor can be affixed to any robot part, e.g., end effectors or arm. We tested several regression and classifier methods to learn the relation between sound wave modulation, the applied force, and its location, respectively and picked the best-performing models for force and location predictions. Our novel tactile sensor yields 93% of the force estimation within 1.5 N tolerances for a range of 0-30+1 N and estimates contact locations with over 96% accuracy. We also demonstrated the performance of STS technology for a real-time gripping force control application.\n    "
    },
    "2304.00933": {
        "title": "Knowledge Accumulation in Continually Learned Representations and the Issue of Feature Forgetting",
        "authors": [
            "Timm Hess",
            "Eli Verwimp",
            "Gido M. van de Ven",
            "Tinne Tuytelaars"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Continual learning research has shown that neural networks suffer from catastrophic forgetting \"at the output level\", but it is debated whether this is also the case at the level of learned representations. Multiple recent studies ascribe representations a certain level of innate robustness against forgetting - that they only forget minimally and no critical information. We revisit and expand upon the experiments that revealed this difference in forgetting and illustrate the coexistence of two phenomena that affect the quality of continually learned representations: knowledge accumulation and feature forgetting. Carefully taking both aspects into account, we show that, even though it is true that feature forgetting can be small in absolute terms, newly learned information tends to be forgotten just as catastrophically at the level of the representation as it is at the output level. Next we show that this feature forgetting is problematic as it substantially slows down knowledge accumulation. Finally, we study how feature forgetting and knowledge accumulation are affected by different types of continual learning methods.\n    "
    },
    "2304.09248": {
        "title": "Real-Time Helmet Violation Detection in AI City Challenge 2023 with Genetic Algorithm-Enhanced YOLOv5",
        "authors": [
            "Elham Soltanikazemi",
            "Ashwin Dhakal",
            "Bijaya Kumar Hatuwal",
            "Imad Eddine Toubal",
            "Armstrong Aboah",
            "Kannappan Palaniappan"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This research focuses on real-time surveillance systems as a means for tackling the issue of non-compliance with helmet regulations, a practice that considerably amplifies the risk for motorcycle drivers or riders. Despite the well-established advantages of helmet usage, achieving widespread compliance remains challenging due to diverse contributing factors. To effectively address this concern, real-time monitoring and enforcement of helmet laws have been proposed as a plausible solution. However, previous attempts at real-time helmet violation detection have been hindered by their limited ability to operate in real-time. To overcome this limitation, the current paper introduces a novel real-time helmet violation detection system that utilizes the YOLOv5 single-stage object detection model. This model is trained on the 2023 NVIDIA AI City Challenge 2023 Track 5 dataset. The optimal hyperparameters for training the model are determined using genetic algorithms. Additionally, data augmentation and various sampling techniques are implemented to enhance the model's performance. The efficacy of the models is evaluated using precision, recall, and mean Average Precision (mAP) metrics. The results demonstrate impressive precision, recall, and mAP scores of 0.848, 0.599, and 0.641, respectively for the training data. Furthermore, the model achieves notable mAP score of 0.6667 for the test datasets, leading to a commendable 4th place rank in the public leaderboard. This innovative approach represents a notable breakthrough in the field and holds immense potential to substantially enhance motorcycle safety. By enabling real-time monitoring and enforcement capabilities, this system has the capacity to contribute towards increased compliance with helmet laws, thereby effectively reducing the risks faced by motorcycle riders and passengers.\n    "
    },
    "2304.10398": {
        "title": "Multi-label Node Classification On Graph-Structured Data",
        "authors": [
            "Tianqi Zhao",
            "Ngan Thi Dong",
            "Alan Hanjalic",
            "Megha Khosla"
        ],
        "comments": "Published in TMLR 2023. Link: this https URL",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Graph Neural Networks (GNNs) have shown state-of-the-art improvements in node classification tasks on graphs. While these improvements have been largely demonstrated in a multi-class classification scenario, a more general and realistic scenario in which each node could have multiple labels has so far received little attention. The first challenge in conducting focused studies on multi-label node classification is the limited number of publicly available multi-label graph datasets. Therefore, as our first contribution, we collect and release three real-world biological datasets and develop a multi-label graph generator to generate datasets with tunable properties. While high label similarity (high homophily) is usually attributed to the success of GNNs, we argue that a multi-label scenario does not follow the usual semantics of homophily and heterophily so far defined for a multi-class scenario. As our second contribution, we define homophily and Cross-Class Neighborhood Similarity for the multi-label scenario and provide a thorough analyses of the collected $9$ multi-label datasets. Finally, we perform a large-scale comparative study with $8$ methods and $9$ datasets and analyse the performances of the methods to assess the progress made by current state of the art in the multi-label node classification scenario. We release our benchmark at this https URL.\n    "
    },
    "2305.02215": {
        "title": "Exploring Linguistic Properties of Monolingual BERTs with Typological Classification among Languages",
        "authors": [
            "Elena Sofia Ruzzetti",
            "Federico Ranaldi",
            "Felicia Logozzo",
            "Michele Mastromattei",
            "Leonardo Ranaldi",
            "Fabio Massimo Zanzotto"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "The impressive achievements of transformers force NLP researchers to delve into how these models represent the underlying structure of natural language. In this paper, we propose a novel standpoint to investigate the above issue: using typological similarities among languages to observe how their respective monolingual models encode structural information. We aim to layer-wise compare transformers for typologically similar languages to observe whether these similarities emerge for particular layers. For this investigation, we propose to use Centered Kernel Alignment to measure similarity among weight matrices. We found that syntactic typological similarity is consistent with the similarity between the weights in the middle layers, which are the pretrained BERT layers to which syntax encoding is generally attributed. Moreover, we observe that a domain adaptation on semantically equivalent texts enhances this similarity among weight matrices.\n    "
    },
    "2305.02531": {
        "title": "Can LLMs Capture Human Preferences?",
        "authors": [
            "Ali Goli",
            "Amandeep Singh"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "We explore the viability of Large Language Models (LLMs), specifically OpenAI's GPT-3.5 and GPT-4, in emulating human survey respondents and eliciting preferences, with a focus on intertemporal choices. Leveraging the extensive literature on intertemporal discounting for benchmarking, we examine responses from LLMs across various languages and compare them to human responses, exploring preferences between smaller, sooner, and larger, later rewards. Our findings reveal that both GPT models demonstrate less patience than humans, with GPT-3.5 exhibiting a lexicographic preference for earlier rewards, unlike human decision-makers. Though GPT-4 does not display lexicographic preferences, its measured discount rates are still considerably larger than those found in humans. Interestingly, GPT models show greater patience in languages with weak future tense references, such as German and Mandarin, aligning with existing literature that suggests a correlation between language structure and intertemporal preferences. We demonstrate how prompting GPT to explain its decisions, a procedure we term \"chain-of-thought conjoint,\" can mitigate, but does not eliminate, discrepancies between LLM and human responses. While directly eliciting preferences using LLMs may yield misleading results, combining chain-of-thought conjoint with topic modeling aids in hypothesis generation, enabling researchers to explore the underpinnings of preferences. Chain-of-thought conjoint provides a structured framework for marketers to use LLMs to identify potential attributes or factors that can explain preference heterogeneity across different customers and contexts.\n    "
    },
    "2305.08381": {
        "title": "Parameter-efficient Tuning of Large-scale Multimodal Foundation Model",
        "authors": [
            "Haixin Wang",
            "Xinlong Yang",
            "Jianlong Chang",
            "Dian Jin",
            "Jinan Sun",
            "Shikun Zhang",
            "Xiao Luo",
            "Qi Tian"
        ],
        "comments": "Accepted by NeurIPS2023",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Driven by the progress of large-scale pre-training, parameter-efficient transfer learning has gained immense popularity across different subfields of Artificial Intelligence. The core is to adapt the model to downstream tasks with only a small set of parameters. Recently, researchers have leveraged such proven techniques in multimodal tasks and achieve promising results. However, two critical issues remain unresolved: how to further reduce the complexity with lightweight design and how to boost alignment between modalities under extremely low parameters. In this paper, we propose A graceful prompt framework for cross-modal transfer (Aurora) to overcome these challenges. Considering the redundancy in existing architectures, we first utilize the mode approximation to generate 0.1M trainable parameters to implement the multimodal prompt tuning, which explores the low intrinsic dimension with only 0.04% parameters of the pre-trained model. Then, for better modality alignment, we propose the Informative Context Enhancement and Gated Query Transformation module under extremely few parameters scenes. A thorough evaluation on six cross-modal benchmarks shows that it not only outperforms the state-of-the-art but even outperforms the full fine-tuning approach. Our code is available at: this https URL.\n    "
    },
    "2305.09675": {
        "title": "Spatial Computing Opportunities in Biomedical Decision Support: The Atlas-EHR Vision",
        "authors": [
            "Majid Farhadloo",
            "Arun Sharma",
            "Shashi Shekhar",
            "Svetomir N. Markovic"
        ],
        "comments": " ",
        "subjects": "Computers and Society (cs.CY)",
        "abstract": "We consider the problem of reducing the time needed by healthcare professionals to understand patient medical history via the next generation of biomedical decision support. This problem is societally important because it has the potential to improve healthcare quality and patient outcomes. However, navigating electronic health records is challenging due to the high patient-doctor ratios, potentially long medical histories, the urgency of treatment for some medical conditions, and patient variability. The current electronic health record systems provides only a longitudinal view of patient medical history, which is time-consuming to browse, and doctors often need to engage nurses, residents, and others for initial analysis. To overcome this limitation, we envision an alternative spatial representation of patients' histories (e.g., electronic health records (EHRs)) and other biomedical data in the form of Atlas-EHR. Just like Google Maps allows a global, national, regional, and local view, the Atlas-EHR may start with an overview of the patient's anatomy and history before drilling down to spatially anatomical sub-systems, their individual components, or sub-components. Atlas-EHR presents a compelling opportunity for spatial computing since healthcare is almost a fifth of the US economy. However, the traditional spatial computing designed for geographic use cases (e.g., navigation, land-surveys, mapping) faces many hurdles in the biomedical domain. This paper presents a number of open research questions under this theme in five broad areas of spatial computing.\n    "
    },
    "2305.11461": {
        "title": "Hint of Thought prompting: an explainable and zero-shot approach to reasoning tasks with LLMs",
        "authors": [
            "Ioktong Lei",
            "Zhidong Deng"
        ],
        "comments": "preprint, under review",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "As a way of communicating with users and any LLMs like GPT or PaLM2, prompting becomes an increasingly important research topic for better utilization of LLMs. Although simple prompting performs well on single-step questions, it cannot permanently activate the correct knowledge path for multi-step reasoning tasks. The chain of thought (CoT), which often contains zero-shot CoT and few-shot CoT, is a recently developed prompting method that can explain the reasoning process to the LLM and outperforms simple prompting in three challenging reasoning tasks, including arithmetic, symbolic, and commonsense reasoning. In this paper, we propose a novel hint of thought (HoT) prompting with explainability and zero-shot generalization. First, it is decomposed into the following three steps: explainable sub-questions, logical reasoning, and answer extraction. Second, such three steps are sequentially ordered in the format of step-by-step hints, which can be easily adjusted and explained to different tasks. Finally, experimental results demonstrate that our HoT prompting has a significant advantage on the zero-shot reasoning task compared to existing zero-shot CoT. We did zero-shot experiments on math tasks like GSM8K, ADDSUB, AQUA, SVAMP and commonsense tasks such as StrategyQA. In particular, the accuracy of the proposed HoT prompting is improved with GSM8K from 40.50% to 67.80%, with AQUA from 31.9% to 46.4%, with SVAMP from 63.7% to 76.9%, and with ADDSUB from 74.7% to 87.34%, respectively, which even defeats the competitive PoT approach on GSM8k, AQUA, and SVAMP.\n    "
    },
    "2306.00964": {
        "title": "Cocktail: Mixing Multi-Modality Controls for Text-Conditional Image Generation",
        "authors": [
            "Minghui Hu",
            "Jianbin Zheng",
            "Daqing Liu",
            "Chuanxia Zheng",
            "Chaoyue Wang",
            "Dacheng Tao",
            "Tat-Jen Cham"
        ],
        "comments": "Project Page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Text-conditional diffusion models are able to generate high-fidelity images with diverse contents. However, linguistic representations frequently exhibit ambiguous descriptions of the envisioned objective imagery, requiring the incorporation of additional control signals to bolster the efficacy of text-guided diffusion models. In this work, we propose Cocktail, a pipeline to mix various modalities into one embedding, amalgamated with a generalized ControlNet (gControlNet), a controllable normalisation (ControlNorm), and a spatial guidance sampling method, to actualize multi-modal and spatially-refined control for text-conditional diffusion models. Specifically, we introduce a hyper-network gControlNet, dedicated to the alignment and infusion of the control signals from disparate modalities into the pre-trained diffusion model. gControlNet is capable of accepting flexible modality signals, encompassing the simultaneous reception of any combination of modality signals, or the supplementary fusion of multiple modality signals. The control signals are then fused and injected into the backbone model according to our proposed ControlNorm. Furthermore, our advanced spatial guidance sampling methodology proficiently incorporates the control signal into the designated region, thereby circumventing the manifestation of undesired objects within the generated image. We demonstrate the results of our method in controlling various modalities, proving high-quality synthesis and fidelity to multiple external signals.\n    "
    },
    "2306.03034": {
        "title": "Tackling Cooperative Incompatibility for Zero-Shot Human-AI Coordination",
        "authors": [
            "Yang Li",
            "Shao Zhang",
            "Jichen Sun",
            "Wenhao Zhang",
            "Yali Du",
            "Ying Wen",
            "Xinbing Wang",
            "Wei Pan"
        ],
        "comments": "46 pages. arXiv admin note: substantial text overlap with arXiv:2302.04831",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Securing coordination between AI agent and teammates (human players or AI agents) in contexts involving unfamiliar humans continues to pose a significant challenge in Zero-Shot Coordination. The issue of cooperative incompatibility becomes particularly prominent when an AI agent is unsuccessful in synchronizing with certain previously unknown partners. Traditional algorithms have aimed to collaborate with partners by optimizing fixed objectives within a population, fostering diversity in strategies and behaviors. However, these techniques may lead to learning loss and an inability to cooperate with specific strategies within the population, a phenomenon named cooperative incompatibility in learning. In order to solve cooperative incompatibility in learning and effectively address the problem in the context of ZSC, we introduce the Cooperative Open-ended LEarning (COLE) framework, which formulates open-ended objectives in cooperative games with two players using perspectives of graph theory to evaluate and pinpoint the cooperative capacity of each strategy. We present two practical algorithms, specifically \\algo and \\algoR, which incorporate insights from game theory and graph theory. We also show that COLE could effectively overcome the cooperative incompatibility from theoretical and empirical analysis. Subsequently, we created an online Overcooked human-AI experiment platform, the COLE platform, which enables easy customization of questionnaires, model weights, and other aspects. Utilizing the COLE platform, we enlist 130 participants for human experiments. Our findings reveal a preference for our approach over state-of-the-art methods using a variety of subjective metrics. Moreover, objective experimental outcomes in the Overcooked game environment indicate that our method surpasses existing ones when coordinating with previously unencountered AI agents and the human proxy model.\n    "
    },
    "2306.16891": {
        "title": "Harnessing the Power of Hugging Face Transformers for Predicting Mental Health Disorders in Social Networks",
        "authors": [
            "Alireza Pourkeyvan",
            "Ramin Safa",
            "Ali Sorourkhah"
        ],
        "comments": "19 pages, 5 figures",
        "subjects": "Information Retrieval (cs.IR)",
        "abstract": "Early diagnosis of mental disorders and intervention can facilitate the prevention of severe injuries and the improvement of treatment results. Using social media and pre-trained language models, this study explores how user-generated data can be used to predict mental disorder symptoms. Our study compares four different BERT models of Hugging Face with standard machine learning techniques used in automatic depression diagnosis in recent literature. The results show that new models outperform the previous approach with an accuracy rate of up to 97%. Analyzing the results while complementing past findings, we find that even tiny amounts of data (like users' bio descriptions) have the potential to predict mental disorders. We conclude that social media data is an excellent source of mental health screening, and pre-trained models can effectively automate this critical task.\n    "
    },
    "2307.07515": {
        "title": "Artificial intelligence is algorithmic mimicry: why artificial \"agents\" are not (and won't be) proper agents",
        "authors": [
            "Johannes Jaeger"
        ],
        "comments": " ",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "What is the prospect of developing artificial general intelligence (AGI)? I investigate this question by systematically comparing living and algorithmic systems, with a special focus on the notion of \"agency.\" There are three fundamental differences to consider: (1) Living systems are autopoietic, that is, self-manufacturing, and therefore able to set their own intrinsic goals, while algorithms exist in a computational environment with target functions that are both provided by an external agent. (2) Living systems are embodied in the sense that there is no separation between their symbolic and physical aspects, while algorithms run on computational architectures that maximally isolate software from hardware. (3) Living systems experience a large world, in which most problems are ill-defined (and not all definable), while algorithms exist in a small world, in which all problems are well-defined. These three differences imply that living and algorithmic systems have very different capabilities and limitations. In particular, it is extremely unlikely that true AGI (beyond mere mimicry) can be developed in the current algorithmic framework of AI research. Consequently, discussions about the proper development and deployment of algorithmic tools should be shaped around the dangers and opportunities of current narrow AI, not the extremely unlikely prospect of the emergence of true agency in artificial systems.\n    "
    },
    "2307.08773": {
        "title": "\"Customization is Key\": Reconfigurable Content Tokens for Accessible Data Visualizations",
        "authors": [
            "Shuli Jones",
            "Isabella Pedraza Pineros",
            "Daniel Hajas",
            "Jonathan Zong",
            "Arvind Satyanarayan"
        ],
        "comments": "14 pages. 6 figures. 2 tables. ACM CHI Conference 2024",
        "subjects": "Human-Computer Interaction (cs.HC)",
        "abstract": "Customization is crucial for making visualizations accessible to blind and low-vision (BLV) people with widely-varying needs. But what makes for usable or useful customization? We identify four design goals for how BLV people should be able to customize screen-reader-accessible visualizations: presence, or what content is included; verbosity, or how concisely content is presented; ordering, or how content is sequenced; and, duration, or how long customizations are active. To meet these goals, we model a customization as a sequence of content tokens, each with a set of adjustable properties. We instantiate our model by extending Olli, an open-source accessible visualization toolkit, with a settings menu and command box for persistent and ephemeral customization respectively. Through a study with 13 BLV participants, we find that customization increases the ease of identifying and remembering information. However, customization also introduces additional complexity, making it more helpful for users familiar with similar tools.\n    "
    },
    "2307.08924": {
        "title": "Towards Task Sampler Learning for Meta-Learning",
        "authors": [
            "Jingyao Wang",
            "Wenwen Qiang",
            "Xingzhe Su",
            "Changwen Zheng",
            "Fuchun Sun",
            "Hui Xiong"
        ],
        "comments": "28 pages, 11 tables, 12 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Meta-learning aims to learn general knowledge with diverse training tasks conducted from limited data, and then transfer it to new tasks. It is commonly believed that increasing task diversity will enhance the generalization ability of meta-learning models. However, this paper challenges this view through empirical and theoretical analysis. We obtain three conclusions: (i) there is no universal task sampling strategy that can guarantee the optimal performance of meta-learning models; (ii) over-constraining task diversity may incur the risk of under-fitting or over-fitting during training; and (iii) the generalization performance of meta-learning models are affected by task diversity, task entropy, and task difficulty. Based on this insight, we design a novel task sampler, called Adaptive Sampler (ASr). ASr is a plug-and-play module that can be integrated into any meta-learning framework. It dynamically adjusts task weights according to task diversity, task entropy, and task difficulty, thereby obtaining the optimal probability distribution for meta-training tasks. Finally, we conduct experiments on a series of benchmark datasets across various scenarios, and the results demonstrate that ASr has clear advantages.\n    "
    },
    "2307.09465": {
        "title": "Occlusion Aware Student Emotion Recognition based on Facial Action Unit Detection",
        "authors": [
            "Shrouk Wally",
            "Ahmed Elsayed",
            "Islam Alkabbany",
            "Asem Ali",
            "Aly Farag"
        ],
        "comments": "it doesn't meet the requirements of the CVIP Lab concerning authorship and acknowledging the funding sources",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Given that approximately half of science, technology, engineering, and mathematics (STEM) undergraduate students in U.S. colleges and universities leave by the end of the first year [15], it is crucial to improve the quality of classroom environments. This study focuses on monitoring students' emotions in the classroom as an indicator of their engagement and proposes an approach to address this issue. The impact of different facial parts on the performance of an emotional recognition model is evaluated through experimentation. To test the proposed model under partial occlusion, an artificially occluded dataset is introduced. The novelty of this work lies in the proposal of an occlusion-aware architecture for facial action units (AUs) extraction, which employs attention mechanism and adaptive feature learning. The AUs can be used later to classify facial expressions in classroom settings.\nThis research paper's findings provide valuable insights into handling occlusion in analyzing facial images for emotional engagement analysis. The proposed experiments demonstrate the significance of considering occlusion and enhancing the reliability of facial analysis models in classroom environments. These findings can also be extended to other settings where occlusions are prevalent.\n    "
    },
    "2307.10811": {
        "title": "\"It Felt Like Having a Second Mind\": Investigating Human-AI Co-creativity in Prewriting with Large Language Models",
        "authors": [
            "Qian Wan",
            "Siying Hu",
            "Yu Zhang",
            "Piaohong Wang",
            "Bo Wen",
            "Zhicong Lu"
        ],
        "comments": "To appear at ACM CSCW 2024; Accepted to PACM HCI (CSCW); 25 pages, 2 figures",
        "subjects": "Human-Computer Interaction (cs.HC)",
        "abstract": "Prewriting is the process of discovering and developing ideas before a first draft, which requires divergent thinking and often implies unstructured strategies such as diagramming, outlining, free-writing, etc. Although large language models (LLMs) have been demonstrated to be useful for a variety of tasks including creative writing, little is known about how users would collaborate with LLMs to support prewriting. The preferred collaborative role and initiative of LLMs during such a creativity process is also unclear. To investigate human-LLM collaboration patterns and dynamics during prewriting, we conducted a three-session qualitative study with 15 participants in two creative tasks: story writing and slogan writing. The findings indicated that during collaborative prewriting, there appears to be a three-stage iterative Human-AI Co-creativity process that includes Ideation, Illumination, and Implementation stages. This collaborative process champions the human in a dominant role, in addition to mixed and shifting levels of initiative that exist between humans and LLMs. This research also reports on collaboration breakdowns that occur during this process, user perceptions of using existing LLMs during Human-AI Co-creativity, and discusses design implications to support this co-creativity process.\n    "
    },
    "2307.16230": {
        "title": "An Unforgeable Publicly Verifiable Watermark for Large Language Models",
        "authors": [
            "Aiwei Liu",
            "Leyi Pan",
            "Xuming Hu",
            "Shu'ang Li",
            "Lijie Wen",
            "Irwin King",
            "Philip S. Yu"
        ],
        "comments": "ICLR2024, 17 pages, 5 figures, 8 tables",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Recently, text watermarking algorithms for large language models (LLMs) have been proposed to mitigate the potential harms of text generated by LLMs, including fake news and copyright issues. However, current watermark detection algorithms require the secret key used in the watermark generation process, making them susceptible to security breaches and counterfeiting during public detection. To address this limitation, we propose an unforgeable publicly verifiable watermark algorithm that uses two different neural networks for watermark generation and detection, instead of using the same key at both stages. Meanwhile, the token embedding parameters are shared between the generation and detection networks, which makes the detection network achieve a high accuracy very efficiently. Experiments demonstrate that our algorithm attains high detection accuracy and computational efficiency through neural networks with a minimized number of parameters. Subsequent analysis confirms the high complexity involved in forging the watermark from the detection network. Our code and data are available at \\href{this https URL}{this https URL\\_watermark}.\n    "
    },
    "2308.04689": {
        "title": "Web crawler strategies for web pages under robot.txt restriction",
        "authors": [
            "Piyush Vyas",
            "Akhilesh Chauhan",
            "Tushar Mandge",
            "Surbhi Hardikar"
        ],
        "comments": " ",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "In the present time, all know about World Wide Web and work over the Internet daily. In this paper, we introduce the search engines working for keywords that are entered by users to find something. The search engine uses different search algorithms for convenient results for providing to the net surfer. Net surfers go with the top search results but how did the results of web pages get higher ranks over search engines? how the search engine got that all the web pages in the database? This paper gives the answers to all these kinds of basic questions. Web crawlers working for search engines and robot exclusion protocol rules for web crawlers are also addressed in this research paper. Webmaster uses different restriction facts in robot.txt file to instruct web crawler, some basic formats of robot.txt are also mentioned in this paper.\n    "
    },
    "2308.05696": {
        "title": "A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment",
        "authors": [
            "Yingxiu Zhao",
            "Bowen Yu",
            "Binyuan Hui",
            "Haiyang Yu",
            "Fei Huang",
            "Yongbin Li",
            "Nevin L. Zhang"
        ],
        "comments": "LREC-Coling 2024",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Training large language models (LLMs) with open-domain instruction data has yielded remarkable success in aligning to end tasks and human preferences. Extensive research has highlighted the importance of the quality and diversity of instruction data. However, the impact of data complexity, as a crucial metric, remains relatively unexplored from three aspects: (1)where the sustainability of performance improvements with increasing complexity is uncertain; (2)whether the improvement brought by complexity merely comes from introducing more training tokens; and (3)where the potential benefits of incorporating instructions from easy to difficult are not yet fully understood. In this paper, we propose Tree-Instruct to systematically enhance the instruction complexity in a controllable manner. By adding a specified number of nodes to instructions' semantic trees, this approach not only yields new instruction data from the modified tree but also allows us to control the difficulty level of modified instructions. Our preliminary experiments reveal the following insights: (1)Increasing complexity consistently leads to sustained performance improvements of LLMs. (2)Under the same token budget, a few complex instructions outperform diverse yet simple instructions. (3)Curriculum instruction tuning might not yield the anticipated results; focusing on increasing complexity appears to be the key.\n    "
    },
    "2308.10562": {
        "title": "Seeing the Intangible: Survey of Image Classification into High-Level and Abstract Categories",
        "authors": [
            "Delfina Sol Martinez Pandiani",
            "Valentina Presutti"
        ],
        "comments": "Preprint",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The field of Computer Vision (CV) is increasingly shifting towards ``high-level'' visual sensemaking tasks, yet the exact nature of these tasks remains unclear and tacit. This survey paper addresses this ambiguity by systematically reviewing research on high-level visual understanding, focusing particularly on Abstract Concepts (ACs) in automatic image classification. Our survey contributes in three main ways: Firstly, it clarifies the tacit understanding of high-level semantics in CV through a multidisciplinary analysis, and categorization into distinct clusters, including commonsense, emotional, aesthetic, and inductive interpretative semantics. Secondly, it identifies and categorizes computer vision tasks associated with high-level visual sensemaking, offering insights into the diverse research areas within this domain. Lastly, it examines how abstract concepts such as values and ideologies are handled in CV, revealing challenges and opportunities in AC-based image classification. Notably, our survey of AC image classification tasks highlights persistent challenges, such as the limited efficacy of massive datasets and the importance of integrating supplementary information and mid-level features. We emphasize the growing relevance of hybrid AI systems in addressing the multifaceted nature of AC image classification tasks. Overall, this survey enhances our understanding of high-level visual reasoning in CV and lays the groundwork for future research endeavors.\n    "
    },
    "2308.11131": {
        "title": "ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation",
        "authors": [
            "Jianghao Lin",
            "Rong Shan",
            "Chenxu Zhu",
            "Kounianhua Du",
            "Bo Chen",
            "Shigang Quan",
            "Ruiming Tang",
            "Yong Yu",
            "Weinan Zhang"
        ],
        "comments": "Accepted by WWW 2024. Full and More Readable Version",
        "subjects": "Information Retrieval (cs.IR)",
        "abstract": "With large language models (LLMs) achieving remarkable breakthroughs in natural language processing (NLP) domains, LLM-enhanced recommender systems have received much attention and have been actively explored currently. In this paper, we focus on adapting and empowering a pure large language model for zero-shot and few-shot recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation domains, i.e., LLMs fail to extract useful information from a textual context of long user behavior sequence, even if the length of context is far from reaching the context limitation of LLMs. To address such an issue and improve the recommendation performance of LLMs, we propose a novel framework, namely Retrieval-enhanced Large Language models (ReLLa) for recommendation tasks in both zero-shot and few-shot settings. For zero-shot recommendation, we perform semantic user behavior retrieval (SUBR) to improve the data quality of testing samples, which greatly reduces the difficulty for LLMs to extract the essential knowledge from user behavior sequences. As for few-shot recommendation, we further design retrieval-enhanced instruction tuning (ReiT) by adopting SUBR as a data augmentation technique for training samples. Specifically, we develop a mixed training dataset consisting of both the original data samples and their retrieval-enhanced counterparts. We conduct extensive experiments on three real-world public datasets to demonstrate the superiority of ReLLa compared with existing baseline models, as well as its capability for lifelong sequential behavior comprehension. To be highlighted, with only less than 10% training samples, few-shot ReLLa can outperform traditional CTR models that are trained on the entire training set (e.g., DCNv2, DIN, SIM). The code is available \\url{this https URL}.\n    "
    },
    "2309.00064": {
        "title": "Ethical Framework for Harnessing the Power of AI in Healthcare and Beyond",
        "authors": [
            "Sidra Nasir",
            "Rizwan Ahmed Khan",
            "Samita Bai"
        ],
        "comments": " ",
        "subjects": "Computers and Society (cs.CY)",
        "abstract": "In the past decade, the deployment of deep learning (Artificial Intelligence (AI)) methods has become pervasive across a spectrum of real-world applications, often in safety-critical contexts. This comprehensive research article rigorously investigates the ethical dimensions intricately linked to the rapid evolution of AI technologies, with a particular focus on the healthcare domain. Delving deeply, it explores a multitude of facets including transparency, adept data management, human oversight, educational imperatives, and international collaboration within the realm of AI advancement. Central to this article is the proposition of a conscientious AI framework, meticulously crafted to accentuate values of transparency, equity, answerability, and a human-centric orientation. The second contribution of the article is the in-depth and thorough discussion of the limitations inherent to AI systems. It astutely identifies potential biases and the intricate challenges of navigating multifaceted contexts. Lastly, the article unequivocally accentuates the pressing need for globally standardized AI ethics principles and frameworks. Simultaneously, it aptly illustrates the adaptability of the ethical framework proposed herein, positioned skillfully to surmount emergent challenges.\n    "
    },
    "2309.05134": {
        "title": "Benchmarking ground truth trajectories with robotic total stations",
        "authors": [
            "Effie Daum",
            "Maxime Vaidis",
            "Fran\u00e7ois Pomerleau"
        ],
        "comments": "Accepted and presented at IROS23, Workshop on Methods for Objective Comparison of Results in Intelligent Robotics Research",
        "subjects": "Robotics (cs.RO)",
        "abstract": "Benchmarks stand as vital cornerstones in elevating SLAM algorithms within mobile robotics. Consequently, ensuring accurate and reproducible ground truth generation is vital for fair evaluation. A majority of outdoor ground truths are generated by GNSS, which can lead to discrepancies over time, especially in covered areas. However, research showed that RTS setups are more precise and can alternatively be used to generate these ground truths. In our work, we compare both RTS and GNSS systems' precision and repeatability through a set of experiments conducted weeks and months apart in the same area. We demonstrated that RTS setups give more reproducible results, with disparities having a median value of 8.6 mm compared to a median value of 10.6 cm coming from a GNSS setup. These results highlight that RTS can be considered to benchmark process for SLAM algorithms with higher precision.\n    "
    },
    "2309.05605": {
        "title": "Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models",
        "authors": [
            "Mansi Sakarvadia",
            "Aswathy Ajith",
            "Arham Khan",
            "Daniel Grzenda",
            "Nathaniel Hudson",
            "Andr\u00e9 Bauer",
            "Kyle Chard",
            "Ian Foster"
        ],
        "comments": "Oral Presentation at BlackboxNLP Workshop at EMNLP 2023",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as \"memories,\" at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%.\n    "
    },
    "2309.06275": {
        "title": "Re-Reading Improves Reasoning in Large Language Models",
        "authors": [
            "Xiaohan Xu",
            "Chongyang Tao",
            "Tao Shen",
            "Can Xu",
            "Hongbo Xu",
            "Guodong Long",
            "Jian-guang Lou"
        ],
        "comments": "25 pages",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "To enhance the reasoning capabilities of off-the-shelf Large Language Models (LLMs), we introduce a simple, yet general and effective prompting method, Re2, i.e., \\textbf{Re}-\\textbf{Re}ading the question as input. Unlike most thought-eliciting prompting methods, such as Chain-of-Thought (CoT), which aim to elicit the reasoning process in the output, Re2 shifts the focus to the input by processing questions twice, thereby enhancing the understanding process. Consequently, Re2 demonstrates strong generality and compatibility with most thought-eliciting prompting methods, including CoT. Crucially, Re2 facilitates a \"bidirectional\" encoding in unidirectional decoder-only LLMs because the first pass could provide global information for the second pass. We begin with a preliminary empirical study as the foundation of Re2, illustrating its potential to enable \"bidirectional\" attention mechanisms. We then evaluate Re2 on extensive reasoning benchmarks across 14 datasets, spanning 112 experiments, to validate its effectiveness and generality. Our findings indicate that, with the exception of a few scenarios on vanilla ChatGPT, Re2 consistently enhances the reasoning performance of LLMs through a simple re-reading strategy. Further analyses reveal Re2's adaptability, showing how it can be effectively integrated with different LLMs, thought-eliciting prompting, and ensemble strategies. Our code is available at \\url{this https URL}\n    "
    },
    "2309.08315": {
        "title": "i-Octree: A Fast, Lightweight, and Dynamic Octree for Proximity Search",
        "authors": [
            "Jun Zhu",
            "Hongyi Li",
            "Zhepeng Wang",
            "Shengjie Wang",
            "Tao Zhang"
        ],
        "comments": "7 pages, 7 figures",
        "subjects": "Robotics (cs.RO)",
        "abstract": "Establishing the correspondences between newly acquired points and historically accumulated data (i.e., map) through nearest neighbors search is crucial in numerous robotic applications. However, static tree data structures are inadequate to handle large and dynamically growing maps in real-time. To address this issue, we present the i-Octree, a dynamic octree data structure that supports both fast nearest neighbor search and real-time dynamic updates, such as point insertion, deletion, and on-tree down-sampling. The i-Octree is built upon a leaf-based octree and has two key features: a local spatially continuous storing strategy that allows for fast access to points while minimizing memory usage, and local on-tree updates that significantly reduce computation time compared to existing static or dynamic tree structures. The experiments show that i-Octree outperforms contemporary state-of-the-art approaches by achieving, on average, a 19% reduction in runtime on realworld open datasets.\n    "
    },
    "2309.09865": {
        "title": "Contrastive Learning for Enhancing Robust Scene Transfer in Vision-based Agile Flight",
        "authors": [
            "Jiaxu Xing",
            "Leonard Bauersfeld",
            "Yunlong Song",
            "Chunwei Xing",
            "Davide Scaramuzza"
        ],
        "comments": " ",
        "subjects": "Robotics (cs.RO)",
        "abstract": "Scene transfer for vision-based mobile robotics applications is a highly relevant and challenging problem. The utility of a robot greatly depends on its ability to perform a task in the real world, outside of a well-controlled lab environment. Existing scene transfer end-to-end policy learning approaches often suffer from poor sample efficiency or limited generalization capabilities, making them unsuitable for mobile robotics applications. This work proposes an adaptive multi-pair contrastive learning strategy for visual representation learning that enables zero-shot scene transfer and real-world deployment. Control policies relying on the embedding are able to operate in unseen environments without the need for finetuning in the deployment environment. We demonstrate the performance of our approach on the task of agile, vision-based quadrotor flight. Extensive simulation and real-world experiments demonstrate that our approach successfully generalizes beyond the training domain and outperforms all baselines.\n    "
    },
    "2309.12309": {
        "title": "Rehearsal: Simulating Conflict to Teach Conflict Resolution",
        "authors": [
            "Omar Shaikh",
            "Valentino Chai",
            "Michele J. Gelfand",
            "Diyi Yang",
            "Michael S. Bernstein"
        ],
        "comments": "CHI 2024",
        "subjects": "Human-Computer Interaction (cs.HC)",
        "abstract": "Interpersonal conflict is an uncomfortable but unavoidable fact of life. Navigating conflict successfully is a skill -- one that can be learned through deliberate practice -- but few have access to effective training or feedback. To expand this access, we introduce Rehearsal, a system that allows users to rehearse conflicts with a believable simulated interlocutor, explore counterfactual \"what if?\" scenarios to identify alternative conversational paths, and learn through feedback on how and when to apply specific conflict strategies. Users can utilize Rehearsal to practice handling a variety of predefined conflict scenarios, from office disputes to relationship issues, or they can choose to create their own setting. To enable Rehearsal, we develop IRP prompting, a method of conditioning output of a large language model on the influential Interest-Rights-Power (IRP) theory from conflict resolution. Rehearsal uses IRP to generate utterances grounded in conflict resolution theory, guiding users towards counterfactual conflict resolution strategies that help de-escalate difficult conversations. In a between-subjects evaluation, 40 participants engaged in an actual conflict with a confederate after training. Compared to a control group with lecture material covering the same IRP theory, participants with simulated training from Rehearsal significantly improved their performance in the unaided conflict: they reduced their use of escalating competitive strategies by an average of 67%, while doubling their use of cooperative strategies. Overall, Rehearsal highlights the potential effectiveness of language models as tools for learning and practicing interpersonal skills.\n    "
    },
    "2309.12444": {
        "title": "Foundation Metrics for Evaluating Effectiveness of Healthcare Conversations Powered by Generative AI",
        "authors": [
            "Mahyar Abbasian",
            "Elahe Khatibi",
            "Iman Azimi",
            "David Oniani",
            "Zahra Shakeri Hossein Abad",
            "Alexander Thieme",
            "Ram Sriram",
            "Zhongqi Yang",
            "Yanshan Wang",
            "Bryant Lin",
            "Olivier Gevaert",
            "Li-Jia Li",
            "Ramesh Jain",
            "Amir M. Rahmani"
        ],
        "comments": "14 pages, 4 figures, 2 tables, journal paper",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Generative Artificial Intelligence is set to revolutionize healthcare delivery by transforming traditional patient care into a more personalized, efficient, and proactive process. Chatbots, serving as interactive conversational models, will probably drive this patient-centered transformation in healthcare. Through the provision of various services, including diagnosis, personalized lifestyle recommendations, and mental health support, the objective is to substantially augment patient health outcomes, all the while mitigating the workload burden on healthcare providers. The life-critical nature of healthcare applications necessitates establishing a unified and comprehensive set of evaluation metrics for conversational models. Existing evaluation metrics proposed for various generic large language models (LLMs) demonstrate a lack of comprehension regarding medical and health concepts and their significance in promoting patients' well-being. Moreover, these metrics neglect pivotal user-centered aspects, including trust-building, ethics, personalization, empathy, user comprehension, and emotional support. The purpose of this paper is to explore state-of-the-art LLM-based evaluation metrics that are specifically applicable to the assessment of interactive conversational models in healthcare. Subsequently, we present an comprehensive set of evaluation metrics designed to thoroughly assess the performance of healthcare chatbots from an end-user perspective. These metrics encompass an evaluation of language processing abilities, impact on real-world clinical tasks, and effectiveness in user-interactive conversations. Finally, we engage in a discussion concerning the challenges associated with defining and implementing these metrics, with particular emphasis on confounding factors such as the target audience, evaluation methods, and prompt techniques involved in the evaluation process.\n    "
    },
    "2309.13192": {
        "title": "Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation",
        "authors": [
            "Kai Huang",
            "Hanyun Yin",
            "Heng Huang",
            "Wei Gao"
        ],
        "comments": "16 pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Fine-tuning is the most effective way of adapting pre-trained large language models (LLMs) to downstream applications. With the fast growth of LLM-enabled AI applications and democratization of open-souced LLMs, fine-tuning has become possible for non-expert individuals, but intensively performed LLM fine-tuning worldwide could result in significantly high energy consumption and carbon footprint, which may bring large environmental impact. Mitigating such environmental impact towards Green AI directly correlates to reducing the FLOPs of fine-tuning, but existing techniques on efficient LLM fine-tuning can only achieve limited reduction of such FLOPs, due to their ignorance of the backpropagation cost in fine-tuning. To address this limitation, in this paper we present GreenTrainer, a new LLM fine-tuning technique that adaptively evaluates different tensors' backpropagation costs and contributions to the fine-tuned model accuracy, to minimize the fine-tuning cost by selecting the most appropriate set of tensors in training. Such selection in GreenTrainer is made based on a given objective of FLOPs reduction, which can flexibly adapt to the carbon footprint in energy supply and the need in Green AI. Experiment results over multiple open-sourced LLM models and abstractive summarization datasets show that, compared to fine-tuning the whole LLM model, GreenTrainer can save up to 64% FLOPs in fine-tuning without any noticeable model accuracy loss. Compared to the existing fine-tuning techniques such as LoRa, GreenTrainer can achieve up to 4% improvement on model accuracy with on-par FLOPs reduction.\n    "
    },
    "2309.17260": {
        "title": "PlaceNav: Topological Navigation through Place Recognition",
        "authors": [
            "Lauri Suomela",
            "Jussi Kalliola",
            "Harry Edelman",
            "Joni-Kristian K\u00e4m\u00e4r\u00e4inen"
        ],
        "comments": "ICRA2024 camera ready",
        "subjects": "Robotics (cs.RO)",
        "abstract": "Recent results suggest that splitting topological navigation into robot-independent and robot-specific components improves navigation performance by enabling the robot-independent part to be trained with data collected by robots of different types. However, the navigation methods' performance is still limited by the scarcity of suitable training data and they suffer from poor computational scaling.\nIn this work, we present PlaceNav, subdividing the robot-independent part into navigation-specific and generic computer vision components. We utilize visual place recognition for the subgoal selection of the topological navigation pipeline. This makes subgoal selection more efficient and enables leveraging large-scale datasets from non-robotics sources, increasing training data availability. Bayesian filtering, enabled by place recognition, further improves navigation performance by increasing the temporal consistency of subgoals. Our experimental results verify the design and the new method obtains a 76% higher success rate in indoor and 23% higher in outdoor navigation tasks with higher computational efficiency.\n    "
    },
    "2310.03560": {
        "title": "Redefining Digital Health Interfaces with Large Language Models",
        "authors": [
            "Fergus Imrie",
            "Paulius Rauba",
            "Mihaela van der Schaar"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Digital health tools have the potential to significantly improve the delivery of healthcare services. However, their adoption remains comparatively limited due, in part, to challenges surrounding usability and trust. Large Language Models (LLMs) have emerged as general-purpose models with the ability to process complex information and produce human-quality text, presenting a wealth of potential applications in healthcare. Directly applying LLMs in clinical settings is not straightforward, however, with LLMs susceptible to providing inconsistent or nonsensical answers. We demonstrate how LLM-based systems can utilize external tools and provide a novel interface between clinicians and digital technologies. This enhances the utility and practical impact of digital healthcare tools and AI models while addressing current issues with using LLMs in clinical settings such as hallucinations. We illustrate LLM-based interfaces with the example of cardiovascular disease risk prediction. We develop a new prognostic tool using automated machine learning and demonstrate how LLMs can provide a unique interface to both our model and existing risk scores, highlighting the benefit compared to traditional interfaces for digital tools.\n    "
    },
    "2310.07555": {
        "title": "Does resistance to style-transfer equal Global Shape Bias? Measuring network sensitivity to global shape configuration",
        "authors": [
            "Ziqi Wen",
            "Tianqin Li",
            "Zhi Jing",
            "Tai Sing Lee"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Deep learning models are known to exhibit a strong texture bias, while human tends to rely heavily on global shape structure for object recognition. The current benchmark for evaluating a model's global shape bias is a set of style-transferred images with the assumption that resistance to the attack of style transfer is related to the development of global structure sensitivity in the model. In this work, we show that networks trained with style-transfer images indeed learn to ignore style, but its shape bias arises primarily from local detail. We provide a \\textbf{Disrupted Structure Testbench (DiST)} as a direct measurement of global structure sensitivity. Our test includes 2400 original images from ImageNet-1K, each of which is accompanied by two images with the global shapes of the original image disrupted while preserving its texture via the texture synthesis program. We found that \\textcolor{black}{(1) models that performed well on the previous cue-conflict dataset do not fare well in the proposed DiST; (2) the supervised trained Vision Transformer (ViT) lose its global spatial information from positional embedding, leading to no significant advantages over Convolutional Neural Networks (CNNs) on DiST. While self-supervised learning methods, especially mask autoencoder significantly improves the global structure sensitivity of ViT. (3) Improving the global structure sensitivity is orthogonal to resistance to style-transfer, indicating that the relationship between global shape structure and local texture detail is not an either/or relationship. Training with DiST images and style-transferred images are complementary, and can be combined to train network together to enhance the global shape sensitivity and robustness of local features.} Our code will be hosted in github: this https URL\n"
    },
    "2310.07576": {
        "title": "Analyzing Trendy Twitter Hashtags in the 2022 French Election",
        "authors": [
            "Aamir Mandviwalla",
            "Lake Yin",
            "Boleslaw K. Szymanski"
        ],
        "comments": "9 pages, 1 figure, published in Complex Networks and their Applications XII",
        "subjects": "Social and Information Networks (cs.SI)",
        "abstract": "Regressions trained to predict the future activity of social media users need rich features for accurate predictions. Many advanced models exist to generate such features; however, the time complexities of their computations are often prohibitive when they run on enormous data-sets. Some studies have shown that simple semantic network features can be rich enough to use for regressions without requiring complex computations. We propose a method for using semantic networks as user-level features for machine learning tasks. We conducted an experiment using a semantic network of 1037 Twitter hashtags from a corpus of 3.7 million tweets related to the 2022 French presidential election. A bipartite graph is formed where hashtags are nodes and weighted edges connect the hashtags reflecting the number of Twitter users that interacted with both hashtags. The graph is then transformed into a maximum-spanning tree with the most popular hashtag as its root node to construct a hierarchy amongst the hashtags. We then provide a vector feature for each user based on this tree. To validate the usefulness of our semantic feature we performed a regression experiment to predict the response rate of each user with six emotions like anger, enjoyment, or disgust. Our semantic feature performs well with the regression with most emotions having $R^2$ above 0.5. These results suggest that our semantic feature could be considered for use in further experiments predicting social media response on big data-sets.\n    "
    },
    "2310.07779": {
        "title": "Social Approval and Network Homophily as Motivators of Online Toxicity",
        "authors": [
            "Julie Jiang",
            "Luca Luceri",
            "Joseph B. Walther",
            "Emilio Ferrara"
        ],
        "comments": " ",
        "subjects": "Social and Information Networks (cs.SI)",
        "abstract": "Online hate messaging is a pervasive issue plaguing the well-being of social media users. This research empirically investigates a novel theory positing that online hate may be driven primarily by the pursuit of social approval rather than a direct desire to harm the targets. Results show that toxicity is homophilous in users' social networks and that a user's propensity for hostility can be predicted by their social networks. We also illustrate how receiving greater or fewer social engagements in the form of likes, retweets, quotes, and replies affects a user's subsequent toxicity. We establish a clear connection between receiving social approval signals and increases in subsequent toxicity. Being retweeted plays a particularly prominent role in escalating toxicity. Results also show that not receiving expected levels of social approval leads to decreased toxicity. We discuss the important implications of our research and opportunities to combat online hate.\n    "
    },
    "2310.08540": {
        "title": "Revisiting the Hypothesis: Do pretrained Transformers Learn In-Context by Gradient Descent?",
        "authors": [
            "Lingfeng Shen",
            "Aayush Mishra",
            "Daniel Khashabi"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "The emergence of In-Context Learning (ICL) in LLMs remains a significant phenomenon with little understanding. To explain ICL, recent studies try to theoretically connect it to Gradient Descent (GD). We ask, does this connection hold up in actual pre-trained models?\nWe highlight the limiting assumptions in prior works that make their context considerably different from the practical context in which language models are trained. For example, the theoretical hand-constructed weights used in these studies have properties that don't match those of real LLMs. Furthermore, their experimental verification uses ICL objective (training models explicitly for ICL), which differs from the emergent ICL in the wild.\nWe also look for evidence in real models. We observe that ICL and GD have different sensitivity to the order in which they observe demonstrations. Finally, we probe and compare the ICL vs. GD hypothesis in a natural setting. We conduct comprehensive empirical analyses on language models pre-trained on natural data (LLaMa-7B). Our comparisons of three performance metrics highlight the inconsistent behavior of ICL and GD as a function of various factors such as datasets, models, and the number of demonstrations. We observe that ICL and GD modify the output distribution of language models differently. These results indicate that the equivalence between ICL and GD remains an open hypothesis and calls for further studies.\n    "
    },
    "2310.18127": {
        "title": "Ask more, know better: Reinforce-Learned Prompt Questions for Decision Making with Large Language Models",
        "authors": [
            "Xue Yan",
            "Yan Song",
            "Xinyu Cui",
            "Filippos Christianos",
            "Haifeng Zhang",
            "David Henry Mguni",
            "Jun Wang"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs) demonstrate their promise in tackling complicated practical challenges by combining action-based policies with chain of thought (CoT) reasoning. Having high-quality prompts on hand, however, is vital to the framework's effectiveness. Currently, these prompts are handcrafted utilising extensive human labor, resulting in CoT policies that frequently fail to generalise. Human intervention is also required to develop grounding functions that ensure low-level controllers appropriately process CoT reasoning. In this paper, we propose a comprehensive training framework for complex task-solving, incorporating human prior knowledge into the learning of action policies. To that purpose, we offer a new leader-follower bilevel framework that is capable of learning to ask relevant questions (prompts) and subsequently undertaking reasoning to guide the learning of actions. The prompt policy is employed to make introspective revisions based on historical findings, leading the CoT process to consider the anticipated goals and generate outputs that lead to decisive, high-performing actions. The action policy subsequently learns to comprehend and integrate the CoT outputs to take actions. Our empirical data reveal that our framework outperforms leading methods in $5$ decision-making tasks such as Overcooked and FourRoom.\n    "
    },
    "2310.20354": {
        "title": "Statistical Complexity of Heterogeneous Geometric Networks",
        "authors": [
            "Keith Malcolm Smith",
            "Jason P. Smith"
        ],
        "comments": "12 pages, 6 figures",
        "subjects": "Social and Information Networks (cs.SI)",
        "abstract": "Heterogeneity and geometry are key explanatory components underlying the structure of real-world networks. The relationship between these components and the statistical complexity of networks is not well understood. We introduce a parsimonious normalised measure of statistical complexity for networks -- normalised hierarchical complexity. The measure is trivially 0 in regular graphs and we prove that this measure tends to 0 in Erd\u00f6s-R\u00e9nyi random graphs in the thermodynamic limit. We go on to demonstrate that greater complexity arises from the combination of hierarchical and geometric components to the network structure than either on their own. Further, the levels of complexity achieved are similar to those found in many real-world networks. We also find that real world networks establish connections in a way which increases hierarchical complexity and which our null models and a range of attachment mechanisms fail to explain. This underlines the non-trivial nature of statistical complexity in real-world networks and provides foundations for the comparative analysis of network complexity within and across disciplines.\n    "
    },
    "2311.01410": {
        "title": "The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing",
        "authors": [
            "Shen Nie",
            "Hanzhong Allan Guo",
            "Cheng Lu",
            "Yuhao Zhou",
            "Chenyu Zheng",
            "Chongxuan Li"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We present a unified probabilistic formulation for diffusion-based image editing, where a latent variable is edited in a task-specific manner and generally deviates from the corresponding marginal distribution induced by the original stochastic or ordinary differential equation (SDE or ODE). Instead, it defines a corresponding SDE or ODE for editing. In the formulation, we prove that the Kullback-Leibler divergence between the marginal distributions of the two SDEs gradually decreases while that for the ODEs remains as the time approaches zero, which shows the promise of SDE in image editing. Inspired by it, we provide the SDE counterparts for widely used ODE baselines in various tasks including inpainting and image-to-image translation, where SDE shows a consistent and substantial improvement. Moreover, we propose SDE-Drag -- a simple yet effective method built upon the SDE formulation for point-based content dragging. We build a challenging benchmark (termed DragBench) with open-set natural, art, and AI-generated images for evaluation. A user study on DragBench indicates that SDE-Drag significantly outperforms our ODE baseline, existing diffusion-based methods, and the renowned DragGAN. Our results demonstrate the superiority and versatility of SDE in image editing and push the boundary of diffusion-based editing methods.\n    "
    },
    "2311.02013": {
        "title": "SMORE: Score Models for Offline Goal-Conditioned Reinforcement Learning",
        "authors": [
            "Harshit Sikchi",
            "Rohan Chitnis",
            "Ahmed Touati",
            "Alborz Geramifard",
            "Amy Zhang",
            "Scott Niekum"
        ],
        "comments": "Published at International Conference of Learning Representations (ICLR) 2024. 26 pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Offline Goal-Conditioned Reinforcement Learning (GCRL) is tasked with learning to achieve multiple goals in an environment purely from offline datasets using sparse reward functions. Offline GCRL is pivotal for developing generalist agents capable of leveraging pre-existing datasets to learn diverse and reusable skills without hand-engineering reward functions. However, contemporary approaches to GCRL based on supervised learning and contrastive learning are often suboptimal in the offline setting. An alternative perspective on GCRL optimizes for occupancy matching, but necessitates learning a discriminator, which subsequently serves as a pseudo-reward for downstream RL. Inaccuracies in the learned discriminator can cascade, negatively influencing the resulting policy. We present a novel approach to GCRL under a new lens of mixture-distribution matching, leading to our discriminator-free method: SMORe. The key insight is combining the occupancy matching perspective of GCRL with a convex dual formulation to derive a learning objective that can better leverage suboptimal offline data. SMORe learns scores or unnormalized densities representing the importance of taking an action at a state for reaching a particular goal. SMORe is principled and our extensive experiments on the fully offline GCRL benchmark composed of robot manipulation and locomotion tasks, including high-dimensional observations, show that SMORe can outperform state-of-the-art baselines by a significant margin.\n    "
    },
    "2311.08631": {
        "title": "Influence of Video Dynamics on EEG-based Single-Trial Video Target Surveillance System",
        "authors": [
            "Heon-Gyu Kwak",
            "Sung-Jin Kim",
            "Hyeon-Taek Han",
            "Ji-Hoon Jeong",
            "Seong-Whan Lee"
        ],
        "comments": "2024 International BCI winter conference accepted paper",
        "subjects": "Human-Computer Interaction (cs.HC)",
        "abstract": "Target detection models are one of the widely used deep learning-based applications for reducing human efforts on video surveillance and patrol. However, the application of conventional computer vision-based target detection models in military usage can result in limited performance, due to the lack of sample data of hostile targets. In this paper, we present the possibility of the electroencephalography-based video target detection model, which could be applied as a supportive module of the military video surveillance system. The proposed framework and detection model showed prospective performance achieving a mean macro F-beta of 0.6522 with asynchronous real-time data from five subjects, in a certain video stimulus, but not on some video stimuli. By analyzing the results of experiments using each video stimulus, we present the factors that would affect the performance of electroencephalography-based video target detection models.\n    "
    },
    "2311.08675": {
        "title": "Refined Coreset Selection: Towards Minimal Coreset Size under Model Performance Constraints",
        "authors": [
            "Xiaobo Xia",
            "Jiale Liu",
            "Shaokun Zhang",
            "Qingyun Wu",
            "Hongxin Wei",
            "Tongliang Liu"
        ],
        "comments": "22 pages, 10 tables, 4 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Coreset selection is powerful in reducing computational costs and accelerating data processing for deep learning algorithms. It strives to identify a small subset from large-scale data, so that training only on the subset practically performs on par with full data. Practitioners regularly desire to identify the smallest possible coreset in realistic scenes while maintaining comparable model performance, to minimize costs and maximize acceleration. Motivated by this desideratum, for the first time, we pose the problem of refined coreset selection, in which the minimal coreset size under model performance constraints is explored. Moreover, to address this problem, we propose an innovative method, which maintains optimization priority order over the model performance and coreset size, and efficiently optimizes them in the coreset selection procedure. Theoretically, we provide the convergence guarantee of the proposed method. Empirically, extensive experiments confirm its superiority compared with previous strategies, often yielding better model performance with smaller coreset sizes.\n    "
    },
    "2311.09758": {
        "title": "OrchestraLLM: Efficient Orchestration of Language Models for Dialogue State Tracking",
        "authors": [
            "Chia-Hsuan Lee",
            "Hao Cheng",
            "Mari Ostendorf"
        ],
        "comments": "updated version",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Large language models (LLMs) have revolutionized the landscape of Natural Language Processing systems, but are computationally expensive. To reduce the cost without sacrificing performance, previous studies have explored various approaches to harness the potential of Small Language Models (SLMs) as cost-effective alternatives to their larger counterparts. Driven by findings that SLMs and LLMs exhibit complementary strengths in a structured knowledge extraction task, this work presents a novel SLM/LLM routing framework designed to improve computational efficiency and enhance task performance. First, exemplar pools are created to represent the types of contexts where each LM provides a more reliable answer, leveraging a sentence embedding fine-tuned so that context similarity is close to dialogue state similarity. Then, during inference, the k-nearest exemplars to the testing instance are retrieved, and the instance is routed according to majority vote. In dialogue state tracking tasks, the proposed routing framework enhances performance substantially compared to relying solely on LLMs, while reducing the computational costs by over 50%.\n    "
    },
    "2311.09827": {
        "title": "Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking",
        "authors": [
            "Nan Xu",
            "Fei Wang",
            "Ben Zhou",
            "Bang Zheng Li",
            "Chaowei Xiao",
            "Muhao Chen"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "While large language models (LLMs) have demonstrated increasing power, they have also given rise to a wide range of harmful behaviors. As representatives, jailbreak attacks can provoke harmful or unethical responses from LLMs, even after safety alignment. In this paper, we investigate a novel category of jailbreak attacks specifically designed to target the cognitive structure and processes of LLMs. Specifically, we analyze the safety vulnerability of LLMs in the face of (1) multilingual cognitive overload, (2) veiled expression, and (3) effect-to-cause reasoning. Different from previous jailbreak attacks, our proposed cognitive overload is a black-box attack with no need for knowledge of model architecture or access to model weights. Experiments conducted on AdvBench and MasterKey reveal that various LLMs, including both popular open-source model Llama 2 and the proprietary model ChatGPT, can be compromised through cognitive overload. Motivated by cognitive psychology work on managing cognitive load, we further investigate defending cognitive overload attack from two perspectives. Empirical studies show that our cognitive overload from three perspectives can jailbreak all studied LLMs successfully, while existing defense strategies can hardly mitigate the caused malicious uses effectively.\n    "
    },
    "2311.13250": {
        "title": "FedHCA$^2$: Towards Hetero-Client Federated Multi-Task Learning",
        "authors": [
            "Yuxiang Lu",
            "Suizhi Huang",
            "Yuwen Yang",
            "Shalayiding Sirejiding",
            "Yue Ding",
            "Hongtao Lu"
        ],
        "comments": "Accepted by CVPR 2024",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Federated Learning (FL) enables joint training across distributed clients using their local data privately. Federated Multi-Task Learning (FMTL) builds on FL to handle multiple tasks, assuming model congruity that identical model architecture is deployed in each client. To relax this assumption and thus extend real-world applicability, we introduce a novel problem setting, Hetero-Client Federated Multi-Task Learning (HC-FMTL), to accommodate diverse task setups. The main challenge of HC-FMTL is the model incongruity issue that invalidates conventional aggregation methods. It also escalates the difficulties in accurate model aggregation to deal with data and task heterogeneity inherent in FMTL. To address these challenges, we propose the FedHCA$^2$ framework, which allows for federated training of personalized models by modeling relationships among heterogeneous clients. Drawing on our theoretical insights into the difference between multi-task and federated optimization, we propose the Hyper Conflict-Averse Aggregation scheme to mitigate conflicts during encoder updates. Additionally, inspired by task interaction in MTL, the Hyper Cross Attention Aggregation scheme uses layer-wise cross attention to enhance decoder interactions while alleviating model incongruity. Moreover, we employ learnable Hyper Aggregation Weights for each client to customize personalized parameter updates. Extensive experiments demonstrate the superior performance of FedHCA$^2$ in various HC-FMTL scenarios compared to representative methods. Our code will be made publicly available.\n    "
    },
    "2311.14175": {
        "title": "Appearance-based gaze estimation enhanced with synthetic images using deep neural networks",
        "authors": [
            "Dmytro Herashchenko",
            "Igor Farka\u0161"
        ],
        "comments": "6 pages, 10 figures, accepted to 2023 IEEE Symposium Series on Computational Intelligence (SSCI). Published version copyrighted by IEEE. This work was funded by the Horizon Europe Twinning project TERAIS G.A. number 101079338, and in part by the national project APVV-21-0105. The link to the code: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Human eye gaze estimation is an important cognitive ingredient for successful human-robot interaction, enabling the robot to read and predict human behavior. We approach this problem using artificial neural networks and build a modular system estimating gaze from separately cropped eyes, taking advantage of existing well-functioning components for face detection (RetinaFace) and head pose estimation (6DRepNet). Our proposed method does not require any special hardware or infrared filters but uses a standard notebook-builtin RGB camera, as often approached with appearance-based methods. Using the MetaHuman tool, we also generated a large synthetic dataset of more than 57,000 human faces and made it publicly available. The inclusion of this dataset (with eye gaze and head pose information) on top of the standard Columbia Gaze dataset into training the model led to better accuracy with a mean average error below two degrees in eye pitch and yaw directions, which compares favourably to related methods. We also verified the feasibility of our model by its preliminary testing in real-world setting using the builtin 4K camera in NICO semi-humanoid robot's eye.\n    "
    },
    "2311.14431": {
        "title": "What you need to know about a learning robot: Identifying the enabling architecture of complex systems",
        "authors": [
            "Helen Beierling",
            "Phillip Richter",
            "Mara Brandt",
            "Lutz Terfloth",
            "Carsten Schulte",
            "Heiko Wersing",
            "Anna-Lisa Vollmer"
        ],
        "comments": "To be published in Cognitive Systems Research",
        "subjects": "Robotics (cs.RO)",
        "abstract": "Nowadays, we are dealing more and more with robots and AI in everyday life. However, their behavior is not always apparent to most lay users, especially in error situations. As a result, there can be misconceptions about the behavior of the technologies in use. This, in turn, can lead to misuse and rejection by users. Explanation, for example, through transparency, can address these misconceptions. However, it would be confusing and overwhelming for users if the entire software or hardware was explained. Therefore, this paper looks at the 'enabling' architecture. It describes those aspects of a robotic system that might need to be explained to enable someone to use the technology effectively. Furthermore, this paper is concerned with the 'explanandum', which is the corresponding misunderstanding or missing concepts of the enabling architecture that needs to be clarified. We have thus developed and present an approach for determining this 'enabling' architecture and the resulting 'explanandum' of complex technologies.\n    "
    },
    "2311.14902": {
        "title": "Parkinson's Disease classification Using Contrastive Graph Cross-View Learning with Multimodal Fusion of SPECT Images and Clinical Features",
        "authors": [
            "Jun-En Ding",
            "Chien-Chin Hsu",
            "Feng Liu"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Parkinson's Disease (PD) affects millions globally, impacting movement. Prior research utilized deep learning for PD prediction, primarily focusing on medical images, neglecting the data's underlying manifold structure. This work proposes a multimodal approach encompassing both image and non-image features, leveraging contrastive cross-view graph fusion for PD classification. We introduce a novel multimodal co-attention module, integrating embeddings from separate graph views derived from low-dimensional representations of images and clinical features. This enables extraction of more robust and structured features for improved multi-view data analysis. Additionally, a simplified contrastive loss-based fusion method is devised to enhance cross-view fusion learning. Our graph-view multimodal approach achieves an accuracy of 91% and an AUC of 92.8% in five-fold cross-validation. It also demonstrates superior predictive capabilities on non-image data compared to solely machine learning-based methods.\n    "
    },
    "2312.00746": {
        "title": "Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games",
        "authors": [
            "Dekun Wu",
            "Haochen Shi",
            "Zhiyuan Sun",
            "Bang Liu"
        ],
        "comments": " ",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "In this study, we explore the application of Large Language Models (LLMs) in \\textit{Jubensha}, a Chinese detective role-playing game and a novel area in Artificial Intelligence (AI) driven gaming. We introduce the first dataset specifically for Jubensha, including character scripts and game rules, to foster AI agent development in this complex narrative environment. Our work also presents a unique multi-agent interaction framework using LLMs, allowing AI agents to autonomously engage in this game. To evaluate the gaming performance of these AI agents, we developed novel methods measuring their mastery of case information and reasoning skills. Furthermore, we incorporated the latest advancements in in-context learning to improve the agents' performance in information gathering, murderer identification, and logical reasoning. The experimental results validate the effectiveness of our proposed methods. This work aims to offer a novel perspective on understanding LLM capabilities and establish a new benchmark for evaluating large language model-based agents.\n    "
    },
    "2312.02528": {
        "title": "Towards Automatic Power Battery Detection: New Challenge, Benchmark Dataset and Baseline",
        "authors": [
            "Xiaoqi Zhao",
            "Youwei Pang",
            "Zhenyu Chen",
            "Qian Yu",
            "Lihe Zhang",
            "Hanqi Liu",
            "Jiaming Zuo",
            "Huchuan Lu"
        ],
        "comments": "Accepted at CVPR2024",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We conduct a comprehensive study on a new task named power battery detection (PBD), which aims to localize the dense cathode and anode plates endpoints from X-ray images to evaluate the quality of power batteries. Existing manufacturers usually rely on human eye observation to complete PBD, which makes it difficult to balance the accuracy and efficiency of detection. To address this issue and drive more attention into this meaningful task, we first elaborately collect a dataset, called X-ray PBD, which has $1,500$ diverse X-ray images selected from thousands of power batteries of $5$ manufacturers, with $7$ different visual interference. Then, we propose a novel segmentation-based solution for PBD, termed multi-dimensional collaborative network (MDCNet). With the help of line and counting predictors, the representation of the point segmentation branch can be improved at both semantic and detail aspects.Besides, we design an effective distance-adaptive mask generation strategy, which can alleviate the visual challenge caused by the inconsistent distribution density of plates to provide MDCNet with stable supervision. Without any bells and whistles, our segmentation-based MDCNet consistently outperforms various other corner detection, crowd counting and general/tiny object detection-based solutions, making it a strong baseline that can help facilitate future research in PBD. Finally, we share some potential difficulties and works for future researches. The source code and datasets will be publicly available at \\href{this https URL}{X-ray PBD}.\n    "
    },
    "2312.03151": {
        "title": "Multitask Learning Can Improve Worst-Group Outcomes",
        "authors": [
            "Atharva Kulkarni",
            "Lucio Dery",
            "Amrith Setlur",
            "Aditi Raghunathan",
            "Ameet Talwalkar",
            "Graham Neubig"
        ],
        "comments": "20 pages, 7 tables, 6 Figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In order to create machine learning systems that serve a variety of users well, it is vital to not only achieve high average performance but also ensure equitable outcomes across diverse groups. However, most machine learning methods are designed to improve a model's average performance on a chosen end task without consideration for their impact on worst group error. Multitask learning (MTL) is one such widely used technique. In this paper, we seek not only to understand the impact of MTL on worst-group accuracy but also to explore its potential as a tool to address the challenge of group-wise fairness. We primarily consider the standard setting of fine-tuning a pre-trained model, where, following recent work \\citep{gururangan2020don, dery2023aang}, we multitask the end task with the pre-training objective constructed from the end task data itself. In settings with few or no group annotations, we find that multitasking often, but not consistently, achieves better worst-group accuracy than Just-Train-Twice (JTT; \\citet{pmlr-v139-liu21f}) -- a representative distributionally robust optimization (DRO) method. Leveraging insights from synthetic data experiments, we propose to modify standard MTL by regularizing the joint multitask representation space. We run a large number of fine-tuning experiments across computer vision and natural language processing datasets and find that our regularized MTL approach \\emph{consistently} outperforms JTT on both average and worst-group outcomes. Our official code can be found here: \\href{this https URL}{\\url{this https URL}}.\n    "
    },
    "2312.05760": {
        "title": "RepViT-SAM: Towards Real-Time Segmenting Anything",
        "authors": [
            "Ao Wang",
            "Hui Chen",
            "Zijia Lin",
            "Jungong Han",
            "Guiguang Ding"
        ],
        "comments": "Technical report of RepViT+SAM in our CVPR 2024 work. Project page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Segment Anything Model (SAM) has shown impressive zero-shot transfer performance for various computer vision tasks recently. However, its heavy computation costs remain daunting for practical applications. MobileSAM proposes to replace the heavyweight image encoder in SAM with TinyViT by employing distillation, which results in a significant reduction in computational requirements. However, its deployment on resource-constrained mobile devices still encounters challenges due to the substantial memory and computational overhead caused by self-attention mechanisms. Recently, RepViT achieves the state-of-the-art performance and latency trade-off on mobile devices by incorporating efficient architectural designs of ViTs into CNNs. Here, to achieve real-time segmenting anything on mobile devices, following MobileSAM, we replace the heavyweight image encoder in SAM with RepViT model, ending up with the RepViT-SAM model. Extensive experiments show that RepViT-SAM can enjoy significantly better zero-shot transfer capability than MobileSAM, along with nearly $10\\times$ faster inference speed. The code and models are available at \\url{this https URL}.\n    "
    },
    "2312.09468": {
        "title": "Safe Reinforcement Learning in a Simulated Robotic Arm",
        "authors": [
            "Luka Kova\u010d",
            "Igor Farka\u0161"
        ],
        "comments": "4 pages, 2 figures. Appeared in 2023 International Conference on Artificial Neural Networks (ICANN) proceedings. Published version copyrighted by Springer. This work was funded by the Horizon Europe Twinning project TERAIS, G.A. number 101079338 and in part by the national project APVV-21-0105. Link to the code: this https URL",
        "subjects": "Robotics (cs.RO)",
        "abstract": "Reinforcement learning (RL) agents need to explore their environments in order to learn optimal policies. In many environments and tasks, safety is of critical importance. The widespread use of simulators offers a number of advantages, including safe exploration which will be inevitable in cases when RL systems need to be trained directly in the physical environment (e.g. in human-robot interaction). The popular Safety Gym library offers three mobile agent types that can learn goal-directed tasks while considering various safety constraints. In this paper, we extend the applicability of safe RL algorithms by creating a customized environment with Panda robotic arm where Safety Gym algorithms can be tested. We performed pilot experiments with the popular PPO algorithm comparing the baseline with the constrained version and show that the constrained version is able to learn the equally good policy while better complying with safety constraints and taking longer training time as expected.\n    "
    },
    "2312.12478": {
        "title": "ProS: Prompting-to-simulate Generalized knowledge for Universal Cross-Domain Retrieval",
        "authors": [
            "Kaipeng Fang",
            "Jingkuan Song",
            "Lianli Gao",
            "Pengpeng Zeng",
            "Zhi-Qi Cheng",
            "Xiyao Li",
            "Heng Tao Shen"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The goal of Universal Cross-Domain Retrieval (UCDR) is to achieve robust performance in generalized test scenarios, wherein data may belong to strictly unknown domains and categories during training. Recently, pre-trained models with prompt tuning have shown strong generalization capabilities and attained noteworthy achievements in various downstream tasks, such as few-shot learning and video-text retrieval. However, applying them directly to UCDR may not sufficiently to handle both domain shift (i.e., adapting to unfamiliar domains) and semantic shift (i.e., transferring to unknown categories). To this end, we propose \\textbf{Pro}mpting-to-\\textbf{S}imulate (ProS), the first method to apply prompt tuning for UCDR. ProS employs a two-step process to simulate Content-aware Dynamic Prompts (CaDP) which can impact models to produce generalized features for UCDR. Concretely, in Prompt Units Learning stage, we introduce two Prompt Units to individually capture domain and semantic knowledge in a mask-and-align way. Then, in Context-aware Simulator Learning stage, we train a Content-aware Prompt Simulator under a simulated test scenarios to produce the corresponding CaDP. Extensive experiments conducted on three benchmark datasets show that our method achieves new state-of-the-art performance without bringing excessive parameters. Our method is publicly available at this https URL.\n    "
    },
    "2312.16475": {
        "title": "Federated Continual Learning via Knowledge Fusion: A Survey",
        "authors": [
            "Xin Yang",
            "Hao Yu",
            "Xin Gao",
            "Hao Wang",
            "Junbo Zhang",
            "Tianrui Li"
        ],
        "comments": "20 pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Data privacy and silos are nontrivial and greatly challenging in many real-world applications. Federated learning is a decentralized approach to training models across multiple local clients without the exchange of raw data from client devices to global servers. However, existing works focus on a static data environment and ignore continual learning from streaming data with incremental tasks. Federated Continual Learning (FCL) is an emerging paradigm to address model learning in both federated and continual learning environments. The key objective of FCL is to fuse heterogeneous knowledge from different clients and retain knowledge of previous tasks while learning on new ones. In this work, we delineate federated learning and continual learning first and then discuss their integration, i.e., FCL, and particular FCL via knowledge fusion. In summary, our motivations are four-fold: we (1) raise a fundamental problem called ''spatial-temporal catastrophic forgetting'' and evaluate its impact on the performance using a well-known method called federated averaging (FedAvg), (2) integrate most of the existing FCL methods into two generic frameworks, namely synchronous FCL and asynchronous FCL, (3) categorize a large number of methods according to the mechanism involved in knowledge fusion, and finally (4) showcase an outlook on the future work of FCL.\n    "
    },
    "2312.16731": {
        "title": "Infinite dSprites for Disentangled Continual Learning: Separating Memory Edits from Generalization",
        "authors": [
            "Sebastian Dziadzio",
            "\u00c7a\u011fatay Y\u0131ld\u0131z",
            "Gido M. van de Ven",
            "Tomasz Trzci\u0144ski",
            "Tinne Tuytelaars",
            "Matthias Bethge"
        ],
        "comments": "10 pages, 10 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The ability of machine learning systems to learn continually is hindered by catastrophic forgetting, the tendency of neural networks to overwrite existing knowledge when learning a new task. Continual learning methods alleviate this problem through regularization, parameter isolation, or rehearsal, but they are typically evaluated on benchmarks comprising only a handful of tasks. In contrast, humans are able to learn continually in dynamic, open-world environments, effortlessly achieving one-shot memorization of unfamiliar objects and reliably recognizing them under various transformations. To make progress towards closing this gap, we introduce Infinite dSprites, a parsimonious tool for creating continual classification and disentanglement benchmarks of arbitrary length and with full control over generative factors. We show that over a sufficiently long time horizon, the performance of all major types of continual learning methods deteriorates on this simple benchmark. Thus, Infinite dSprites highlights an important aspect of continual learning that has not received enough attention so far: given a finite modelling capacity and an arbitrarily long learning horizon, efficient learning requires memorizing class-specific information and accumulating knowledge about general mechanisms. In a simple setting with direct supervision on the generative factors, we show how learning class-agnostic transformations offers a way to circumvent catastrophic forgetting and improve classification accuracy over time. Our approach sets the stage for continual learning over hundreds of tasks with explicit control over memorization and forgetting, emphasizing open-set classification and one-shot generalization.\n    "
    },
    "2401.02982": {
        "title": "BIBench: Benchmarking Data Analysis Knowledge of Large Language Models",
        "authors": [
            "Shu Liu",
            "Shangqing Zhao",
            "Chenghao Jia",
            "Xinlin Zhuang",
            "Zhaoguang Long",
            "Qingquan Wu",
            "Chong Yang",
            "Aimin Zhou",
            "Man Lan"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of tasks. However, their proficiency and reliability in the specialized domain of Data Analysis, particularly with a focus on data-driven thinking, remain uncertain. To bridge this gap, we introduce BIBench, a comprehensive benchmark designed to evaluate the data analysis capabilities of LLMs within the context of Business Intelligence (BI). BIBench assesses LLMs across three dimensions: 1) BI foundational knowledge, evaluating the models' numerical reasoning and familiarity with financial concepts; 2) BI knowledge application, determining the models' ability to quickly comprehend textual information and generate analysis questions from multiple views; and 3) BI technical skills, examining the models' use of technical knowledge to address real-world data analysis challenges. BIBench comprises 11 sub-tasks, spanning three categories of task types: classification, extraction, and generation. Additionally, we've developed BIChat, a domain-specific dataset with over a million data points, to fine-tune LLMs. We will release BIBenchmark, BIChat, and the evaluation scripts at \\url{this https URL}. This benchmark aims to provide a measure for in-depth analysis of LLM abilities and foster the advancement of LLMs in the field of data analysis.\n    "
    },
    "2401.09181": {
        "title": "Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with Positive Forward Transfer",
        "authors": [
            "Junhao Zheng",
            "Qianli Ma",
            "Zhen Liu",
            "Binquan Wu",
            "Huawen Feng"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Multimodal Continual Instruction Tuning (MCIT) enables Multimodal Large Language Models (MLLMs) to meet continuously emerging requirements without expensive retraining. MCIT faces two major obstacles: catastrophic forgetting (where old knowledge is forgotten) and negative forward transfer (where the performance of future tasks is degraded). Although existing methods have greatly alleviated catastrophic forgetting, they still suffer from negative forward transfer. By performing singular value decomposition (SVD) on input embeddings, we discover a large discrepancy in different input embeddings. The discrepancy results in the model learning irrelevant information for old and pre-trained tasks, which leads to catastrophic forgetting and negative forward transfer. To address these issues, we propose Fwd-Prompt, a prompt-based method projecting prompt gradient to the residual space to minimize the interference between tasks and to the pre-trained subspace for reusing pre-trained knowledge. Our experiments demonstrate that Fwd-Prompt achieves state-of-the-art performance while updating fewer parameters and requiring no old samples. Our research sheds light on the potential of continuously adapting MLLMs to new tasks under the instruction tuning paradigm and encourages future studies to explore MCIT. The code will soon be publicly available.\n    "
    },
    "2401.12202": {
        "title": "OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics",
        "authors": [
            "Peiqi Liu",
            "Yaswanth Orru",
            "Jay Vakil",
            "Chris Paxton",
            "Nur Muhammad Mahi Shafiullah",
            "Lerrel Pinto"
        ],
        "comments": "Github repo: this https URL",
        "subjects": "Robotics (cs.RO)",
        "abstract": "Remarkable progress has been made in recent years in the fields of vision, language, and robotics. We now have vision models capable of recognizing objects based on language queries, navigation systems that can effectively control mobile systems, and grasping models that can handle a wide range of objects. Despite these advancements, general-purpose applications of robotics still lag behind, even though they rely on these fundamental capabilities of recognition, navigation, and grasping. In this paper, we adopt a systems-first approach to develop a new Open Knowledge-based robotics framework called OK-Robot. By combining Vision-Language Models (VLMs) for object detection, navigation primitives for movement, and grasping primitives for object manipulation, OK-Robot offers a integrated solution for pick-and-drop operations without requiring any training. To evaluate its performance, we run OK-Robot in 10 real-world home environments. The results demonstrate that OK-Robot achieves a 58.5% success rate in open-ended pick-and-drop tasks, representing a new state-of-the-art in Open Vocabulary Mobile Manipulation (OVMM) with nearly 1.8x the performance of prior work. On cleaner, uncluttered environments, OK-Robot's performance increases to 82%. However, the most important insight gained from OK-Robot is the critical role of nuanced details when combining Open Knowledge systems like VLMs with robotic modules. Videos of our experiments and code are available on our website: this https URL\n"
    },
    "2401.12413": {
        "title": "How Far Can 100 Samples Go? Unlocking Overall Zero-Shot Multilingual Translation via Tiny Multi-Parallel Data",
        "authors": [
            "Di Wu",
            "Shaomu Tan",
            "Yan Meng",
            "David Stap",
            "Christof Monz"
        ],
        "comments": "15 pages, 5 figures",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Zero-shot translation aims to translate between language pairs not seen during training in Multilingual Machine Translation (MMT) and is largely considered an open problem. A common, albeit resource-consuming, solution is to add as many related translation directions as possible to the training corpus. In this paper, we show that for an English-centric model, surprisingly large zero-shot improvements can be achieved by simply fine-tuning with a very small amount of multi-parallel data. For example, on the EC30 dataset, we obtain up to +21.7 ChrF non-English overall improvements (870 directions) by using only 100 multi-parallel samples while preserving English-centric translation quality. When investigating the size effect of fine-tuning data and its transfer capabilities, we found that already a small, randomly sampled set of fine-tuning directions is sufficient to achieve comparable improvements. The resulting non-English performance is close to the complete translation upper bound. Even in a minimal setting -- fine-tuning with only one single sample -- the well-known off-target issue is almost completely resolved, explaining parts -- but not all -- of the observed improvements in translation quality.\n    "
    },
    "2401.13172": {
        "title": "ADMap: Anti-disturbance framework for reconstructing online vectorized HD map",
        "authors": [
            "Haotian Hu",
            "Fanyi Wang",
            "Yaonong Wang",
            "Laifeng Hu",
            "Jingwei Xu",
            "Zhiwang Zhang"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In the field of autonomous driving, online high-definition (HD) map reconstruction is crucial for planning tasks. Recent research has developed several high-performance HD map reconstruction models to meet this necessity. However, the point sequences within the instance vectors may be jittery or jagged due to prediction bias, which can impact subsequent tasks. Therefore, this paper proposes the Anti-disturbance Map reconstruction framework (ADMap). To mitigate point-order jitter, the framework consists of three modules: Multi-Scale Perception Neck, Instance Interactive Attention (IIA), and Vector Direction Difference Loss (VDDL). By exploring the point-order relationships between and within instances in a cascading manner, the model can monitor the point-order prediction process more effectively. ADMap achieves state-of-the-art performance on the nuScenes and Argoverse2 datasets. Extensive results demonstrate its ability to produce stable and reliable map elements in complex and changing driving scenarios. Code and more demos are available at this https URL.\n    "
    },
    "2401.13919": {
        "title": "WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models",
        "authors": [
            "Hongliang He",
            "Wenlin Yao",
            "Kaixin Ma",
            "Wenhao Yu",
            "Yong Dai",
            "Hongming Zhang",
            "Zhenzhong Lan",
            "Dong Yu"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "The rapid advancement of large language models (LLMs) has led to a new era marked by the development of autonomous applications in real-world scenarios, which drives innovation in creating advanced web agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we establish a new benchmark by compiling real-world tasks from 15 popular websites and introduce an automatic evaluation protocol leveraging multimodal understanding abilities of GPT-4V to evaluate open-ended web agents. We show that WebVoyager achieves a 59.1% task success rate on our benchmark, significantly surpassing the performance of both GPT-4 (All Tools) and the WebVoyager (text-only) setups, underscoring the exceptional capability of WebVoyager. The proposed automatic evaluation metric achieves 85.3% agreement with human judgment, indicating its effectiveness in providing reliable and accurate assessments of web agents.\n    "
    },
    "2401.14292": {
        "title": "Single and bi-layered 2-D acoustic soft tactile skin (AST2)",
        "authors": [
            "Vishnu Rajendran",
            "Simon Parsons",
            "Amir Ghalamzan E"
        ],
        "comments": "IEEE Robosoft conference 2024 (accepted)",
        "subjects": "Robotics (cs.RO)",
        "abstract": "This paper aims to present an innovative and cost-effective design for Acoustic Soft Tactile (AST) Skin, with the primary goal of significantly enhancing the accuracy of 2-D tactile feature estimation. The existing challenge lies in achieving precise tactile feature estimation, especially concerning contact geometry characteristics, using cost-effective solutions. We hypothesise that by harnessing acoustic energy through dedicated acoustic channels in 2 layers beneath the sensing surface and analysing amplitude modulation, we can effectively decode interactions on the sensory surface, thereby improving tactile feature estimation. Our approach involves the distinct separation of hardware components responsible for emitting and receiving acoustic signals, resulting in a modular and highly customizable skin design. Practical tests demonstrate the effectiveness of this novel design, achieving remarkable precision in estimating contact normal forces (MAE < 0.8 N), 2D contact localisation (MAE < 0.7 mm), and contact surface diameter (MAE < 0.3 mm). In conclusion, the AST skin, with its innovative design and modular architecture, successfully addresses the challenge of tactile feature estimation. The presented results showcase its ability to precisely estimate various tactile features, making it a practical and cost-effective solution for robotic applications.\n    "
    },
    "2401.15721": {
        "title": "A Study of Acquisition Functions for Medical Imaging Deep Active Learning",
        "authors": [
            "Bonaventure F. P. Dossou"
        ],
        "comments": "Best Poster Award at Deep Learning Indaba 2023 Conference",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The Deep Learning revolution has enabled groundbreaking achievements in recent years. From breast cancer detection to protein folding, deep learning algorithms have been at the core of very important advancements. However, these modern advancements are becoming more and more data-hungry, especially on labeled data whose availability is scarce: this is even more prevalent in the medical context. In this work, we show how active learning could be very effective in data scarcity situations, where obtaining labeled data (or annotation budget is very limited). We compare several selection criteria (BALD, MeanSTD, and MaxEntropy) on the ISIC 2016 dataset. We also explored the effect of acquired pool size on the model's performance. Our results suggest that uncertainty is useful to the Melanoma detection task, and confirms the hypotheses of the author of the paper of interest, that \\textit{bald} performs on average better than other acquisition functions. Our extended analyses however revealed that all acquisition functions perform badly on the positive (cancerous) samples, suggesting exploitation of class unbalance, which could be crucial in real-world settings. We finish by suggesting future work directions that would be useful to improve this current work. The code of our implementation is open-sourced at \\url{this https URL}\n    "
    },
    "2401.15861": {
        "title": "BPDec: Unveiling the Potential of Masked Language Modeling Decoder in BERT pretraining",
        "authors": [
            "Wen Liang",
            "Youzhi Liang"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "BERT (Bidirectional Encoder Representations from Transformers) has revolutionized the field of natural language processing through its exceptional performance on numerous tasks. Yet, the majority of researchers have mainly concentrated on enhancements related to the model structure, such as relative position embedding and more efficient attention mechanisms. Others have delved into pretraining tricks associated with Masked Language Modeling, including whole word masking. DeBERTa introduced an enhanced decoder adapted for BERT's encoder model for pretraining, proving to be highly effective. We argue that the design and research around enhanced masked language modeling decoders have been underappreciated. In this paper, we propose several designs of enhanced decoders and introduce BPDec (BERT Pretraining Decoder), a novel method for modeling training. Typically, a pretrained BERT model is fine-tuned for specific Natural Language Understanding (NLU) tasks. In our approach, we utilize the original BERT model as the encoder, making only changes to the decoder without altering the encoder. This approach does not necessitate extensive modifications to the model's architecture and can be seamlessly integrated into existing fine-tuning pipelines and services, offering an efficient and effective enhancement strategy. Compared to other methods, while we also incur a moderate training cost for the decoder during the pretraining process, our approach does not introduce additional training costs during the fine-tuning phase. We test multiple enhanced decoder structures after pretraining and evaluate their performance on the GLUE benchmark. Our results demonstrate that BPDec, having only undergone subtle refinements to the model structure during pretraining, significantly enhances model performance without escalating the inference time and serving budget.\n    "
    },
    "2402.00128": {
        "title": "Real-time Traffic Object Detection for Autonomous Driving",
        "authors": [
            "Abdul Hannan Khan",
            "Syed Tahseen Raza Rizvi",
            "Andreas Dengel"
        ],
        "comments": "\\c{opyright} 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "With recent advances in computer vision, it appears that autonomous driving will be part of modern society sooner rather than later. However, there are still a significant number of concerns to address. Although modern computer vision techniques demonstrate superior performance, they tend to prioritize accuracy over efficiency, which is a crucial aspect of real-time applications. Large object detection models typically require higher computational power, which is achieved by using more sophisticated onboard hardware. For autonomous driving, these requirements translate to increased fuel costs and, ultimately, a reduction in mileage. Further, despite their computational demands, the existing object detectors are far from being real-time. In this research, we assess the robustness of our previously proposed, highly efficient pedestrian detector LSFM on well-established autonomous driving benchmarks, including diverse weather conditions and nighttime scenes. Moreover, we extend our LSFM model for general object detection to achieve real-time object detection in traffic scenes. We evaluate its performance, low latency, and generalizability on traffic object detection datasets. Furthermore, we discuss the inadequacy of the current key performance indicator employed by object detection systems in the context of autonomous driving and propose a more suitable alternative that incorporates real-time requirements.\n    "
    },
    "2402.01109": {
        "title": "Vaccine: Perturbation-aware Alignment for Large Language Model",
        "authors": [
            "Tiansheng Huang",
            "Sihao Hu",
            "Ling Liu"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The new paradigm of finetuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the finetuning to produce an alignment-broken model. We conduct an empirical analysis and uncover a \\textit{harmful embedding drift} phenomenon, showing a probable cause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users finetuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from un-sanitized user data in the finetuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompts. Our code is available at \\url{this https URL}.\n    "
    },
    "2402.01368": {
        "title": "LIR: A Lightweight Baseline for Image Restoration",
        "authors": [
            "Dongqi Fan",
            "Ting Yue",
            "Xin Zhao",
            "Liang Chang"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recently, there have been significant advancements in Image Restoration based on CNN and transformer. However, the inherent characteristics of the Image Restoration task are often overlooked. Many works, instead, only focus on the basic block design and stack numerous such blocks to the model, leading to parameters redundant and computations unnecessary. Thus, the efficiency of the image restoration is hindered. In this paper, we propose a Lightweight Baseline for Image Restoration called LIR to efficiently reconstruct the image and remove degradations (blur, rain, noise, haze). First of all, LIR addresses the degradations existing in the local and global residual connections that are ignored by modern networks, through a simple structural design. Then, to achieve lightweight, a Lightweight Adaptive Attention (LAA) Block is introduced depending on the inherent characteristics of the Image Restoration, which is mainly composed of proposed Adaptive Filters and Attention Blocks. LAA is capable of adaptively sharpening contours, removing degradation, and capturing global information in various Image Restoration scenes in a computation-friendly manner. Extensive experiments demonstrate that our LIR achieves comparable performance to state-of-the-art models with fewer parameters and computations in certain tasks. In addition, it is worth noting that our LIR produces better visual results than state-of-the-art networks that are more in line with the human aesthetic.\n    "
    },
    "2402.03279": {
        "title": "Stepping into the Right Shoes: The Effects of User-Matched Avatar Ethnicity and Gender on Sense of Embodiment in Virtual Reality",
        "authors": [
            "Tiffany D. Do",
            "Camille Isabella Protko",
            "Ryan P. McMahan"
        ],
        "comments": "To appear in IEEE Transactions on Visualization and Computer Graphics",
        "subjects": "Human-Computer Interaction (cs.HC)",
        "abstract": "In many consumer virtual reality (VR) applications, users embody predefined characters that offer minimal customization options, frequently emphasizing storytelling over user choice. We explore whether matching a user's physical characteristics, specifically ethnicity and gender, with their virtual self-avatar affects their sense of embodiment in VR. We conducted a 2 x 2 within-subjects experiment (n=32) with a diverse user population to explore the impact of matching or not matching a user's self-avatar to their ethnicity and gender on their sense of embodiment. Our results indicate that matching the ethnicity of the user and their self-avatar significantly enhances sense of embodiment regardless of gender, extending across various aspects, including appearance, response, and ownership. We also found that matching gender significantly enhanced ownership, suggesting that this aspect is influenced by matching both ethnicity and gender. Interestingly, we found that matching ethnicity specifically affects self-location while matching gender specifically affects one's body ownership.\n    "
    },
    "2402.04140": {
        "title": "Advancing Legal Reasoning: The Integration of AI to Navigate Complexities and Biases in Global Jurisprudence with Semi-Automated Arbitration Processes (SAAPs)",
        "authors": [
            "Michael De'Shazer"
        ],
        "comments": " ",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "This study consists of a novel approach toward the analysis of court judgments spanning five countries, including the United States, the United Kingdom, Rwanda, Sweden and Hong Kong. This study also explores the intersection of the latest advancements in artificial intelligence (AI) and legal analysis, emphasizing the role of AI (specifically generative AI) in identifying human biases and facilitating automated, valid, and coherent multisided argumentation of court judgments with the goal of ensuring consistent application of laws in and across various jurisdictions. By incorporating Advanced Language Models (ALMs) and a newly introduced human-AI collaborative framework, this paper seeks to analyze Grounded Theory-based research design with Advanced Language Models (ALMs) in the practice of law. SHIRLEY is the name of the AI-based application (built on top of OpenAI's GPT technology), focusing on detecting logical inconsistencies and biases across various legal decisions. SHIRLEY analysis is aggregated and is accompanied by a comparison-oriented AI-based application called SAM (also an ALM) to identify relative deviations in SHIRLEY bias detections. Further, a CRITIC is generated within semi-autonomous arbitration process via the ALM, SARA. A novel approach is introduced in the utilization of an AI arbitrator to critically evaluate biases and qualitative-in-nature nuances identified by the aforementioned AI applications (SAM in concert with SHIRLEY), based on the Hague Rules on Business and Human Rights Arbitration. This Semi-Automated Arbitration Process (SAAP) aims to uphold the integrity and fairness of legal judgments by ensuring a nuanced debate-resultant \"understanding\" through a hybrid system of AI and human-based collaborative analysis.\n    "
    },
    "2402.05699": {
        "title": "Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation",
        "authors": [
            "Xianghe Pang",
            "Shuo Tang",
            "Rui Ye",
            "Yuxin Xiong",
            "Bolun Zhang",
            "Yanfeng Wang",
            "Siheng Chen"
        ],
        "comments": "36 pages, 9 figures",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Aligning large language models (LLMs) with human values is imperative to mitigate potential adverse effects resulting from their misuse. Drawing from the sociological insight that acknowledging all parties' concerns is a key factor in shaping human values, this paper proposes a novel direction to align LLMs by themselves: social scene simulation. To achieve this, we present MATRIX, a novel social scene simulator that emulates realistic scenes around a user's input query, enabling the LLM to take social consequences into account before responding. MATRIX serves as a virtual rehearsal space, akin to a Monopolylogue, where the LLM performs diverse roles related to the query and practice by itself. To inject this alignment, we fine-tune the LLM with MATRIX-simulated data, ensuring adherence to human values without compromising inference speed. We theoretically show that the LLM with MATRIX outperforms Constitutional AI under mild assumptions. Finally, extensive experiments validate that our method outperforms over 10 baselines across 4 benchmarks. As evidenced by 875 user ratings, our tuned 13B-size LLM exceeds GPT-4 in aligning with human values. Our project page is available at this https URL.\n    "
    },
    "2402.08208": {
        "title": "Inherent Diverse Redundant Safety Mechanisms for AI-based Software Elements in Automotive Applications",
        "authors": [
            "Mandar Pitale",
            "Alireza Abbaspour",
            "Devesh Upadhyay"
        ],
        "comments": "This article is accepted for the SAE WCX 2024 conference proceedings",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "This paper explores the role and challenges of Artificial Intelligence (AI) algorithms, specifically AI-based software elements, in autonomous driving systems. These AI systems are fundamental in executing real-time critical functions in complex and high-dimensional environments. They handle vital tasks like multi-modal perception, cognition, and decision-making tasks such as motion planning, lane keeping, and emergency braking. A primary concern relates to the ability (and necessity) of AI models to generalize beyond their initial training data. This generalization issue becomes evident in real-time scenarios, where models frequently encounter inputs not represented in their training or validation data. In such cases, AI systems must still function effectively despite facing distributional or domain shifts. This paper investigates the risk associated with overconfident AI models in safety-critical applications like autonomous driving. To mitigate these risks, methods for training AI models that help maintain performance without overconfidence are proposed. This involves implementing certainty reporting architectures and ensuring diverse training data. While various distribution-based methods exist to provide safety mechanisms for AI models, there is a noted lack of systematic assessment of these methods, especially in the context of safety-critical automotive applications. Many methods in the literature do not adapt well to the quick response times required in safety-critical edge applications. This paper reviews these methods, discusses their suitability for safety-critical applications, and highlights their strengths and limitations. The paper also proposes potential improvements to enhance the safety and reliability of AI algorithms in autonomous vehicles in the context of rapid and accurate decision-making processes.\n    "
    },
    "2402.09164": {
        "title": "Less is More: Fewer Interpretable Region via Submodular Subset Selection",
        "authors": [
            "Ruoyu Chen",
            "Hua Zhang",
            "Siyuan Liang",
            "Jingzhi Li",
            "Xiaochun Cao"
        ],
        "comments": "Accepted to ICLR 2024 (Oral)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Image attribution algorithms aim to identify important regions that are highly relevant to model decisions. Although existing attribution solutions can effectively assign importance to target elements, they still face the following challenges: 1) existing attribution methods generate inaccurate small regions thus misleading the direction of correct attribution, and 2) the model cannot produce good attribution results for samples with wrong predictions. To address the above challenges, this paper re-models the above image attribution problem as a submodular subset selection problem, aiming to enhance model interpretability using fewer regions. To address the lack of attention to local regions, we construct a novel submodular function to discover more accurate small interpretation regions. To enhance the attribution effect for all samples, we also impose four different constraints on the selection of sub-regions, i.e., confidence, effectiveness, consistency, and collaboration scores, to assess the importance of various subsets. Moreover, our theoretical analysis substantiates that the proposed function is in fact submodular. Extensive experiments show that the proposed method outperforms SOTA methods on two face datasets (Celeb-A and VGG-Face2) and one fine-grained dataset (CUB-200-2011). For correctly predicted samples, the proposed method improves the Deletion and Insertion scores with an average of 4.9% and 2.5% gain relative to HSIC-Attribution. For incorrectly predicted samples, our method achieves gains of 81.0% and 18.4% compared to the HSIC-Attribution algorithm in the average highest confidence and Insertion score respectively. The code is released at this https URL.\n    "
    },
    "2402.10153": {
        "title": "Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study for Diabetes Patients",
        "authors": [
            "Mahyar Abbasian",
            "Zhongqi Yang",
            "Elahe Khatibi",
            "Pengfei Zhang",
            "Nitish Nagesh",
            "Iman Azimi",
            "Ramesh Jain",
            "Amir M. Rahmani"
        ],
        "comments": "4 pages, 3 figures, and 2 tables, conference paper",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Effective diabetes management is crucial for maintaining health in diabetic patients. Large Language Models (LLMs) have opened new avenues for diabetes management, facilitating their efficacy. However, current LLM-based approaches are limited by their dependence on general sources and lack of integration with domain-specific knowledge, leading to inaccurate responses. In this paper, we propose a knowledge-infused LLM-powered conversational health agent (CHA) for diabetic patients. We customize and leverage the open-source openCHA framework, enhancing our CHA with external knowledge and analytical capabilities. This integration involves two key components: 1) incorporating the American Diabetes Association dietary guidelines and the Nutritionix information and 2) deploying analytical tools that enable nutritional intake calculation and comparison with the guidelines. We compare the proposed CHA with GPT4. Our evaluation includes 100 diabetes-related questions on daily meal choices and assessing the potential risks associated with the suggested diet. Our findings show that the proposed agent demonstrates superior performance in generating responses to manage essential nutrients.\n    "
    },
    "2402.10401": {
        "title": "ManiFPT: Defining and Analyzing Fingerprints of Generative Models",
        "authors": [
            "Hae Jin Song",
            "Mahyar Khayatkhoei",
            "Wael AbdAlmageed"
        ],
        "comments": "Accepted to CVPR 2024",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Recent works have shown that generative models leave traces of their underlying generative process on the generated samples, broadly referred to as fingerprints of a generative model, and have studied their utility in detecting synthetic images from real ones. However, the extend to which these fingerprints can distinguish between various types of synthetic image and help identify the underlying generative process remain under-explored. In particular, the very definition of a fingerprint remains unclear, to our knowledge. To that end, in this work, we formalize the definition of artifact and fingerprint in generative models, propose an algorithm for computing them in practice, and finally study its effectiveness in distinguishing a large array of different generative models. We find that using our proposed definition can significantly improve the performance on the task of identifying the underlying generative process from samples (model attribution) compared to existing methods. Additionally, we study the structure of the fingerprints, and observe that it is very predictive of the effect of different design choices on the generative process.\n    "
    },
    "2402.10487": {
        "title": "Random Projection Layers for Multidimensional Time Series Forecasting",
        "authors": [
            "Chin-Chia Michael Yeh",
            "Yujie Fan",
            "Xin Dai",
            "Vivian Lai",
            "Prince Osei Aboagye",
            "Junpeng Wang",
            "Huiyuan Chen",
            "Yan Zheng",
            "Zhongfang Zhuang",
            "Liang Wang",
            "Wei Zhang"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "All-Multi-Layer Perceptron (all-MLP) mixer models have been shown to be effective for time series forecasting problems. However, when such a model is applied to high-dimensional time series (e.g., the time series in a spatial-temporal dataset), its performance is likely to degrade due to overfitting issues. In this paper, we propose an all-MLP time series forecasting architecture, referred to as RPMixer. Our method leverages the ensemble-like behavior of deep neural networks, where each individual block within the network acts like a base learner in an ensemble model, especially when identity mapping residual connections are incorporated. By integrating random projection layers into our model, we increase the diversity among the blocks' outputs, thereby enhancing the overall performance of RPMixer. Extensive experiments conducted on large-scale spatial-temporal forecasting benchmark datasets demonstrate that our proposed method outperforms alternative methods, including both spatial-temporal graph models and general forecasting models.\n    "
    },
    "2402.11194": {
        "title": "Evaluating LLMs' Mathematical Reasoning in Financial Document Question Answering",
        "authors": [
            "Pragya Srivastava",
            "Manuj Malik",
            "Vivek Gupta",
            "Tanuja Ganu",
            "Dan Roth"
        ],
        "comments": "25 pages, 17 figures",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Large Language Models (LLMs), excel in natural language understanding, but their capability for complex mathematical reasoning with an amalgamation of structured tables and unstructured text is uncertain. This study explores LLMs' mathematical reasoning on four financial tabular question-answering datasets: TATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments with various models and prompting techniques, we assess how LLMs adapt to complex tables and mathematical tasks. We focus on sensitivity to table complexity and performance variations with an increasing number of arithmetic reasoning steps. The results provide insights into LLMs' capabilities and limitations in handling complex mathematical scenarios for semi-structured tables. Ultimately, we introduce a novel prompting technique tailored to semi-structured documents, matching or outperforming other baselines in performance while providing a nuanced understanding of LLMs abilities for such a task.\n    "
    },
    "2402.12374": {
        "title": "Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding",
        "authors": [
            "Zhuoming Chen",
            "Avner May",
            "Ruslan Svirschevski",
            "Yuhsun Huang",
            "Max Ryabinin",
            "Zhihao Jia",
            "Beidi Chen"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "As the usage of large language models (LLMs) grows, performing efficient inference with these models becomes increasingly important. While speculative decoding has recently emerged as a promising direction for speeding up inference, existing methods are limited in their ability to scale to larger speculation budgets, and adapt to different hyperparameters and hardware. This paper introduces Sequoia, a scalable, robust, and hardware-aware algorithm for speculative decoding. To attain better scalability, Sequoia introduces a dynamic programming algorithm to find the optimal tree structure for the speculated tokens. To achieve robust speculative performance, Sequoia uses a novel sampling and verification method that outperforms prior work across different decoding temperatures. Finally, Sequoia introduces a hardware-aware tree optimizer that maximizes speculative performance by automatically selecting the token tree size and depth for a given hardware platform. Evaluation shows that Sequoia improves the decoding speed of Llama2-7B, Llama2-13B, and Vicuna-33B on an A100 by up to $4.04\\times$, $3.73\\times$, and $2.27\\times$. For offloading setting on L40, Sequoia achieves as low as 0.56 s/token for exact Llama2-70B inference latency, which is $9.96\\times$ on our optimized offloading system (5.6 s/token), $9.7\\times$ than DeepSpeed-Zero-Inference, $19.5\\times$ than Huggingface Accelerate.\n    "
    },
    "2402.13616": {
        "title": "YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information",
        "authors": [
            "Chien-Yao Wang",
            "I-Hau Yeh",
            "Hong-Yuan Mark Liao"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Today's deep learning methods focus on how to design the most appropriate objective functions so that the prediction results of the model can be closest to the ground truth. Meanwhile, an appropriate architecture that can facilitate acquisition of enough information for prediction has to be designed. Existing methods ignore a fact that when input data undergoes layer-by-layer feature extraction and spatial transformation, large amount of information will be lost. This paper will delve into the important issues of data loss when data is transmitted through deep networks, namely information bottleneck and reversible functions. We proposed the concept of programmable gradient information (PGI) to cope with the various changes required by deep networks to achieve multiple objectives. PGI can provide complete input information for the target task to calculate objective function, so that reliable gradient information can be obtained to update network weights. In addition, a new lightweight network architecture -- Generalized Efficient Layer Aggregation Network (GELAN), based on gradient path planning is designed. GELAN's architecture confirms that PGI has gained superior results on lightweight models. We verified the proposed GELAN and PGI on MS COCO dataset based object detection. The results show that GELAN only uses conventional convolution operators to achieve better parameter utilization than the state-of-the-art methods developed based on depth-wise convolution. PGI can be used for variety of models from lightweight to large. It can be used to obtain complete information, so that train-from-scratch models can achieve better results than state-of-the-art models pre-trained using large datasets, the comparison results are shown in Figure 1. The source codes are at: this https URL.\n    "
    },
    "2402.14614": {
        "title": "Two Counterexamples to Tokenization and the Noiseless Channel",
        "authors": [
            "Marco Cognetta",
            "Vil\u00e9m Zouhar",
            "Sangwhan Moon",
            "Naoaki Okazaki"
        ],
        "comments": "9 pages, 2 figures, to appear in LREC-COLING 2024, de-texified metadata",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "In Tokenization and the Noiseless Channel (Zouhar et al., 2023a), R\u00e9nyi efficiency is suggested as an intrinsic mechanism for evaluating a tokenizer: for NLP tasks, the tokenizer which leads to the highest R\u00e9nyi efficiency of the unigram distribution should be chosen. The R\u00e9nyi efficiency is thus treated as a predictor of downstream performance (e.g., predicting BLEU for a machine translation task), without the expensive step of training multiple models with different tokenizers. Although useful, the predictive power of this metric is not perfect, and the authors note there are additional qualities of a good tokenization scheme that R\u00e9nyi efficiency alone cannot capture.\nWe describe two variants of BPE tokenization which can arbitrarily increase R\u00e9nyi efficiency while decreasing the downstream model performance. These counterexamples expose cases where R\u00e9nyi efficiency fails as an intrinsic tokenization metric and thus give insight for building more accurate predictors.\n    "
    },
    "2402.14808": {
        "title": "RelayAttention for Efficient Large Language Model Serving with Long System Prompts",
        "authors": [
            "Lei Zhu",
            "Xinjiang Wang",
            "Wayne Zhang",
            "Rynson W.H. Lau"
        ],
        "comments": "fix typos; add code link",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Practical large language model (LLM) services may involve a long system prompt, which specifies the instructions, examples, and knowledge documents of the task and is reused across numerous requests. However, the long system prompt causes throughput/latency bottlenecks as the cost of generating the next token grows w.r.t. the sequence length. This paper aims to improve the efficiency of LLM services that involve long system prompts. Our key observation is that handling these system prompts requires heavily redundant memory accesses in existing causal attention computation algorithms. Specifically, for batched requests, the cached hidden states (i.e., key-value pairs) of system prompts are transferred from off-chip DRAM to on-chip SRAM multiple times, each corresponding to an individual request. To eliminate such a redundancy, we propose RelayAttention, an attention algorithm that allows reading these hidden states from DRAM exactly once for a batch of input tokens. RelayAttention is a free lunch: it maintains the generation quality while requiring no model retraining, as it is based on a mathematical reformulation of causal attention. Code is available at \\url{this https URL}.\n    "
    },
    "2402.16041": {
        "title": "Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy",
        "authors": [
            "Shuhai Zhang",
            "Yiliao Song",
            "Jiahao Yang",
            "Yuanqing Li",
            "Bo Han",
            "Mingkui Tan"
        ],
        "comments": "Accepted at ICLR 2024",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Large language models (LLMs) such as ChatGPT have exhibited remarkable performance in generating human-like texts. However, machine-generated texts (MGTs) may carry critical risks, such as plagiarism issues, misleading information, or hallucination issues. Therefore, it is very urgent and important to detect MGTs in many situations. Unfortunately, it is challenging to distinguish MGTs and human-written texts because the distributional discrepancy between them is often very subtle due to the remarkable performance of LLMs. In this paper, we seek to exploit \\textit{maximum mean discrepancy} (MMD) to address this issue in the sense that MMD can well identify distributional discrepancies. However, directly training a detector with MMD using diverse MGTs will incur a significantly increased variance of MMD since MGTs may contain \\textit{multiple text populations} due to various LLMs. This will severely impair MMD's ability to measure the difference between two samples. To tackle this, we propose a novel \\textit{multi-population} aware optimization method for MMD called MMD-MP, which can \\textit{avoid variance increases} and thus improve the stability to measure the distributional discrepancy. Relying on MMD-MP, we develop two methods for paragraph-based and sentence-based detection, respectively. Extensive experiments on various LLMs, \\eg, GPT2 and ChatGPT, show superior detection performance of our MMD-MP. The source code is available at \\url{this https URL}.\n    "
    },
    "2402.16192": {
        "title": "Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing",
        "authors": [
            "Jiabao Ji",
            "Bairu Hou",
            "Alexander Robey",
            "George J. Pappas",
            "Hamed Hassani",
            "Yang Zhang",
            "Eric Wong",
            "Shiyu Chang"
        ],
        "comments": "37 pages",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Aligned large language models (LLMs) are vulnerable to jailbreaking attacks, which bypass the safeguards of targeted LLMs and fool them into generating objectionable content. While initial defenses show promise against token-based threat models, there do not exist defenses that provide robustness against semantic attacks and avoid unfavorable trade-offs between robustness and nominal performance. To meet this need, we propose SEMANTICSMOOTH, a smoothing-based defense that aggregates the predictions of multiple semantically transformed copies of a given input prompt. Experimental results demonstrate that SEMANTICSMOOTH achieves state-of-the-art robustness against GCG, PAIR, and AutoDAN attacks while maintaining strong nominal performance on instruction following benchmarks such as InstructionFollowing and AlpacaEval. The codes will be publicly available at this https URL.\n    "
    },
    "2402.16235": {
        "title": "Human-AI Co-Creation of Worked Examples for Programming Classes",
        "authors": [
            "Mohammad Hassany",
            "Peter Brusilovsky",
            "Jiaze Ke",
            "Kamil Akhuseyinoglu",
            "Arun Balajiee Lekshmi Narayanan"
        ],
        "comments": "arXiv admin note: substantial text overlap with arXiv:2312.02105",
        "subjects": "Human-Computer Interaction (cs.HC)",
        "abstract": "Worked examples (solutions to typical programming problems presented as a source code in a certain language and are used to explain the topics from a programming class) are among the most popular types of learning content in programming classes. Most approaches and tools for presenting these examples to students are based on line-by-line explanations of the example code. However, instructors rarely have time to provide line-by-line explanations for a large number of examples typically used in a programming class. In this paper, we explore and assess a human-AI collaboration approach to authoring worked examples for Java programming. We introduce an authoring system for creating Java worked examples that generates a starting version of code explanations and presents it to the instructor to edit if necessary.We also present a study that assesses the quality of explanations created with this approach\n    "
    },
    "2402.16459": {
        "title": "Defending LLMs against Jailbreaking Attacks via Backtranslation",
        "authors": [
            "Yihan Wang",
            "Zhouxing Shi",
            "Andrew Bai",
            "Cho-Jui Hsieh"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Although many large language models (LLMs) have been trained to refuse harmful requests, they are still vulnerable to jailbreaking attacks, which rewrite the original prompt to conceal its harmful intent. In this paper, we propose a new method for defending LLMs against jailbreaking attacks by ``backtranslation''. Specifically, given an initial response generated by the target LLM from an input prompt, our backtranslation prompts a language model to infer an input prompt that can lead to the response. The inferred prompt is called the backtranslated prompt which tends to reveal the actual intent of the original prompt, since it is generated based on the LLM's response and is not directly manipulated by the attacker. We then run the target LLM again on the backtranslated prompt, and we refuse the original prompt if the model refuses the backtranslated prompt. We explain that the proposed defense provides several benefits on its effectiveness and efficiency. We empirically demonstrate that our defense significantly outperforms the baselines, in the cases that are hard for the baselines, and our defense also has little impact on the generation quality for benign input prompts.\n    "
    },
    "2402.16860": {
        "title": "Interactive Mars Image Content-Based Search with Interpretable Machine Learning",
        "authors": [
            "Bhavan Vasu",
            "Steven Lu",
            "Emily Dunkel",
            "Kiri L. Wagstaff",
            "Kevin Grimes",
            "Michael McAuley"
        ],
        "comments": "7 pages, 6 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The NASA Planetary Data System (PDS) hosts millions of images of planets, moons, and other bodies collected throughout many missions. The ever-expanding nature of data and user engagement demands an interpretable content classification system to support scientific discovery and individual curiosity. In this paper, we leverage a prototype-based architecture to enable users to understand and validate the evidence used by a classifier trained on images from the Mars Science Laboratory (MSL) Curiosity rover mission. In addition to providing explanations, we investigate the diversity and correctness of evidence used by the content-based classifier. The work presented in this paper will be deployed on the PDS Image Atlas, replacing its non-interpretable counterpart.\n    "
    },
    "2402.17110": {
        "title": "Sinkhorn Distance Minimization for Knowledge Distillation",
        "authors": [
            "Xiao Cui",
            "Yulei Qin",
            "Yuting Gao",
            "Enwei Zhang",
            "Zihan Xu",
            "Tong Wu",
            "Ke Li",
            "Xing Sun",
            "Wengang Zhou",
            "Houqiang Li"
        ],
        "comments": "Accepted by COLING 2024",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Knowledge distillation (KD) has been widely adopted to compress large language models (LLMs). Existing KD methods investigate various divergence measures including the Kullback-Leibler (KL), reverse Kullback-Leibler (RKL), and Jensen-Shannon (JS) divergences. However, due to limitations inherent in their assumptions and definitions, these measures fail to deliver effective supervision when few distribution overlap exists between the teacher and the student. In this paper, we show that the aforementioned KL, RKL, and JS divergences respectively suffer from issues of mode-averaging, mode-collapsing, and mode-underestimation, which deteriorates logits-based KD for diverse NLP tasks. We propose the Sinkhorn Knowledge Distillation (SinKD) that exploits the Sinkhorn distance to ensure a nuanced and precise assessment of the disparity between teacher and student distributions. Besides, profit by properties of the Sinkhorn metric, we can get rid of sample-wise KD that restricts the perception of divergence in each teacher-student sample pair. Instead, we propose a batch-wise reformulation to capture geometric intricacies of distributions across samples in the high-dimensional space. Comprehensive evaluation on GLUE and SuperGLUE, in terms of comparability, validity, and generalizability, highlights our superiority over state-of-the-art methods on all kinds of LLMs with encoder-only, encoder-decoder, and decoder-only architectures.\n    "
    },
    "2402.17229": {
        "title": "Preserving Fairness Generalization in Deepfake Detection",
        "authors": [
            "Li Lin",
            "Xinan He",
            "Yan Ju",
            "Xin Wang",
            "Feng Ding",
            "Shu Hu"
        ],
        "comments": "Accepted by The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2024)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Although effective deepfake detection models have been developed in recent years, recent studies have revealed that these models can result in unfair performance disparities among demographic groups, such as race and gender. This can lead to particular groups facing unfair targeting or exclusion from detection, potentially allowing misclassified deepfakes to manipulate public opinion and undermine trust in the model. The existing method for addressing this problem is providing a fair loss function. It shows good fairness performance for intra-domain evaluation but does not maintain fairness for cross-domain testing. This highlights the significance of fairness generalization in the fight against deepfakes. In this work, we propose the first method to address the fairness generalization problem in deepfake detection by simultaneously considering features, loss, and optimization aspects. Our method employs disentanglement learning to extract demographic and domain-agnostic forgery features, fusing them to encourage fair learning across a flattened loss landscape. Extensive experiments on prominent deepfake datasets demonstrate our method's effectiveness, surpassing state-of-the-art approaches in preserving fairness during cross-domain deepfake detection. The code is available at this https URL\n"
    },
    "2402.17236": {
        "title": "A Review of Data Mining in Personalized Education: Current Trends and Future Prospects",
        "authors": [
            "Zhang Xiong",
            "Haoxuan Li",
            "Zhuang Liu",
            "Zhuofan Chen",
            "Hao Zhou",
            "Wenge Rong",
            "Yuanxin Ouyang"
        ],
        "comments": "25 pages, 5 figures",
        "subjects": "Computers and Society (cs.CY)",
        "abstract": "Personalized education, tailored to individual student needs, leverages educational technology and artificial intelligence (AI) in the digital age to enhance learning effectiveness. The integration of AI in educational platforms provides insights into academic performance, learning preferences, and behaviors, optimizing the personal learning process. Driven by data mining techniques, it not only benefits students but also provides educators and institutions with tools to craft customized learning experiences. To offer a comprehensive review of recent advancements in personalized educational data mining, this paper focuses on four primary scenarios: educational recommendation, cognitive diagnosis, knowledge tracing, and learning analysis. This paper presents a structured taxonomy for each area, compiles commonly used datasets, and identifies future research directions, emphasizing the role of data mining in enhancing personalized education and paving the way for future exploration and innovation.\n    "
    },
    "2402.17279": {
        "title": "DiFashion: Towards Personalized Outfit Generation and Recommendation",
        "authors": [
            "Yiyan Xu",
            "Wenjie Wang",
            "Fuli Feng",
            "Yunshan Ma",
            "Jizhi Zhang",
            "Xiangnan He"
        ],
        "comments": " ",
        "subjects": "Information Retrieval (cs.IR)",
        "abstract": "The evolution of Outfit Recommendation (OR) in the realm of fashion has progressed through two distinct phases: Pre-defined Outfit Recommendation and Personalized Outfit Composition. Despite these advancements, both phases face limitations imposed by existing fashion products, hindering their effectiveness in meeting users' diverse fashion needs. The emergence of AI-generated content has paved the way for OR to overcome these constraints, demonstrating the potential for personalized outfit generation.\nIn pursuit of this, we introduce an innovative task named Generative Outfit Recommendation (GOR), with the goal of synthesizing a set of fashion images and assembling them to form visually harmonious outfits customized to individual users. The primary objectives of GOR revolve around achieving high fidelity, compatibility, and personalization of the generated outfits. To accomplish these, we propose DiFashion, a generative outfit recommender model that harnesses exceptional diffusion models for the simultaneous generation of multiple fashion images. To ensure the fulfillment of these objectives, three types of conditions are designed to guide the parallel generation process and Classifier-Free-Guidance are employed to enhance the alignment between generated images and conditions. DiFashion is applied to both personalized Fill-In-The-Blank and GOR tasks, and extensive experiments are conducted on the iFashion and Polyvore-U datasets. The results of quantitative and human-involved qualitative evaluations highlight the superiority of DiFashion over competitive baselines.\n    "
    },
    "2402.17289": {
        "title": "Active propulsion noise shaping for multi-rotor aircraft localization",
        "authors": [
            "Gabriele Serussi",
            "Tamir Shor",
            "Tom Hirshberg",
            "Chaim Baskin",
            "Alex Bronstein"
        ],
        "comments": " ",
        "subjects": "Robotics (cs.RO)",
        "abstract": "Multi-rotor aerial autonomous vehicles (MAVs) primarily rely on vision for navigation purposes. However, visual localization and odometry techniques suffer from poor performance in low or direct sunlight, a limited field of view, and vulnerability to occlusions. Acoustic sensing can serve as a complementary or even alternative modality for vision in many situations, and it also has the added benefits of lower system cost and energy footprint, which is especially important for micro aircraft. This paper proposes actively controlling and shaping the aircraft propulsion noise generated by the rotors to benefit localization tasks, rather than considering it a harmful nuisance. We present a neural network architecture for selfnoise-based localization in a known environment. We show that training it simultaneously with learning time-varying rotor phase modulation achieves accurate and robust localization. The proposed methods are evaluated using a computationally affordable simulation of MAV rotor noise in 2D acoustic environments that is fitted to real recordings of rotor pressure fields.\n    "
    },
    "2402.17311": {
        "title": "SKT5SciSumm -- A Hybrid Generative Approach for Multi-Document Scientific Summarization",
        "authors": [
            "Huy Quoc To",
            "Hung-Nghiep Tran",
            "Andr'e Greiner-Petter",
            "Felix Beierle",
            "Akiko Aizawa"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Summarization for scientific text has shown significant benefits both for the research community and human society. Given the fact that the nature of scientific text is distinctive and the input of the multi-document summarization task is substantially long, the task requires sufficient embedding generation and text truncation without losing important information. To tackle these issues, in this paper, we propose SKT5SciSumm - a hybrid framework for multi-document scientific summarization (MDSS). We leverage the Sentence-Transformer version of Scientific Paper Embeddings using Citation-Informed Transformers (SPECTER) to encode and represent textual sentences, allowing for efficient extractive summarization using k-means clustering. We employ the T5 family of models to generate abstractive summaries using extracted sentences. SKT5SciSumm achieves state-of-the-art performance on the Multi-XScience dataset. Through extensive experiments and evaluation, we showcase the benefits of our model by using less complicated models to achieve remarkable results, thereby highlighting its potential in advancing the field of multi-document summarization for scientific text.\n    "
    },
    "2402.17316": {
        "title": "Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via Selective Entropy Distillation",
        "authors": [
            "Yaofo Chen",
            "Shuaicheng Niu",
            "Shoukai Xu",
            "Hengjie Song",
            "Yaowei Wang",
            "Mingkui Tan"
        ],
        "comments": "Published in ICLR 2024",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The conventional deep learning paradigm often involves training a deep model on a server and then deploying the model or its distilled ones to resource-limited edge devices. Usually, the models shall remain fixed once deployed (at least for some period) due to the potential high cost of model adaptation for both the server and edge sides. However, in many real-world scenarios, the test environments may change dynamically (known as distribution shifts), which often results in degraded performance. Thus, one has to adapt the edge models promptly to attain promising performance. Moreover, with the increasing data collected at the edge, this paradigm also fails to further adapt the cloud model for better performance. To address these, we encounter two primary challenges: 1) the edge model has limited computation power and may only support forward propagation; 2) the data transmission budget between cloud and edge devices is limited in latency-sensitive scenarios. In this paper, we establish a Cloud-Edge Elastic Model Adaptation (CEMA) paradigm in which the edge models only need to perform forward propagation and the edge models can be adapted online. In our CEMA, to reduce the communication burden, we devise two criteria to exclude unnecessary samples from uploading to the cloud, i.e., dynamic unreliable and low-informative sample exclusion. Based on the uploaded samples, we update and distribute the affine parameters of normalization layers by distilling from the stronger foundation model to the edge model with a sample replay strategy. Extensive experimental results on ImageNet-C and ImageNet-R verify the effectiveness of our CEMA.\n    "
    },
    "2402.17323": {
        "title": "SDDGR: Stable Diffusion-based Deep Generative Replay for Class Incremental Object Detection",
        "authors": [
            "Junsu Kim",
            "Hoseong Cho",
            "Jihyeon Kim",
            "Yihalem Yimolal Tiruneh",
            "Seungryul Baek"
        ],
        "comments": "Accepted to CVPR 2024. We will post a camera-ready version later",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In the field of class incremental learning (CIL), generative replay has become increasingly prominent as a method to mitigate the catastrophic forgetting, alongside the continuous improvements in generative models. However, its application in class incremental object detection (CIOD) has been significantly limited, primarily due to the complexities of scenes involving multiple labels. In this paper, we propose a novel approach called stable diffusion deep generative replay (SDDGR) for CIOD. Our method utilizes a diffusion-based generative model with pre-trained text-to-diffusion networks to generate realistic and diverse synthetic images. SDDGR incorporates an iterative refinement strategy to produce high-quality images encompassing old classes. Additionally, we adopt an L2 knowledge distillation technique to improve the retention of prior knowledge in synthetic images. Furthermore, our approach includes pseudo-labeling for old objects within new task images, preventing misclassification as background elements. Extensive experiments on the COCO 2017 dataset demonstrate that SDDGR significantly outperforms existing algorithms, achieving a new state-of-the-art in various CIOD scenarios. The source code will be made available to the public.\n    "
    },
    "2402.17389": {
        "title": "FairBelief -- Assessing Harmful Beliefs in Language Models",
        "authors": [
            "Mattia Setzu",
            "Marta Marchiori Manerba",
            "Pasquale Minervini",
            "Debora Nozza"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Language Models (LMs) have been shown to inherit undesired biases that might hurt minorities and underrepresented groups if such systems were integrated into real-world applications without careful fairness auditing. This paper proposes FairBelief, an analytical approach to capture and assess beliefs, i.e., propositions that an LM may embed with different degrees of confidence and that covertly influence its predictions. With FairBelief, we leverage prompting to study the behavior of several state-of-the-art LMs across different previously neglected axes, such as model scale and likelihood, assessing predictions on a fairness dataset specifically designed to quantify LMs' outputs' hurtfulness. Finally, we conclude with an in-depth qualitative assessment of the beliefs emitted by the models. We apply FairBelief to English LMs, revealing that, although these architectures enable high performances on diverse natural language processing tasks, they show hurtful beliefs about specific genders. Interestingly, training procedure and dataset, model scale, and architecture induce beliefs of different degrees of hurtfulness.\n    "
    },
    "2402.17392": {
        "title": "Spot the bot: Coarse-Grained Partition of Semantic Paths for Bots and Humans",
        "authors": [
            "Vasilii A. Gromov",
            "Alexandra S. Kogan"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Nowadays, technology is rapidly advancing: bots are writing comments, articles, and reviews. Due to this fact, it is crucial to know if the text was written by a human or by a bot. This paper focuses on comparing structures of the coarse-grained partitions of semantic paths for human-written and bot-generated texts. We compare the clusterizations of datasets of n-grams from literary texts and texts generated by several bots. The hypothesis is that the structures and clusterizations are different. Our research supports the hypothesis. As the semantic structure may be different for different languages, we investigate Russian, English, German, and Vietnamese languages.\n    "
    },
    "2402.17485": {
        "title": "EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions",
        "authors": [
            "Linrui Tian",
            "Qi Wang",
            "Bang Zhang",
            "Liefeng Bo"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In this work, we tackle the challenge of enhancing the realism and expressiveness in talking head video generation by focusing on the dynamic and nuanced relationship between audio cues and facial movements. We identify the limitations of traditional techniques that often fail to capture the full spectrum of human expressions and the uniqueness of individual facial styles. To address these issues, we propose EMO, a novel framework that utilizes a direct audio-to-video synthesis approach, bypassing the need for intermediate 3D models or facial landmarks. Our method ensures seamless frame transitions and consistent identity preservation throughout the video, resulting in highly expressive and lifelike animations. Experimental results demonsrate that EMO is able to produce not only convincing speaking videos but also singing videos in various styles, significantly outperforming existing state-of-the-art methodologies in terms of expressiveness and realism.\n    "
    },
    "2402.17758": {
        "title": "ADL4D: Towards A Contextually Rich Dataset for 4D Activities of Daily Living",
        "authors": [
            "Marsil Zakour",
            "Partha Pratim Nath",
            "Ludwig Lohmer",
            "Emre Faik G\u00f6k\u00e7e",
            "Martin Piccolrovazzi",
            "Constantin Patsch",
            "Yuankai Wu",
            "Rahul Chaudhari",
            "Eckehard Steinbach"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Hand-Object Interactions (HOIs) are conditioned on spatial and temporal contexts like surrounding objects, previous actions, and future intents (for example, grasping and handover actions vary greatly based on objects proximity and trajectory obstruction). However, existing datasets for 4D HOI (3D HOI over time) are limited to one subject interacting with one object only. This restricts the generalization of learning-based HOI methods trained on those datasets. We introduce ADL4D, a dataset of up to two subjects interacting with different sets of objects performing Activities of Daily Living (ADL) like breakfast or lunch preparation activities. The transition between multiple objects to complete a certain task over time introduces a unique context lacking in existing datasets. Our dataset consists of 75 sequences with a total of 1.1M RGB-D frames, hand and object poses, and per-hand fine-grained action annotations. We develop an automatic system for multi-view multi-hand 3D pose annotation capable of tracking hand poses over time. We integrate and test it against publicly available datasets. Finally, we evaluate our dataset on the tasks of Hand Mesh Recovery (HMR) and Hand Action Segmentation (HAS).\n    "
    },
    "2402.18065": {
        "title": "A Probabilistic Motion Model for Skid-Steer Wheeled Mobile Robot Navigation on Off-Road Terrains",
        "authors": [
            "Ananya Trivedi",
            "Mark Zolotas",
            "Adeeb Abbas",
            "Sarvesh Prajapati",
            "Salah Bazzi",
            "Task\u0131n Pad\u0131r"
        ],
        "comments": "Accepted for publication at IEEE ICRA 2024",
        "subjects": "Robotics (cs.RO)",
        "abstract": "Skid-Steer Wheeled Mobile Robots (SSWMRs) are increasingly being used for off-road autonomy applications. When turning at high speeds, these robots tend to undergo significant skidding and slipping. In this work, using Gaussian Process Regression (GPR) and Sigma-Point Transforms, we estimate the non-linear effects of tire-terrain interaction on robot velocities in a probabilistic fashion. Using the mean estimates from GPR, we propose a data-driven dynamic motion model that is more accurate at predicting future robot poses than conventional kinematic motion models. By efficiently solving a convex optimization problem based on the history of past robot motion, the GPR augmented motion model generalizes to previously unseen terrain conditions. The output distribution from the proposed motion model can be used for local motion planning approaches, such as stochastic model predictive control, leveraging model uncertainty to make safe decisions. We validate our work on a benchmark real-world multi-terrain SSWMR dataset. Our results show that the model generalizes to three different terrains while significantly reducing errors in linear and angular motion predictions. As shown in the attached video, we perform a separate set of experiments on a physical robot to demonstrate the robustness of the proposed algorithm.\n    "
    },
    "2402.18101": {
        "title": "Assessing the Efficacy of Grammar Error Correction: A Human Evaluation Approach in the Japanese Context",
        "authors": [
            "Qiao Wang",
            "Zheng Yuan"
        ],
        "comments": "2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "In this study, we evaluated the performance of the state-of-the-art sequence tagging grammar error detection and correction model (SeqTagger) using Japanese university students' writing samples. With an automatic annotation toolkit, ERRANT, we first evaluated SeqTagger's performance on error correction with human expert correction as the benchmark. Then a human-annotated approach was adopted to evaluate Seqtagger's performance in error detection using a subset of the writing dataset. Results indicated a precision of 63.66% and a recall of 20.19% for error correction in the full dataset. For the subset, after manual exclusion of irrelevant errors such as semantic and mechanical ones, the model shows an adjusted precision of 97.98% and an adjusted recall of 42.98% for error detection, indicating the model's high accuracy but also its conservativeness. Thematic analysis on errors undetected by the model revealed that determiners and articles, especially the latter, were predominant. Specifically, in terms of context-independent errors, the model occasionally overlooked basic ones and faced challenges with overly erroneous or complex structures. Meanwhile, context-dependent errors, notably those related to tense and noun number, as well as those possibly influenced by the students' first language (L1), remained particularly challenging.\n    "
    },
    "2402.18169": {
        "title": "MIKO: Multimodal Intention Knowledge Distillation from Large Language Models for Social-Media Commonsense Discovery",
        "authors": [
            "Feihong Lu",
            "Weiqi Wang",
            "Yangyifei Luo",
            "Ziqin Zhu",
            "Qingyun Sun",
            "Baixuan Xu",
            "Haochen Shi",
            "Shiqi Gao",
            "Qian Li",
            "Yangqiu Song",
            "Jianxin Li"
        ],
        "comments": "11 pages, 5 figures",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Social media has become a ubiquitous tool for connecting with others, staying updated with news, expressing opinions, and finding entertainment. However, understanding the intention behind social media posts remains challenging due to the implicitness of intentions in social media posts, the need for cross-modality understanding of both text and images, and the presence of noisy information such as hashtags, misspelled words, and complicated abbreviations. To address these challenges, we present MIKO, a Multimodal Intention Kowledge DistillatiOn framework that collaboratively leverages a Large Language Model (LLM) and a Multimodal Large Language Model (MLLM) to uncover users' intentions. Specifically, we use an MLLM to interpret the image and an LLM to extract key information from the text and finally instruct the LLM again to generate intentions. By applying MIKO to publicly available social media datasets, we construct an intention knowledge base featuring 1,372K intentions rooted in 137,287 posts. We conduct a two-stage annotation to verify the quality of the generated knowledge and benchmark the performance of widely used LLMs for intention generation. We further apply MIKO to a sarcasm detection dataset and distill a student model to demonstrate the downstream benefits of applying intention knowledge.\n    "
    },
    "2402.18222": {
        "title": "HearHere: Mitigating Echo Chambers in News Consumption through an AI-based Web System",
        "authors": [
            "Youngseung Jeon",
            "Jaehoon Kim",
            "Sohyun Park",
            "Yunyong Ko",
            "Seongeun Ryu",
            "Sang-Wook Kim",
            "Kyungsik Han"
        ],
        "comments": "34 pages, 6 figures, 6 tables, CSCW 2024",
        "subjects": "Human-Computer Interaction (cs.HC)",
        "abstract": "Considerable efforts are currently underway to mitigate the negative impacts of echo chambers, such as increased susceptibility to fake news and resistance towards accepting scientific evidence. Prior research has presented the development of computer systems that support the consumption of news information from diverse political perspectives to mitigate the echo chamber effect. However, existing studies still lack the ability to effectively support the key processes of news information consumption and quantitatively identify a political stance towards the information. In this paper, we present HearHere, an AI-based web system designed to help users accommodate information and opinions from diverse perspectives. HearHere facilitates the key processes of news information consumption through two visualizations. Visualization 1 provides political news with quantitative political stance information, derived from our graph-based political classification model, and users can experience diverse perspectives (Hear). Visualization 2 allows users to express their opinions on specific political issues in a comment form and observe the position of their own opinions relative to pro-liberal and pro-conservative comments presented on a map interface (Here). Through a user study with 94 participants, we demonstrate the feasibility of HearHere in supporting the consumption of information from various perspectives. Our findings highlight the importance of providing political stance information and quantifying users' political status as a means to mitigate political polarization. In addition, we propose design implications for system development, including the consideration of demographics such as political interest and providing users with initiatives.\n    "
    },
    "2402.18321": {
        "title": "Privacy Policies and Consent Management Platforms: Growth and Users' Interactions over Time",
        "authors": [
            "Nikhil Jha",
            "Martino Trevisan",
            "Marco Mellia",
            "Daniel Fernandez",
            "Rodrigo Irarrazaval"
        ],
        "comments": " ",
        "subjects": "Computers and Society (cs.CY)",
        "abstract": "In response to growing concerns about user privacy, legislators have introduced new regulations and laws such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) that force websites to obtain user consent before activating personal data collection, fundamental to providing targeted advertising. The cornerstone of this consent-seeking process involves the use of Privacy Banners, the technical mechanism to collect users' approval for data collection practices. Consent management platforms (CMPs) have emerged as practical solutions to make it easier for website administrators to properly manage consent, allowing them to outsource the complexities of managing user consent and activating advertising features.\nThis paper presents a detailed and longitudinal analysis of the evolution of CMPs spanning nine years. We take a twofold perspective: Firstly, thanks to the HTTP Archive dataset, we provide insights into the growth, market share, and geographical spread of CMPs. Noteworthy observations include the substantial impact of GDPR on the proliferation of CMPs in Europe. Secondly, we analyse millions of user interactions with a medium-sized CMP present in thousands of websites worldwide. We observe how even small changes in the design of Privacy Banners have a critical impact on the user's giving or denying their consent to data collection. For instance, over 60% of users do not consent when offered a simple \"one-click reject-all\" option. Conversely, when opting out requires more than one click, about 90% of users prefer to simply give their consent. The main objective is in fact to eliminate the annoying privacy banner rather the make an informed decision. Curiously, we observe iOS users exhibit a higher tendency to accept cookies compared to Android users, possibly indicating greater confidence in the privacy offered by Apple devices.\n    "
    },
    "2402.18402": {
        "title": "A Modular System for Enhanced Robustness of Multimedia Understanding Networks via Deep Parametric Estimation",
        "authors": [
            "Francesco Barbato",
            "Umberto Michieli",
            "Mehmet Kerim Yucel",
            "Pietro Zanuttigh",
            "Mete Ozay"
        ],
        "comments": "Accepted at ACM MMSys'24. 10 pages, 7 figures, 8 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In multimedia understanding tasks, corrupted samples pose a critical challenge, because when fed to machine learning models they lead to performance degradation. In the past, three groups of approaches have been proposed to handle noisy data: i) enhancer and denoiser modules to improve the quality of the noisy data, ii) data augmentation approaches, and iii) domain adaptation strategies. All the aforementioned approaches come with drawbacks that limit their applicability; the first has high computational costs and requires pairs of clean-corrupted data for training, while the others only allow deployment of the same task/network they were trained on (\\ie, when upstream and downstream task/network are the same). In this paper, we propose SyMPIE to solve these shortcomings. To this end, we design a small, modular, and efficient (just 2GFLOPs to process a Full HD image) system to enhance input data for robust downstream multimedia understanding with minimal computational cost. Our SyMPIE is pre-trained on an upstream task/network that should not match the downstream ones and does not need paired clean-corrupted samples. Our key insight is that most input corruptions found in real-world tasks can be modeled through global operations on color channels of images or spatial filters with small kernels. We validate our approach on multiple datasets and tasks, such as image classification (on ImageNetC, ImageNetC-Bar, VizWiz, and a newly proposed mixed corruption benchmark named ImageNetC-mixed) and semantic segmentation (on Cityscapes, ACDC, and DarkZurich) with consistent improvements of about 5\\% relative accuracy gain across the board. The code of our approach and the new ImageNetC-mixed benchmark will be made available upon publication.\n    "
    },
    "2402.18409": {
        "title": "A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models",
        "authors": [
            "Xiujie Song",
            "Mengyue Wu",
            "Kenny Q. Zhu",
            "Chunhao Zhang",
            "Yanyi Chen"
        ],
        "comments": " ",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Large Vision Language Models (LVLMs), despite their recent success, are hardly comprehensively tested for their cognitive abilities. Inspired by the prevalent use of the \"Cookie Theft\" task in human cognition test, we propose a novel evaluation benchmark to evaluate high-level cognitive ability of LVLMs using images with rich semantics. It defines eight reasoning capabilities and consists of an image description task and a visual question answering task. Our evaluation on well-known LVLMs shows that there is still a large gap in cognitive ability between LVLMs and humans.\n    "
    },
    "2402.18495": {
        "title": "ROG$_{PL}$: Robust Open-Set Graph Learning via Region-Based Prototype Learning",
        "authors": [
            "Qin Zhang",
            "Xiaowei Li",
            "Jiexin Lu",
            "Liping Qiu",
            "Shirui Pan",
            "Xiaojun Chen",
            "Junyang Chen"
        ],
        "comments": "9 pages, 5 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Open-set graph learning is a practical task that aims to classify the known class nodes and to identify unknown class samples as unknowns. Conventional node classification methods usually perform unsatisfactorily in open-set scenarios due to the complex data they encounter, such as out-of-distribution (OOD) data and in-distribution (IND) noise. OOD data are samples that do not belong to any known classes. They are outliers if they occur in training (OOD noise), and open-set samples if they occur in testing. IND noise are training samples which are assigned incorrect labels. The existence of IND noise and OOD noise is prevalent, which usually cause the ambiguity problem, including the intra-class variety problem and the inter-class confusion problem. Thus, to explore robust open-set learning methods is necessary and difficult, and it becomes even more difficult for non-IID graph this http URL this end, we propose a unified framework named ROG$_{PL}$ to achieve robust open-set learning on complex noisy graph data, by introducing prototype learning. In specific, ROG$_{PL}$ consists of two modules, i.e., denoising via label propagation and open-set prototype learning via regions. The first module corrects noisy labels through similarity-based label propagation and removes low-confidence samples, to solve the intra-class variety problem caused by noise. The second module learns open-set prototypes for each known class via non-overlapped regions and remains both interior and border prototypes to remedy the inter-class confusion problem.The two modules are iteratively updated under the constraints of classification loss and prototype diversity loss. To the best of our knowledge, the proposed ROG$_{PL}$ is the first robust open-set node classification method for graph data with complex noise.\n    "
    },
    "2402.18496": {
        "title": "Language Models Represent Beliefs of Self and Others",
        "authors": [
            "Wentao Zhu",
            "Zhining Zhang",
            "Yizhou Wang"
        ],
        "comments": "project page: this https URL",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Understanding and attributing mental states, known as Theory of Mind (ToM), emerges as a fundamental capability for human social reasoning. While Large Language Models (LLMs) appear to possess certain ToM abilities, the mechanisms underlying these capabilities remain elusive. In this study, we discover that it is possible to linearly decode the belief status from the perspectives of various agents through neural activations of language models, indicating the existence of internal representations of self and others' beliefs. By manipulating these representations, we observe dramatic changes in the models' ToM performance, underscoring their pivotal role in the social reasoning process. Additionally, our findings extend to diverse social reasoning tasks that involve different causal inference patterns, suggesting the potential generalizability of these representations.\n    "
    },
    "2402.18577": {
        "title": "Motion Guided Token Compression for Efficient Masked Video Modeling",
        "authors": [
            "Yukun Feng",
            "Yangming Shi",
            "Fengze Liu",
            "Tan Yan"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recent developments in Transformers have achieved notable strides in enhancing video comprehension. Nonetheless, the O($N^2$) computation complexity associated with attention mechanisms presents substantial computational hurdles when dealing with the high dimensionality of videos. This challenge becomes particularly pronounced when striving to increase the frames per second (FPS) to enhance the motion capturing capabilities. Such a pursuit is likely to introduce redundancy and exacerbate the existing computational limitations. In this paper, we initiate by showcasing the enhanced performance achieved through an escalation in the FPS rate. Additionally, we present a novel approach, Motion Guided Token Compression (MGTC), to empower Transformer models to utilize a smaller yet more representative set of tokens for comprehensive video representation. Consequently, this yields substantial reductions in computational burden and remains seamlessly adaptable to increased FPS rates. Specifically, we draw inspiration from video compression algorithms and scrutinize the variance between patches in consecutive video frames across the temporal dimension. The tokens exhibiting a disparity below a predetermined threshold are then masked. Notably, this masking strategy effectively addresses video redundancy while conserving essential information. Our experiments, conducted on widely examined video recognition datasets, Kinetics-400, UCF101 and HMDB51, demonstrate that elevating the FPS rate results in a significant top-1 accuracy score improvement of over 1.6, 1.6 and 4.0. By implementing MGTC with the masking ratio of 25\\%, we further augment accuracy by 0.1 and simultaneously reduce computational costs by over 31\\% on Kinetics-400. Even within a fixed computational budget, higher FPS rates paired with MGTC sustain performance gains when compared to lower FPS settings.\n    "
    },
    "2402.18587": {
        "title": "At the Dawn of Generative AI Era: A Tutorial-cum-Survey on New Frontiers in 6G Wireless Intelligence",
        "authors": [
            "Abdulkadir Celik",
            "Ahmed M. Eltawil"
        ],
        "comments": " ",
        "subjects": "Networking and Internet Architecture (cs.NI)",
        "abstract": "The majority of data-driven wireless research leans heavily on discriminative AI (DAI) that requires vast real-world datasets. Unlike the DAI, Generative AI (GenAI) pertains to generative models (GMs) capable of discerning the underlying data distribution, patterns, and features of the input data. This makes GenAI a crucial asset in wireless domain wherein real-world data is often scarce, incomplete, costly to acquire, and hard to model or comprehend. With these appealing attributes, GenAI can replace or supplement DAI methods in various capacities. Accordingly, this combined tutorial-survey paper commences with preliminaries of 6G and wireless intelligence by outlining candidate 6G applications and services, presenting a taxonomy of state-of-the-art DAI models, exemplifying prominent DAI use cases, and elucidating the multifaceted ways through which GenAI enhances DAI. Subsequently, we present a tutorial on GMs by spotlighting seminal examples such as generative adversarial networks, variational autoencoders, flow-based GMs, diffusion-based GMs, generative transformers, large language models, to name a few. Contrary to the prevailing belief that GenAI is a nascent trend, our exhaustive review of approximately 120 technical papers demonstrates the scope of research across core wireless research areas, including physical layer design; network optimization, organization, and management; network traffic analytics; cross-layer network security; and localization & positioning. Furthermore, we outline the central role of GMs in pioneering areas of 6G network research, including semantic/THz/near-field communications, ISAC, extremely large antenna arrays, digital twins, AI-generated content services, mobile edge computing and edge AI, adversarial ML, and trustworthy AI. Lastly, we shed light on the multifarious challenges ahead, suggesting potential strategies and promising remedies.\n    "
    },
    "2402.18599": {
        "title": "Meta-Tasks: An alternative view on Meta-Learning Regularization",
        "authors": [
            "Mohammad Rostami",
            "Atik Faysal",
            "Huaxia Wang",
            "Avimanyu Sahoo",
            "Ryan Antle"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Few-shot learning (FSL) is a challenging machine learning problem due to a scarcity of labeled data. The ability to generalize effectively on both novel and training tasks is a significant barrier to FSL. This paper proposes a novel solution that can generalize to both training and novel tasks while also utilizing unlabeled samples. The method refines the embedding model before updating the outer loop using unsupervised techniques as ``meta-tasks''. The experimental results show that our proposed method performs well on novel and training tasks, with faster and better convergence, lower generalization, and standard deviation error, indicating its potential for practical applications in FSL. The experimental results show that the proposed method outperforms prototypical networks by 3.9%.\n    "
    },
    "2402.18605": {
        "title": "FORML: A Riemannian Hessian-free Method for Meta-learning with Orthogonality Constraint",
        "authors": [
            "Hadi Tabealhojeh",
            "Soumava Kumar Roy",
            "Peyman Adibi",
            "Hossein Karshenas"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Meta-learning problem is usually formulated as a bi-level optimization in which the task-specific and the meta-parameters are updated in the inner and outer loops of optimization, respectively. However, performing the optimization in the Riemannian space, where the parameters and meta-parameters are located on Riemannian manifolds is computationally intensive. Unlike the Euclidean methods, the Riemannian backpropagation needs computing the second-order derivatives that include backward computations through the Riemannian operators such as retraction and orthogonal projection. This paper introduces a Hessian-free approach that uses a first-order approximation of derivatives on the Stiefel manifold. Our method significantly reduces the computational load and memory footprint. We show how using a Stiefel fully-connected layer that enforces orthogonality constraint on the parameters of the last classification layer as the head of the backbone network, strengthens the representation reuse of the gradient-based meta-learning methods. Our experimental results across various few-shot learning datasets, demonstrate the superiority of our proposed method compared to the state-of-the-art methods, especially MAML, its Euclidean counterpart.\n    "
    },
    "2402.18621": {
        "title": "Unveiling News Publishers Trustworthiness Through Social Interactions",
        "authors": [
            "Manuel Pratelli",
            "Fabio Saracco",
            "Marinella Petrocchi"
        ],
        "comments": "A pre-final version of the paper accepted at WebSci'24",
        "subjects": "Social and Information Networks (cs.SI)",
        "abstract": "With the primary goal of raising readers' awareness of misinformation phenomena, extensive efforts have been made by both academic institutions and independent organizations to develop methodologies for assessing the trustworthiness of online news publishers. Unfortunately, existing approaches are costly and face critical scalability challenges. This study presents a novel framework for assessing the trustworthiness of online news publishers using user interactions on social media platforms. The proposed methodology provides a versatile solution that serves the dual purpose of i) identifying verifiable online publishers and ii) automatically performing an initial estimation of the trustworthiness of previously unclassified online news outlets.\n    "
    },
    "2402.18650": {
        "title": "The Grasp Reset Mechanism: An Automated Apparatus for Conducting Grasping Trials",
        "authors": [
            "Kyle DuFrene",
            "Keegan Nave",
            "Joshua Campbell",
            "Ravi Balasubramanian",
            "Cindy Grimm"
        ],
        "comments": "Accepted to the 2024 IEEE International Conference on Robotics and Automation (ICRA2024)",
        "subjects": "Robotics (cs.RO)",
        "abstract": "Advancing robotic grasping and manipulation requires the ability to test algorithms and/or train learning models on large numbers of grasps. Towards the goal of more advanced grasping, we present the Grasp Reset Mechanism (GRM), a fully automated apparatus for conducting large-scale grasping trials. The GRM automates the process of resetting a grasping environment, repeatably placing an object in a fixed location and controllable 1-D orientation. It also collects data and swaps between multiple objects enabling robust dataset collection with no human intervention. We also present a standardized state machine interface for control, which allows for integration of most manipulators with minimal effort. In addition to the physical design and corresponding software, we include a dataset of 1,020 grasps. The grasps were created with a Kinova Gen3 robot arm and Robotiq 2F-85 Adaptive Gripper to enable training of learning models and to demonstrate the capabilities of the GRM. The dataset includes ranges of grasps conducted across four objects and a variety of orientations. Manipulator states, object pose, video, and grasp success data are provided for every trial.\n    "
    },
    "2402.18659": {
        "title": "Large Language Models and Games: A Survey and Roadmap",
        "authors": [
            "Roberto Gallotta",
            "Graham Todd",
            "Marvin Zammit",
            "Sam Earle",
            "Antonios Liapis",
            "Julian Togelius",
            "Georgios N. Yannakakis"
        ],
        "comments": "13 pages, 4 figures",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Recent years have seen an explosive increase in research on large language models (LLMs), and accompanying public engagement on the topic. While starting as a niche area within natural language processing, LLMs have shown remarkable potential across a broad range of applications and domains, including games. This paper surveys the current state of the art across the various applications of LLMs in and for games, and identifies the different roles LLMs can take within a game. Importantly, we discuss underexplored areas and promising directions for future uses of LLMs in games and we reconcile the potential and limitations of LLMs within the games domain. As the first comprehensive survey and roadmap at the intersection of LLMs and games, we are hopeful that this paper will serve as the basis for groundbreaking research and innovation in this exciting new field.\n    "
    },
    "2402.18667": {
        "title": "FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability",
        "authors": [
            "Congying Xia",
            "Chen Xing",
            "Jiangshu Du",
            "Xinyi Yang",
            "Yihao Feng",
            "Ran Xu",
            "Wenpeng Yin",
            "Caiming Xiong"
        ],
        "comments": "The first two authors contributed equally",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "This paper presents FoFo, a pioneering benchmark for evaluating large language models' (LLMs) ability to follow complex, domain-specific formats, a crucial yet underexamined capability for their application as AI agents. Despite LLMs' advancements, existing benchmarks fail to assess their format-following proficiency adequately. FoFo fills this gap with a diverse range of real-world formats and instructions, developed through an AI-Human collaborative method. Our evaluation across both open-source (e.g., Llama 2, WizardLM) and closed-source (e.g., GPT-4, PALM2, Gemini) LLMs highlights three key findings: open-source models significantly lag behind closed-source ones in format adherence; LLMs' format-following performance is independent of their content generation quality; and LLMs' format proficiency varies across different domains. These insights suggest the need for specialized tuning for format-following skills and highlight FoFo's role in guiding the selection of domain-specific AI agents. FoFo is released here at this https URL.\n    "
    },
    "2402.18668": {
        "title": "Simple linear attention language models balance the recall-throughput tradeoff",
        "authors": [
            "Simran Arora",
            "Sabri Eyuboglu",
            "Michael Zhang",
            "Aman Timalsina",
            "Silas Alberti",
            "Dylan Zinsley",
            "James Zou",
            "Atri Rudra",
            "Christopher R\u00e9"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train language models up to 1.3b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points. Implementations of linear attention are often less efficient than optimized standard attention implementations. To make BASED competitive, we develop IO-aware algorithms that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models. Code for this work is provided at: this https URL.\n    "
    },
    "2402.18675": {
        "title": "Robot Body Schema Learning from Full-body Extero/Proprioception Sensors",
        "authors": [
            "Shuo Jiang",
            "Jinkun Zhang",
            "Lawson Wong"
        ],
        "comments": " ",
        "subjects": "Robotics (cs.RO)",
        "abstract": "For a robot, its body structure is an a-prior knowledge when it is designed. However, when such information is not available, can a robot recognize it by itself? In this paper, we aim to grant a robot such ability to learn its body structure from exteroception and proprioception data collected from on-body sensors. By a novel machine learning method, the robot can learn a binary Heterogeneous Dependency Matrix from its sensor readings. We showed such matrix is equivalent to a Heterogeneous out-tree structure which can uniquely represent the robot body topology. We explored the properties of such matrix and the out-tree, and proposed a remedy to fix them when they are contaminated by partial observability or data noise. We ran our algorithm on 6 different robots with different body structures in simulation and 1 real robot. Our algorithm correctly recognized their body structures with only on-body sensor readings but no topology prior knowledge.\n    "
    },
    "2402.18682": {
        "title": "Acoustic tactile sensing for mobile robot wheels",
        "authors": [
            "Wilfred Mason",
            "David Brenken",
            "Falcon Z. Dai",
            "Ricardo Gonzalo Cruz Castillo",
            "Olivier St-Martin Cormier",
            "Audrey Sedal"
        ],
        "comments": "12 pages, 12 figures",
        "subjects": "Robotics (cs.RO)",
        "abstract": "Tactile sensing in mobile robots remains under-explored, mainly due to challenges related to sensor integration and the complexities of distributed sensing. In this work, we present a tactile sensing architecture for mobile robots based on wheel-mounted acoustic waveguides. Our sensor architecture enables tactile sensing along the entire circumference of a wheel with a single active component: an off-the-shelf acoustic rangefinder. We present findings showing that our sensor, mounted on the wheel of a mobile robot, is capable of discriminating between different terrains, detecting and classifying obstacles with different geometries, and performing collision detection via contact localization. We also present a comparison between our sensor and sensors traditionally used in mobile robots, and point to the potential for sensor fusion approaches that leverage the unique capabilities of our tactile sensing architecture. Our findings demonstrate that autonomous mobile robots can further leverage our sensor architecture for diverse mapping tasks requiring knowledge of terrain material, surface topology, and underlying structure.\n    "
    },
    "2402.18688": {
        "title": "Exploring AI Problem Formulation with Children via Teachable Machines",
        "authors": [
            "Utkarsh Dwivedi",
            "Salma Elsayed-Ali",
            "Elizabeth Bonsignore",
            "Hernisa Kacorri"
        ],
        "comments": " ",
        "subjects": "Human-Computer Interaction (cs.HC)",
        "abstract": "Emphasizing problem formulation in AI literacy activities with children is vital, yet we lack empirical studies on their structure and affordances. We propose that participatory design involving teachable machines facilitates problem formulation activities. To test this, we integrated problem reduction heuristics into storyboarding and invited a university-based intergenerational design team of 10 children (ages 8-13) and 9 adults to co-design a teachable machine. We find that children draw from personal experiences when formulating AI problems; they assume voice and video capabilities, explore diverse machine learning approaches, and plan for error handling. Their ideas promote human involvement in AI, though some are drawn to more autonomous systems. Their designs prioritize values like capability, logic, helpfulness, responsibility, and obedience, and a preference for a comfortable life, family security, inner harmony, and excitement as end-states. We conclude by discussing how these results can inform the design of future participatory AI activities.\n    "
    },
    "2402.18695": {
        "title": "Grounding Language Models for Visual Entity Recognition",
        "authors": [
            "Zilin Xiao",
            "Ming Gong",
            "Paola Cascante-Bonilla",
            "Xingyao Zhang",
            "Jie Wu",
            "Vicente Ordonez"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We introduce AutoVER, an Autoregressive model for Visual Entity Recognition. Our model extends an autoregressive Multi-modal Large Language Model by employing retrieval augmented constrained generation. It mitigates low performance on out-of-domain entities while excelling in queries that require visually-situated reasoning. Our method learns to distinguish similar entities within a vast label space by contrastively training on hard negative pairs in parallel with a sequence-to-sequence objective without an external retriever. During inference, a list of retrieved candidate answers explicitly guides language generation by removing invalid decoding paths. The proposed method achieves significant improvements across different dataset splits in the recently proposed Oven-Wiki benchmark. Accuracy on the Entity seen split rises from 32.7% to 61.5%. It also demonstrates superior performance on the unseen and query splits by a substantial double-digit margin.\n    "
    },
    "2402.18698": {
        "title": "Spatial Coherence Loss for Salient and Camouflaged Object Detection and Beyond",
        "authors": [
            "Ziyun Yang",
            "Kevin Choy",
            "Sina Farsiu"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Generic object detection is a category-independent task that relies on accurate modeling of objectness. Most relevant CNN-based models of objectness utilize loss functions (e.g., binary cross entropy) that focus on the single-response, i.e., the loss response of a single pixel. Inspired by the human visual system, which first discerns the boundaries of ambiguous regions (i.e., hard regions) before delving into the semantic meaning, we propose a novel loss function, Spatial Coherence Loss (SCLoss), that uses the mutual response between adjacent pixels to suppress or emphasize the single-response of pixels. We demonstrate that the proposed SCLoss can gradually learn the hard regions by detecting and emphasizing their boundaries. Through comprehensive experiments, we demonstrate that replacing popular loss functions with SCLoss can improve the performance of current state-of-the-art (SOTA) salient or camouflaged object detection (SOD or COD) models. We also demonstrate that combining SCLoss with other loss functions can further improve performance and result in the SOTA outcomes for different applications. Finally, as a demonstrative example of the potential uses for other related tasks, we show an application of SCLoss for semantic segmentation.\n    "
    },
    "2402.18705": {
        "title": "How Platform Exchange and Safeguards Matter: The Case of Sexual Risk in Airbnb and Couchsurfing",
        "authors": [
            "Skyler Wang"
        ],
        "comments": " ",
        "subjects": "Human-Computer Interaction (cs.HC)",
        "abstract": "Recent work in CHI and CSCW has devoted increasing attention to how the design of network hospitality platforms shapes user experiences and relational outcomes. In this article, I interrogate how different risk factors emerge based on the type of exchanges these platforms facilitate. To do so, I juxtapose two prominent network hospitality platforms: one facilitating negotiated exchange (i.e., Airbnb) with another facilitating reciprocal exchange (i.e., Couchsurfing) between users. Homing in on sexual risk, an underexplored form of platform danger, and drawing on interviews with 40 female dual-platform users, I argue that the provision of binding negotiated exchange and institutional safeguards by Airbnb reduces risk through three mechanisms: casting initial guest-host relation into a buyer-seller arrangement, stabilizing interactional scripts, and formalizing sexual violence recourse. Conversely, Couchsurfing's reciprocal exchange and lack of safeguards increase sexual precarity for users both on- and off-platform. This study demonstrates how platforms with strong prosocial motivations can jeopardize sociality and concludes with implications for designs that better protect vulnerable user populations.\n    "
    },
    "2402.18707": {
        "title": "Embodied Supervision: Haptic Display of Automation Command to Improve Supervisory Performance",
        "authors": [
            "Alia Gilbert",
            "Sachit Krishnan",
            "R. Brent Gillespie"
        ],
        "comments": "IEEE Haptics Symposium 2024",
        "subjects": "Human-Computer Interaction (cs.HC)",
        "abstract": "A human operator using a manual control interface has ready access to their own command signal, both by efference copy and proprioception. In contrast, a human supervisor typically relies on visual information alone. We propose supplying a supervisor with a copy of the operators command signal, hypothesizing improved performance, especially when that copy is provided through haptic display. We experimentally compared haptic with visual access to the command signal, quantifying the performance of N equals 10 participants attempting to determine which of three reference signals was being tracked by an operator. Results indicate an improved accuracy in identifying the tracked target when haptic display was available relative to visual display alone. We conjecture the benefit follows from the relationship of haptics to the supervisor's own experience, perhaps muscle memory, as an operator.\n    "
    },
    "2402.18710": {
        "title": "Hefty: A Modular Reconfigurable Robot for Advancing Robot Manipulation in Agriculture",
        "authors": [
            "Dominic Guri",
            "Moonyoung Lee",
            "Oliver Kroemer",
            "George Kantor"
        ],
        "comments": "8 pages, 11 figures",
        "subjects": "Robotics (cs.RO)",
        "abstract": "This paper presents a modular, reconfigurable robot platform for robot manipulation in agriculture. While robot manipulation promises great advancements in automating challenging, complex tasks that are currently best left to humans, it is also an expensive capital investment for researchers and users because it demands significantly varying robot configurations depending on the task. Modular robots provide a way to obtain multiple configurations and reduce costs by enabling incremental acquisition of only the necessary modules. The robot we present, Hefty, is designed to be modular and reconfigurable. It is designed for both researchers and end-users as a means to improve technology transfer from research to real-world application. This paper provides a detailed design and integration process, outlining the critical design decisions that enable modularity in the mobility of the robot as well as its sensor payload, power systems, computing, and fixture mounting. We demonstrate the utility of the robot by presenting five configurations used in multiple real-world agricultural robotics applications.\n    "
    },
    "2402.18728": {
        "title": "Not All the Same: Understanding and Informing Similarity Estimation in Tile-Based Video Games",
        "authors": [
            "Sebastian Berns",
            "Vanessa Volz",
            "Laurissa Tokarchuk",
            "Sam Snodgrass",
            "Christian Guckelsberger"
        ],
        "comments": "Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI '24), 11-16 May 2024, Honolulu, HI, USA",
        "subjects": "Human-Computer Interaction (cs.HC)",
        "abstract": "Similarity estimation is essential for many game AI applications, from the procedural generation of distinct assets to automated exploration with game-playing agents. While similarity metrics often substitute human evaluation, their alignment with our judgement is unclear. Consequently, the result of their application can fail human expectations, leading to e.g. unappreciated content or unbelievable agent behaviour. We alleviate this gap through a multi-factorial study of two tile-based games in two representations, where participants (N=456) judged the similarity of level triplets. Based on this data, we construct domain-specific perceptual spaces, encoding similarity-relevant attributes. We compare 12 metrics to these spaces and evaluate their approximation quality through several quantitative lenses. Moreover, we conduct a qualitative labelling study to identify the features underlying the human similarity judgement in this popular genre. Our findings inform the selection of existing metrics and highlight requirements for the design of new similarity metrics benefiting game development and research.\n    "
    },
    "2402.18732": {
        "title": "GAIA: Categorical Foundations of Generative AI",
        "authors": [
            "Sridhar Mahadevan"
        ],
        "comments": "65 pages. arXiv admin note: text overlap with arXiv:2212.08981",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "In this paper, we propose GAIA, a generative AI architecture based on category theory. GAIA is based on a hierarchical model where modules are organized as a simplicial complex. Each simplicial complex updates its internal parameters biased on information it receives from its superior simplices and in turn relays updates to its subordinate sub-simplices. Parameter updates are formulated in terms of lifting diagrams over simplicial sets, where inner and outer horn extensions correspond to different types of learning problems. Backpropagation is modeled as an endofunctor over the category of parameters, leading to a coalgebraic formulation of deep learning.\n    "
    },
    "2402.18742": {
        "title": "Comparing Importance Sampling Based Methods for Mitigating the Effect of Class Imbalance",
        "authors": [
            "Indu Panigrahi",
            "Richard Zhu"
        ],
        "comments": "Preprint",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Most state-of-the-art computer vision models heavily depend on data. However, many datasets exhibit extreme class imbalance which has been shown to negatively impact model performance. Among the training-time and data-generation solutions that have been explored, one subset that leverages existing data is importance sampling. A good deal of this work focuses primarily on the CIFAR-10 and CIFAR-100 datasets which fail to be representative of the scale, composition, and complexity of current state-of-the-art datasets. In this work, we explore and compare three techniques that derive from importance sampling: loss reweighting, undersampling, and oversampling. Specifically, we compare the effect of these techniques on the performance of two encoders on an impactful satellite imagery dataset, Planet's Amazon Rainforest dataset, in preparation for another work. Furthermore, we perform supplemental experimentation on a scene classification dataset, ADE20K, to test on a contrasting domain and clarify our results. Across both types of encoders, we find that up-weighting the loss for and undersampling has a negigible effect on the performance on underrepresented classes. Additionally, our results suggest oversampling generally improves performance for the same underrepresented classes. Interestingly, our findings also indicate that there may exist some redundancy in data in the Planet dataset. Our work aims to provide a foundation for further work on the Planet dataset and similar domain-specific datasets. We open-source our code at this https URL for future work on other satellite imagery datasets as well.\n    "
    },
    "2402.18743": {
        "title": "A revision on Multi-Criteria Decision Making methods for Multi-UAV Mission Planning Support",
        "authors": [
            "Cristian Ramirez-Atencia",
            "Victor Rodriguez-Fernandez",
            "David Camacho"
        ],
        "comments": "Preprint submitted and acepted in Expert Systems with Applications",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Over the last decade, Unmanned Aerial Vehicles (UAVs) have been extensively used in many commercial applications due to their manageability and risk avoidance. One of the main problems considered is the Mission Planning for multiple UAVs, where a solution plan must be found satisfying the different constraints of the problem. This problem has multiple variables that must be optimized simultaneously, such as the makespan, the cost of the mission or the risk. Therefore, the problem has a lot of possible optimal solutions, and the operator must select the final solution to be executed among them. In order to reduce the workload of the operator in this decision process, a Decision Support System (DSS) becomes necessary. In this work, a DSS consisting of ranking and filtering systems, which order and reduce the optimal solutions, has been designed. With regard to the ranking system, a wide range of Multi-Criteria Decision Making (MCDM) methods, including some fuzzy MCDM, are compared on a multi-UAV mission planning scenario, in order to study which method could fit better in a multi-UAV decision support system. Expert operators have evaluated the solutions returned, and the results show, on the one hand, that fuzzy methods generally achieve better average scores, and on the other, that all of the tested methods perform better when the preferences of the operators are biased towards a specific variable, and worse when their preferences are balanced. For the filtering system, a similarity function based on the proximity of the solutions has been designed, and on top of that, a threshold is tuned empirically to decide how to filter solutions without losing much of the hypervolume of the space of solutions.\n    "
    },
    "2402.18747": {
        "title": "Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains",
        "authors": [
            "Vil\u00e9m Zouhar",
            "Shuoyang Ding",
            "Anna Currey",
            "Tatyana Badeka",
            "Jenyuan Wang",
            "Brian Thompson"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "We introduce a new, extensive multidimensional quality metrics (MQM) annotated dataset covering 11 language pairs in the biomedical domain. We use this dataset to investigate whether machine translation (MT) metrics which are fine-tuned on human-generated MT quality judgements are robust to domain shifts between training and inference. We find that fine-tuned metrics exhibit a substantial performance drop in the unseen domain scenario relative to metrics that rely on the surface form, as well as pre-trained metrics which are not fine-tuned on MT quality judgments.\n    "
    },
    "2402.18751": {
        "title": "Multi-Sensor and Multi-temporal High-Throughput Phenotyping for Monitoring and Early Detection of Water-Limiting Stress in Soybean",
        "authors": [
            "Sarah E. Jones",
            "Timilehin Ayanlade",
            "Benjamin Fallen",
            "Talukder Z. Jubery",
            "Arti Singh",
            "Baskar Ganapathysubramanian",
            "Soumik Sarkar",
            "Asheesh K. Singh"
        ],
        "comments": "25 pages, 5 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Soybean production is susceptible to biotic and abiotic stresses, exacerbated by extreme weather events. Water limiting stress, i.e. drought, emerges as a significant risk for soybean production, underscoring the need for advancements in stress monitoring for crop breeding and production. This project combines multi-modal information to identify the most effective and efficient automated methods to investigate drought response. We investigated a set of diverse soybean accessions using multiple sensors in a time series high-throughput phenotyping manner to: (1) develop a pipeline for rapid classification of soybean drought stress symptoms, and (2) investigate methods for early detection of drought stress. We utilized high-throughput time-series phenotyping using UAVs and sensors in conjunction with machine learning (ML) analytics, which offered a swift and efficient means of phenotyping. The red-edge and green bands were most effective to classify canopy wilting stress. The Red-Edge Chlorophyll Vegetation Index (RECI) successfully differentiated susceptible and tolerant soybean accessions prior to visual symptom development. We report pre-visual detection of soybean wilting using a combination of different vegetation indices. These results can contribute to early stress detection methodologies and rapid classification of drought responses in screening nurseries for breeding and production applications.\n    "
    },
    "2402.18753": {
        "title": "Like-minded, like-bodied: How users (18-26) trust online eating and health information",
        "authors": [
            "Rachel Xu",
            "Nhu Le",
            "Rebekah Park",
            "Laura Murray"
        ],
        "comments": "10 pages",
        "subjects": "Human-Computer Interaction (cs.HC)",
        "abstract": "This paper investigates the relationship between social media and eating practices amongst 42 internet users aged 18-26. We conducted an ethnography in the US and India to observe how they navigated eating and health information online. We found that participants portrayed themselves online through a vocabulary we have labeled \"the good life\": performing holistic health by displaying a socially-ideal body. In doing so, participants unconsciously engaged in behaviors of disordered eating while actively eschewing them. They also valued personal testimonies, and readily tested tips from content creators who shared similar beliefs and bodies to them. In doing so, they discarded probabilistic thinking and opened themselves to harm. Our study found that their social media feeds did not unidirectionally influence participants - they also reflected participants' internalized views of health, in an intertwined, non-linear journey. Reducing the online spread of disordered eating practices requires addressing it within young people's social context.\n    "
    },
    "2402.18754": {
        "title": "Extending QGroundControl for Automated Mission Planning of UAVs",
        "authors": [
            "Cristian Ramirez-Atencia",
            "David Camacho"
        ],
        "comments": "Preprint submitted and accepted in Sensors",
        "subjects": "Robotics (cs.RO)",
        "abstract": "Unmanned Aerial Vehicle (UAVs) have become very popular in the last decade due to some advantages such as strong terrain adaptation, low cost, zero casualties, and so on. One of the most interesting advances in this field is the automation of mission planning (task allocation) and real-time replanning, which are highly useful to increase the autonomy of the vehicle and reduce the operator workload. These automated mission planning and replanning systems require a Human Computer Interface (HCI) that facilitates the visualization and selection of plans that will be executed by the vehicles. In addition, most missions should be assessed before their real-life execution. This paper extends QGroundControl, an open-source simulation environment for flight control of multiple vehicles, by adding a mission designer that permits the operator to build complex missions with tasks and other scenario items; an interface for automated mission planning and replanning, which works as a test bed for different algorithms, and a Decision Support System (DSS) that helps the operator in the selection of the plan. In this work, a complete guide of these systems and some practical use cases are provided.\n    "
    },
    "2402.18756": {
        "title": "How Much Annotation is Needed to Compare Summarization Models?",
        "authors": [
            "Chantal Shaib",
            "Joe Barrow",
            "Alexa F. Siu",
            "Byron C. Wallace",
            "Ani Nenkova"
        ],
        "comments": "Preprint",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Modern instruction-tuned models have become highly capable in text generation tasks such as summarization, and are expected to be released at a steady pace. In practice one may now wish to choose confidently, but with minimal effort, the best performing summarization model when applied to a new domain or purpose. In this work, we empirically investigate the test sample size necessary to select a preferred model in the context of news summarization. Empirical results reveal that comparative evaluation converges quickly for both automatic and human evaluation, with clear preferences for a system emerging from under 100 examples. The human preference data allows us to quantify how well automatic scores can reproduce preference rankings across a variety of downstream summarization tasks. We find that, while automatic metrics are stable at smaller sample sizes, only some automatic metrics are able to moderately predict model win rates according to human preference.\n    "
    },
    "2402.18762": {
        "title": "Disentangling the Causes of Plasticity Loss in Neural Networks",
        "authors": [
            "Clare Lyle",
            "Zeyu Zheng",
            "Khimya Khetarpal",
            "Hado van Hasselt",
            "Razvan Pascanu",
            "James Martens",
            "Will Dabney"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Underpinning the past decades of work on the design, initialization, and optimization of neural networks is a seemingly innocuous assumption: that the network is trained on a \\textit{stationary} data distribution. In settings where this assumption is violated, e.g.\\ deep reinforcement learning, learning algorithms become unstable and brittle with respect to hyperparameters and even random seeds. One factor driving this instability is the loss of plasticity, meaning that updating the network's predictions in response to new information becomes more difficult as training progresses. While many recent works provide analyses and partial solutions to this phenomenon, a fundamental question remains unanswered: to what extent do known mechanisms of plasticity loss overlap, and how can mitigation strategies be combined to best maintain the trainability of a network? This paper addresses these questions, showing that loss of plasticity can be decomposed into multiple independent mechanisms and that, while intervening on any single mechanism is insufficient to avoid the loss of plasticity in all cases, intervening on multiple mechanisms in conjunction results in highly robust learning algorithms. We show that a combination of layer normalization and weight decay is highly effective at maintaining plasticity in a variety of synthetic nonstationary learning tasks, and further demonstrate its effectiveness on naturally arising nonstationarities, including reinforcement learning in the Arcade Learning Environment.\n    "
    },
    "2402.18780": {
        "title": "A Quantitative Evaluation of Score Distillation Sampling Based Text-to-3D",
        "authors": [
            "Xiaohan Fei",
            "Chethan Parameshwara",
            "Jiawei Mo",
            "Xiaolong Li",
            "Ashwin Swaminathan",
            "CJ Taylor",
            "Paolo Favaro",
            "Stefano Soatto"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The development of generative models that create 3D content from a text prompt has made considerable strides thanks to the use of the score distillation sampling (SDS) method on pre-trained diffusion models for image generation. However, the SDS method is also the source of several artifacts, such as the Janus problem, the misalignment between the text prompt and the generated 3D model, and 3D model inaccuracies. While existing methods heavily rely on the qualitative assessment of these artifacts through visual inspection of a limited set of samples, in this work we propose more objective quantitative evaluation metrics, which we cross-validate via human ratings, and show analysis of the failure cases of the SDS technique. We demonstrate the effectiveness of this analysis by designing a novel computationally efficient baseline model that achieves state-of-the-art performance on the proposed metrics while addressing all the above-mentioned artifacts.\n    "
    },
    "2402.18786": {
        "title": "OpticalDR: A Deep Optical Imaging Model for Privacy-Protective Depression Recognition",
        "authors": [
            "Yuchen Pan",
            "Junjun Jiang",
            "Kui Jiang",
            "Zhihao Wu",
            "Keyuan Yu",
            "Xianming Liu"
        ],
        "comments": "Accepted by CVPR 2024",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Depression Recognition (DR) poses a considerable challenge, especially in the context of the growing concerns surrounding privacy. Traditional automatic diagnosis of DR technology necessitates the use of facial images, undoubtedly expose the patient identity features and poses privacy risks. In order to mitigate the potential risks associated with the inappropriate disclosure of patient facial images, we design a new imaging system to erase the identity information of captured facial images while retain disease-relevant features. It is irreversible for identity information recovery while preserving essential disease-related characteristics necessary for accurate DR. More specifically, we try to record a de-identified facial image (erasing the identifiable features as much as possible) by a learnable lens, which is optimized in conjunction with the following DR task as well as a range of face analysis related auxiliary tasks in an end-to-end manner. These aforementioned strategies form our final Optical deep Depression Recognition network (OpticalDR). Experiments on CelebA, AVEC 2013, and AVEC 2014 datasets demonstrate that our OpticalDR has achieved state-of-the-art privacy protection performance with an average AUC of 0.51 on popular facial recognition models, and competitive results for DR with MAE/RMSE of 7.53/8.48 on AVEC 2013 and 7.89/8.82 on AVEC 2014, respectively.\n    "
    },
    "2402.18796": {
        "title": "MOSAIC: A Modular System for Assistive and Interactive Cooking",
        "authors": [
            "Huaxiaoyue Wang",
            "Kushal Kedia",
            "Juntao Ren",
            "Rahma Abdullah",
            "Atiksh Bhardwaj",
            "Angela Chao",
            "Kelly Y Chen",
            "Nathaniel Chin",
            "Prithwish Dan",
            "Xinyi Fan",
            "Gonzalo Gonzalez-Pumariega",
            "Aditya Kompella",
            "Maximus Adrian Pace",
            "Yash Sharma",
            "Xiangwan Sun",
            "Neha Sunkara",
            "Sanjiban Choudhury"
        ],
        "comments": "22 pages, 13 figures",
        "subjects": "Robotics (cs.RO)",
        "abstract": "We present MOSAIC, a modular architecture for home robots to perform complex collaborative tasks, such as cooking with everyday users. MOSAIC tightly collaborates with humans, interacts with users using natural language, coordinates multiple robots, and manages an open vocabulary of everyday objects. At its core, MOSAIC employs modularity: it leverages multiple large-scale pre-trained models for general tasks like language and image recognition, while using streamlined modules designed for task-specific control. We extensively evaluate MOSAIC on 60 end-to-end trials where two robots collaborate with a human user to cook a combination of 6 recipes. We also extensively test individual modules with 180 episodes of visuomotor picking, 60 episodes of human motion forecasting, and 46 online user evaluations of the task planner. We show that MOSAIC is able to efficiently collaborate with humans by running the overall system end-to-end with a real human user, completing 68.3% (41/60) collaborative cooking trials of 6 different recipes with a subtask completion rate of 91.6%. Finally, we discuss the limitations of the current system and exciting open challenges in this domain. The project's website is at this https URL\n"
    },
    "2402.18797": {
        "title": "ARTiST: Automated Text Simplification for Task Guidance in Augmented Reality",
        "authors": [
            "Guande Wu",
            "Jing Qian",
            "Sonia Castelo",
            "Shaoyu Chen",
            "Joao Rulff",
            "Claudio Silva"
        ],
        "comments": "Conditionally accepted by CHI '24",
        "subjects": "Human-Computer Interaction (cs.HC)",
        "abstract": "Text presented in augmented reality provides in-situ, real-time information for users. However, this content can be challenging to apprehend quickly when engaging in cognitively demanding AR tasks, especially when it is presented on a head-mounted display. We propose ARTiST, an automatic text simplification system that uses a few-shot prompt and GPT-3 models to specifically optimize the text length and semantic content for augmented reality. Developed out of a formative study that included seven users and three experts, our system combines a customized error calibration model with a few-shot prompt to integrate the syntactic, lexical, elaborative, and content simplification techniques, and generate simplified AR text for head-worn displays. Results from a 16-user empirical study showed that ARTiST lightens the cognitive load and improves performance significantly over both unmodified text and text modified via traditional methods. Our work constitutes a step towards automating the optimization of batch text data for readability and performance in augmented reality.\n    "
    },
    "2402.18803": {
        "title": "To Pool or Not To Pool: Analyzing the Regularizing Effects of Group-Fair Training on Shared Models",
        "authors": [
            "Cyrus Cousins",
            "I. Elizabeth Kumar",
            "Suresh Venkatasubramanian"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In fair machine learning, one source of performance disparities between groups is over-fitting to groups with relatively few training samples. We derive group-specific bounds on the generalization error of welfare-centric fair machine learning that benefit from the larger sample size of the majority group. We do this by considering group-specific Rademacher averages over a restricted hypothesis class, which contains the family of models likely to perform well with respect to a fair learning objective (e.g., a power-mean). Our simulations demonstrate these bounds improve over a naive method, as expected by theory, with particularly significant improvement for smaller group sizes.\n    "
    },
    "2402.18807": {
        "title": "On the Decision-Making Abilities in Role-Playing using Large Language Models",
        "authors": [
            "Chenglei Shen",
            "Guofu Xie",
            "Xiao Zhang",
            "Jun Xu"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Large language models (LLMs) are now increasingly utilized for role-playing tasks, especially in impersonating domain-specific experts, primarily through role-playing prompts. When interacting in real-world scenarios, the decision-making abilities of a role significantly shape its behavioral patterns. In this paper, we concentrate on evaluating the decision-making abilities of LLMs post role-playing thereby validating the efficacy of role-playing. Our goal is to provide metrics and guidance for enhancing the decision-making abilities of LLMs in role-playing tasks. Specifically, we first use LLMs to generate virtual role descriptions corresponding to the 16 personality types of Myers-Briggs Type Indicator (abbreviated as MBTI) representing a segmentation of the population. Then we design specific quantitative operations to evaluate the decision-making abilities of LLMs post role-playing from four aspects: adaptability, exploration$\\&$exploitation trade-off ability, reasoning ability, and safety. Finally, we analyze the association between the performance of decision-making and the corresponding MBTI types through GPT-4. Extensive experiments demonstrate stable differences in the four aspects of decision-making abilities across distinct roles, signifying a robust correlation between decision-making abilities and the roles emulated by LLMs. These results underscore that LLMs can effectively impersonate varied roles while embodying their genuine sociological characteristics.\n    "
    },
    "2402.18811": {
        "title": "BFRFormer: Transformer-based generator for Real-World Blind Face Restoration",
        "authors": [
            "Guojing Ge",
            "Qi Song",
            "Guibo Zhu",
            "Yuting Zhang",
            "Jinglu Chen",
            "Miao Xin",
            "Ming Tang",
            "Jinqiao Wang"
        ],
        "comments": "Accepted by ICASSP 2024",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Blind face restoration is a challenging task due to the unknown and complex degradation. Although face prior-based methods and reference-based methods have recently demonstrated high-quality results, the restored images tend to contain over-smoothed results and lose identity-preserved details when the degradation is severe. It is observed that this is attributed to short-range dependencies, the intrinsic limitation of convolutional neural networks. To model long-range dependencies, we propose a Transformer-based blind face restoration method, named BFRFormer, to reconstruct images with more identity-preserved details in an end-to-end manner. In BFRFormer, to remove blocking artifacts, the wavelet discriminator and aggregated attention module are developed, and spectral normalization and balanced consistency regulation are adaptively applied to address the training instability and over-fitting problem, respectively. Extensive experiments show that our method outperforms state-of-the-art methods on a synthetic dataset and four real-world datasets. The source code, Casia-Test dataset, and pre-trained models are released at this https URL.\n    "
    },
    "2402.18815": {
        "title": "How do Large Language Models Handle Multilingualism?",
        "authors": [
            "Yiran Zhao",
            "Wenxuan Zhang",
            "Guizhen Chen",
            "Kenji Kawaguchi",
            "Lidong Bing"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Large language models (LLMs) demonstrate remarkable performance across a spectrum of languages. In this work, we delve into the question: How do LLMs handle multilingualism? We introduce a framework that depicts LLMs' processing of multilingual inputs: In the first several layers, LLMs understand the question, converting multilingual inputs into English to facilitate the task-solving phase. In the intermediate layers, LLMs engage in problem-solving by thinking in English and incorporating multilingual knowledge to obtain factual content, leveraging the self-attention and feed-forward structures, respectively. In the last several layers, LLMs generate responses that align with the original language of the query. In addition, we investigate the existence of language-specific neurons when processing a certain language. To detect neurons activated by the input language, even without labels, we innovatively design a Parallel Language specific Neuron Detection ($\\texttt{PLND}$) method that effectively measures the significance of neurons when handling multilingual inputs. By comprehensive ablation analysis through deactivating neurons of different layers and structures, we verify the framework that we propose. Additionally, we demonstrate that we can utilize such a framework to effectively enhance the multilingual ability with much less training effort.\n    "
    },
    "2402.18819": {
        "title": "Dual Operating Modes of In-Context Learning",
        "authors": [
            "Ziqian Lin",
            "Kangwook Lee"
        ],
        "comments": "53 pages, 20 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In-context learning (ICL) exhibits dual operating modes: task learning, i.e., acquiring a new skill from in-context samples, and task retrieval, i.e., locating and activating a relevant pretrained skill. Recent theoretical work investigates various mathematical models to analyze ICL, but existing models explain only one operating mode at a time. We introduce a probabilistic model, with which one can explain the dual operating modes of ICL simultaneously. Focusing on in-context learning of linear functions, we extend existing models for pretraining data by introducing multiple task groups and task-dependent input distributions. We then analyze the behavior of the optimally pretrained model under the squared loss, i.e., the MMSE estimator of the label given in-context examples. Regarding pretraining task distribution as prior and in-context examples as the observation, we derive the closed-form expression of the task posterior distribution. With the closed-form expression, we obtain a quantitative understanding of the two operating modes of ICL. Furthermore, we shed light on an unexplained phenomenon observed in practice: under certain settings, the ICL risk initially increases and then decreases with more in-context examples. Our model offers a plausible explanation for this \"early ascent\" phenomenon: a limited number of in-context samples may lead to the retrieval of an incorrect skill, thereby increasing the risk, which will eventually diminish as task learning takes effect with more in-context samples. We also theoretically analyze ICL with biased labels, e.g., zero-shot ICL, where in-context examples are assigned random labels. Lastly, we validate our findings and predictions via experiments involving Transformers and large language models.\n    "
    },
    "2402.18821": {
        "title": "Debiased Novel Category Discovering and Localization",
        "authors": [
            "Juexiao Feng",
            "Yuhong Yang",
            "Yanchun Xie",
            "Yaqian Li",
            "Yandong Guo",
            "Yuchen Guo",
            "Yuwei He",
            "Liuyu Xiang",
            "Guiguang Ding"
        ],
        "comments": "Accepted by AAAI 2024",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In recent years, object detection in deep learning has experienced rapid development. However, most existing object detection models perform well only on closed-set datasets, ignoring a large number of potential objects whose categories are not defined in the training set. These objects are often identified as background or incorrectly classified as pre-defined categories by the detectors. In this paper, we focus on the challenging problem of Novel Class Discovery and Localization (NCDL), aiming to train detectors that can detect the categories present in the training data, while also actively discover, localize, and cluster new categories. We analyze existing NCDL methods and identify the core issue: object detectors tend to be biased towards seen objects, and this leads to the neglect of unseen targets. To address this issue, we first propose an Debiased Region Mining (DRM) approach that combines class-agnostic Region Proposal Network (RPN) and class-aware RPN in a complementary manner. Additionally, we suggest to improve the representation network through semi-supervised contrastive learning by leveraging unlabeled data. Finally, we adopt a simple and efficient mini-batch K-means clustering method for novel class discovery. We conduct extensive experiments on the NCDL benchmark, and the results demonstrate that the proposed DRM approach significantly outperforms previous methods, establishing a new state-of-the-art.\n    "
    },
    "2402.18824": {
        "title": "Batch size invariant Adam",
        "authors": [
            "Xi Wang",
            "Laurence Aitchison"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We propose a batch size invariant version of Adam, for use in large-scale, distributed settings, in which the mini-batch is divided into micro-batches which are distributed among worker nodes. For the v term, standard Adam first computes the average over micro-batch gradients, then squares, while in the batch size invariant Adam proposed here, we first square the micro-batch gradients, then average. Previous work (e.g. Malladi et al. 2022) used an alternative approach that involved a square-root scaling of the learning rate, but this approach requires strong assumptions to work; in particular that the gradient variance dominates the square of the expected gradient. In contrast, the approach proposed here gives batch size invariance without this assumption. We confirm that in practice our scheme gives batch size invariance in a much larger range of scenarios than the previous approach.\n    "
    },
    "2402.18835": {
        "title": "Envisioning the Applications and Implications of Generative AI for News Media",
        "authors": [
            "Sachita Nishal",
            "Nicholas Diakopoulos"
        ],
        "comments": "Accepted to CHI 2023 Workshop on Generative AI and HCI; 8 pages",
        "subjects": "Computers and Society (cs.CY)",
        "abstract": "This article considers the increasing use of algorithmic decision-support systems and synthetic media in the newsroom, and explores how generative models can help reporters and editors across a range of tasks from the conception of a news story to its distribution. Specifically, we draw from a taxonomy of tasks associated with news production, and discuss where generative models could appropriately support reporters, the journalistic and ethical values that must be preserved within these interactions, and the resulting implications for design contributions in this area in the future. Our essay is relevant to practitioners and researchers as they consider using generative AI systems to support different tasks and workflows.\n    "
    },
    "2402.18836": {
        "title": "A Model-Based Approach for Improving Reinforcement Learning Efficiency Leveraging Expert Observations",
        "authors": [
            "Erhan Can Ozcan",
            "Vittorio Giammarino",
            "James Queeney",
            "Ioannis Ch. Paschalidis"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This paper investigates how to incorporate expert observations (without explicit information on expert actions) into a deep reinforcement learning setting to improve sample efficiency. First, we formulate an augmented policy loss combining a maximum entropy reinforcement learning objective with a behavioral cloning loss that leverages a forward dynamics model. Then, we propose an algorithm that automatically adjusts the weights of each component in the augmented loss function. Experiments on a variety of continuous control tasks demonstrate that the proposed algorithm outperforms various benchmarks by effectively utilizing available expert observations.\n    "
    },
    "2402.18842": {
        "title": "ViewFusion: Towards Multi-View Consistency via Interpolated Denoising",
        "authors": [
            "Xianghui Yang",
            "Yan Zuo",
            "Sameera Ramasinghe",
            "Loris Bazzani",
            "Gil Avraham",
            "Anton van den Hengel"
        ],
        "comments": "CVPR2024,homepage:this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Novel-view synthesis through diffusion models has demonstrated remarkable potential for generating diverse and high-quality images. Yet, the independent process of image generation in these prevailing methods leads to challenges in maintaining multiple-view consistency. To address this, we introduce ViewFusion, a novel, training-free algorithm that can be seamlessly integrated into existing pre-trained diffusion models. Our approach adopts an auto-regressive method that implicitly leverages previously generated views as context for the next view generation, ensuring robust multi-view consistency during the novel-view generation process. Through a diffusion process that fuses known-view information via interpolated denoising, our framework successfully extends single-view conditioned models to work in multiple-view conditional settings without any additional fine-tuning. Extensive experimental results demonstrate the effectiveness of ViewFusion in generating consistent and detailed novel views.\n    "
    },
    "2402.18846": {
        "title": "Multi-Fidelity Residual Neural Processes for Scalable Surrogate Modeling",
        "authors": [
            "Ruijia Niu",
            "Dongxia Wu",
            "Kai Kim",
            "Yi-An Ma",
            "Duncan Watson-Parris",
            "Rose Yu"
        ],
        "comments": "A novel probabilistic inference approach for scalable multi-fidelity surrogate modeling",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Multi-fidelity surrogate modeling aims to learn an accurate surrogate at the highest fidelity level by combining data from multiple sources. Traditional methods relying on Gaussian processes can hardly scale to high-dimensional data. Deep learning approaches utilize neural network based encoders and decoders to improve scalability. These approaches share encoded representations across fidelities without including corresponding decoder parameters. At the highest fidelity, the representations are decoded with different parameters, making the shared information inherently inaccurate. This hinders inference performance, especially in out-of-distribution scenarios when the highest fidelity data has limited domain coverage. To address these limitations, we propose Multi-fidelity Residual Neural Processes (MFRNP), a novel multi-fidelity surrogate modeling framework. MFRNP optimizes lower fidelity decoders for accurate information sharing by aggregating lower fidelity surrogate outputs and models residual between the aggregation and ground truth on the highest fidelity. We show that MFRNP significantly outperforms current state-of-the-art in learning partial differential equations and a real-world climate modeling task.\n    "
    },
    "2402.18848": {
        "title": "SwitchLight: Co-design of Physics-driven Architecture and Pre-training Framework for Human Portrait Relighting",
        "authors": [
            "Hoon Kim",
            "Minje Jang",
            "Wonjun Yoon",
            "Jisoo Lee",
            "Donghyun Na",
            "Sanghyun Woo"
        ],
        "comments": "CVPR2024. Live demos available at this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We introduce a co-designed approach for human portrait relighting that combines a physics-guided architecture with a pre-training framework. Drawing on the Cook-Torrance reflectance model, we have meticulously configured the architecture design to precisely simulate light-surface interactions. Furthermore, to overcome the limitation of scarce high-quality lightstage data, we have developed a self-supervised pre-training strategy. This novel combination of accurate physical modeling and expanded training dataset establishes a new benchmark in relighting realism.\n    "
    },
    "2402.18849": {
        "title": "Enhancing Steganographic Text Extraction: Evaluating the Impact of NLP Models on Accuracy and Semantic Coherence",
        "authors": [
            "Mingyang Li",
            "Maoqin Yuan",
            "Luyao Li",
            "Han Pengsihua"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This study discusses a new method combining image steganography technology with Natural Language Processing (NLP) large models, aimed at improving the accuracy and robustness of extracting steganographic text. Traditional Least Significant Bit (LSB) steganography techniques face challenges in accuracy and robustness of information extraction when dealing with complex character encoding, such as Chinese characters. To address this issue, this study proposes an innovative LSB-NLP hybrid framework. This framework integrates the advanced capabilities of NLP large models, such as error detection, correction, and semantic consistency analysis, as well as information reconstruction techniques, thereby significantly enhancing the robustness of steganographic text extraction. Experimental results show that the LSB-NLP hybrid framework excels in improving the extraction accuracy of steganographic text, especially in handling Chinese characters. The findings of this study not only confirm the effectiveness of combining image steganography technology and NLP large models but also propose new ideas for research and application in the field of information hiding. The successful implementation of this interdisciplinary approach demonstrates the great potential of integrating image steganography technology with natural language processing technology in solving complex information processing problems.\n    "
    },
    "2402.18853": {
        "title": "Rethinking Multi-domain Generalization with A General Learning Objective",
        "authors": [
            "Zhaorui Tan",
            "Xi Yang",
            "Kaizhu Huang"
        ],
        "comments": "Accepted by CVPR24",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Multi-domain generalization (mDG) is universally aimed to minimize the discrepancy between training and testing distributions to enhance marginal-to-label distribution mapping. However, existing mDG literature lacks a general learning objective paradigm and often imposes constraints on static target marginal distributions. In this paper, we propose to leverage a $Y$-mapping to relax the constraint. We rethink the learning objective for mDG and design a new \\textbf{general learning objective} to interpret and analyze most existing mDG wisdom. This general objective is bifurcated into two synergistic amis: learning domain-independent conditional features and maximizing a posterior. Explorations also extend to two effective regularization terms that incorporate prior information and suppress invalid causality, alleviating the issues that come with relaxed constraints. We theoretically contribute an upper bound for the domain alignment of domain-independent conditional features, disclosing that many previous mDG endeavors actually \\textbf{optimize partially the objective} and thus lead to limited performance. As such, our study distills a general learning objective into four practical components, providing a general, robust, and flexible mechanism to handle complex domain shifts. Extensive empirical results indicate that the proposed objective with $Y$-mapping leads to substantially better mDG performance in various downstream tasks, including regression, segmentation, and classification.\n    "
    },
    "2402.18865": {
        "title": "Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning",
        "authors": [
            "Weijieying Ren",
            "Xinlong Li",
            "Lei Wang",
            "Tianxiang Zhao",
            "Wei Qin"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Existing research has shown that large language models (LLMs) exhibit remarkable performance in language understanding and generation. However, when LLMs are continuously fine-tuned on complex and diverse domain-specific downstream tasks, the inference performance on historical tasks decreases dramatically, which is known as a catastrophic forgetting problem. A trade-off needs to be kept between learning plasticity and memory stability. Plenty of existing works have explored strategies like memory replay, regularization and parameter isolation, but little is known about the geometric connection of various adjacent minima in the continual LLMs fine-tuning scenarios. In this work, we investigate the geometric connections of different minima through the lens of mode connectivity, which means different minima can be connected by a low-loss valley. Through extensive experiments, we uncover the mode connectivity phenomenon in the LLMs continual learning scenario and find that it can strike a balance between plasticity and stability. Building upon these findings, we propose a simple yet effective method called Interpolation-based LoRA (I-LoRA), which constructs a dual-memory experience replay framework based on LoRA parameter interpolations. Extensive experiments and analysis on eight domain-specific CL benchmarks demonstrate that I-LoRA consistently show significant improvement over the previous state-of-the-art approaches with up to $11\\%$ performance gains, providing a strong baseline and insights for future research on the large language model continual learning problem. Our code is available at \\url{this https URL}.\n    "
    },
    "2402.18866": {
        "title": "Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming",
        "authors": [
            "Hany Hamed",
            "Subin Kim",
            "Dongyeong Kim",
            "Jaesik Yoon",
            "Sungjin Ahn"
        ],
        "comments": "First two authors contributed equally",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Model-based reinforcement learning (MBRL) has been a primary approach to ameliorating the sample efficiency issue as well as to make a generalist agent. However, there has not been much effort toward enhancing the strategy of dreaming itself. Therefore, it is a question whether and how an agent can \"dream better\" in a more structured and strategic way. In this paper, inspired by the observation from cognitive science suggesting that humans use a spatial divide-and-conquer strategy in planning, we propose a new MBRL agent, called Dr. Strategy, which is equipped with a novel Dreaming Strategy. The proposed agent realizes a version of divide-and-conquer-like strategy in dreaming. This is achieved by learning a set of latent landmarks and then utilizing these to learn a landmark-conditioned highway policy. With the highway policy, the agent can first learn in the dream to move to a landmark, and from there it tackles the exploration and achievement task in a more focused way. In experiments, we show that the proposed model outperforms prior pixel-based MBRL methods in various visually complex and partially observable navigation tasks. The source code will be available at this https URL\n"
    },
    "2402.18873": {
        "title": "Reducing Hallucinations in Entity Abstract Summarization with Facts-Template Decomposition",
        "authors": [
            "Fangwei Zhu",
            "Peiyi Wang",
            "Zhifang Sui"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Entity abstract summarization aims to generate a coherent description of a given entity based on a set of relevant Internet documents. Pretrained language models (PLMs) have achieved significant success in this task, but they may suffer from hallucinations, i.e. generating non-factual information about the entity. To address this issue, we decompose the summary into two components: Facts that represent the factual information about the given entity, which PLMs are prone to fabricate; and Template that comprises generic content with designated slots for facts, which PLMs can generate competently. Based on the facts-template decomposition, we propose SlotSum, an explainable framework for entity abstract summarization. SlotSum first creates the template and then predicts the fact for each template slot based on the input documents. Benefiting from our facts-template decomposition, SlotSum can easily locate errors and further rectify hallucinated predictions with external knowledge. We construct a new dataset WikiFactSum to evaluate the performance of SlotSum. Experimental results demonstrate that SlotSum could generate summaries that are significantly more factual with credible external knowledge.\n    "
    },
    "2402.18875": {
        "title": "Loss-aware Curriculum Learning for Heterogeneous Graph Neural Networks",
        "authors": [
            "Zhen Hao Wong",
            "Hansi Yang",
            "Xiaoyi Fu",
            "Quanming Yao"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Heterogeneous Graph Neural Networks (HGNNs) are a class of deep learning models designed specifically for heterogeneous graphs, which are graphs that contain different types of nodes and edges. This paper investigates the application of curriculum learning techniques to improve the performance and robustness of Heterogeneous Graph Neural Networks (GNNs). To better classify the quality of the data, we design a loss-aware training schedule, named LTS that measures the quality of every nodes of the data and incorporate the training dataset into the model in a progressive manner that increases difficulty step by step. LTS can be seamlessly integrated into various frameworks, effectively reducing bias and variance, mitigating the impact of noisy data, and enhancing overall accuracy. Our findings demonstrate the efficacy of curriculum learning in enhancing HGNNs capabilities for analyzing complex graph-structured data. The code is public at https: //github.com/LARS-research/CLGNN/.\n    "
    },
    "2402.18877": {
        "title": "Principal Component Analysis as a Sanity Check for Bayesian Phylolinguistic Reconstruction",
        "authors": [
            "Yugo Murawaki"
        ],
        "comments": "Accepted at LREC-COLING 2024",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Bayesian approaches to reconstructing the evolutionary history of languages rely on the tree model, which assumes that these languages descended from a common ancestor and underwent modifications over time. However, this assumption can be violated to different extents due to contact and other factors. Understanding the degree to which this assumption is violated is crucial for validating the accuracy of phylolinguistic inference. In this paper, we propose a simple sanity check: projecting a reconstructed tree onto a space generated by principal component analysis. By using both synthetic and real data, we demonstrate that our method effectively visualizes anomalies, particularly in the form of jogging.\n    "
    },
    "2402.18879": {
        "title": "Dose Prediction Driven Radiotherapy Paramters Regression via Intra- and Inter-Relation Modeling",
        "authors": [
            "Jiaqi Cui",
            "Yuanyuan Xu",
            "Jianghong Xiao",
            "Yuchen Fei",
            "Jiliu Zhou",
            "Xingcheng Peng",
            "Yan Wang"
        ],
        "comments": "Accepted by ISBI 2024",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Deep learning has facilitated the automation of radiotherapy by predicting accurate dose distribution maps. However, existing methods fail to derive the desirable radiotherapy parameters that can be directly input into the treatment planning system (TPS), impeding the full automation of radiotherapy. To enable more thorough automatic radiotherapy, in this paper, we propose a novel two-stage framework to directly regress the radiotherapy parameters, including a dose map prediction stage and a radiotherapy parameters regression stage. In stage one, we combine transformer and convolutional neural network (CNN) to predict realistic dose maps with rich global and local information, providing accurate dosimetric knowledge for the subsequent parameters regression. In stage two, two elaborate modules, i.e., an intra-relation modeling (Intra-RM) module and an inter-relation modeling (Inter-RM) module, are designed to exploit the organ-specific and organ-shared features for precise parameters regression. Experimental results on a rectal cancer dataset demonstrate the effectiveness of our method.\n    "
    },
    "2402.18892": {
        "title": "Aligning Knowledge Graph with Visual Perception for Object-goal Navigation",
        "authors": [
            "Nuo Xu",
            "Wen Wang",
            "Rong Yang",
            "Mengjie Qin",
            "Zheyuan Lin",
            "Wei Song",
            "Chunlong Zhang",
            "Jason Gu",
            "Chao Li"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Object-goal navigation is a challenging task that requires guiding an agent to specific objects based on first-person visual observations. The ability of agent to comprehend its surroundings plays a crucial role in achieving successful object finding. However, existing knowledge-graph-based navigators often rely on discrete categorical one-hot vectors and vote counting strategy to construct graph representation of the scenes, which results in misalignment with visual images. To provide more accurate and coherent scene descriptions and address this misalignment issue, we propose the Aligning Knowledge Graph with Visual Perception (AKGVP) method for object-goal navigation. Technically, our approach introduces continuous modeling of the hierarchical scene architecture and leverages visual-language pre-training to align natural language description with visual perception. The integration of a continuous knowledge graph architecture and multimodal feature alignment empowers the navigator with a remarkable zero-shot navigation capability. We extensively evaluate our method using the AI2-THOR simulator and conduct a series of experiments to demonstrate the effectiveness and efficiency of our navigator. Code available: this https URL.\n    "
    },
    "2402.18899": {
        "title": "Aligning Language Models for Versatile Text-based Item Retrieval",
        "authors": [
            "Yuxuan Lei",
            "Jianxun Lian",
            "Jing Yao",
            "Mingqi Wu",
            "Defu Lian",
            "Xing Xie"
        ],
        "comments": "4 pages,1 figures, 4 tables",
        "subjects": "Information Retrieval (cs.IR)",
        "abstract": "This paper addresses the gap between general-purpose text embeddings and the specific demands of item retrieval tasks. We demonstrate the shortcomings of existing models in capturing the nuances necessary for zero-shot performance on item retrieval tasks. To overcome these limitations, we propose generate in-domain dataset from ten tasks tailored to unlocking models' representation ability for item retrieval. Our empirical studies demonstrate that fine-tuning embedding models on the dataset leads to remarkable improvements in a variety of retrieval tasks. We also illustrate the practical application of our refined model in a conversational setting, where it enhances the capabilities of LLM-based Recommender Agents like Chat-Rec. Our code is available at this https URL.\n    "
    },
    "2402.18909": {
        "title": "Updating Language Models with Unstructured Facts: Towards Practical Knowledge Editing",
        "authors": [
            "Xiaobao Wu",
            "Liangming Pan",
            "William Yang Wang",
            "Anh Tuan Luu"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Knowledge editing aims to inject knowledge updates into language models to keep them correct and up-to-date. However, its current evaluation strategies are notably impractical: they solely update with well-curated structured facts (triplets with subjects, relations, and objects), whereas real-world knowledge updates commonly emerge in unstructured texts like news articles. In this paper, we propose a new benchmark, Unstructured Knowledge Editing (UKE). It evaluates editing performance directly using unstructured texts as knowledge updates, termed unstructured facts. Hence UKE avoids the laborious construction of structured facts and enables efficient and responsive knowledge editing, becoming a more practical benchmark. We conduct extensive experiments on newly built datasets and demonstrate that UKE poses a significant challenge to state-of-the-art knowledge editing methods, resulting in their critical performance declines. We further show that this challenge persists even if we extract triplets as structured facts. Our analysis discloses key insights to motivate future research in UKE for more practical knowledge editing.\n    "
    },
    "2402.18913": {
        "title": "AdaMergeX: Cross-Lingual Transfer with Large Language Models via Adaptive Adapter Merging",
        "authors": [
            "Yiran Zhao",
            "Wenxuan Zhang",
            "Huiming Wang",
            "Kenji Kawaguchi",
            "Lidong Bing"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "As an effective alternative to the direct fine-tuning on target tasks in specific languages, cross-lingual transfer addresses the challenges of limited training data by decoupling ''task ability'' and ''language ability'' by fine-tuning on the target task in the source language and another selected task in the target language, respectively. However, they fail to fully separate the task ability from the source language or the language ability from the chosen task. In this paper, we acknowledge the mutual reliance between task ability and language ability and direct our attention toward the gap between the target language and the source language on tasks. As the gap removes the impact of tasks, we assume that it remains consistent across tasks. Based on this assumption, we propose a new cross-lingual transfer method called $\\texttt{AdaMergeX}$ that utilizes adaptive adapter merging. By introducing a reference task, we can determine that the divergence of adapters fine-tuned on the reference task in both languages follows the same distribution as the divergence of adapters fine-tuned on the target task in both languages. Hence, we can obtain target adapters by combining the other three adapters. Furthermore, we propose a structure-adaptive adapter merging method. Our empirical results demonstrate that our approach yields new and effective cross-lingual transfer, outperforming existing methods across all settings.\n    "
    },
    "2402.18917": {
        "title": "Stop Relying on No-Choice and Do not Repeat the Moves: Optimal, Efficient and Practical Algorithms for Assortment Optimization",
        "authors": [
            "Aadirupa Saha",
            "Pierre Gaillard"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We address the problem of active online assortment optimization problem with preference feedback, which is a framework for modeling user choices and subsetwise utility maximization. The framework is useful in various real-world applications including ad placement, online retail, recommender systems, fine-tuning language models, amongst many. The problem, although has been studied in the past, lacks an intuitive and practical solution approach with simultaneously efficient algorithm and optimal regret guarantee. E.g., popularly used assortment selection algorithms often require the presence of a `strong reference' which is always included in the choice sets, further they are also designed to offer the same assortments repeatedly until the reference item gets selected -- all such requirements are quite unrealistic for practical applications. In this paper, we designed efficient algorithms for the problem of regret minimization in assortment selection with \\emph{Plackett Luce} (PL) based user choices. We designed a novel concentration guarantee for estimating the score parameters of the PL model using `\\emph{Pairwise Rank-Breaking}', which builds the foundation of our proposed algorithms. Moreover, our methods are practical, provably optimal, and devoid of the aforementioned limitations of the existing methods. Empirical evaluations corroborate our findings and outperform the existing baselines.\n    "
    },
    "2402.18922": {
        "title": "A Simple yet Effective Network based on Vision Transformer for Camouflaged Object and Salient Object Detection",
        "authors": [
            "Chao Hao",
            "Zitong Yu",
            "Xin Liu",
            "Jun Xu",
            "Huanjing Yue",
            "Jingyu Yang"
        ],
        "comments": "submitted to IEEE TIP",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Camouflaged object detection (COD) and salient object detection (SOD) are two distinct yet closely-related computer vision tasks widely studied during the past decades. Though sharing the same purpose of segmenting an image into binary foreground and background regions, their distinction lies in the fact that COD focuses on concealed objects hidden in the image, while SOD concentrates on the most prominent objects in the image. Previous works achieved good performance by stacking various hand-designed modules and multi-scale features. However, these carefully-designed complex networks often performed well on one task but not on another. In this work, we propose a simple yet effective network (SENet) based on vision Transformer (ViT), by employing a simple design of an asymmetric ViT-based encoder-decoder structure, we yield competitive results on both tasks, exhibiting greater versatility than meticulously crafted ones. Furthermore, to enhance the Transformer's ability to model local information, which is important for pixel-level binary segmentation tasks, we propose a local information capture module (LICM). We also propose a dynamic weighted loss (DW loss) based on Binary Cross-Entropy (BCE) and Intersection over Union (IoU) loss, which guides the network to pay more attention to those smaller and more difficult-to-find target objects according to their size. Moreover, we explore the issue of joint training of SOD and COD, and propose a preliminary solution to the conflict in joint training, further improving the performance of SOD. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our method. The code is available at this https URL.\n    "
    },
    "2402.18925": {
        "title": "PCDepth: Pattern-based Complementary Learning for Monocular Depth Estimation by Best of Both Worlds",
        "authors": [
            "Haotian Liu",
            "Sanqing Qu",
            "Fan Lu",
            "Zongtao Bu",
            "Florian Roehrbein",
            "Alois Knoll",
            "Guang Chen"
        ],
        "comments": "Under Review",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Event cameras can record scene dynamics with high temporal resolution, providing rich scene details for monocular depth estimation (MDE) even at low-level illumination. Therefore, existing complementary learning approaches for MDE fuse intensity information from images and scene details from event data for better scene understanding. However, most methods directly fuse two modalities at pixel level, ignoring that the attractive complementarity mainly impacts high-level patterns that only occupy a few pixels. For example, event data is likely to complement contours of scene objects. In this paper, we discretize the scene into a set of high-level patterns to explore the complementarity and propose a Pattern-based Complementary learning architecture for monocular Depth estimation (PCDepth). Concretely, PCDepth comprises two primary components: a complementary visual representation learning module for discretizing the scene into high-level patterns and integrating complementary patterns across modalities and a refined depth estimator aimed at scene reconstruction and depth prediction while maintaining an efficiency-accuracy balance. Through pattern-based complementary learning, PCDepth fully exploits two modalities and achieves more accurate predictions than existing methods, especially in challenging nighttime scenarios. Extensive experiments on MVSEC and DSEC datasets verify the effectiveness and superiority of our PCDepth. Remarkably, compared with state-of-the-art, PCDepth achieves a 37.9% improvement in accuracy in MVSEC nighttime scenarios.\n    "
    },
    "2402.18944": {
        "title": "SemEval 2024 -- Task 10: Emotion Discovery and Reasoning its Flip in Conversation (EDiReF)",
        "authors": [
            "Shivani Kumar",
            "Md Shad Akhtar",
            "Erik Cambria",
            "Tanmoy Chakraborty"
        ],
        "comments": "11 pages, 3 figures, 7 tables",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "We present SemEval-2024 Task 10, a shared task centred on identifying emotions and finding the rationale behind their flips within monolingual English and Hindi-English code-mixed dialogues. This task comprises three distinct subtasks - emotion recognition in conversation for code-mixed dialogues, emotion flip reasoning for code-mixed dialogues, and emotion flip reasoning for English dialogues. Participating systems were tasked to automatically execute one or more of these subtasks. The datasets for these tasks comprise manually annotated conversations focusing on emotions and triggers for emotion shifts (The task data is available at this https URL). A total of 84 participants engaged in this task, with the most adept systems attaining F1-scores of 0.70, 0.79, and 0.76 for the respective subtasks. This paper summarises the results and findings from 24 teams alongside their system descriptions.\n    "
    },
    "2402.18949": {
        "title": "Improving Group Connectivity for Generalization of Federated Deep Learning",
        "authors": [
            "Zexi Li",
            "Jie Lin",
            "Zhiqi Li",
            "Didi Zhu",
            "Chao Wu"
        ],
        "comments": "Preprint",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Federated learning (FL) involves multiple heterogeneous clients collaboratively training a global model via iterative local updates and model fusion. The generalization of FL's global model has a large gap compared with centralized training, which is its bottleneck for broader applications. In this paper, we study and improve FL's generalization through a fundamental ``connectivity'' perspective, which means how the local models are connected in the parameter region and fused into a generalized global model. The term ``connectivity'' is derived from linear mode connectivity (LMC), studying the interpolated loss landscape of two different solutions (e.g., modes) of neural networks. Bridging the gap between LMC and FL, in this paper, we leverage fixed anchor models to empirically and theoretically study the transitivity property of connectivity from two models (LMC) to a group of models (model fusion in FL). Based on the findings, we propose FedGuCci and FedGuCci+, improving group connectivity for better generalization. It is shown that our methods can boost the generalization of FL under client heterogeneity across various tasks (4 CV datasets and 6 NLP datasets), models (both convolutional and transformer-based), and training paradigms (both from-scratch and pretrain-finetune).\n    "
    },
    "2402.18950": {
        "title": "PopALM: Popularity-Aligned Language Models for Social Media Trendy Response Prediction",
        "authors": [
            "Erxin Yu",
            "Jing Li",
            "Chunpu Xu"
        ],
        "comments": "Accepted by COLING 2024",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Social media platforms are daily exhibiting millions of events. To preliminarily predict the mainstream public reaction to these events, we study trendy response prediction to automatically generate top-liked user replies to social media events. While previous works focus on generating responses without factoring in popularity, we propose Popularity-Aligned Language Models (PopALM) to distinguish responses liked by a larger audience through reinforcement learning. Recognizing the noisy labels from user \"likes\", we tailor-make curriculum learning in proximal policy optimization (PPO) to help models capture the essential samples for easy-to-hard training. In experiments, we build a large-scale Weibo dataset for trendy response prediction, and its results show that PopALM can help boost the performance of advanced language models.\n    "
    },
    "2402.18951": {
        "title": "Percept, Chat, and then Adapt: Multimodal Knowledge Transfer of Foundation Models for Open-World Video Recognition",
        "authors": [
            "Boyu Chen",
            "Siran Chen",
            "Kunchang Li",
            "Qinglin Xu",
            "Yu Qiao",
            "Yali Wang"
        ],
        "comments": "35 pages, 6 figures, 8 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Open-world video recognition is challenging since traditional networks are not generalized well on complex environment variations. Alternatively, foundation models with rich knowledge have recently shown their generalization power. However, how to apply such knowledge has not been fully explored for open-world video recognition. To this end, we propose a generic knowledge transfer pipeline, which progressively exploits and integrates external multimodal knowledge from foundation models to boost open-world video recognition. We name it PCA, based on three stages of Percept, Chat, and Adapt. First, we perform Percept process to reduce the video domain gap and obtain external visual knowledge. Second, we generate rich linguistic semantics as external textual knowledge in Chat stage. Finally, we blend external multimodal knowledge in Adapt stage, by inserting multimodal knowledge adaptation modules into networks. We conduct extensive experiments on three challenging open-world video benchmarks, i.e., TinyVIRAT, ARID, and QV-Pipe. Our approach achieves state-of-the-art performance on all three datasets.\n    "
    },
    "2402.18958": {
        "title": "Boosting Semi-Supervised Object Detection in Remote Sensing Images With Active Teaching",
        "authors": [
            "Boxuan Zhang",
            "Zengmao Wang",
            "Bo Du"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The lack of object-level annotations poses a significant challenge for object detection in remote sensing images (RSIs). To address this issue, active learning (AL) and semi-supervised learning (SSL) techniques have been proposed to enhance the quality and quantity of annotations. AL focuses on selecting the most informative samples for annotation, while SSL leverages the knowledge from unlabeled samples. In this letter, we propose a novel AL method to boost semi-supervised object detection (SSOD) for remote sensing images with a teacher student network, called SSOD-AT. The proposed method incorporates an RoI comparison module (RoICM) to generate high-confidence pseudo-labels for regions of interest (RoIs). Meanwhile, the RoICM is utilized to identify the top-K uncertain images. To reduce redundancy in the top-K uncertain images for human labeling, a diversity criterion is introduced based on object-level prototypes of different categories using both labeled and pseudo-labeled images. Extensive experiments on DOTA and DIOR, two popular datasets, demonstrate that our proposed method outperforms state-of-the-art methods for object detection in RSIs. Compared with the best performance in the SOTA methods, the proposed method achieves 1 percent improvement in most cases in the whole AL.\n    "
    },
    "2402.18960": {
        "title": "Towards Out-of-Distribution Detection for breast cancer classification in Point-of-Care Ultrasound Imaging",
        "authors": [
            "Jennie Karlsson",
            "Marisa Wodrich",
            "Niels Christian Overgaard",
            "Freja Sahlin",
            "Kristina L\u00e5ng",
            "Anders Heyden",
            "Ida Arvidsson"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Deep learning has shown to have great potential in medical applications. In critical domains as such, it is of high interest to have trustworthy algorithms which are able to tell when reliable assessments cannot be guaranteed. Detecting out-of-distribution (OOD) samples is a crucial step towards building a safe classifier. Following a previous study, showing that it is possible to classify breast cancer in point-of-care ultrasound images, this study investigates OOD detection using three different methods: softmax, energy score and deep ensembles. All methods are tested on three different OOD data sets. The results show that the energy score method outperforms the softmax method, performing well on two of the data sets. The ensemble method is the most robust, performing the best at detecting OOD samples for all three OOD data sets.\n    "
    },
    "2402.18969": {
        "title": "OHTA: One-shot Hand Avatar via Data-driven Implicit Priors",
        "authors": [
            "Xiaozheng Zheng",
            "Chao Wen",
            "Zhuo Su",
            "Zeran Xu",
            "Zhaohu Li",
            "Yang Zhao",
            "Zhou Xue"
        ],
        "comments": "Accepted to CVPR 2024. Project page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In this paper, we delve into the creation of one-shot hand avatars, attaining high-fidelity and drivable hand representations swiftly from a single image. With the burgeoning domains of the digital human, the need for quick and personalized hand avatar creation has become increasingly critical. Existing techniques typically require extensive input data and may prove cumbersome or even impractical in certain scenarios. To enhance accessibility, we present a novel method OHTA (One-shot Hand avaTAr) that enables the creation of detailed hand avatars from merely one image. OHTA tackles the inherent difficulties of this data-limited problem by learning and utilizing data-driven hand priors. Specifically, we design a hand prior model initially employed for 1) learning various hand priors with available data and subsequently for 2) the inversion and fitting of the target identity with prior knowledge. OHTA demonstrates the capability to create high-fidelity hand avatars with consistent animatable quality, solely relying on a single image. Furthermore, we illustrate the versatility of OHTA through diverse applications, encompassing text-to-avatar conversion, hand editing, and identity latent space manipulation.\n    "
    },
    "2402.18970": {
        "title": "PrivatEyes: Appearance-based Gaze Estimation Using Federated Secure Multi-Party Computation",
        "authors": [
            "Mayar Elfares",
            "Pascal Reisert",
            "Zhiming Hu",
            "Wenwu Tang",
            "Ralf K\u00fcsters",
            "Andreas Bulling"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Latest gaze estimation methods require large-scale training data but their collection and exchange pose significant privacy risks. We propose PrivatEyes - the first privacy-enhancing training approach for appearance-based gaze estimation based on federated learning (FL) and secure multi-party computation (MPC). PrivatEyes enables training gaze estimators on multiple local datasets across different users and server-based secure aggregation of the individual estimators' updates. PrivatEyes guarantees that individual gaze data remains private even if a majority of the aggregating servers is malicious. We also introduce a new data leakage attack DualView that shows that PrivatEyes limits the leakage of private training data more effectively than previous approaches. Evaluations on the MPIIGaze, MPIIFaceGaze, GazeCapture, and NVGaze datasets further show that the improved privacy does not lead to a lower gaze estimation accuracy or substantially higher computational costs - both of which are on par with its non-secure counterparts.\n    "
    },
    "2402.18974": {
        "title": "Graph Generation via Spectral Diffusion",
        "authors": [
            "Giorgia Minello",
            "Alessandro Bicciato",
            "Luca Rossi",
            "Andrea Torsello",
            "Luca Cosmo"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In this paper, we present GRASP, a novel graph generative model based on 1) the spectral decomposition of the graph Laplacian matrix and 2) a diffusion process. Specifically, we propose to use a denoising model to sample eigenvectors and eigenvalues from which we can reconstruct the graph Laplacian and adjacency matrix. Our permutation invariant model can also handle node features by concatenating them to the eigenvectors of each node. Using the Laplacian spectrum allows us to naturally capture the structural characteristics of the graph and work directly in the node space while avoiding the quadratic complexity bottleneck that limits the applicability of other methods. This is achieved by truncating the spectrum, which as we show in our experiments results in a faster yet accurate generative process. An extensive set of experiments on both synthetic and real world graphs demonstrates the strengths of our model against state-of-the-art alternatives.\n    "
    },
    "2402.18998": {
        "title": "COFT-AD: COntrastive Fine-Tuning for Few-Shot Anomaly Detection",
        "authors": [
            "Jingyi Liao",
            "Xun Xu",
            "Manh Cuong Nguyen",
            "Adam Goodge",
            "Chuan Sheng Foo"
        ],
        "comments": "IEEE Transactions on Image Processing",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Existing approaches towards anomaly detection~(AD) often rely on a substantial amount of anomaly-free data to train representation and density models. However, large anomaly-free datasets may not always be available before the inference stage; in which case an anomaly detection model must be trained with only a handful of normal samples, a.k.a. few-shot anomaly detection (FSAD). In this paper, we propose a novel methodology to address the challenge of FSAD which incorporates two important techniques. Firstly, we employ a model pre-trained on a large source dataset to initialize model weights. Secondly, to ameliorate the covariate shift between source and target domains, we adopt contrastive training to fine-tune on the few-shot target domain data. To learn suitable representations for the downstream AD task, we additionally incorporate cross-instance positive pairs to encourage a tight cluster of the normal samples, and negative pairs for better separation between normal and synthesized negative samples. We evaluate few-shot anomaly detection on on 3 controlled AD tasks and 4 real-world AD tasks to demonstrate the effectiveness of the proposed method.\n    "
    },
    "2402.19002": {
        "title": "GoalNet: Goal Areas Oriented Pedestrian Trajectory Prediction",
        "authors": [
            "Ching-Lin Lee",
            "Zhi-Xuan Wang",
            "Kuan-Ting Lai",
            "Amar Fadillah"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Predicting the future trajectories of pedestrians on the road is an important task for autonomous driving. The pedestrian trajectory prediction is affected by scene paths, pedestrian's intentions and decision-making, which is a multi-modal problem. Most recent studies use past trajectories to predict a variety of potential future trajectory distributions, which do not account for the scene context and pedestrian targets. Instead of predicting the future trajectory directly, we propose to use scene context and observed trajectory to predict the goal points first, and then reuse the goal points to predict the future trajectories. By leveraging the information from scene context and observed trajectory, the uncertainty can be limited to a few target areas, which represent the \"goals\" of the pedestrians. In this paper, we propose GoalNet, a new trajectory prediction neural network based on the goal areas of a pedestrian. Our network can predict both pedestrian's trajectories and bounding boxes. The overall model is efficient and modular, and its outputs can be changed according to the usage scenario. Experimental results show that GoalNet significantly improves the previous state-of-the-art performance by 48.7% on the JAAD and 40.8% on the PIE dataset.\n    "
    },
    "2402.19007": {
        "title": "DOZE: A Dataset for Open-Vocabulary Zero-Shot Object Navigation in Dynamic Environments",
        "authors": [
            "Ji Ma",
            "Hongming Dai",
            "Yao Mu",
            "Pengying Wu",
            "Hao Wang",
            "Xiaowei Chi",
            "Yang Fei",
            "Shanghang Zhang",
            "Chang Liu"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Zero-Shot Object Navigation (ZSON) requires agents to autonomously locate and approach unseen objects in unfamiliar environments and has emerged as a particularly challenging task within the domain of Embodied AI. Existing datasets for developing ZSON algorithms lack consideration of dynamic obstacles, object attribute diversity, and scene texts, thus exhibiting noticeable discrepancy from real-world situations. To address these issues, we propose a Dataset for Open-Vocabulary Zero-Shot Object Navigation in Dynamic Environments (DOZE) that comprises ten high-fidelity 3D scenes with over 18k tasks, aiming to mimic complex, dynamic real-world scenarios. Specifically, DOZE scenes feature multiple moving humanoid obstacles, a wide array of open-vocabulary objects, diverse distinct-attribute objects, and valuable textual hints. Besides, different from existing datasets that only provide collision checking between the agent and static obstacles, we enhance DOZE by integrating capabilities for detecting collisions between the agent and moving obstacles. This novel functionality enables evaluation of the agents' collision avoidance abilities in dynamic environments. We test four representative ZSON methods on DOZE, revealing substantial room for improvement in existing approaches concerning navigation efficiency, safety, and object recognition accuracy. Our dataset could be found at this https URL.\n    "
    },
    "2402.19009": {
        "title": "Generating, Reconstructing, and Representing Discrete and Continuous Data: Generalized Diffusion with Learnable Encoding-Decoding",
        "authors": [
            "Guangyi Liu",
            "Yu Wang",
            "Zeyu Feng",
            "Qiyu Wu",
            "Liping Tang",
            "Yuan Gao",
            "Zhen Li",
            "Shuguang Cui",
            "Julian McAuley",
            "Eric P. Xing",
            "Zichao Yang",
            "Zhiting Hu"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The vast applications of deep generative models are anchored in three core capabilities -- generating new instances, reconstructing inputs, and learning compact representations -- across various data types, such as discrete text/protein sequences and continuous images. Existing model families, like Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), autoregressive models, and diffusion models, generally excel in specific capabilities and data types but fall short in others. We introduce generalized diffusion with learnable encoder-decoder (DiLED), that seamlessly integrates the core capabilities for broad applicability and enhanced performance. DiLED generalizes the Gaussian noising-denoising in standard diffusion by introducing parameterized encoding-decoding. Crucially, DiLED is compatible with the well-established diffusion model objective and training recipes, allowing effective learning of the encoder-decoder parameters jointly with diffusion. By choosing appropriate encoder/decoder (e.g., large language models), DiLED naturally applies to different data types. Extensive experiments on text, proteins, and images demonstrate DiLED's flexibility to handle diverse data and tasks and its strong improvement over various existing models.\n    "
    },
    "2402.19014": {
        "title": "Enhancing Visual Document Understanding with Contrastive Learning in Large Visual-Language Models",
        "authors": [
            "Xin Li",
            "Yunfei Wu",
            "Xinghua Jiang",
            "Zhihao Guo",
            "Mingming Gong",
            "Haoyu Cao",
            "Yinsong Liu",
            "Deqiang Jiang",
            "Xing Sun"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recently, the advent of Large Visual-Language Models (LVLMs) has received increasing attention across various domains, particularly in the field of visual document understanding (VDU). Different from conventional vision-language tasks, VDU is specifically concerned with text-rich scenarios containing abundant document elements. Nevertheless, the importance of fine-grained features remains largely unexplored within the community of LVLMs, leading to suboptimal performance in text-rich scenarios. In this paper, we abbreviate it as the fine-grained feature collapse issue. With the aim of filling this gap, we propose a contrastive learning framework, termed Document Object COntrastive learning (DoCo), specifically tailored for the downstream tasks of VDU. DoCo leverages an auxiliary multimodal encoder to obtain the features of document objects and align them to the visual features generated by the vision encoder of LVLM, which enhances visual representation in text-rich scenarios. It can represent that the contrastive learning between the visual holistic representations and the multimodal fine-grained features of document objects can assist the vision encoder in acquiring more effective visual cues, thereby enhancing the comprehension of text-rich documents in LVLMs. We also demonstrate that the proposed DoCo serves as a plug-and-play pre-training method, which can be employed in the pre-training of various LVLMs without inducing any increase in computational complexity during the inference process. Extensive experimental results on multiple benchmarks of VDU reveal that LVLMs equipped with our proposed DoCo can achieve superior performance and mitigate the gap between VDU and generic vision-language tasks.\n    "
    },
    "2402.19025": {
        "title": "Combination of Weak Learners eXplanations to Improve Random Forest eXplicability Robustness",
        "authors": [
            "Riccardo Pala",
            "Esteban Garc\u00eda-Cuesta"
        ],
        "comments": "8 pages, 10 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The notion of robustness in XAI refers to the observed variations in the explanation of the prediction of a learned model with respect to changes in the input leading to that prediction. Intuitively, if the input being explained is modified slightly subtly enough so as to not change the prediction of the model too much, then we would expect that the explanation provided for that new input does not change much either. We argue that a combination through discriminative averaging of ensembles weak learners explanations can improve the robustness of explanations in ensemble methods.This approach has been implemented and tested with post-hoc SHAP method and Random Forest ensemble with successful results. The improvements obtained have been measured quantitatively and some insights into the explicability robustness in ensemble methods are presented.\n    "
    },
    "2402.19026": {
        "title": "Progressive Contrastive Learning with Multi-Prototype for Unsupervised Visible-Infrared Person Re-identification",
        "authors": [
            "Jiangming Shi",
            "Xiangbo Yin",
            "Yaoxing Wang",
            "Xiaofeng Liu",
            "Yuan Xie",
            "Yanyun Qu"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match specified people in infrared images to visible images without annotation, and vice versa. USVI-ReID is a challenging yet under-explored task. Most existing methods address the USVI-ReID problem using cluster-based contrastive learning, which simply employs the cluster center as a representation of a person. However, the cluster center primarily focuses on shared information, overlooking disparity. To address the problem, we propose a Progressive Contrastive Learning with Multi-Prototype (PCLMP) method for USVI-ReID. In brief, we first generate the hard prototype by selecting the sample with the maximum distance from the cluster center. This hard prototype is used in the contrastive loss to emphasize disparity. Additionally, instead of rigidly aligning query images to a specific prototype, we generate the dynamic prototype by randomly picking samples within a cluster. This dynamic prototype is used to retain the natural variety of features while reducing instability in the simultaneous learning of both common and disparate information. Finally, we introduce a progressive learning strategy to gradually shift the model's attention towards hard samples, avoiding cluster deterioration. Extensive experiments conducted on the publicly available SYSU-MM01 and RegDB datasets validate the effectiveness of the proposed method. PCLMP outperforms the existing state-of-the-art method with an average mAP improvement of 3.9%. The source codes will be released.\n    "
    },
    "2402.19033": {
        "title": "High-Speed Motion Planning for Aerial Swarms in Unknown and Cluttered Environments",
        "authors": [
            "Charbel Toumieh",
            "Dario Floreano"
        ],
        "comments": " ",
        "subjects": "Robotics (cs.RO)",
        "abstract": "Coordinated flight of multiple drones allows to achieve tasks faster such as search and rescue and infrastructure inspection. Thus, pushing the state-of-the-art of aerial swarms in navigation speed and robustness is of tremendous benefit. In particular, being able to account for unexplored/unknown environments when planning trajectories allows for safer flight. In this work, we propose the first high-speed, decentralized, and synchronous motion planning framework (HDSM) for an aerial swarm that explicitly takes into account the unknown/undiscovered parts of the environment. The proposed approach generates an optimized trajectory for each planning agent that avoids obstacles and other planning agents while moving and exploring the environment. The only global information that each agent has is the target location. The generated trajectory is high-speed, safe from unexplored spaces, and brings the agent closer to its goal. The proposed method outperforms four recent state-of-the-art methods in success rate (100% success in reaching the target location), flight speed (67% faster), and flight time (42% lower). Finally, the method is validated on a set of Crazyflie nano-drones as a proof of concept.\n    "
    },
    "2402.19044": {
        "title": "DMSA -- Dense Multi Scan Adjustment for LiDAR Inertial Odometry and Global Optimization",
        "authors": [
            "David Skuddis",
            "Norbert Haala"
        ],
        "comments": "accepted for ICRA 2024",
        "subjects": "Robotics (cs.RO)",
        "abstract": "We propose a new method for fine registering multiple point clouds simultaneously. The approach is characterized by being dense, therefore point clouds are not reduced to pre-selected features in advance. Furthermore, the approach is robust against small overlaps and dynamic objects, since no direct correspondences are assumed between point clouds. Instead, all points are merged into a global point cloud, whose scattering is then iteratively reduced. This is achieved by dividing the global point cloud into uniform grid cells whose contents are subsequently modeled by normal distributions. We show that the proposed approach can be used in a sliding window continuous trajectory optimization combined with IMU measurements to obtain a highly accurate and robust LiDAR inertial odometry estimation. Furthermore, we show that the proposed approach is also suitable for large scale keyframe optimization to increase accuracy. We provide the source code and some experimental data on this https URL.\n    "
    },
    "2402.19052": {
        "title": "Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: A Benchmark Study",
        "authors": [
            "Prottay Kumar Adhikary",
            "Aseem Srivastava",
            "Shivani Kumar",
            "Salam Michael Singh",
            "Puneet Manuja",
            "Jini K Gopinath",
            "Vijay Krishnan",
            "Swati Kedia",
            "Koushik Sinha Deb",
            "Tanmoy Chakraborty"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Comprehensive summaries of sessions enable an effective continuity in mental health counseling, facilitating informed therapy planning. Yet, manual summarization presents a significant challenge, diverting experts' attention from the core counseling process. This study evaluates the effectiveness of state-of-the-art Large Language Models (LLMs) in selectively summarizing various components of therapy sessions through aspect-based summarization, aiming to benchmark their performance. We introduce MentalCLOUDS, a counseling-component guided summarization dataset consisting of 191 counseling sessions with summaries focused on three distinct counseling components (aka counseling aspects). Additionally, we assess the capabilities of 11 state-of-the-art LLMs in addressing the task of component-guided summarization in counseling. The generated summaries are evaluated quantitatively using standard summarization metrics and verified qualitatively by mental health professionals. Our findings demonstrate the superior performance of task-specific LLMs such as MentalLlama, Mistral, and MentalBART in terms of standard quantitative metrics such as Rouge-1, Rouge-2, Rouge-L, and BERTScore across all aspects of counseling components. Further, expert evaluation reveals that Mistral supersedes both MentalLlama and MentalBART based on six parameters -- affective attitude, burden, ethicality, coherence, opportunity costs, and perceived effectiveness. However, these models share the same weakness by demonstrating a potential for improvement in the opportunity costs and perceived effectiveness metrics.\n    "
    },
    "2402.19058": {
        "title": "On the Design of Human-Robot Collaboration Gestures",
        "authors": [
            "Anas Shrinah",
            "Masoud S. Bahraini",
            "Fahad Khan",
            "Seemal Asif",
            "Niels Lohse",
            "Kerstin Eder"
        ],
        "comments": " ",
        "subjects": "Robotics (cs.RO)",
        "abstract": "Effective communication between humans and collaborative robots is essential for seamless Human-Robot Collaboration (HRC). In noisy industrial settings, nonverbal communication, such as gestures, plays a key role in conveying commands and information to robots efficiently. While existing literature has thoroughly examined gesture recognition and robots' responses to these gestures, there is a notable gap in exploring the design of these gestures. The criteria for creating efficient HRC gestures are scattered across numerous studies. This paper surveys the design principles of HRC gestures, as contained in the literature, aiming to consolidate a set of criteria for HRC gesture design. It also examines the methods used for designing and evaluating HRC gestures to highlight research gaps and present directions for future research in this area.\n    "
    },
    "2402.19064": {
        "title": "The Influence of Color Stimuli on Adolescents' Emotion Playing Mobile Games",
        "authors": [
            "Leonie Kallabis",
            "Bruno Baruque-Zan\u00f3n",
            "Heinrich Klocke",
            "Ana Mar\u00eda Lara-Palma",
            "Boris Naujoks"
        ],
        "comments": "17 pages, 12 figures, 1 table",
        "subjects": "Human-Computer Interaction (cs.HC)",
        "abstract": "Video games elicit emotions which can be influenced by color stimuli as shown by previous studies. However, little research has been conducted on whether this applies to mobile games played by adolescents. Therefore, we examined the influence of color stimuli hue and saturation on mobile game play. Adolescents (n=21) played a mobile platformer game with varying hue and saturation per level for about 25 minutes. We gathered data on emotional states after each level using the Self-Assessment Manikin questionnaire, recorded time spent in each level, and collected participant self-reports on their video game experience. We performed statistical tests, such as ANOVA, which depict no significant influence of hue and/or saturation on the emotional state of our players. We conclude that it is possible that color alone is not an effective measure for eliciting emotion in mobile games, and further research is needed to consider measures such as time spent in the game and screen size, as these are unique to mobile games. There was a noticeable variance in emotional response between male and female players, with a significant interaction of hue and saturation among male players for valence ratings. This may be an indication that color preference influences perceived pleasantness.\n    "
    },
    "2402.19071": {
        "title": "FATE in MMLA: A Student-Centred Exploration of Fairness, Accountability, Transparency, and Ethics in Multimodal Learning Analytics",
        "authors": [
            "Yueqiao Jin",
            "Vanessa Echeverria",
            "Lixiang Yan",
            "Linxuan Zhao",
            "Riordan Alfredo",
            "Yi-Shan Tsai",
            "Dragan Ga\u0161evi\u0107",
            "Roberto Martinez-Maldonado"
        ],
        "comments": "16 pages, 1 figure",
        "subjects": "Computers and Society (cs.CY)",
        "abstract": "Multimodal Learning Analytics (MMLA) integrates novel sensing technologies and artificial intelligence algorithms, providing opportunities to enhance student reflection during complex, collaborative learning experiences. Although recent advancements in MMLA have shown its capability to generate insights into diverse learning behaviours across various learning settings, little research has been conducted to evaluate these systems in authentic learning contexts, particularly regarding students' perceived fairness, accountability, transparency, and ethics (FATE). Understanding these perceptions is essential to using MMLA effectively without introducing ethical complications or negatively affecting how students learn. This study aimed to address this gap by assessing the FATE of MMLA in an authentic, collaborative learning context. We conducted semi-structured interviews with 14 undergraduate students who used MMLA visualisations for post-activity reflection. The findings highlighted the significance of accurate and comprehensive data representation to ensure visualisation fairness, the need for different levels of data access to foster accountability, the imperative of measuring and cultivating transparency with students, and the necessity of transforming informed consent from dichotomous to continuous and measurable scales. While students value the benefits of MMLA, they also emphasise the importance of ethical considerations, highlighting a pressing need for the LA and MMLA community to investigate and address FATE issues actively.\n    "
    },
    "2402.19072": {
        "title": "TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables",
        "authors": [
            "Yuxuan Wang",
            "Haixu Wu",
            "Jiaxiang Dong",
            "Yong Liu",
            "Yunzhong Qiu",
            "Haoran Zhang",
            "Jianmin Wang",
            "Mingsheng Long"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Recent studies have demonstrated remarkable performance in time series forecasting. However, due to the partially-observed nature of real-world applications, solely focusing on the target of interest, so-called endogenous variables, is usually insufficient to guarantee accurate forecasting. Notably, a system is often recorded into multiple variables, where the exogenous series can provide valuable external information for endogenous variables. Thus, unlike prior well-established multivariate or univariate forecasting that either treats all the variables equally or overlooks exogenous information, this paper focuses on a practical setting, which is time series forecasting with exogenous variables. We propose a novel framework, TimeXer, to utilize external information to enhance the forecasting of endogenous variables. With a deftly designed embedding layer, TimeXer empowers the canonical Transformer architecture with the ability to reconcile endogenous and exogenous information, where patch-wise self-attention and variate-wise cross-attention are employed. Moreover, a global endogenous variate token is adopted to effectively bridge the exogenous series into endogenous temporal patches. Experimentally, TimeXer significantly improves time series forecasting with exogenous variables and achieves consistent state-of-the-art performance in twelve real-world forecasting benchmarks.\n    "
    },
    "2402.19076": {
        "title": "Pointing out the Shortcomings of Relation Extraction Models with Semantically Motivated Adversarials",
        "authors": [
            "Gennaro Nolano",
            "Moritz Blum",
            "Basil Ell",
            "Philipp Cimiano"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "In recent years, large language models have achieved state-of-the-art performance across various NLP tasks. However, investigations have shown that these models tend to rely on shortcut features, leading to inaccurate predictions and causing the models to be unreliable at generalization to out-of-distribution (OOD) samples. For instance, in the context of relation extraction (RE), we would expect a model to identify the same relation independently of the entities involved in it. For example, consider the sentence \"Leonardo da Vinci painted the Mona Lisa\" expressing the created(Leonardo_da_Vinci, Mona_Lisa) relation. If we substiute \"Leonardo da Vinci\" with \"Barack Obama\", then the sentence still expresses the created relation. A robust model is supposed to detect the same relation in both cases. In this work, we describe several semantically-motivated strategies to generate adversarial examples by replacing entity mentions and investigate how state-of-the-art RE models perform under pressure. Our analyses show that the performance of these models significantly deteriorates on the modified datasets (avg. of -48.5% in F1), which indicates that these models rely to a great extent on shortcuts, such as surface forms (or patterns therein) of entities, without making full use of the information present in the sentences.\n    "
    },
    "2402.19082": {
        "title": "VideoMAC: Video Masked Autoencoders Meet ConvNets",
        "authors": [
            "Gensheng Pei",
            "Tao Chen",
            "Xiruo Jiang",
            "Huafeng Liu",
            "Zeren Sun",
            "Yazhou Yao"
        ],
        "comments": "accepted by IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recently, the advancement of self-supervised learning techniques, like masked autoencoders (MAE), has greatly influenced visual representation learning for images and videos. Nevertheless, it is worth noting that the predominant approaches in existing masked image / video modeling rely excessively on resource-intensive vision transformers (ViTs) as the feature encoder. In this paper, we propose a new approach termed as \\textbf{VideoMAC}, which combines video masked autoencoders with resource-friendly ConvNets. Specifically, VideoMAC employs symmetric masking on randomly sampled pairs of video frames. To prevent the issue of mask pattern dissipation, we utilize ConvNets which are implemented with sparse convolutional operators as encoders. Simultaneously, we present a simple yet effective masked video modeling (MVM) approach, a dual encoder architecture comprising an online encoder and an exponential moving average target encoder, aimed to facilitate inter-frame reconstruction consistency in videos. Additionally, we demonstrate that VideoMAC, empowering classical (ResNet) / modern (ConvNeXt) convolutional encoders to harness the benefits of MVM, outperforms ViT-based approaches on downstream tasks, including video object segmentation (+\\textbf{5.2\\%} / \\textbf{6.4\\%} $\\mathcal{J}\\&\\mathcal{F}$), body part propagation (+\\textbf{6.3\\%} / \\textbf{3.1\\%} mIoU), and human pose tracking (+\\textbf{10.2\\%} / \\textbf{11.1\\%} PCK@0.1).\n    "
    },
    "2402.19090": {
        "title": "Best Arm Identification with Resource Constraints",
        "authors": [
            "Zitian Li",
            "Wang Chi Cheung"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Motivated by the cost heterogeneity in experimentation across different alternatives, we study the Best Arm Identification with Resource Constraints (BAIwRC) problem. The agent aims to identify the best arm under resource constraints, where resources are consumed for each arm pull. We make two novel contributions. We design and analyze the Successive Halving with Resource Rationing algorithm (SH-RR). The SH-RR achieves a near-optimal non-asymptotic rate of convergence in terms of the probability of successively identifying an optimal arm. Interestingly, we identify a difference in convergence rates between the cases of deterministic and stochastic resource consumption.\n    "
    },
    "2402.19091": {
        "title": "Leveraging Representations from Intermediate Encoder-blocks for Synthetic Image Detection",
        "authors": [
            "Christos Koutlis",
            "Symeon Papadopoulos"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The recently developed and publicly available synthetic image generation methods and services make it possible to create extremely realistic imagery on demand, raising great risks for the integrity and safety of online information. State-of-the-art Synthetic Image Detection (SID) research has led to strong evidence on the advantages of feature extraction from foundation models. However, such extracted features mostly encapsulate high-level visual semantics instead of fine-grained details, which are more important for the SID task. On the contrary, shallow layers encode low-level visual information. In this work, we leverage the image representations extracted by intermediate Transformer blocks of CLIP's image-encoder via a lightweight network that maps them to a learnable forgery-aware vector space capable of generalizing exceptionally well. We also employ a trainable module to incorporate the importance of each Transformer block to the final prediction. Our method is compared against the state-of-the-art by evaluating it on 20 test datasets and exhibits an average +10.6% absolute performance improvement. Notably, the best performing models require just a single epoch for training (~8 minutes). Code available at this https URL.\n    "
    },
    "2402.19097": {
        "title": "TEncDM: Understanding the Properties of Diffusion Model in the Space of Language Model Encodings",
        "authors": [
            "Alexander Shabalin",
            "Viacheslav Meshchaninov",
            "Tingir Badmaev",
            "Dmitry Molchanov",
            "Grigory Bartosh",
            "Sergey Markov",
            "Dmitry Vetrov"
        ],
        "comments": "14 pages, 8 figures, submitted to ACL 2024",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Drawing inspiration from the success of diffusion models in various domains, numerous research papers proposed methods for adapting them to text data. Despite these efforts, none of them has managed to achieve the quality of the large language models. In this paper, we conduct a comprehensive analysis of key components of the text diffusion models and introduce a novel approach named Text Encoding Diffusion Model (TEncDM). Instead of the commonly used token embedding space, we train our model in the space of the language model encodings. Additionally, we propose to use a Transformer-based decoder that utilizes contextual information for text reconstruction. We also analyse self-conditioning and find that it increases the magnitude of the model outputs, allowing the reduction of the number of denoising steps at the inference stage. Evaluation of TEncDM on two downstream text generation tasks, QQP and XSum, demonstrates its superiority over existing non-autoregressive models.\n    "
    },
    "2402.19101": {
        "title": "Effective Two-Stage Knowledge Transfer for Multi-Entity Cross-Domain Recommendation",
        "authors": [
            "Jianyu Guan",
            "Zongming Yin",
            "Tianyi Zhang",
            "Leihui Chen",
            "Yin Zhang",
            "Fei Huang",
            "Jufeng Chen",
            "Shuguang Han"
        ],
        "comments": " ",
        "subjects": "Information Retrieval (cs.IR)",
        "abstract": "In recent years, the recommendation content on e-commerce platforms has become increasingly rich -- a single user feed may contain multiple entities, such as selling products, short videos, and content posts. To deal with the multi-entity recommendation problem, an intuitive solution is to adopt the shared-network-based architecture for joint training. The idea is to transfer the extracted knowledge from one type of entity (source entity) to another (target entity). However, different from the conventional same-entity cross-domain recommendation, multi-entity knowledge transfer encounters several important issues: (1) data distributions of the source entity and target entity are naturally different, making the shared-network-based joint training susceptible to the negative transfer issue, (2) more importantly, the corresponding feature schema of each entity is not exactly aligned (e.g., price is an essential feature for selling product while missing for content posts), making the existing methods no longer appropriate. Recent researchers have also experimented with the pre-training and fine-tuning paradigm. Again, they only consider the scenarios with the same entity type and feature systems, which is inappropriate in our case. To this end, we design a pre-training & fine-tuning based Multi-entity Knowledge Transfer framework called MKT. MKT utilizes a multi-entity pre-training module to extract transferable knowledge across different entities. In particular, a feature alignment module is first applied to scale and align different feature schemas. Afterward, a couple of knowledge extractors are employed to extract the common and entity-specific knowledge. In the end, the extracted common knowledge is adopted for target entity model training. Through extensive offline and online experiments, we demonstrated the superiority of MKT over multiple State-Of-The-Art methods.\n    "
    },
    "2402.19102": {
        "title": "FlatNAS: optimizing Flatness in Neural Architecture Search for Out-of-Distribution Robustness",
        "authors": [
            "Matteo Gambella",
            "Fabrizio Pittorino",
            "Manuel Roveri"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Neural Architecture Search (NAS) paves the way for the automatic definition of Neural Network (NN) architectures, attracting increasing research attention and offering solutions in various scenarios. This study introduces a novel NAS solution, called Flat Neural Architecture Search (FlatNAS), which explores the interplay between a novel figure of merit based on robustness to weight perturbations and single NN optimization with Sharpness-Aware Minimization (SAM). FlatNAS is the first work in the literature to systematically explore flat regions in the loss landscape of NNs in a NAS procedure, while jointly optimizing their performance on in-distribution data, their out-of-distribution (OOD) robustness, and constraining the number of parameters in their architecture. Differently from current studies primarily concentrating on OOD algorithms, FlatNAS successfully evaluates the impact of NN architectures on OOD robustness, a crucial aspect in real-world applications of machine and deep learning. FlatNAS achieves a good trade-off between performance, OOD generalization, and the number of parameters, by using only in-distribution data in the NAS exploration. The OOD robustness of the NAS-designed models is evaluated by focusing on robustness to input data corruptions, using popular benchmark datasets in the literature.\n    "
    },
    "2402.19103": {
        "title": "Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models",
        "authors": [
            "Hongbang Yuan",
            "Pengfei Cao",
            "Zhuoran Jin",
            "Yubo Chen",
            "Daojian Zeng",
            "Kang Liu",
            "Jun Zhao"
        ],
        "comments": "12 pages, 5 figures, 5 tables",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted with false premise questions. In this paper, we perform a comprehensive analysis of the false premise hallucination and elucidate its internal working mechanism: a small subset of attention heads (which we designate as false premise heads) disturb the knowledge extraction process, leading to the occurrence of false premise hallucination. Based on our analysis, we propose \\textbf{FAITH} (\\textbf{F}alse premise \\textbf{A}ttention head constra\\textbf{I}ining for mi\\textbf{T}igating \\textbf{H}allucinations), a novel and effective method to mitigate false premise hallucinations. It constrains the false premise attention heads during the model inference process. Impressively, extensive experiments demonstrate that constraining only approximately $1\\%$ of the attention heads in the model yields a notable increase of nearly $20\\%$ of model performance.\n    "
    },
    "2402.19105": {
        "title": "CollaFuse: Navigating Limited Resources and Privacy in Collaborative Generative AI",
        "authors": [
            "Domenique Zipperling",
            "Simeon Allmendinger",
            "Lukas Struppek",
            "Niklas K\u00fchl"
        ],
        "comments": "Thirty-Second European Conference on Information Systems (ECIS 2024)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In the landscape of generative artificial intelligence, diffusion-based models present challenges for socio-technical systems in data requirements and privacy. Traditional approaches like federated learning distribute the learning process but strain individual clients, especially with constrained resources (e.g., edge devices). In response to these challenges, we introduce CollaFuse, a novel framework inspired by split learning. Tailored for efficient and collaborative use of denoising diffusion probabilistic models, CollaFuse enables shared server training and inference, alleviating client computational burdens. This is achieved by retaining data and computationally inexpensive GPU processes locally at each client while outsourcing the computationally expensive processes to the shared server. Demonstrated in a healthcare context, CollaFuse enhances privacy by highly reducing the need for sensitive information sharing. These capabilities hold the potential to impact various application areas, such as the design of edge computing solutions, healthcare research, or autonomous driving. In essence, our work advances distributed machine learning, shaping the future of collaborative GenAI networks.\n    "
    },
    "2402.19108": {
        "title": "DeepEraser: Deep Iterative Context Mining for Generic Text Eraser",
        "authors": [
            "Hao Feng",
            "Wendi Wang",
            "Shaokai Liu",
            "Jiajun Deng",
            "Wengang Zhou",
            "Houqiang Li"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In this work, we present DeepEraser, an effective deep network for generic text removal. DeepEraser utilizes a recurrent architecture that erases the text in an image via iterative operations. Our idea comes from the process of erasing pencil script, where the text area designated for removal is subject to continuous monitoring and the text is attenuated progressively, ensuring a thorough and clean erasure. Technically, at each iteration, an innovative erasing module is deployed, which not only explicitly aggregates the previous erasing progress but also mines additional semantic context to erase the target text. Through iterative refinements, the text regions are progressively replaced with more appropriate content and finally converge to a relatively accurate status. Furthermore, a custom mask generation strategy is introduced to improve the capability of DeepEraser for adaptive text removal, as opposed to indiscriminately removing all the text in an image. Our DeepEraser is notably compact with only 1.4M parameters and trained in an end-to-end manner. To verify its effectiveness, extensive experiments are conducted on several prevalent benchmarks, including SCUT-Syn, SCUT-EnsText, and Oxford Synthetic text dataset. The quantitative and qualitative results demonstrate the effectiveness of our DeepEraser over the state-of-the-art methods, as well as its strong generalization ability in custom mask text removal. The codes and pre-trained models are available at this https URL\n"
    },
    "2402.19118": {
        "title": "Continuous Sign Language Recognition Based on Motor attention mechanism and frame-level Self-distillation",
        "authors": [
            "Qidan Zhu",
            "Jing Li",
            "Fei Yuan",
            "Quan Gan"
        ],
        "comments": "10 pages, 7 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Changes in facial expression, head movement, body movement and gesture movement are remarkable cues in sign language recognition, and most of the current continuous sign language recognition(CSLR) research methods mainly focus on static images in video sequences at the frame-level feature extraction stage, while ignoring the dynamic changes in the images. In this paper, we propose a novel motor attention mechanism to capture the distorted changes in local motion regions during sign language expression, and obtain a dynamic representation of image changes. And for the first time, we apply the self-distillation method to frame-level feature extraction for continuous sign language, which improves the feature expression without increasing the computational resources by self-distilling the features of adjacent stages and using the higher-order features as teachers to guide the lower-order features. The combination of the two constitutes our proposed holistic model of CSLR Based on motor attention mechanism and frame-level Self-Distillation (MAM-FSD), which improves the inference ability and robustness of the model. We conduct experiments on three publicly available datasets, and the experimental results show that our proposed method can effectively extract the sign language motion information in videos, improve the accuracy of CSLR and reach the state-of-the-art level.\n    "
    },
    "2402.19133": {
        "title": "Evaluating Webcam-based Gaze Data as an Alternative for Human Rationale Annotations",
        "authors": [
            "Stephanie Brandl",
            "Oliver Eberle",
            "Tiago Ribeiro",
            "Anders S\u00f8gaard",
            "Nora Hollenstein"
        ],
        "comments": "Accepted to LREC-COLING 2024",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Rationales in the form of manually annotated input spans usually serve as ground truth when evaluating explainability methods in NLP. They are, however, time-consuming and often biased by the annotation process. In this paper, we debate whether human gaze, in the form of webcam-based eye-tracking recordings, poses a valid alternative when evaluating importance scores. We evaluate the additional information provided by gaze data, such as total reading times, gaze entropy, and decoding accuracy with respect to human rationale annotations. We compare WebQAmGaze, a multilingual dataset for information-seeking QA, with attention and explainability-based importance scores for 4 different multilingual Transformer-based language models (mBERT, distil-mBERT, XLMR, and XLMR-L) and 3 languages (English, Spanish, and German). Our pipeline can easily be applied to other tasks and languages. Our findings suggest that gaze data offers valuable linguistic insights that could be leveraged to infer task difficulty and further show a comparable ranking of explainability methods to that of human rationales.\n    "
    },
    "2402.19135": {
        "title": "Think Fast, Think Slow, Think Critical: Designing an Automated Propaganda Detection Tool",
        "authors": [
            "Liudmila Zavolokina",
            "Kilian Sprenkamp",
            "Zoya Katashinskaya",
            "Daniel Gordon Jones",
            "Gerhard Schwabe"
        ],
        "comments": "The paper is accepted for publication in proceedings of the CHI Conference on Human Factors in Computing Systems (2024)",
        "subjects": "Human-Computer Interaction (cs.HC)",
        "abstract": "In today's digital age, characterized by rapid news consumption and increasing vulnerability to propaganda, fostering citizens' critical thinking is crucial for stable democracies. This paper introduces the design of ClarifAI, a novel automated propaganda detection tool designed to nudge readers towards more critical news consumption by activating the analytical mode of thinking, following Kahneman's dual-system theory of cognition. Using Large Language Models, ClarifAI detects propaganda in news articles and provides context-rich explanations, enhancing users' understanding and critical thinking. Our contribution is threefold: first, we propose the design of ClarifAI; second, in an online experiment, we demonstrate that this design effectively encourages news readers to engage in more critical reading; and third, we emphasize the value of explanations for fostering critical thinking. The study thus offers both a practical tool and useful design knowledge for mitigating propaganda in digital news.\n    "
    },
    "2402.19142": {
        "title": "ProtoP-OD: Explainable Object Detection with Prototypical Parts",
        "authors": [
            "Pavlos Rath-Manakidis",
            "Frederik Strothmann",
            "Tobias Glasmachers",
            "Laurenz Wiskott"
        ],
        "comments": "9 pages, 11 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Interpretation and visualization of the behavior of detection transformers tends to highlight the locations in the image that the model attends to, but it provides limited insight into the \\emph{semantics} that the model is focusing on. This paper introduces an extension to detection transformers that constructs prototypical local features and uses them in object detection. These custom features, which we call prototypical parts, are designed to be mutually exclusive and align with the classifications of the model. The proposed extension consists of a bottleneck module, the prototype neck, that computes a discretized representation of prototype activations and a new loss term that matches prototypes to object classes. This setup leads to interpretable representations in the prototype neck, allowing visual inspection of the image content perceived by the model and a better understanding of the model's reliability. We show experimentally that our method incurs only a limited performance penalty, and we provide examples that demonstrate the quality of the explanations provided by our method, which we argue outweighs the performance penalty.\n    "
    },
    "2402.19144": {
        "title": "Weakly Supervised Monocular 3D Detection with a Single-View Image",
        "authors": [
            "Xueying Jiang",
            "Sheng Jin",
            "Lewei Lu",
            "Xiaoqin Zhang",
            "Shijian Lu"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Monocular 3D detection (M3D) aims for precise 3D object localization from a single-view image which usually involves labor-intensive annotation of 3D detection boxes. Weakly supervised M3D has recently been studied to obviate the 3D annotation process by leveraging many existing 2D annotations, but it often requires extra training data such as LiDAR point clouds or multi-view images which greatly degrades its applicability and usability in various applications. We propose SKD-WM3D, a weakly supervised monocular 3D detection framework that exploits depth information to achieve M3D with a single-view image exclusively without any 3D annotations or other training data. One key design in SKD-WM3D is a self-knowledge distillation framework, which transforms image features into 3D-like representations by fusing depth information and effectively mitigates the inherent depth ambiguity in monocular scenarios with little computational overhead in inference. In addition, we design an uncertainty-aware distillation loss and a gradient-targeted transfer modulation strategy which facilitate knowledge acquisition and knowledge transfer, respectively. Extensive experiments show that SKD-WM3D surpasses the state-of-the-art clearly and is even on par with many fully supervised methods.\n    "
    },
    "2402.19145": {
        "title": "A SAM-guided Two-stream Lightweight Model for Anomaly Detection",
        "authors": [
            "Chenghao Li",
            "Lei Qi",
            "Xin Geng"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In industrial anomaly detection, model efficiency and mobile-friendliness become the primary concerns in real-world applications. Simultaneously, the impressive generalization capabilities of Segment Anything (SAM) have garnered broad academic attention, making it an ideal choice for localizing unseen anomalies and diverse real-world patterns. In this paper, considering these two critical factors, we propose a SAM-guided Two-stream Lightweight Model for unsupervised anomaly detection (STLM) that not only aligns with the two practical application requirements but also harnesses the robust generalization capabilities of SAM. We employ two lightweight image encoders, i.e., our two-stream lightweight module, guided by SAM's knowledge. To be specific, one stream is trained to generate discriminative and general feature representations in both normal and anomalous regions, while the other stream reconstructs the same images without anomalies, which effectively enhances the differentiation of two-stream representations when facing anomalous regions. Furthermore, we employ a shared mask decoder and a feature aggregation module to generate anomaly maps. Our experiments conducted on MVTec AD benchmark show that STLM, with about 16M parameters and achieving an inference time in 20ms, competes effectively with state-of-the-art methods in terms of performance, 98.26% on pixel-level AUC and 94.92% on PRO. We further experiment on more difficult datasets, e.g., VisA and DAGM, to demonstrate the effectiveness and generalizability of STLM.\n    "
    },
    "2402.19155": {
        "title": "Beyond Language Models: Byte Models are Digital World Simulators",
        "authors": [
            "Shangda Wu",
            "Xu Tan",
            "Zili Wang",
            "Rui Wang",
            "Xiaobing Li",
            "Maosong Sun"
        ],
        "comments": "19 pages, 5 figures, 5 tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Traditional deep learning often overlooks bytes, the basic units of the digital world, where all forms of information and operations are encoded and manipulated in binary format. Inspired by the success of next token prediction in natural language processing, we introduce bGPT, a model with next byte prediction to simulate the digital world. bGPT matches specialized models in performance across various modalities, including text, audio, and images, and offers new possibilities for predicting, simulating, and diagnosing algorithm or hardware behaviour. It has almost flawlessly replicated the process of converting symbolic music data, achieving a low error rate of 0.0011 bits per byte in converting ABC notation to MIDI format. In addition, bGPT demonstrates exceptional capabilities in simulating CPU behaviour, with an accuracy exceeding 99.99% in executing various operations. Leveraging next byte prediction, models like bGPT can directly learn from vast binary data, effectively simulating the intricate patterns of the digital world.\n    "
    },
    "2402.19166": {
        "title": "Conversational Language Models for Human-in-the-Loop Multi-Robot Coordination",
        "authors": [
            "William Hunt",
            "Toby Godfrey",
            "Mohammad D. Soorati"
        ],
        "comments": " ",
        "subjects": "Robotics (cs.RO)",
        "abstract": "With the increasing prevalence and diversity of robots interacting in the real world, there is need for flexible, on-the-fly planning and cooperation. Large Language Models are starting to be explored in a multimodal setup for communication, coordination, and planning in robotics. Existing approaches generally use a single agent building a plan, or have multiple homogeneous agents coordinating for a simple task. We present a decentralised, dialogical approach in which a team of agents with different abilities plans solutions through peer-to-peer and human-robot discussion. We suggest that argument-style dialogues are an effective way to facilitate adaptive use of each agent's abilities within a cooperative team. Two robots discuss how to solve a cleaning problem set by a human, define roles, and agree on paths they each take. Each step can be interrupted by a human advisor and agents check their plans with the human. Agents then execute this plan in the real world, collecting rubbish from people in each room. Our implementation uses text at every step, maintaining transparency and effective human-multi-robot interaction.\n    "
    },
    "2402.19167": {
        "title": "Teaching Large Language Models an Unseen Language on the Fly",
        "authors": [
            "Chen Zhang",
            "Xiao Liu",
            "Jiuheng Lin",
            "Yansong Feng"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Existing large language models struggle to support numerous low-resource languages, particularly the extremely low-resource ones where there is minimal training data available for effective parameter updating. We thus investigate whether LLMs can learn a new language on the fly solely through prompting. To study this question, we collect a research suite for Zhuang, a language supported by no LLMs currently. We introduce \\textsc{DiPMT++}, a framework for adapting LLMs to unseen languages by in-context learning. Using a dictionary and only 5K parallel sentences, \\textsc{DiPMT++} significantly enhances the performance of GPT-4 from 0 to 16 BLEU for Chinese-to-Zhuang translation and achieves 32 BLEU for Zhuang-to-Chinese translation. Furthermore, we demonstrate the practical utility of this framework in aiding humans to translate completely unseen languages, which could contribute to the preservation of linguistic diversity.\n    "
    },
    "2402.19186": {
        "title": "Disentangling representations of retinal images with generative models",
        "authors": [
            "Sarah M\u00fcller",
            "Lisa M. Koch",
            "Hendrik P. A. Lensch",
            "Philipp Berens"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Retinal fundus images play a crucial role in the early detection of eye diseases and, using deep learning approaches, recent studies have even demonstrated their potential for detecting cardiovascular risk factors and neurological disorders. However, the impact of technical factors on these images can pose challenges for reliable AI applications in ophthalmology. For example, large fundus cohorts are often confounded by factors like camera type, image quality or illumination level, bearing the risk of learning shortcuts rather than the causal relationships behind the image generation process. Here, we introduce a novel population model for retinal fundus images that effectively disentangles patient attributes from camera effects, thus enabling controllable and highly realistic image generation. To achieve this, we propose a novel disentanglement loss based on distance correlation. Through qualitative and quantitative analyses, we demonstrate the effectiveness of this novel loss function in disentangling the learned subspaces. Our results show that our model provides a new perspective on the complex relationship between patient attributes and technical confounders in retinal fundus image generation.\n    "
    },
    "2402.19189": {
        "title": "Link Recommendation to Augment Influence Diffusion with Provable Guarantees",
        "authors": [
            "Xiaolong Chen",
            "Yifan Song",
            "Jing Tang"
        ],
        "comments": "TheWebConf'24; Corresponding author: Jing Tang",
        "subjects": "Social and Information Networks (cs.SI)",
        "abstract": "Link recommendation systems in online social networks (OSNs), such as Facebook's ``People You May Know'', Twitter's ``Who to Follow'', and Instagram's ``Suggested Accounts'', facilitate the formation of new connections among users. This paper addresses the challenge of link recommendation for the purpose of social influence maximization. In particular, given a graph $G$ and the seed set $S$, our objective is to select $k$ edges that connect seed nodes and ordinary nodes to optimize the influence dissemination of the seed set. This problem, referred to as influence maximization with augmentation (IMA), has been proven to be NP-hard.\nIn this paper, we propose an algorithm, namely \\textsf{AIS}, consisting of an efficient estimator for augmented influence estimation and an accelerated sampling approach. \\textsf{AIS} provides a $(1-1/\\mathrm{e}-\\varepsilon)$-approximate solution with a high probability of $1-\\delta$, and runs in $O(k^2 (m+n) \\log (n / \\delta) / \\varepsilon^2 + k \\left|E_{\\mathcal{C}}\\right|)$ time assuming that the influence of any singleton node is smaller than that of the seed set. To the best of our knowledge, this is the first algorithm that can be implemented on large graphs containing millions of nodes while preserving strong theoretical guarantees. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our proposed algorithm.\n    "
    },
    "2402.19195": {
        "title": "Negative Sampling in Knowledge Graph Representation Learning: A Review",
        "authors": [
            "Tiroshan Madushanka",
            "Ryutaro Ichise"
        ],
        "comments": " ",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Knowledge graph representation learning (KGRL) or knowledge graph embedding (KGE) plays a crucial role in AI applications for knowledge construction and information exploration. These models aim to encode entities and relations present in a knowledge graph into a lower-dimensional vector space. During the training process of KGE models, using positive and negative samples becomes essential for discrimination purposes. However, obtaining negative samples directly from existing knowledge graphs poses a challenge, emphasizing the need for effective generation techniques. The quality of these negative samples greatly impacts the accuracy of the learned embeddings, making their generation a critical aspect of KGRL. This comprehensive survey paper systematically reviews various negative sampling (NS) methods and their contributions to the success of KGRL. Their respective advantages and disadvantages are outlined by categorizing existing NS methods into five distinct categories. Moreover, this survey identifies open research questions that serve as potential directions for future investigations. By offering a generalization and alignment of fundamental NS concepts, this survey provides valuable insights for designing effective NS methods in the context of KGRL and serves as a motivating force for further advancements in the field.\n    "
    },
    "2402.19197": {
        "title": "Fine Structure-Aware Sampling: A New Sampling Training Scheme for Pixel-Aligned Implicit Models in Single-View Human Reconstruction",
        "authors": [
            "Kennard Yanting Chan",
            "Fayao Liu",
            "Guosheng Lin",
            "Chuan Sheng Foo",
            "Weisi Lin"
        ],
        "comments": "Accepted in Proceedings of the AAAI Conference on Artificial Intelligence, 2024 (AAAI 2024)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Pixel-aligned implicit models, such as PIFu, PIFuHD, and ICON, are used for single-view clothed human reconstruction. These models need to be trained using a sampling training scheme. Existing sampling training schemes either fail to capture thin surfaces (e.g. ears, fingers) or cause noisy artefacts in reconstructed meshes. To address these problems, we introduce Fine Structured-Aware Sampling (FSS), a new sampling training scheme to train pixel-aligned implicit models for single-view human reconstruction. FSS resolves the aforementioned problems by proactively adapting to the thickness and complexity of surfaces. In addition, unlike existing sampling training schemes, FSS shows how normals of sample points can be capitalized in the training process to improve results. Lastly, to further improve the training process, FSS proposes a mesh thickness loss signal for pixel-aligned implicit models. It becomes computationally feasible to introduce this loss once a slight reworking of the pixel-aligned implicit function framework is carried out. Our results show that our methods significantly outperform SOTA methods qualitatively and quantitatively. Our code is publicly available at this https URL.\n    "
    },
    "2402.19204": {
        "title": "PeLLE: Encoder-based language models for Brazilian Portuguese based on open data",
        "authors": [
            "Guilherme Lamartine de Mello",
            "Marcelo Finger",
            "and Felipe Serras",
            "Miguel de Mello Carpi",
            "Marcos Menon Jose",
            "Pedro Henrique Domingues",
            "Paulo Cavalim"
        ],
        "comments": "15 pages",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "In this paper we present PeLLE, a family of large language models based on the RoBERTa architecture, for Brazilian Portuguese, trained on curated, open data from the Carolina corpus. Aiming at reproducible results, we describe details of the pretraining of the models. We also evaluate PeLLE models against a set of existing multilingual and PT-BR refined pretrained Transformer-based LLM encoders, contrasting performance of large versus smaller-but-curated pretrained models in several downstream tasks. We conclude that several tasks perform better with larger models, but some tasks benefit from smaller-but-curated data in its pretraining.\n    "
    },
    "2402.19218": {
        "title": "Memory-Augmented Generative Adversarial Transformers",
        "authors": [
            "Stephan Raaijmakers",
            "Roos Bakker",
            "Anita Cremers",
            "Roy de Kleijn",
            "Tom Kouwenhoven",
            "Tessa Verhoef"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Conversational AI systems that rely on Large Language Models, like Transformers, have difficulty interweaving external data (like facts) with the language they generate. Vanilla Transformer architectures are not designed for answering factual questions with high accuracy. This paper investigates a possible route for addressing this problem. We propose to extend the standard Transformer architecture with an additional memory bank holding extra information (such as facts drawn from a knowledge base), and an extra attention layer for addressing this memory. We add this augmented memory to a Generative Adversarial Network-inspired Transformer architecture. This setup allows for implementing arbitrary felicity conditions on the generated language of the Transformer. We first demonstrate how this machinery can be deployed for handling factual questions in goal-oriented dialogues. Secondly, we demonstrate that our approach can be useful for applications like {\\it style adaptation} as well: the adaptation of utterances according to certain stylistic (external) constraints, like social properties of human interlocutors in dialogues.\n    "
    },
    "2402.19229": {
        "title": "CAPTURE-24: A large dataset of wrist-worn activity tracker data collected in the wild for human activity recognition",
        "authors": [
            "Shing Chan",
            "Hang Yuan",
            "Catherine Tong",
            "Aidan Acquah",
            "Abram Schonfeldt",
            "Jonathan Gershuny",
            "Aiden Doherty"
        ],
        "comments": " ",
        "subjects": "Human-Computer Interaction (cs.HC)",
        "abstract": "Existing activity tracker datasets for human activity recognition are typically obtained by having participants perform predefined activities in an enclosed environment under supervision. This results in small datasets with a limited number of activities and heterogeneity, lacking the mixed and nuanced movements normally found in free-living scenarios. As such, models trained on laboratory-style datasets may not generalise out of sample. To address this problem, we introduce a new dataset involving wrist-worn accelerometers, wearable cameras, and sleep diaries, enabling data collection for over 24 hours in a free-living setting. The result is CAPTURE-24, a large activity tracker dataset collected in the wild from 151 participants, amounting to 3883 hours of accelerometer data, of which 2562 hours are annotated. CAPTURE-24 is two to three orders of magnitude larger than existing publicly available datasets, which is critical to developing accurate human activity recognition models.\n    "
    },
    "2402.19233": {
        "title": "Shared lightweight autonomous vehicles for urban food deliveries: A simulation study",
        "authors": [
            "Ainhoa Genua Cervi\u00f1o",
            "Naroa Coretti Sanchez",
            "Elaine Liu Wang",
            "Arnaud Grignard",
            "Kent Larson"
        ],
        "comments": "17 pages, 25 including abstract, 16 figures, journal paper",
        "subjects": "Computers and Society (cs.CY)",
        "abstract": "In recent years, the rapid growth of on-demand deliveries, especially in food deliveries, has spurred the exploration of innovative mobility solutions. In this context, lightweight autonomous vehicles have emerged as a potential alternative. However, their fleet-level behavior remains largely unexplored. To address this gap, we have developed an agent-based model and an environmental impact study assessing the fleet performance of lightweight autonomous food delivery vehicles. This model explores critical factors such as fleet sizing, service level, operational strategies, and environmental impacts. We have applied this model to a case study in Cambridge, MA, USA, where results indicate that there could be environmental benefits in replacing traditional car-based deliveries with shared lightweight autonomous vehicle fleets. Lastly, we introduce an interactive platform that offers a user-friendly means of comprehending the model's performance and potential trade-offs, which can help inform decision-makers in the evolving landscape of food delivery innovation.\n    "
    },
    "2402.19237": {
        "title": "Context-based Interpretable Spatio-Temporal Graph Convolutional Network for Human Motion Forecasting",
        "authors": [
            "Edgar Medina",
            "Leyong Loh",
            "Namrata Gurung",
            "Kyung Hun Oh",
            "Niels Heller"
        ],
        "comments": "10 pages, 6 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Human motion prediction is still an open problem extremely important for autonomous driving and safety applications. Due to the complex spatiotemporal relation of motion sequences, this remains a challenging problem not only for movement prediction but also to perform a preliminary interpretation of the joint connections. In this work, we present a Context-based Interpretable Spatio-Temporal Graph Convolutional Network (CIST-GCN), as an efficient 3D human pose forecasting model based on GCNs that encompasses specific layers, aiding model interpretability and providing information that might be useful when analyzing motion distribution and body behavior. Our architecture extracts meaningful information from pose sequences, aggregates displacements and accelerations into the input model, and finally predicts the output displacements. Extensive experiments on Human 3.6M, AMASS, 3DPW, and ExPI datasets demonstrate that CIST-GCN outperforms previous methods in human motion prediction and robustness. Since the idea of enhancing interpretability for motion prediction has its merits, we showcase experiments towards it and provide preliminary evaluations of such insights here. available code: this https URL\n"
    },
    "2402.19249": {
        "title": "Mirage: Cross-Embodiment Zero-Shot Policy Transfer with Cross-Painting",
        "authors": [
            "Lawrence Yunliang Chen",
            "Kush Hari",
            "Karthik Dharmarajan",
            "Chenfeng Xu",
            "Quan Vuong",
            "Ken Goldberg"
        ],
        "comments": "Project page: this https URL",
        "subjects": "Robotics (cs.RO)",
        "abstract": "The ability to reuse collected data and transfer trained policies between robots could alleviate the burden of additional data collection and training. While existing approaches such as pretraining plus finetuning and co-training show promise, they do not generalize to robots unseen in training. Focusing on common robot arms with similar workspaces and 2-jaw grippers, we investigate the feasibility of zero-shot transfer. Through simulation studies on 8 manipulation tasks, we find that state-based Cartesian control policies can successfully zero-shot transfer to a target robot after accounting for forward dynamics. To address robot visual disparities for vision-based policies, we introduce Mirage, which uses \"cross-painting\"--masking out the unseen target robot and inpainting the seen source robot--during execution in real time so that it appears to the policy as if the trained source robot were performing the task. Despite its simplicity, our extensive simulation and physical experiments provide strong evidence that Mirage can successfully zero-shot transfer between different robot arms and grippers with only minimal performance degradation on a variety of manipulation tasks such as picking, stacking, and assembly, significantly outperforming a generalist policy. Project website: this https URL\n"
    },
    "2402.19250": {
        "title": "Feature boosting with efficient attention for scene parsing",
        "authors": [
            "Vivek Singh",
            "Shailza Sharma",
            "Fabio Cuzzolin"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The complexity of scene parsing grows with the number of object and scene classes, which is higher in unrestricted open scenes. The biggest challenge is to model the spatial relation between scene elements while succeeding in identifying objects at smaller scales. This paper presents a novel feature-boosting network that gathers spatial context from multiple levels of feature extraction and computes the attention weights for each level of representation to generate the final class labels. A novel `channel attention module' is designed to compute the attention weights, ensuring that features from the relevant extraction stages are boosted while the others are attenuated. The model also learns spatial context information at low resolution to preserve the abstract spatial relationships among scene elements and reduce computation cost. Spatial attention is subsequently concatenated into a final feature set before applying feature boosting. Low-resolution spatial attention features are trained using an auxiliary task that helps learning a coarse global scene structure. The proposed model outperforms all state-of-the-art models on both the ADE20K and the Cityscapes datasets.\n    "
    },
    "2402.19251": {
        "title": "A Cognitive-Based Trajectory Prediction Approach for Autonomous Driving",
        "authors": [
            "Haicheng Liao",
            "Yongkang Li",
            "Zhenning Li",
            "Chengyue Wang",
            "Zhiyong Cui",
            "Shengbo Eben Li",
            "Chengzhong Xu"
        ],
        "comments": " ",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "In autonomous vehicle (AV) technology, the ability to accurately predict the movements of surrounding vehicles is paramount for ensuring safety and operational efficiency. Incorporating human decision-making insights enables AVs to more effectively anticipate the potential actions of other vehicles, significantly improving prediction accuracy and responsiveness in dynamic environments. This paper introduces the Human-Like Trajectory Prediction (HLTP) model, which adopts a teacher-student knowledge distillation framework inspired by human cognitive processes. The HLTP model incorporates a sophisticated teacher-student knowledge distillation framework. The \"teacher\" model, equipped with an adaptive visual sector, mimics the visual processing of the human brain, particularly the functions of the occipital and temporal lobes. The \"student\" model focuses on real-time interaction and decision-making, drawing parallels to prefrontal and parietal cortex functions. This approach allows for dynamic adaptation to changing driving scenarios, capturing essential perceptual cues for accurate prediction. Evaluated using the Macao Connected and Autonomous Driving (MoCAD) dataset, along with the NGSIM and HighD benchmarks, HLTP demonstrates superior performance compared to existing models, particularly in challenging environments with incomplete data. The project page is available at Github.\n    "
    },
    "2402.19255": {
        "title": "GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers",
        "authors": [
            "Qintong Li",
            "Leyang Cui",
            "Xueliang Zhao",
            "Lingpeng Kong",
            "Wei Bi"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Large language models (LLMs) have achieved impressive performance across various mathematical reasoning benchmarks. However, there are increasing debates regarding whether these models truly understand and apply mathematical knowledge or merely rely on shortcuts for mathematical reasoning. One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly. This motivates us to evaluate the robustness of LLMs' math reasoning capability by testing a wide range of question variations. We introduce the adversarial grade school math (\\datasetname) dataset, an extension of GSM8K augmented with various mathematical perturbations. Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust. In particular, even for problems that have been solved in GSM8K, LLMs can make mistakes when new statements are added or the question targets are altered. We also explore whether more robust performance can be achieved by composing existing prompting methods, in which we try an iterative method that generates and verifies each intermediate thought based on its reasoning goal and calculation result. Code and data are available at \\url{this https URL}.\n    "
    },
    "2402.19258": {
        "title": "MaskFi: Unsupervised Learning of WiFi and Vision Representations for Multimodal Human Activity Recognition",
        "authors": [
            "Jianfei Yang",
            "Shijie Tang",
            "Yuecong Xu",
            "Yunjiao Zhou",
            "Lihua Xie"
        ],
        "comments": "9 pages",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Human activity recognition (HAR) has been playing an increasingly important role in various domains such as healthcare, security monitoring, and metaverse gaming. Though numerous HAR methods based on computer vision have been developed to show prominent performance, they still suffer from poor robustness in adverse visual conditions in particular low illumination, which motivates WiFi-based HAR to serve as a good complementary modality. Existing solutions using WiFi and vision modalities rely on massive labeled data that are very cumbersome to collect. In this paper, we propose a novel unsupervised multimodal HAR solution, MaskFi, that leverages only unlabeled video and WiFi activity data for model training. We propose a new algorithm, masked WiFi-vision modeling (MI2M), that enables the model to learn cross-modal and single-modal features by predicting the masked sections in representation learning. Benefiting from our unsupervised learning procedure, the network requires only a small amount of annotated data for finetuning and can adapt to the new environment with better performance. We conduct extensive experiments on two WiFi-vision datasets collected in-house, and our method achieves human activity recognition and human identification in terms of both robustness and accuracy.\n    "
    },
    "2402.19262": {
        "title": "Masks, Signs, And Learning Rate Rewinding",
        "authors": [
            "Advait Gadhikar",
            "Rebekka Burkholz"
        ],
        "comments": "Accepted for publishing at ICLR 2024",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Learning Rate Rewinding (LRR) has been established as a strong variant of Iterative Magnitude Pruning (IMP) to find lottery tickets in deep overparameterized neural networks. While both iterative pruning schemes couple structure and parameter learning, understanding how LRR excels in both aspects can bring us closer to the design of more flexible deep learning algorithms that can optimize diverse sets of sparse architectures. To this end, we conduct experiments that disentangle the effect of mask learning and parameter optimization and how both benefit from overparameterization. The ability of LRR to flip parameter signs early and stay robust to sign perturbations seems to make it not only more effective in mask identification but also in optimizing diverse sets of masks, including random ones. In support of this hypothesis, we prove in a simplified single hidden neuron setting that LRR succeeds in more cases than IMP, as it can escape initially problematic sign configurations.\n    "
    },
    "2402.19263": {
        "title": "Spinal Osteophyte Detection via Robust Patch Extraction on minimally annotated X-rays",
        "authors": [
            "Soumya Snigdha Kundu",
            "Yuanhan Mo",
            "Nicharee Srikijkasemwat",
            "Bart\u0142omiej W. Papiez"
        ],
        "comments": "ISBI'24 Full Paper",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The development and progression of arthritis is strongly associated with osteophytes, which are small and elusive bone growths. This paper presents one of the first efforts towards automated spinal osteophyte detection in spinal X-rays. A novel automated patch extraction process, called SegPatch, has been proposed based on deep learning-driven vertebrae segmentation and the enlargement of mask contours. A final patch classification accuracy of 84.5\\% is secured, surpassing a baseline tiling-based patch generation technique by 9.5%. This demonstrates that even with limited annotations, SegPatch can deliver superior performance for detection of tiny structures such as osteophytes. The proposed approach has potential to assist clinicians in expediting the process of manually identifying osteophytes in spinal X-ray.\n    "
    },
    "2402.19264": {
        "title": "T3DNet: Compressing Point Cloud Models for Lightweight 3D Recognition",
        "authors": [
            "Zhiyuan Yang",
            "Yunjiao Zhou",
            "Lihua Xie",
            "Jianfei Yang"
        ],
        "comments": "12 pages",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "3D point cloud has been widely used in many mobile application scenarios, including autonomous driving and 3D sensing on mobile devices. However, existing 3D point cloud models tend to be large and cumbersome, making them hard to deploy on edged devices due to their high memory requirements and non-real-time latency. There has been a lack of research on how to compress 3D point cloud models into lightweight models. In this paper, we propose a method called T3DNet (Tiny 3D Network with augmEntation and disTillation) to address this issue. We find that the tiny model after network augmentation is much easier for a teacher to distill. Instead of gradually reducing the parameters through techniques such as pruning or quantization, we pre-define a tiny model and improve its performance through auxiliary supervision from augmented networks and the original model. We evaluate our method on several public datasets, including ModelNet40, ShapeNet, and ScanObjectNN. Our method can achieve high compression rates without significant accuracy sacrifice, achieving state-of-the-art performances on three datasets against existing methods. Amazingly, our T3DNet is 58 times smaller and 54 times faster than the original model yet with only 1.4% accuracy descent on the ModelNet40 dataset.\n    "
    },
    "2402.19267": {
        "title": "Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation",
        "authors": [
            "Seunghyun Ji",
            "Hagai Raja Sinulingga",
            "Darongsae Kwon"
        ],
        "comments": "Submitted to SIGUL 2024, a satellite workshop of LREC-COLING 2024",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Employing extensive datasets enables the training of multilingual machine translation models; however, these models often fail to accurately translate sentences within specialized domains. Although obtaining and translating domain-specific data incurs high costs, it is inevitable for high-quality translations. Hence, finding the most 'effective' data with an unsupervised setting becomes a practical strategy for reducing labeling costs. Recent research indicates that this effective data could be found by selecting 'properly difficult data' based on its volume. This means the data should not be excessively challenging or overly simplistic, especially if the amount of data is limited. However, we found that establishing a criterion for unsupervised data selection remains challenging, as the 'proper difficulty' might vary based on the data domain being trained on. We introduce a novel unsupervised data selection method, 'Capturing Perplexing Named Entities', which adopts the maximum inference entropy in translated named entities as a selection measure. The motivation was that named entities in domain-specific data are considered the most complex portion of the data and should be predicted with high confidence. When verified with the 'Korean-English Parallel Corpus of Specialized Domains,' our method served as a robust guidance for unsupervised data selection, in contrast to existing methods.\n    "
    },
    "2402.19273": {
        "title": "PlanGPT: Enhancing Urban Planning with Tailored Language Model and Efficient Retrieval",
        "authors": [
            "He Zhu",
            "Wenjia Zhang",
            "Nuoxian Huang",
            "Boyang Li",
            "Luyao Niu",
            "Zipei Fan",
            "Tianle Lun",
            "Yicheng Tao",
            "Junyou Su",
            "Zhaoya Gong",
            "Chenyu Fang",
            "Xing Liu"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "In the field of urban planning, general-purpose large language models often struggle to meet the specific needs of planners. Tasks like generating urban planning texts, retrieving related information, and evaluating planning documents pose unique challenges. To enhance the efficiency of urban professionals and overcome these obstacles, we introduce PlanGPT, the first specialized Large Language Model tailored for urban and spatial planning. Developed through collaborative efforts with institutions like the Chinese Academy of Urban Planning, PlanGPT leverages a customized local database retrieval framework, domain-specific fine-tuning of base models, and advanced tooling capabilities. Empirical tests demonstrate that PlanGPT has achieved advanced performance, delivering responses of superior quality precisely tailored to the intricacies of urban planning.\n    "
    },
    "2402.19279": {
        "title": "SIFT-Aided Rectified 2D-DIC for Displacement and Strain Measurements in Asphalt Concrete Testing",
        "authors": [
            "Zehui Zhu",
            "Imad L. Al-Qadi"
        ],
        "comments": "Journal of Transportation Engineering, Part B: Pavements",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Two-dimensional digital image correlation (2D-DIC) is a widely used optical technique to measure displacement and strain during asphalt concrete (AC) testing. An accurate 2-D DIC measurement can only be achieved when the camera's principal axis is perpendicular to the planar specimen surface. However, this requirement may not be met during testing due to device constraints. This paper proposes a simple and reliable method to correct errors induced by non-perpendicularity. The method is based on image feature matching and rectification. No additional equipment is needed. A theoretical error analysis was conducted to quantify the effect of a non-perpendicular camera alignment on measurement accuracy. The proposed method was validated numerically using synthetic images and experimentally in an AC fracture test. It achieved relatively high accuracy, even under considerable camera rotation angle and large deformation. As a pre-processing technique, the proposed method showed promising performance in assisting the recently developed CrackPropNet for automated crack propagation measurement under a non-perpendicular camera alignment.\n    "
    },
    "2402.19287": {
        "title": "StiefelGen: A Simple, Model Agnostic Approach for Time Series Data Augmentation over Riemannian Manifolds",
        "authors": [
            "Prasad Cheema",
            "Mahito Sugiyama"
        ],
        "comments": "61 pages, 41 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Data augmentation is an area of research which has seen active development in many machine learning fields, such as in image-based learning models, reinforcement learning for self driving vehicles, and general noise injection for point cloud data. However, convincing methods for general time series data augmentation still leaves much to be desired, especially since the methods developed for these models do not readily cross-over. Three common approaches for time series data augmentation include: (i) Constructing a physics-based model and then imbuing uncertainty over the coefficient space (for example), (ii) Adding noise to the observed data set(s), and, (iii) Having access to ample amounts of time series data sets from which a robust generative neural network model can be trained. However, for many practical problems that work with time series data in the industry: (i) One usually does not have access to a robust physical model, (ii) The addition of noise can in of itself require large or difficult assumptions (for example, what probability distribution should be used? Or, how large should the noise variance be?), and, (iii) In practice, it can be difficult to source a large representative time series data base with which to train the neural network model for the underlying problem. In this paper, we propose a methodology which attempts to simultaneously tackle all three of these previous limitations to a large extent. The method relies upon the well-studied matrix differential geometry of the Stiefel manifold, as it proposes a simple way in which time series signals can placed on, and then smoothly perturbed over the manifold. We attempt to clarify how this method works by showcasing several potential use cases which in particular work to take advantage of the unique properties of this underlying manifold.\n    "
    },
    "2402.19294": {
        "title": "Degradation Modeling and Prognostic Analysis Under Unknown Failure Modes",
        "authors": [
            "Ying Fu",
            "Ye Kwon Huh",
            "Kaibo Liu"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Operating units often experience various failure modes in complex systems, leading to distinct degradation paths. Relying on a prognostic model trained on a single failure mode may lead to poor generalization performance across multiple failure modes. Therefore, accurately identifying the failure mode is of critical importance. Current prognostic approaches either ignore failure modes during degradation or assume known failure mode labels, which can be challenging to acquire in practice. Moreover, the high dimensionality and complex relations of sensor signals make it challenging to identify the failure modes accurately. To address these issues, we propose a novel failure mode diagnosis method that leverages a dimension reduction technique called UMAP (Uniform Manifold Approximation and Projection) to project and visualize each unit's degradation trajectory into a lower dimension. Then, using these degradation trajectories, we develop a time series-based clustering method to identify the training units' failure modes. Finally, we introduce a monotonically constrained prognostic model to predict the failure mode labels and RUL of the test units simultaneously using the obtained failure modes of the training units. The proposed prognostic model provides failure mode-specific RUL predictions while preserving the monotonic property of the RUL predictions across consecutive time steps. We evaluate the proposed model using a case study with the aircraft gas turbine engine dataset.\n    "
    },
    "2402.19295": {
        "title": "Anomaly Detection in Offshore Wind Turbine Structures using Hierarchical Bayesian Modelling",
        "authors": [
            "S. M. Smith",
            "A. J. Hughes",
            "T. A. Dardeno",
            "L. A. Bull",
            "N. Dervilis",
            "K. Worden"
        ],
        "comments": "Submitted to International Workshop on Structural Health Monitoring 2023, Stanford University, California, USA",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Population-based structural health monitoring (PBSHM), aims to share information between members of a population. An offshore wind (OW) farm could be considered as a population of nominally-identical wind-turbine structures. However, benign variations exist among members, such as geometry, sea-bed conditions and temperature differences. These factors could influence structural properties and therefore the dynamic response, making it more difficult to detect structural problems via traditional SHM techniques. This paper explores the use of a hierarchical Bayesian model to infer expected soil stiffness distributions at both population and local levels, as a basis to perform anomaly detection, in the form of scour, for new and existing turbines. To do this, observations of natural frequency will be generated as though they are from a small population of wind turbines. Differences between individual observations will be introduced by postulating distributions over the soil stiffness and measurement noise, as well as reducing soil depth (to represent scour), in the case of anomaly detection.\n    "
    },
    "2402.19296": {
        "title": "An AI based Digital Score of Tumour-Immune Microenvironment Predicts Benefit to Maintenance Immunotherapy in Advanced Oesophagogastric Adenocarcinoma",
        "authors": [
            "Quoc Dang Vu",
            "Caroline Fong",
            "Anderley Gordon",
            "Tom Lund",
            "Tatiany L Silveira",
            "Daniel Rodrigues",
            "Katharina von Loga",
            "Shan E Ahmed Raza",
            "David Cunningham",
            "Nasir Rajpoot"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Gastric and oesophageal (OG) cancers are the leading causes of cancer mortality worldwide. In OG cancers, recent studies have showed that PDL1 immune checkpoint inhibitors (ICI) in combination with chemotherapy improves patient survival. However, our understanding of the tumour immune microenvironment in OG cancers remains limited. In this study, we interrogate multiplex immunofluorescence (mIF) images taken from patients with advanced Oesophagogastric Adenocarcinoma (OGA) who received first-line fluoropyrimidine and platinum-based chemotherapy in the PLATFORM trial (NCT02678182) to predict the efficacy of the treatment and to explore the biological basis of patients responding to maintenance durvalumab (PDL1 inhibitor). Our proposed Artificial Intelligence (AI) based marker successfully identified responder from non-responder (p < 0.05) as well as those who could potentially benefit from ICI with statistical significance (p < 0.05) for both progression free and overall survival. Our findings suggest that T cells that express FOXP3 seem to heavily influence the patient treatment response and survival outcome. We also observed that higher levels of CD8+PD1+ cells are consistently linked to poor prognosis for both OS and PFS, regardless of ICI.\n    "
    },
    "2402.19299": {
        "title": "RL-GPT: Integrating Reinforcement Learning and Code-as-policy",
        "authors": [
            "Shaoteng Liu",
            "Haoqi Yuan",
            "Minda Hu",
            "Yanwei Li",
            "Yukang Chen",
            "Shu Liu",
            "Zongqing Lu",
            "Jiaya Jia"
        ],
        "comments": " ",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) have demonstrated proficiency in utilizing various tools by coding, yet they face limitations in handling intricate logic and precise control. In embodied tasks, high-level planning is amenable to direct coding, while low-level actions often necessitate task-specific refinement, such as Reinforcement Learning (RL). To seamlessly integrate both modalities, we introduce a two-level hierarchical framework, RL-GPT, comprising a slow agent and a fast agent. The slow agent analyzes actions suitable for coding, while the fast agent executes coding tasks. This decomposition effectively focuses each agent on specific tasks, proving highly efficient within our pipeline. Our approach outperforms traditional RL methods and existing GPT agents, demonstrating superior efficiency. In the Minecraft game, it rapidly obtains diamonds within a single day on an RTX3090. Additionally, it achieves SOTA performance across all designated MineDojo tasks.\n    "
    },
    "2402.19302": {
        "title": "DiffAssemble: A Unified Graph-Diffusion Model for 2D and 3D Reassembly",
        "authors": [
            "Gianluca Scarpellini",
            "Stefano Fiorini",
            "Francesco Giuliari",
            "Pietro Morerio",
            "Alessio Del Bue"
        ],
        "comments": "Accepted at CVPR2024",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Reassembly tasks play a fundamental role in many fields and multiple approaches exist to solve specific reassembly problems. In this context, we posit that a general unified model can effectively address them all, irrespective of the input data type (images, 3D, etc.). We introduce DiffAssemble, a Graph Neural Network (GNN)-based architecture that learns to solve reassembly tasks using a diffusion model formulation. Our method treats the elements of a set, whether pieces of 2D patch or 3D object fragments, as nodes of a spatial graph. Training is performed by introducing noise into the position and rotation of the elements and iteratively denoising them to reconstruct the coherent initial pose. DiffAssemble achieves state-of-the-art (SOTA) results in most 2D and 3D reassembly tasks and is the first learning-based approach that solves 2D puzzles for both rotation and translation. Furthermore, we highlight its remarkable reduction in run-time, performing 11 times faster than the quickest optimization-based method for puzzle solving. Code available at this https URL\n"
    },
    "2402.19305": {
        "title": "HyenaPixel: Global Image Context with Convolutions",
        "authors": [
            "Julian Spravil",
            "Sebastian Houben",
            "Sven Behnke"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In vision tasks, a larger effective receptive field (ERF) is associated with better performance. While attention natively supports global context, convolution requires multiple stacked layers and a hierarchical structure for large context. In this work, we extend Hyena, a convolution-based attention replacement, from causal sequences to the non-causal two-dimensional image space. We scale the Hyena convolution kernels beyond the feature map size up to 191$\\times$191 to maximize the ERF while maintaining sub-quadratic complexity in the number of pixels. We integrate our two-dimensional Hyena, HyenaPixel, and bidirectional Hyena into the MetaFormer framework. For image categorization, HyenaPixel and bidirectional Hyena achieve a competitive ImageNet-1k top-1 accuracy of 83.0% and 83.5%, respectively, while outperforming other large-kernel networks. Combining HyenaPixel with attention further increases accuracy to 83.6%. We attribute the success of attention to the lack of spatial bias in later stages and support this finding with bidirectional Hyena.\n    "
    },
    "2402.19308": {
        "title": "Loss-Free Machine Unlearning",
        "authors": [
            "Jack Foster",
            "Stefan Schoepf",
            "Alexandra Brintrup"
        ],
        "comments": "Accepted as a Tiny Paper at ICLR 2024",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We present a machine unlearning approach that is both retraining- and label-free. Most existing machine unlearning approaches require a model to be fine-tuned to remove information while preserving performance. This is computationally expensive and necessitates the storage of the whole dataset for the lifetime of the model. Retraining-free approaches often utilise Fisher information, which is derived from the loss and requires labelled data which may not be available. Thus, we present an extension to the Selective Synaptic Dampening algorithm, substituting the diagonal of the Fisher information matrix for the gradient of the l2 norm of the model output to approximate sensitivity. We evaluate our method in a range of experiments using ResNet18 and Vision Transformer. Results show our label-free method is competitive with existing state-of-the-art approaches.\n    "
    },
    "2402.19318": {
        "title": "DISCERN: Designing Decision Support Interfaces to Investigate the Complexities of Workplace Social Decision-Making With Line Managers",
        "authors": [
            "Pranav Khadpe",
            "Lindy Le",
            "Kate Nowak",
            "Shamsi T. Iqbal",
            "Jina Suh"
        ],
        "comments": "CHI 2024",
        "subjects": "Human-Computer Interaction (cs.HC)",
        "abstract": "Line managers form the first level of management in organizations, and must make complex decisions, while maintaining relationships with those impacted by their decisions. Amidst growing interest in technology-supported decision-making at work, their needs remain understudied. Further, most existing design knowledge for supporting social decision-making comes from domains where decision-makers are more socially detached from those they decide for. We conducted iterative design research with line managers within a technology organization, investigating decision-making practices, and opportunities for technological support. Through formative research, development of a decision-representation tool -- DISCERN -- and user enactments, we identify their communication and analysis needs that lack adequate support. We found they preferred tools for externalizing reasoning rather than tools that replace interpersonal interactions, and they wanted tools to support a range of intuitive and calculative decision-making. We discuss how design of social decision-making supports, especially in the workplace, can more explicitly support highly interactional social decision-making.\n    "
    },
    "2402.19328": {
        "title": "Seeking Soulmate via Voice: Understanding Promises and Challenges of Online Synchronized Voice-Based Mobile Dating",
        "authors": [
            "Chenxinran Shen",
            "Yan Xu",
            "Ray LC",
            "Zhicong Lu"
        ],
        "comments": "14 pages, 2 figures. Accepted to ACM CHI 2024. In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI '24)",
        "subjects": "Human-Computer Interaction (cs.HC)",
        "abstract": "Online dating has become a popular way for individuals to connect with potential romantic partners. Many dating apps use personal profiles that include a headshot and self-description, allowing users to present themselves and search for compatible matches. However, this traditional model often has limitations. In this study, we explore a non-traditional voice-based dating app called \"Soul\". Unlike traditional platforms that rely heavily on profile information, Soul facilitates user interactions through voice-based communication. We conducted semi-structured interviews with 18 dedicated Soul users to investigate how they engage with the platform and perceive themselves and others in this unique dating environment. Our findings indicate that the role of voice as a moderator influences impression management and shapes perceptions between the sender and the receiver of the voice. Additionally, the synchronous voice-based and community-based dating model offers benefits to users in the Chinese cultural context. Our study contributes to understanding the affordances introduced by voice-based interactions in online dating in China.\n    "
    },
    "2402.19334": {
        "title": "Here's a Free Lunch: Sanitizing Backdoored Models with Model Merge",
        "authors": [
            "Ansh Arora",
            "Xuanli He",
            "Maximilian Mozes",
            "Srinibas Swain",
            "Mark Dras",
            "Qiongkai Xu"
        ],
        "comments": "work in progress",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "The democratization of pre-trained language models through open-source initiatives has rapidly advanced innovation and expanded access to cutting-edge technologies. However, this openness also brings significant security risks, including backdoor attacks, where hidden malicious behaviors are triggered by specific inputs, compromising natural language processing (NLP) system integrity and reliability. This paper suggests that merging a backdoored model with other homogeneous models can remediate backdoor vulnerabilities even if such models are not entirely secure. In our experiments, we explore various models (BERT-Base, RoBERTa-Large, Llama2-7B, and Mistral-7B) and datasets (SST-2, OLID, AG News, and QNLI). Compared to multiple advanced defensive approaches, our method offers an effective and efficient inference-stage defense against backdoor attacks without additional resources or specific knowledge. Our approach consistently outperforms the other advanced baselines, leading to an average of 75% reduction in the attack success rate. Since model merging has been an established approach for improving model performance, the extra advantage it provides regarding defense can be seen as a cost-free bonus.\n    "
    },
    "2402.19339": {
        "title": "Stitching Gaps: Fusing Situated Perceptual Knowledge with Vision Transformers for High-Level Image Classification",
        "authors": [
            "Delfina Sol Martinez Pandiani",
            "Nicolas Lazzari",
            "Valentina Presutti"
        ],
        "comments": "Preprint",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The increasing demand for automatic high-level image understanding, particularly in detecting abstract concepts (AC) within images, underscores the necessity for innovative and more interpretable approaches. These approaches need to harmonize traditional deep vision methods with the nuanced, context-dependent knowledge humans employ to interpret images at intricate semantic levels. In this work, we leverage situated perceptual knowledge of cultural images to enhance performance and interpretability in AC image classification. We automatically extract perceptual semantic units from images, which we then model and integrate into the ARTstract Knowledge Graph (AKG). This resource captures situated perceptual semantics gleaned from over 14,000 cultural images labeled with ACs. Additionally, we enhance the AKG with high-level linguistic frames. We compute KG embeddings and experiment with relative representations and hybrid approaches that fuse these embeddings with visual transformer embeddings. Finally, for interpretability, we conduct posthoc qualitative analyses by examining model similarities with training instances. Our results show that our hybrid KGE-ViT methods outperform existing techniques in AC image classification. The posthoc interpretability analyses reveal the visual transformer's proficiency in capturing pixel-level visual attributes, contrasting with our method's efficacy in representing more abstract and semantic scene elements. We demonstrate the synergy and complementarity between KGE embeddings' situated perceptual knowledge and deep visual model's sensory-perceptual understanding for AC image classification. This work suggests a strong potential of neuro-symbolic methods for knowledge integration and robust image representation for use in downstream intricate visual comprehension tasks. All the materials and code are available online.\n    "
    },
    "2402.19347": {
        "title": "#PoetsOfInstagram: Navigating The Practices And Challenges Of Novice Poets On Instagram",
        "authors": [
            "Ankolika De",
            "Zhicong Lu"
        ],
        "comments": "16 pages, 2 figures; Accepted to ACM CHI 2024. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (CHI'24)",
        "subjects": "Human-Computer Interaction (cs.HC)",
        "abstract": "Commencing as a photo-sharing platform, Instagram has since become multifaceted, accommodating diverse art forms, with poetry emerging as a prominent one. However, the academic understanding of Instagram's poetry community is limited, yet its significance emerges from its distinctive utilization of a primarily visual social media platform guided by recommendation algorithms for disseminating poetry, further characterized by a predominantly novice creative population. We employ qualitative analysis to explore motivations, experiences, and algorithmic influence within Instagram's poetry community. We demonstrate that participants prioritize conforming to algorithmic constraints for visibility, yet maintain their community's values of integrity and originality, illustrating the tension between algorithmic growth and participant authenticity. We introduce the concept of Algorithmically Mediated Creative Labor, a phenomenon specific to non-monetizing creative users who are impacted by the prioritization of professional creators and continually adapt their creative endeavors to align with platform logic, thereby affecting their motivation and creative outputs.\n    "
    },
    "2402.19348": {
        "title": "Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook",
        "authors": [
            "Xingchen Zou",
            "Yibo Yan",
            "Xixuan Hao",
            "Yuehong Hu",
            "Haomin Wen",
            "Erdong Liu",
            "Junbo Zhang",
            "Yong Li",
            "Tianrui Li",
            "Yu Zheng",
            "Yuxuan Liang"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "As cities continue to burgeon, Urban Computing emerges as a pivotal discipline for sustainable development by harnessing the power of cross-domain data fusion from diverse sources (e.g., geographical, traffic, social media, and environmental data) and modalities (e.g., spatio-temporal, visual, and textual modalities). Recently, we are witnessing a rising trend that utilizes various deep-learning methods to facilitate cross-domain data fusion in smart cities. To this end, we propose the first survey that systematically reviews the latest advancements in deep learning-based data fusion methods tailored for urban computing. Specifically, we first delve into data perspective to comprehend the role of each modality and data source. Secondly, we classify the methodology into four primary categories: feature-based, alignment-based, contrast-based, and generation-based fusion methods. Thirdly, we further categorize multi-modal urban applications into seven types: urban planning, transportation, economy, public safety, society, environment, and energy. Compared with previous surveys, we focus more on the synergy of deep learning methods with urban computing applications. Furthermore, we shed light on the interplay between Large Language Models (LLMs) and urban computing, postulating future research directions that could revolutionize the field. We firmly believe that the taxonomy, progress, and prospects delineated in our survey stand poised to significantly enrich the research community. The summary of the comprehensive and up-to-date paper list can be found at this https URL.\n    "
    },
    "2402.19369": {
        "title": "Structure Preserving Diffusion Models",
        "authors": [
            "Haoye Lu",
            "Spencer Szabados",
            "Yaoliang Yu"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Diffusion models have become the leading distribution-learning method in recent years. Herein, we introduce structure-preserving diffusion processes, a family of diffusion processes for learning distributions that possess additional structure, such as group symmetries, by developing theoretical conditions under which the diffusion transition steps preserve said symmetry. While also enabling equivariant data sampling trajectories, we exemplify these results by developing a collection of different symmetry equivariant diffusion models capable of learning distributions that are inherently symmetric. Empirical studies, over both synthetic and real-world datasets, are used to validate the developed models adhere to the proposed theory and are capable of achieving improved performance over existing methods in terms of sample equality. We also show how the proposed models can be used to achieve theoretically guaranteed equivariant image noise reduction without prior knowledge of the image orientation.\n    "
    },
    "2402.19371": {
        "title": "OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models",
        "authors": [
            "Jenish Maharjan",
            "Anurag Garikipati",
            "Navan Preet Singh",
            "Leo Cyrus",
            "Mayank Sharma",
            "Madalina Ciobanu",
            "Gina Barnes",
            "Rahul Thapa",
            "Qingqing Mao",
            "Ritankar Das"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "LLMs have become increasingly capable at accomplishing a range of specialized-tasks and can be utilized to expand equitable access to medical knowledge. Most medical LLMs have involved extensive fine-tuning, leveraging specialized medical data and significant, thus costly, amounts of computational power. Many of the top performing LLMs are proprietary and their access is limited to very few research groups. However, open-source (OS) models represent a key area of growth for medical LLMs due to significant improvements in performance and an inherent ability to provide the transparency and compliance required in healthcare. We present OpenMedLM, a prompting platform which delivers state-of-the-art (SOTA) performance for OS LLMs on medical benchmarks. We evaluated a range of OS foundation LLMs (7B-70B) on four medical benchmarks (MedQA, MedMCQA, PubMedQA, MMLU medical-subset). We employed a series of prompting strategies, including zero-shot, few-shot, chain-of-thought (random selection and kNN selection), and ensemble/self-consistency voting. We found that OpenMedLM delivers OS SOTA results on three common medical LLM benchmarks, surpassing the previous best performing OS models that leveraged computationally costly extensive fine-tuning. The model delivers a 72.6% accuracy on the MedQA benchmark, outperforming the previous SOTA by 2.4%, and achieves 81.7% accuracy on the MMLU medical-subset, establishing itself as the first OS LLM to surpass 80% accuracy on this benchmark. Our results highlight medical-specific emergent properties in OS LLMs which have not yet been documented to date elsewhere, and showcase the benefits of further leveraging prompt engineering to improve the performance of accessible LLMs for medical applications.\n    "
    },
    "2402.19375": {
        "title": "Unveiling Internet Censorship: Analysing the Impact of Nation States' Content Control Efforts on Internet Architecture and Routing Patterns",
        "authors": [
            "Joshua Levett",
            "Vassilios Vassilakis",
            "Poonam Yadav"
        ],
        "comments": " ",
        "subjects": "Networking and Internet Architecture (cs.NI)",
        "abstract": "Heightened interest from nation states to perform content censorship make it evermore critical to identify the impact of censorship efforts on the Internet. We undertake a study of Internet architecture, capturing the state of Internet topology with greater completeness than existing state-of-the-art. We describe our methodology for this, including the tooling we create to collect and process data from a wide range of sources. We analyse this data to find key patterns in nation states with higher censorship, discovering a funnelling effect wherein higher Internet censorship effort is reflected in a constraining effect on a state's Internet routing architecture. However, there are a small number of nation states that do not follow this trend, for which we provide an analysis and explanation, demonstrating a relationship between geographical factors in addition to geopolitics. In summary, our work provides a deeper understanding of how these censorship measures impact the overall functioning and dynamics of the Internet.\n    "
    },
    "2402.19385": {
        "title": "Towards Safe and Reliable Autonomous Driving: Dynamic Occupancy Set Prediction",
        "authors": [
            "Wenbo Shao",
            "Jiahui Xu",
            "Wenhao Yu",
            "Jun Li",
            "Hong Wang"
        ],
        "comments": "9 pages, 5 figures",
        "subjects": "Robotics (cs.RO)",
        "abstract": "In the rapidly evolving field of autonomous driving, accurate trajectory prediction is pivotal for vehicular safety. However, trajectory predictions often deviate from actual paths, particularly in complex and challenging environments, leading to significant errors. To address this issue, our study introduces a novel method for Dynamic Occupancy Set (DOS) prediction, enhancing trajectory prediction capabilities. This method effectively combines advanced trajectory prediction networks with a DOS prediction module, overcoming the shortcomings of existing models. It provides a comprehensive and adaptable framework for predicting the potential occupancy sets of traffic participants. The main contributions of this research include: 1) A novel DOS prediction model tailored for complex scenarios, augmenting traditional trajectory prediction; 2) The development of unique DOS representations and evaluation metrics; 3) Extensive validation through experiments, demonstrating enhanced performance and adaptability. This research contributes to the advancement of safer and more efficient intelligent vehicle and transportation systems.\n    "
    },
    "2402.19401": {
        "title": "Assessing Visually-Continuous Corruption Robustness of Neural Networks Relative to Human Performance",
        "authors": [
            "Huakun Shen",
            "Boyue Caroline Hu",
            "Krzysztof Czarnecki",
            "Lina Marsso",
            "Marsha Chechik"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "While Neural Networks (NNs) have surpassed human accuracy in image classification on ImageNet, they often lack robustness against image corruption, i.e., corruption robustness. Yet such robustness is seemingly effortless for human perception. In this paper, we propose visually-continuous corruption robustness (VCR) -- an extension of corruption robustness to allow assessing it over the wide and continuous range of changes that correspond to the human perceptive quality (i.e., from the original image to the full distortion of all perceived visual information), along with two novel human-aware metrics for NN evaluation. To compare VCR of NNs with human perception, we conducted extensive experiments on 14 commonly used image corruptions with 7,718 human participants and state-of-the-art robust NN models with different training objectives (e.g., standard, adversarial, corruption robustness), different architectures (e.g., convolution NNs, vision transformers), and different amounts of training data augmentation. Our study showed that: 1) assessing robustness against continuous corruption can reveal insufficient robustness undetected by existing benchmarks; as a result, 2) the gap between NN and human robustness is larger than previously known; and finally, 3) some image corruptions have a similar impact on human perception, offering opportunities for more cost-effective robustness assessments. Our validation set with 14 image corruptions, human robustness data, and the evaluation code is provided as a toolbox and a benchmark.\n    "
    },
    "2402.19402": {
        "title": "A Scalable and Transferable Time Series Prediction Framework for Demand Forecasting",
        "authors": [
            "Young-Jin Park",
            "Donghyun Kim",
            "Fr\u00e9d\u00e9ric Odermatt",
            "Juho Lee",
            "Kyung-Min Kim"
        ],
        "comments": "Published as a full paper at ICDM 2022",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Time series forecasting is one of the most essential and ubiquitous tasks in many business problems, including demand forecasting and logistics optimization. Traditional time series forecasting methods, however, have resulted in small models with limited expressive power because they have difficulty in scaling their model size up while maintaining high accuracy. In this paper, we propose Forecasting orchestra (Forchestra), a simple but powerful framework capable of accurately predicting future demand for a diverse range of items. We empirically demonstrate that the model size is scalable to up to 0.8 billion parameters. The proposed method not only outperforms existing forecasting models with a significant margin, but it could generalize well to unseen data points when evaluated in a zero-shot fashion on downstream datasets. Last but not least, we present extensive qualitative and quantitative studies to analyze how the proposed model outperforms baseline models and differs from conventional approaches. The original paper was presented as a full paper at ICDM 2022 and is available at: this https URL.\n    "
    },
    "2402.19407": {
        "title": "MENTOR: Multi-level Self-supervised Learning for Multimodal Recommendation",
        "authors": [
            "Jinfeng Xu",
            "Zheyu Chen",
            "Shuo Yang",
            "Jinze Li",
            "Hewei Wang",
            "Edith C.-H. Ngai"
        ],
        "comments": " ",
        "subjects": "Information Retrieval (cs.IR)",
        "abstract": "With the increasing multimedia information, multimodal recommendation has received extensive attention. It utilizes multimodal information to alleviate the data sparsity problem in recommendation systems, thus improving recommendation accuracy. However, the reliance on labeled data severely limits the performance of multimodal recommendation models. Recently, self-supervised learning has been used in multimodal recommendations to mitigate the label sparsity problem. Nevertheless, the state-of-the-art methods cannot avoid the modality noise when aligning multimodal information due to the large differences in the distributions of different modalities. To this end, we propose a Multi-level sElf-supervised learNing for mulTimOdal Recommendation (MENTOR) method to address the label sparsity problem and the modality alignment problem. Specifically, MENTOR first enhances the specific features of each modality using the graph convolutional network (GCN) and fuses the visual and textual modalities. It then enhances the item representation via the item semantic graph for all modalities, including the fused modality. Then, it introduces two multilevel self-supervised tasks: the multilevel cross-modal alignment task and the general feature enhancement task. The multilevel cross-modal alignment task aligns each modality under the guidance of the ID embedding from multiple levels while maintaining the historical interaction information. The general feature enhancement task enhances the general feature from both the graph and feature perspectives to improve the robustness of our model. Extensive experiments on three publicly available datasets demonstrate the effectiveness of our method. Our code is publicly available at this https URL.\n    "
    },
    "2402.19411": {
        "title": "PaECTER: Patent-level Representation Learning using Citation-informed Transformers",
        "authors": [
            "Mainak Ghosh",
            "Sebastian Erhardt",
            "Michael E. Rose",
            "Erik Buunk",
            "Dietmar Harhoff"
        ],
        "comments": "7 pages, 3 figures",
        "subjects": "Information Retrieval (cs.IR)",
        "abstract": "PaECTER is a publicly available, open-source document-level encoder specific for patents. We fine-tune BERT for Patents with examiner-added citation information to generate numerical representations for patent documents. PaECTER performs better in similarity tasks than current state-of-the-art models used in the patent domain. More specifically, our model outperforms the next-best patent specific pre-trained language model (BERT for Patents) on our patent citation prediction test dataset on two different rank evaluation metrics. PaECTER predicts at least one most similar patent at a rank of 1.32 on average when compared against 25 irrelevant patents. Numerical representations generated by PaECTER from patent text can be used for downstream tasks such as classification, tracing knowledge flows, or semantic similarity search. Semantic similarity search is especially relevant in the context of prior art search for both inventors and patent examiners. PaECTER is available on Hugging Face.\n    "
    },
    "2402.19423": {
        "title": "Leveraging AI Predicted and Expert Revised Annotations in Interactive Segmentation: Continual Tuning or Full Training?",
        "authors": [
            "Tiezheng Zhang",
            "Xiaoxi Chen",
            "Chongyu Qu",
            "Alan Yuille",
            "Zongwei Zhou"
        ],
        "comments": "IEEE International Symposium on Biomedical Imaging (ISBI)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Interactive segmentation, an integration of AI algorithms and human expertise, premises to improve the accuracy and efficiency of curating large-scale, detailed-annotated datasets in healthcare. Human experts revise the annotations predicted by AI, and in turn, AI improves its predictions by learning from these revised annotations. This interactive process continues to enhance the quality of annotations until no major revision is needed from experts. The key challenge is how to leverage AI predicted and expert revised annotations to iteratively improve the AI. Two problems arise: (1) The risk of catastrophic forgetting--the AI tends to forget the previously learned classes if it is only retrained using the expert revised classes. (2) Computational inefficiency when retraining the AI using both AI predicted and expert revised annotations; moreover, given the dominant AI predicted annotations in the dataset, the contribution of newly revised annotations--often account for a very small fraction--to the AI training remains marginal. This paper proposes Continual Tuning to address the problems from two perspectives: network design and data reuse. Firstly, we design a shared network for all classes followed by class-specific networks dedicated to individual classes. To mitigate forgetting, we freeze the shared network for previously learned classes and only update the class-specific network for revised classes. Secondly, we reuse a small fraction of data with previous annotations to avoid over-computing. The selection of such data relies on the importance estimate of each data. The importance score is computed by combining the uncertainty and consistency of AI predictions. Our experiments demonstrate that Continual Tuning achieves a speed 16x greater than repeatedly training AI from scratch without compromising the performance.\n    "
    },
    "2402.19427": {
        "title": "Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models",
        "authors": [
            "Soham De",
            "Samuel L. Smith",
            "Anushan Fernando",
            "Aleksandar Botev",
            "George Cristian-Muraru",
            "Albert Gu",
            "Ruba Haroun",
            "Leonard Berrada",
            "Yutian Chen",
            "Srivatsan Srinivasan",
            "Guillaume Desjardins",
            "Arnaud Doucet",
            "David Budden",
            "Yee Whye Teh",
            "Razvan Pascanu",
            "Nando De Freitas",
            "Caglar Gulcehre"
        ],
        "comments": "25 pages, 11 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.\n    "
    },
    "2402.19432": {
        "title": "Pushing the Limits of Cross-Embodiment Learning for Manipulation and Navigation",
        "authors": [
            "Jonathan Yang",
            "Catherine Glossop",
            "Arjun Bhorkar",
            "Dhruv Shah",
            "Quan Vuong",
            "Chelsea Finn",
            "Dorsa Sadigh",
            "Sergey Levine"
        ],
        "comments": "16 pages, 9 figures",
        "subjects": "Robotics (cs.RO)",
        "abstract": "Recent years in robotics and imitation learning have shown remarkable progress in training large-scale foundation models by leveraging data across a multitude of embodiments. The success of such policies might lead us to wonder: just how diverse can the robots in the training set be while still facilitating positive transfer? In this work, we study this question in the context of heterogeneous embodiments, examining how even seemingly very different domains, such as robotic navigation and manipulation, can provide benefits when included in the training data for the same model. We train a single goal-conditioned policy that is capable of controlling robotic arms, quadcopters, quadrupeds, and mobile bases. We then investigate the extent to which transfer can occur across navigation and manipulation on these embodiments by framing them as a single goal-reaching task. We find that co-training with navigation data can enhance robustness and performance in goal-conditioned manipulation with a wrist-mounted camera. We then deploy our policy trained only from navigation-only and static manipulation-only data on a mobile manipulator, showing that it can control a novel embodiment in a zero-shot manner. These results provide evidence that large-scale robotic policies can benefit from data collected across various embodiments. Further information and robot videos can be found on our project website this http URL.\n    "
    },
    "2402.19446": {
        "title": "ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL",
        "authors": [
            "Yifei Zhou",
            "Andrea Zanette",
            "Jiayi Pan",
            "Sergey Levine",
            "Aviral Kumar"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "A broad use case of large language models (LLMs) is in goal-directed decision-making tasks (or \"agent\" tasks), where an LLM needs to not just generate completions for a given prompt, but rather make intelligent decisions over a multi-turn interaction to accomplish a task (e.g., when interacting with the web, using tools, or providing customer support). Reinforcement learning (RL) provides a general paradigm to address such agent tasks, but current RL methods for LLMs largely focus on optimizing single-turn rewards. By construction, most single-turn RL methods cannot endow LLMs with the ability to intelligently seek information over multiple turns, perform credit assignment, or reason about their past actions -- all of which are critical in agent tasks. This raises the question: how can we design effective and efficient multi-turn RL algorithms for LLMs? In this paper, we develop a framework for building multi-turn RL algorithms for fine-tuning LLMs, that preserves the flexibility of existing single-turn RL methods for LLMs (e.g., proximal policy optimization), while accommodating multiple turns, long horizons, and delayed rewards effectively. To do this, our framework adopts a hierarchical RL approach and runs two RL algorithms in parallel: a high-level off-policy value-based RL algorithm to aggregate reward over utterances, and a low-level RL algorithm that utilizes this high-level value function to train a token policy within each utterance or turn. Our hierarchical framework, Actor-Critic Framework with a Hierarchical Structure (ArCHer), can also give rise to other RL methods. Empirically, we find that ArCHer significantly improves efficiency and performance on agent tasks, attaining a sample efficiency of about 100x over existing methods, while also improving with larger model capacity (upto the 7 billion scale that we tested on).\n    "
    },
    "2402.19450": {
        "title": "Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap",
        "authors": [
            "Saurabh Srivastava",
            "Annarose M B",
            "Anto P V",
            "Shashank Menon",
            "Ajay Sukumar",
            "Adwaith Samod T",
            "Alan Philipose",
            "Stevin Prince",
            "Sooraj Thomas"
        ],
        "comments": "37 pages, 10 figures",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "We propose a framework for robust evaluation of reasoning capabilities of language models, using functional variants of benchmarks. Models that solve a reasoning test should exhibit no difference in performance over the static version of a problem compared to a snapshot of the functional variant. We have rewritten the relevant fragment of the MATH benchmark into its functional variant MATH(), with functionalization of other benchmarks to follow. When evaluating current state-of-the-art models over snapshots of MATH(), we find a reasoning gap -- the percentage difference between the static and functional accuracies. We find reasoning gaps from 58.35% to 80.31% among the state-of-the-art closed and open weights models that perform well on static benchmarks, with the caveat that the gaps are likely to be smaller with more sophisticated prompting strategies. Here we show that models which anecdotally have good reasoning performance over real-world tasks, have quantifiable lower gaps, motivating the open problem of building \"gap 0\" models. Code for evaluation and new evaluation datasets, three MATH() snapshots, are publicly available at this https URL.\n    "
    },
    "2402.19464": {
        "title": "Curiosity-driven Red-teaming for Large Language Models",
        "authors": [
            "Zhang-Wei Hong",
            "Idan Shenfeld",
            "Tsun-Hsuan Wang",
            "Yung-Sung Chuang",
            "Aldo Pareja",
            "James Glass",
            "Akash Srivastava",
            "Pulkit Agrawal"
        ],
        "comments": "Published at ICLR 2024",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content. To probe when an LLM generates unwanted content, the current paradigm is to recruit a \\textit{red team} of human testers to design input prompts (i.e., test cases) that elicit undesirable responses from LLMs. However, relying solely on human testers is expensive and time-consuming. Recent works automate red teaming by training a separate red team LLM with reinforcement learning (RL) to generate test cases that maximize the chance of eliciting undesirable responses from the target LLM. However, current RL methods are only able to generate a small number of effective test cases resulting in a low coverage of the span of prompts that elicit undesirable responses from the target LLM. To overcome this limitation, we draw a connection between the problem of increasing the coverage of generated test cases and the well-studied approach of curiosity-driven exploration that optimizes for novelty. Our method of curiosity-driven red teaming (CRT) achieves greater coverage of test cases while mantaining or increasing their effectiveness compared to existing methods. Our method, CRT successfully provokes toxic responses from LLaMA2 model that has been heavily fine-tuned using human preferences to avoid toxic outputs. Code is available at \\url{this https URL}\n    "
    },
    "2402.19465": {
        "title": "Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models",
        "authors": [
            "Chen Qian",
            "Jie Zhang",
            "Wei Yao",
            "Dongrui Liu",
            "Zhenfei Yin",
            "Yu Qiao",
            "Yong Liu",
            "Jing Shao"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Ensuring the trustworthiness of large language models (LLMs) is crucial. Most studies concentrate on fully pre-trained LLMs to better understand and improve LLMs' trustworthiness. In this paper, to reveal the untapped potential of pre-training, we pioneer the exploration of LLMs' trustworthiness during this period, focusing on five key dimensions: reliability, privacy, toxicity, fairness, and robustness. To begin with, we apply linear probing to LLMs. The high probing accuracy suggests that \\textit{LLMs in early pre-training can already distinguish concepts in each trustworthiness dimension}. Therefore, to further uncover the hidden possibilities of pre-training, we extract steering vectors from a LLM's pre-training checkpoints to enhance the LLM's trustworthiness. Finally, inspired by~\\citet{choi2023understanding} that mutual information estimation is bounded by linear probing accuracy, we also probe LLMs with mutual information to investigate the dynamics of trustworthiness during pre-training. We are the first to observe a similar two-phase phenomenon: fitting and compression~\\citep{shwartz2017opening}. This research provides an initial exploration of trustworthiness modeling during LLM pre-training, seeking to unveil new insights and spur further developments in the field. We will make our code publicly accessible at \\url{this https URL}.\n    "
    },
    "2402.19469": {
        "title": "Humanoid Locomotion as Next Token Prediction",
        "authors": [
            "Ilija Radosavovic",
            "Bike Zhang",
            "Baifeng Shi",
            "Jathushan Rajasegaran",
            "Sarthak Kamat",
            "Trevor Darrell",
            "Koushil Sreenath",
            "Jitendra Malik"
        ],
        "comments": " ",
        "subjects": "Robotics (cs.RO)",
        "abstract": "We cast real-world humanoid control as a next token prediction problem, akin to predicting the next word in language. Our model is a causal transformer trained via autoregressive prediction of sensorimotor trajectories. To account for the multi-modal nature of the data, we perform prediction in a modality-aligned way, and for each input token predict the next token from the same modality. This general formulation enables us to leverage data with missing modalities, like video trajectories without actions. We train our model on a collection of simulated trajectories coming from prior neural network policies, model-based controllers, motion capture data, and YouTube videos of humans. We show that our model enables a full-sized humanoid to walk in San Francisco zero-shot. Our model can transfer to the real world even when trained on only 27 hours of walking data, and can generalize to commands not seen during training like walking backward. These findings suggest a promising path toward learning challenging real-world control tasks by generative modeling of sensorimotor trajectories.\n    "
    },
    "2402.19471": {
        "title": "Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling",
        "authors": [
            "Gabriel Grand",
            "Valerio Pepe",
            "Jacob Andreas",
            "Joshua B. Tenenbaum"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Questions combine our mastery of language with our remarkable facility for reasoning about uncertainty. How do people navigate vast hypothesis spaces to pose informative questions given limited cognitive resources? We study these tradeoffs in a classic grounded question-asking task based on the board game Battleship. Our language-informed program sampling (LIPS) model uses large language models (LLMs) to generate natural language questions, translate them into symbolic programs, and evaluate their expected information gain. We find that with a surprisingly modest resource budget, this simple Monte Carlo optimization strategy yields informative questions that mirror human performance across varied Battleship board scenarios. In contrast, LLM-only baselines struggle to ground questions in the board state; notably, GPT-4V provides no improvement over non-visual baselines. Our results illustrate how Bayesian models of question-asking can leverage the statistics of language to capture human priors, while highlighting some shortcomings of pure LLMs as grounded reasoners.\n    "
    },
    "2402.19472": {
        "title": "Lifelong Benchmarks: Efficient Model Evaluation in an Era of Rapid Progress",
        "authors": [
            "Ameya Prabhu",
            "Vishaal Udandarao",
            "Philip Torr",
            "Matthias Bethge",
            "Adel Bibi",
            "Samuel Albanie"
        ],
        "comments": " ",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Standardized benchmarks drive progress in machine learning. However, with repeated testing, the risk of overfitting grows as algorithms over-exploit benchmark idiosyncrasies. In our work, we seek to mitigate this challenge by compiling ever-expanding large-scale benchmarks called Lifelong Benchmarks. As exemplars of our approach, we create Lifelong-CIFAR10 and Lifelong-ImageNet, containing (for now) 1.69M and 1.98M test samples, respectively. While reducing overfitting, lifelong benchmarks introduce a key challenge: the high cost of evaluating a growing number of models across an ever-expanding sample set. To address this challenge, we also introduce an efficient evaluation framework: Sort \\& Search (S&S), which reuses previously evaluated models by leveraging dynamic programming algorithms to selectively rank and sub-select test samples, enabling cost-effective lifelong benchmarking. Extensive empirical evaluations across 31,000 models demonstrate that S&S achieves highly-efficient approximate accuracy measurement, reducing compute cost from 180 GPU days to 5 GPU hours (1000x reduction) on a single A100 GPU, with low approximation error. As such, lifelong benchmarks offer a robust, practical solution to the \"benchmark exhaustion\" problem.\n    "
    },
    "2402.19479": {
        "title": "Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers",
        "authors": [
            "Tsai-Shien Chen",
            "Aliaksandr Siarohin",
            "Willi Menapace",
            "Ekaterina Deyneka",
            "Hsiang-wei Chao",
            "Byung Eun Jeon",
            "Yuwei Fang",
            "Hsin-Ying Lee",
            "Jian Ren",
            "Ming-Hsuan Yang",
            "Sergey Tulyakov"
        ],
        "comments": "CVPR 2024. Project Page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The quality of the data and annotation upper-bounds the quality of a downstream model. While there exist large text corpora and image-text pairs, high-quality video-text data is much harder to collect. First of all, manual labeling is more time-consuming, as it requires an annotator to watch an entire video. Second, videos have a temporal dimension, consisting of several scenes stacked together, and showing multiple actions. Accordingly, to establish a video dataset with high-quality captions, we propose an automatic approach leveraging multimodal inputs, such as textual video description, subtitles, and individual video frames. Specifically, we curate 3.8M high-resolution videos from the publicly available HD-VILA-100M dataset. We then split them into semantically consistent video clips, and apply multiple cross-modality teacher models to obtain captions for each video. Next, we finetune a retrieval model on a small subset where the best caption of each video is manually selected and then employ the model in the whole dataset to select the best caption as the annotation. In this way, we get 70M videos paired with high-quality text captions. We dub the dataset as Panda-70M. We show the value of the proposed dataset on three downstream tasks: video captioning, video and text retrieval, and text-driven video generation. The models trained on the proposed data score substantially better on the majority of metrics across all the tasks.\n    "
    }
}