{
    "2307.07515": {
        "title": "Artificial intelligence is algorithmic mimicry: why artificial \"agents\" are not (and won't be) proper agents",
        "authors": [
            "Johannes Jaeger"
        ],
        "comments": " ",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "What is the prospect of developing artificial general intelligence (AGI)? I investigate this question by systematically comparing living and algorithmic systems, with a special focus on the notion of \"agency.\" There are three fundamental differences to consider: (1) Living systems are autopoietic, that is, self-manufacturing, and therefore able to set their own intrinsic goals, while algorithms exist in a computational environment with target functions that are both provided by an external agent. (2) Living systems are embodied in the sense that there is no separation between their symbolic and physical aspects, while algorithms run on computational architectures that maximally isolate software from hardware. (3) Living systems experience a large world, in which most problems are ill-defined (and not all definable), while algorithms exist in a small world, in which all problems are well-defined. These three differences imply that living and algorithmic systems have very different capabilities and limitations. In particular, it is extremely unlikely that true AGI (beyond mere mimicry) can be developed in the current algorithmic framework of AI research. Consequently, discussions about the proper development and deployment of algorithmic tools should be shaped around the dangers and opportunities of current narrow AI, not the extremely unlikely prospect of the emergence of true agency in artificial systems.\n    "
    },
    "2401.12413": {
        "title": "How Far Can 100 Samples Go? Unlocking Overall Zero-Shot Multilingual Translation via Tiny Multi-Parallel Data",
        "authors": [
            "Di Wu",
            "Shaomu Tan",
            "Yan Meng",
            "David Stap",
            "Christof Monz"
        ],
        "comments": "15 pages, 5 figures",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Zero-shot translation aims to translate between language pairs not seen during training in Multilingual Machine Translation (MMT) and is largely considered an open problem. A common, albeit resource-consuming, solution is to add as many related translation directions as possible to the training corpus. In this paper, we show that for an English-centric model, surprisingly large zero-shot improvements can be achieved by simply fine-tuning with a very small amount of multi-parallel data. For example, on the EC30 dataset, we obtain up to +21.7 ChrF non-English overall improvements (870 directions) by using only 100 multi-parallel samples while preserving English-centric translation quality. When investigating the size effect of fine-tuning data and its transfer capabilities, we found that already a small, randomly sampled set of fine-tuning directions is sufficient to achieve comparable improvements. The resulting non-English performance is close to the complete translation upper bound. Even in a minimal setting -- fine-tuning with only one single sample -- the well-known off-target issue is almost completely resolved, explaining parts -- but not all -- of the observed improvements in translation quality.\n    "
    },
    "2402.16860": {
        "title": "Interactive Mars Image Content-Based Search with Interpretable Machine Learning",
        "authors": [
            "Bhavan Vasu",
            "Steven Lu",
            "Emily Dunkel",
            "Kiri L. Wagstaff",
            "Kevin Grimes",
            "Michael McAuley"
        ],
        "comments": "7 pages, 6 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The NASA Planetary Data System (PDS) hosts millions of images of planets, moons, and other bodies collected throughout many missions. The ever-expanding nature of data and user engagement demands an interpretable content classification system to support scientific discovery and individual curiosity. In this paper, we leverage a prototype-based architecture to enable users to understand and validate the evidence used by a classifier trained on images from the Mars Science Laboratory (MSL) Curiosity rover mission. In addition to providing explanations, we investigate the diversity and correctness of evidence used by the content-based classifier. The work presented in this paper will be deployed on the PDS Image Atlas, replacing its non-interpretable counterpart.\n    "
    },
    "2402.17110": {
        "title": "Sinkhorn Distance Minimization for Knowledge Distillation",
        "authors": [
            "Xiao Cui",
            "Yulei Qin",
            "Yuting Gao",
            "Enwei Zhang",
            "Zihan Xu",
            "Tong Wu",
            "Ke Li",
            "Xing Sun",
            "Wengang Zhou",
            "Houqiang Li"
        ],
        "comments": "Accepted by COLING 2024",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Knowledge distillation (KD) has been widely adopted to compress large language models (LLMs). Existing KD methods investigate various divergence measures including the Kullback-Leibler (KL), reverse Kullback-Leibler (RKL), and Jensen-Shannon (JS) divergences. However, due to limitations inherent in their assumptions and definitions, these measures fail to deliver effective supervision when few distribution overlap exists between the teacher and the student. In this paper, we show that the aforementioned KL, RKL, and JS divergences respectively suffer from issues of mode-averaging, mode-collapsing, and mode-underestimation, which deteriorates logits-based KD for diverse NLP tasks. We propose the Sinkhorn Knowledge Distillation (SinKD) that exploits the Sinkhorn distance to ensure a nuanced and precise assessment of the disparity between teacher and student distributions. Besides, profit by properties of the Sinkhorn metric, we can get rid of sample-wise KD that restricts the perception of divergence in each teacher-student sample pair. Instead, we propose a batch-wise reformulation to capture geometric intricacies of distributions across samples in the high-dimensional space. Comprehensive evaluation on GLUE and SuperGLUE, in terms of comparability, validity, and generalizability, highlights our superiority over state-of-the-art methods on all kinds of LLMs with encoder-only, encoder-decoder, and decoder-only architectures.\n    "
    },
    "2402.17229": {
        "title": "Preserving Fairness Generalization in Deepfake Detection",
        "authors": [
            "Li Lin",
            "Xinan He",
            "Yan Ju",
            "Xin Wang",
            "Feng Ding",
            "Shu Hu"
        ],
        "comments": "Accepted by The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2024)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Although effective deepfake detection models have been developed in recent years, recent studies have revealed that these models can result in unfair performance disparities among demographic groups, such as race and gender. This can lead to particular groups facing unfair targeting or exclusion from detection, potentially allowing misclassified deepfakes to manipulate public opinion and undermine trust in the model. The existing method for addressing this problem is providing a fair loss function. It shows good fairness performance for intra-domain evaluation but does not maintain fairness for cross-domain testing. This highlights the significance of fairness generalization in the fight against deepfakes. In this work, we propose the first method to address the fairness generalization problem in deepfake detection by simultaneously considering features, loss, and optimization aspects. Our method employs disentanglement learning to extract demographic and domain-agnostic forgery features, fusing them to encourage fair learning across a flattened loss landscape. Extensive experiments on prominent deepfake datasets demonstrate our method's effectiveness, surpassing state-of-the-art approaches in preserving fairness during cross-domain deepfake detection. The code is available at this https URL\n"
    },
    "2402.17236": {
        "title": "A Review of Data Mining in Personalized Education: Current Trends and Future Prospects",
        "authors": [
            "Zhang Xiong",
            "Haoxuan Li",
            "Zhuang Liu",
            "Zhuofan Chen",
            "Hao Zhou",
            "Wenge Rong",
            "Yuanxin Ouyang"
        ],
        "comments": "25 pages, 5 figures",
        "subjects": "Computers and Society (cs.CY)",
        "abstract": "Personalized education, tailored to individual student needs, leverages educational technology and artificial intelligence (AI) in the digital age to enhance learning effectiveness. The integration of AI in educational platforms provides insights into academic performance, learning preferences, and behaviors, optimizing the personal learning process. Driven by data mining techniques, it not only benefits students but also provides educators and institutions with tools to craft customized learning experiences. To offer a comprehensive review of recent advancements in personalized educational data mining, this paper focuses on four primary scenarios: educational recommendation, cognitive diagnosis, knowledge tracing, and learning analysis. This paper presents a structured taxonomy for each area, compiles commonly used datasets, and identifies future research directions, emphasizing the role of data mining in enhancing personalized education and paving the way for future exploration and innovation.\n    "
    },
    "2402.17311": {
        "title": "SKT5SciSumm -- A Hybrid Generative Approach for Multi-Document Scientific Summarization",
        "authors": [
            "Huy Quoc To",
            "Hung-Nghiep Tran",
            "Andr'e Greiner-Petter",
            "Felix Beierle",
            "Akiko Aizawa"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Summarization for scientific text has shown significant benefits both for the research community and human society. Given the fact that the nature of scientific text is distinctive and the input of the multi-document summarization task is substantially long, the task requires sufficient embedding generation and text truncation without losing important information. To tackle these issues, in this paper, we propose SKT5SciSumm - a hybrid framework for multi-document scientific summarization (MDSS). We leverage the Sentence-Transformer version of Scientific Paper Embeddings using Citation-Informed Transformers (SPECTER) to encode and represent textual sentences, allowing for efficient extractive summarization using k-means clustering. We employ the T5 family of models to generate abstractive summaries using extracted sentences. SKT5SciSumm achieves state-of-the-art performance on the Multi-XScience dataset. Through extensive experiments and evaluation, we showcase the benefits of our model by using less complicated models to achieve remarkable results, thereby highlighting its potential in advancing the field of multi-document summarization for scientific text.\n    "
    },
    "2402.17323": {
        "title": "SDDGR: Stable Diffusion-based Deep Generative Replay for Class Incremental Object Detection",
        "authors": [
            "Junsu Kim",
            "Hoseong Cho",
            "Jihyeon Kim",
            "Yihalem Yimolal Tiruneh",
            "Seungryul Baek"
        ],
        "comments": "Accepted to CVPR 2024. We will post a camera-ready version later",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In the field of class incremental learning (CIL), generative replay has become increasingly prominent as a method to mitigate the catastrophic forgetting, alongside the continuous improvements in generative models. However, its application in class incremental object detection (CIOD) has been significantly limited, primarily due to the complexities of scenes involving multiple labels. In this paper, we propose a novel approach called stable diffusion deep generative replay (SDDGR) for CIOD. Our method utilizes a diffusion-based generative model with pre-trained text-to-diffusion networks to generate realistic and diverse synthetic images. SDDGR incorporates an iterative refinement strategy to produce high-quality images encompassing old classes. Additionally, we adopt an L2 knowledge distillation technique to improve the retention of prior knowledge in synthetic images. Furthermore, our approach includes pseudo-labeling for old objects within new task images, preventing misclassification as background elements. Extensive experiments on the COCO 2017 dataset demonstrate that SDDGR significantly outperforms existing algorithms, achieving a new state-of-the-art in various CIOD scenarios. The source code will be made available to the public.\n    "
    },
    "2402.17389": {
        "title": "FairBelief -- Assessing Harmful Beliefs in Language Models",
        "authors": [
            "Mattia Setzu",
            "Marta Marchiori Manerba",
            "Pasquale Minervini",
            "Debora Nozza"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Language Models (LMs) have been shown to inherit undesired biases that might hurt minorities and underrepresented groups if such systems were integrated into real-world applications without careful fairness auditing. This paper proposes FairBelief, an analytical approach to capture and assess beliefs, i.e., propositions that an LM may embed with different degrees of confidence and that covertly influence its predictions. With FairBelief, we leverage prompting to study the behavior of several state-of-the-art LMs across different previously neglected axes, such as model scale and likelihood, assessing predictions on a fairness dataset specifically designed to quantify LMs' outputs' hurtfulness. Finally, we conclude with an in-depth qualitative assessment of the beliefs emitted by the models. We apply FairBelief to English LMs, revealing that, although these architectures enable high performances on diverse natural language processing tasks, they show hurtful beliefs about specific genders. Interestingly, training procedure and dataset, model scale, and architecture induce beliefs of different degrees of hurtfulness.\n    "
    },
    "2402.17392": {
        "title": "Spot the bot: Coarse-Grained Partition of Semantic Paths for Bots and Humans",
        "authors": [
            "Vasilii A. Gromov",
            "Alexandra S. Kogan"
        ],
        "comments": " ",
        "subjects": "Computation and Language (cs.CL)",
        "abstract": "Nowadays, technology is rapidly advancing: bots are writing comments, articles, and reviews. Due to this fact, it is crucial to know if the text was written by a human or by a bot. This paper focuses on comparing structures of the coarse-grained partitions of semantic paths for human-written and bot-generated texts. We compare the clusterizations of datasets of n-grams from literary texts and texts generated by several bots. The hypothesis is that the structures and clusterizations are different. Our research supports the hypothesis. As the semantic structure may be different for different languages, we investigate Russian, English, German, and Vietnamese languages.\n    "
    },
    "2402.17485": {
        "title": "EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions",
        "authors": [
            "Linrui Tian",
            "Qi Wang",
            "Bang Zhang",
            "Liefeng Bo"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In this work, we tackle the challenge of enhancing the realism and expressiveness in talking head video generation by focusing on the dynamic and nuanced relationship between audio cues and facial movements. We identify the limitations of traditional techniques that often fail to capture the full spectrum of human expressions and the uniqueness of individual facial styles. To address these issues, we propose EMO, a novel framework that utilizes a direct audio-to-video synthesis approach, bypassing the need for intermediate 3D models or facial landmarks. Our method ensures seamless frame transitions and consistent identity preservation throughout the video, resulting in highly expressive and lifelike animations. Experimental results demonsrate that EMO is able to produce not only convincing speaking videos but also singing videos in various styles, significantly outperforming existing state-of-the-art methodologies in terms of expressiveness and realism.\n    "
    },
    "2402.17758": {
        "title": "ADL4D: Towards A Contextually Rich Dataset for 4D Activities of Daily Living",
        "authors": [
            "Marsil Zakour",
            "Partha Pratim Nath",
            "Ludwig Lohmer",
            "Emre Faik G\u00f6k\u00e7e",
            "Martin Piccolrovazzi",
            "Constantin Patsch",
            "Yuankai Wu",
            "Rahul Chaudhari",
            "Eckehard Steinbach"
        ],
        "comments": " ",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Hand-Object Interactions (HOIs) are conditioned on spatial and temporal contexts like surrounding objects, previous actions, and future intents (for example, grasping and handover actions vary greatly based on objects proximity and trajectory obstruction). However, existing datasets for 4D HOI (3D HOI over time) are limited to one subject interacting with one object only. This restricts the generalization of learning-based HOI methods trained on those datasets. We introduce ADL4D, a dataset of up to two subjects interacting with different sets of objects performing Activities of Daily Living (ADL) like breakfast or lunch preparation activities. The transition between multiple objects to complete a certain task over time introduces a unique context lacking in existing datasets. Our dataset consists of 75 sequences with a total of 1.1M RGB-D frames, hand and object poses, and per-hand fine-grained action annotations. We develop an automatic system for multi-view multi-hand 3D pose annotation capable of tracking hand poses over time. We integrate and test it against publicly available datasets. Finally, we evaluate our dataset on the tasks of Hand Mesh Recovery (HMR) and Hand Action Segmentation (HAS).\n    "
    }
}