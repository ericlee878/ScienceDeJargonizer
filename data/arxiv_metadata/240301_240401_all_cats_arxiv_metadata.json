{
    "0710.3901v3": {
        "url": "http://arxiv.org/abs/0710.3901v3",
        "title": "A recursive linear time modular decomposition algorithm via LexBFS",
        "summary": "A module of a graph G is a set of vertices that have the same set of\nneighbours outside. Modules of a graphs form a so-called partitive family and\nthereby can be represented by a unique tree MD(G), called the modular\ndecomposition tree. Motivated by the central role of modules in numerous\nalgorithmic graph theory questions, the problem of efficiently computing MD(G)\nhas been investigated since the early 70's. To date the best algorithms run in\nlinear time but are all rather complicated. By combining previous algorithmic\nparadigms developed for the problem, we are able to present a simpler\nlinear-time that relies on very simple data-structures, namely slice\ndecomposition and sequences of rooted ordered trees.",
        "updated": "2024-03-01T16:03:35Z",
        "published": "2007-10-21T03:30:05Z",
        "authors": [
            "Derek Corneil",
            "Michel Habib",
            "Christophe Paul",
            "Marc Tedder"
        ],
        "comments": "An EA of this work appeared in ICALP'08. The arXiv v2 contains an\n  appendix with some sketches of proofs. To date, complete proofs can only be\n  found in the PhD of M. Tedder and spread over several chapters. This is the\n  first self-contained version. To ease the understanding, the noveI\n  presentation enlights the combinatorial objects involved in the algorithm,\n  which still relies on the same ideas",
        "categories": [
            "cs.DM"
        ],
        "primary_category": "cs.DM"
    },
    "0802.3284v1": {
        "url": "http://arxiv.org/abs/0802.3284v1",
        "title": "Tur\u00e1n Graphs, Stability Number, and Fibonacci Index",
        "summary": "The Fibonacci index of a graph is the number of its stable sets. This\nparameter is widely studied and has applications in chemical graph theory. In\nthis paper, we establish tight upper bounds for the Fibonacci index in terms of\nthe stability number and the order of general graphs and connected graphs.\nTur\\'an graphs frequently appear in extremal graph theory. We show that Tur\\'an\ngraphs and a connected variant of them are also extremal for these particular\nproblems.",
        "updated": "2008-02-22T11:22:50Z",
        "published": "2008-02-22T11:22:50Z",
        "authors": [
            "V\u00e9ronique Bruy\u00e8re",
            "Hadrien M\u00e9lot"
        ],
        "comments": "11 pages, 3 figures",
        "categories": [
            "cs.DM"
        ],
        "primary_category": "cs.DM",
        "doi": "10.1007/978-3-540-85097-7_12"
    },
    "0802.3414v4": {
        "url": "http://arxiv.org/abs/0802.3414v4",
        "title": "A Universal In-Place Reconfiguration Algorithm for Sliding Cube-Shaped\n  Robots in a Quadratic Number of Moves",
        "summary": "In the modular robot reconfiguration problem, we are given $n$ cube-shaped\nmodules (or robots) as well as two configurations, i.e., placements of the $n$\nmodules so that their union is face-connected. The goal is to find a sequence\nof moves that reconfigures the modules from one configuration to the other\nusing \"sliding moves,\" in which a module slides over the face or edge of a\nneighboring module, maintaining connectivity of the configuration at all times.\n  For many years it has been known that certain module configurations in this\nmodel require at least $\\Omega(n^2)$ moves to reconfigure between them. In this\npaper, we introduce the first universal reconfiguration algorithm -- i.e., we\nshow that any $n$-module configuration can reconfigure itself into any\nspecified $n$-module configuration using just sliding moves. Our algorithm\nachieves reconfiguration in $O(n^2)$ moves, making it asymptotically tight. We\nalso present a variation that reconfigures in-place, it ensures that throughout\nthe reconfiguration process, all modules, except for one, will be contained in\nthe union of the bounding boxes of the start and end configuration.",
        "updated": "2024-03-14T15:57:07Z",
        "published": "2008-02-23T00:54:13Z",
        "authors": [
            "Zachary Abel",
            "Hugo A. Akitaya",
            "Scott Duke Kominers",
            "Matias Korman",
            "Frederick Stock"
        ],
        "comments": "23 pages, 11 figures",
        "categories": [
            "cs.CG",
            "cs.MA",
            "cs.RO"
        ],
        "primary_category": "cs.CG"
    },
    "0811.1449v1": {
        "url": "http://arxiv.org/abs/0811.1449v1",
        "title": "Fibonacci Index and Stability Number of Graphs: a Polyhedral Study",
        "summary": "The Fibonacci index of a graph is the number of its stable sets. This\nparameter is widely studied and has applications in chemical graph theory. In\nthis paper, we establish tight upper bounds for the Fibonacci index in terms of\nthe stability number and the order of general graphs and connected graphs.\nTur\\'an graphs frequently appear in extremal graph theory. We show that Tur\\'an\ngraphs and a connected variant of them are also extremal for these particular\nproblems. We also make a polyhedral study by establishing all the optimal\nlinear inequalities for the stability number and the Fibonacci index, inside\nthe classes of general and connected graphs of order $n$.",
        "updated": "2008-11-10T11:46:09Z",
        "published": "2008-11-10T11:46:09Z",
        "authors": [
            "V\u00e9ronique Bruy\u00e8re",
            "Hadrien M\u00e9lot"
        ],
        "categories": [
            "cs.DM"
        ],
        "primary_category": "cs.DM",
        "doi": "10.1007/s10878-009-9228-7"
    },
    "1303.2033v14": {
        "url": "http://arxiv.org/abs/1303.2033v14",
        "title": "Extended Fourier analysis of signals",
        "summary": "This summary of the doctoral thesis is created to emphasize the close\nconnection of the proposed spectral analysis method with the Discrete Fourier\nTransform (DFT), the most extensively studied and frequently used approach in\nthe history of signal processing. It is shown that in a typical application\ncase, where uniform data readings are transformed to the same number of\nuniformly spaced frequencies, the results of the classical DFT and proposed\napproach coincide. The difference in performance appears when the length of the\nDFT is selected to be greater than the length of the data. The DFT solves the\nunknown data problem by padding readings with zeros up to the length of the\nDFT, while the proposed Extended DFT (EDFT) deals with this situation in a\ndifferent way, it uses the Fourier integral transform as a target and optimizes\nthe transform basis in the extended frequency range without putting such\nrestrictions on the time domain. Consequently, the Inverse DFT (IDFT) applied\nto the result of EDFT returns not only known readings, but also the\nextrapolated data, where classical DFT is able to give back just zeros, and\nhigher resolution are achieved at frequencies where the data has been\nsuccessfully extended. It has been demonstrated that EDFT able to process data\nwith missing readings or gaps inside or even nonuniformly distributed data.\nThus, EDFT significantly extends the usability of the DFT-based methods, where\npreviously these approaches have been considered as not applicable. The EDFT\nfounds the solution in an iterative way and requires repeated calculations to\nget the adaptive basis, and this makes it numerical complexity much higher\ncompared to DFT. This disadvantage was a serious problem in the 1990s, when the\nmethod has been proposed. Fortunately, since then the power of computers has\nincreased so much that nowadays EDFT application could be considered as a real\nalternative.",
        "updated": "2024-03-09T19:45:58Z",
        "published": "2013-03-08T15:47:28Z",
        "authors": [
            "Vilnis Liepins"
        ],
        "comments": "32 pages, 9 figures",
        "categories": [
            "cs.DS",
            "cs.IT",
            "cs.NA",
            "math.IT"
        ],
        "primary_category": "cs.DS"
    },
    "1403.6207v2": {
        "url": "http://arxiv.org/abs/1403.6207v2",
        "title": "Cluster Before You Hallucinate: Approximating Node-Capacitated Network\n  Design and Energy Efficient Routing",
        "summary": "We consider the following node-capacitated network design problem. The input\nis an undirected graph, set of demands, uniform node capacity and arbitrary\nnode costs. The goal is to find a minimum node-cost subgraph that supports all\ndemands concurrently subject to the node capacities. We consider both single\nand multi-commodity demands, and provide the first poly-logarithmic\napproximation guarantees. For single-commodity demands (i.e., all request pairs\nhave the same sink node), we obtain an $O(\\log^2 n)$ approximation to the cost\nwith an $O(\\log^3 n)$ factor violation in node capacities. For multi-commodity\ndemands, we obtain an $O(\\log^4 n)$ approximation to the cost with an\n$O(\\log^{10} n)$ factor violation in node capacities. We use a variety of\ntechniques, including single-sink confluent flows, low-load set cover, random\nsampling and cut-sparsification. We also develop new techniques for clustering\nmulticommodity demands into (nearly) node-disjoint clusters, which may be of\nindependent interest.\n  Moreover, this network design problem has applications to energy-efficient\nvirtual circuit routing. In this setting, there is a network of routers that\nare speed scalable, and that may be shutdown when idle. We assume the standard\nmodel for power: the power consumed by a router with load (speed) $s$ is\n$\\sigma + s^\\alpha$ where $\\sigma$ is the static power and the exponent $\\alpha\n> 1$. We obtain the first poly-logarithmic approximation algorithms for this\nproblem when speed-scaling occurs on nodes of a network.",
        "updated": "2024-03-10T12:51:54Z",
        "published": "2014-03-25T01:38:59Z",
        "authors": [
            "Ravishankar Krishnaswamy",
            "Viswanath Nagarajan",
            "Kirk Pruhs",
            "Cliff Stein"
        ],
        "comments": "38 pages, 4 figures (full version, to appear in SICOMP)",
        "categories": [
            "cs.DS",
            "68W25, 05C21, 05C85",
            "F.2.2; G.2.2"
        ],
        "primary_category": "cs.DS"
    },
    "1404.0736v2": {
        "url": "http://arxiv.org/abs/1404.0736v2",
        "title": "Exploiting Linear Structure Within Convolutional Networks for Efficient\n  Evaluation",
        "summary": "We present techniques for speeding up the test-time evaluation of large\nconvolutional networks, designed for object recognition tasks. These models\ndeliver impressive accuracy but each image evaluation requires millions of\nfloating point operations, making their deployment on smartphones and\nInternet-scale clusters problematic. The computation is dominated by the\nconvolution operations in the lower layers of the model. We exploit the linear\nstructure present within the convolutional filters to derive approximations\nthat significantly reduce the required computation. Using large\nstate-of-the-art models, we demonstrate we demonstrate speedups of\nconvolutional layers on both CPU and GPU by a factor of 2x, while keeping the\naccuracy within 1% of the original model.",
        "updated": "2014-06-09T15:53:55Z",
        "published": "2014-04-02T23:31:12Z",
        "authors": [
            "Remi Denton",
            "Wojciech Zaremba",
            "Joan Bruna",
            "Yann LeCun",
            "Rob Fergus"
        ],
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "primary_category": "cs.CV"
    },
    "1505.02681v2": {
        "url": "http://arxiv.org/abs/1505.02681v2",
        "title": "Socio-Spatial Group Queries for Impromptu Activity Planning",
        "summary": "The development and integration of social networking services and smartphones\nhave made it easy for individuals to organize impromptu social activities\nanywhere and anytime. Main challenges arising in organizing impromptu\nactivities are mostly due to the requirements of making timely invitations in\naccordance with the potential activity locations, corresponding to the\nlocations of and the relationship among the candidate attendees. Various\ncombinations of candidate attendees and activity locations create a large\nsolution space. Thus, in this paper, we propose Multiple Rally-Point Social\nSpatial Group Query (MRGQ), to select an appropriate activity location for a\ngroup of nearby attendees with tight social relationships. Although MRGQ is\nNP-hard, the number of attendees in practice is usually small enough such that\nan optimal solution can be found efficiently. Therefore, we first propose an\nInteger Linear Programming optimization model for MRGQ. We then design an\nefficient algorithm, called MAGS, which employs effective search space\nexploration and pruning strategies to reduce the running time for finding the\noptimal solution. We also propose to further optimize efficiency by indexing\nthe potential activity locations. A user study demonstrates the strength of\nusing MAGS over manual coordination in terms of both solution quality and\nefficiency. Experimental results on real datasets show that our algorithms can\nprocess MRGQ efficiently and significantly outperform other baseline\nalgorithms, including one based on the commercial parallel optimizer IBM CPLEX.",
        "updated": "2015-05-13T10:35:11Z",
        "published": "2015-05-11T15:58:31Z",
        "authors": [
            "Chih-Ya Shen",
            "De-Nian Yang",
            "Liang-Hao Huang",
            "Wang-Chien Lee",
            "Ming-Syan Chen"
        ],
        "categories": [
            "cs.DS",
            "cs.DB"
        ],
        "primary_category": "cs.DS",
        "doi": "10.1109/TKDE.2015.2468726"
    },
    "1511.03086v2": {
        "url": "http://arxiv.org/abs/1511.03086v2",
        "title": "The CTU Prague Relational Learning Repository",
        "summary": "The aim of the Prague Relational Learning Repository is to support machine\nlearning research with multi-relational data. The repository currently contains\n148 SQL databases hosted on a public MySQL server located at\n\\url{https://relational-data.org}. The server is provided by getML to support\nthe relational machine learning community (\\url{www.getml.com}). A searchable\nmeta-database provides metadata (e.g., the number of tables in the database,\nthe number of rows and columns in the tables, the number of\nself-relationships).",
        "updated": "2024-03-11T17:09:24Z",
        "published": "2015-11-10T12:30:42Z",
        "authors": [
            "Jan Motl",
            "Oliver Schulte"
        ],
        "comments": "7 pages",
        "categories": [
            "cs.LG",
            "cs.DB",
            "I.2.6; H.2.8"
        ],
        "primary_category": "cs.LG"
    },
    "1607.01327v8": {
        "url": "http://arxiv.org/abs/1607.01327v8",
        "title": "Feature Selection Library (MATLAB Toolbox)",
        "summary": "The Feature Selection Library (FSLib) introduces a comprehensive suite of\nfeature selection (FS) algorithms for MATLAB, aimed at improving machine\nlearning and data mining tasks. FSLib encompasses filter, embedded, and wrapper\nmethods to cater to diverse FS requirements. Filter methods focus on the\ninherent characteristics of features, embedded methods incorporate FS within\nmodel training, and wrapper methods assess features through model performance\nmetrics. By enabling effective feature selection, FSLib addresses the curse of\ndimensionality, reduces computational load, and enhances model\ngeneralizability. The elimination of redundant features through FSLib\nstreamlines the training process, improving efficiency and scalability. This\nfacilitates faster model development and boosts key performance indicators such\nas accuracy, precision, and recall by focusing on vital features. Moreover,\nFSLib contributes to data interpretability by revealing important features,\naiding in pattern recognition and understanding. Overall, FSLib provides a\nversatile framework that not only simplifies feature selection but also\nsignificantly benefits the machine learning and data mining ecosystem by\noffering a wide range of algorithms, reducing dimensionality, accelerating\nmodel training, improving model outcomes, and enhancing data insights.",
        "updated": "2024-03-12T11:24:45Z",
        "published": "2016-07-05T16:50:42Z",
        "authors": [
            "Giorgio Roffo"
        ],
        "comments": "Feature Selection Library (FSLib) 2024",
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2204.10438v4": {
        "url": "http://arxiv.org/abs/2204.10438v4",
        "title": "EVOTER: Evolution of Transparent Explainable Rule-sets",
        "summary": "Most AI systems are black boxes generating reasonable outputs for given\ninputs. Some domains, however, have explainability and trustworthiness\nrequirements that cannot be directly met by these approaches. Various methods\nhave therefore been developed to interpret black-box models after training.\nThis paper advocates an alternative approach where the models are transparent\nand explainable to begin with. This approach, EVOTER, evolves rule-sets based\non simple logical expressions. The approach is evaluated in several\nprediction/classification and prescription/policy search domains with and\nwithout a surrogate. It is shown to discover meaningful rule sets that perform\nsimilarly to black-box models. The rules can provide insight into the domain,\nand make biases hidden in the data explicit. It may also be possible to edit\nthem directly to remove biases and add constraints. EVOTER thus forms a\npromising foundation for building trustworthy AI systems for real-world\napplications in the future.",
        "updated": "2024-03-25T02:37:09Z",
        "published": "2022-04-21T23:59:17Z",
        "authors": [
            "Hormoz Shahrzad",
            "Babak Hodjat",
            "Risto Miikkulainen"
        ],
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.NE"
        ],
        "primary_category": "cs.AI"
    },
    "2204.11041v3": {
        "url": "http://arxiv.org/abs/2204.11041v3",
        "title": "Learning by Erasing: Conditional Entropy based Transferable\n  Out-Of-Distribution Detection",
        "summary": "Out-of-distribution (OOD) detection is essential to handle the distribution\nshifts between training and test scenarios. For a new in-distribution (ID)\ndataset, existing methods require retraining to capture the dataset-specific\nfeature representation or data distribution. In this paper, we propose a deep\ngenerative models (DGM) based transferable OOD detection method, which is\nunnecessary to retrain on a new ID dataset. We design an image erasing strategy\nto equip exclusive conditional entropy distribution for each ID dataset, which\ndetermines the discrepancy of DGM's posteriori ucertainty distribution on\ndifferent ID datasets. Owing to the powerful representation capacity of\nconvolutional neural networks, the proposed model trained on complex dataset\ncan capture the above discrepancy between ID datasets without retraining and\nthus achieve transferable OOD detection. We validate the proposed method on\nfive datasets and verity that ours achieves comparable performance to the\nstate-of-the-art group based OOD detection methods that need to be retrained to\ndeploy on new ID datasets. Our code is available at\nhttps://github.com/oOHCIOo/CETOOD.",
        "updated": "2024-03-27T14:29:27Z",
        "published": "2022-04-23T10:19:58Z",
        "authors": [
            "Meng Xing",
            "Zhiyong Feng",
            "Yong Su",
            "Changjae Oh"
        ],
        "comments": "update new experimental results",
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2204.11970v3": {
        "url": "http://arxiv.org/abs/2204.11970v3",
        "title": "Visual Acuity Prediction on Real-Life Patient Data Using a Machine\n  Learning Based Multistage System",
        "summary": "In ophthalmology, intravitreal operative medication therapy (IVOM) is a\nwidespread treatment for diseases related to the age-related macular\ndegeneration (AMD), the diabetic macular edema (DME), as well as the retinal\nvein occlusion (RVO). However, in real-world settings, patients often suffer\nfrom loss of vision on time scales of years despite therapy, whereas the\nprediction of the visual acuity (VA) and the earliest possible detection of\ndeterioration under real-life conditions is challenging due to heterogeneous\nand incomplete data. In this contribution, we present a workflow for the\ndevelopment of a research-compatible data corpus fusing different IT systems of\nthe department of ophthalmology of a German maximum care hospital. The\nextensive data corpus allows predictive statements of the expected progression\nof a patient and his or her VA in each of the three diseases. For the disease\nAMD, we found out a significant deterioration of the visual acuity over time.\nWithin our proposed multistage system, we subsequently classify the VA\nprogression into the three groups of therapy \"winners\", \"stabilizers\", and\n\"losers\" (WSL classification scheme). Our OCT biomarker classification using an\nensemble of deep neural networks results in a classification accuracy\n(F1-score) of over 98 %, enabling us to complete incomplete OCT documentations\nwhile allowing us to exploit them for a more precise VA modelling process. Our\nVA prediction requires at least four VA examinations and optionally OCT\nbiomarkers from the same time period to predict the VA progression within a\nforecasted time frame, whereas our prediction is currently restricted to IVOM /\nno therapy. We achieve a final prediction accuracy of 69 % in macro average\nF1-score, while being in the same range as the ophthalmologists with 57.8 and\n50 +- 10.7 % F1-score.",
        "updated": "2024-03-27T22:02:30Z",
        "published": "2022-04-25T21:20:27Z",
        "authors": [
            "Tobias Schlosser",
            "Frederik Beuth",
            "Trixy Meyer",
            "Arunodhayan Sampath Kumar",
            "Gabriel Stolze",
            "Olga Furashova",
            "Katrin Engelmann",
            "Danny Kowerko"
        ],
        "comments": "Preprint for journal Scientific Reports (Springer)",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.IR",
            "cs.LG"
        ],
        "primary_category": "eess.IV"
    },
    "2204.12095v2": {
        "url": "http://arxiv.org/abs/2204.12095v2",
        "title": "PyGOD: A Python Library for Graph Outlier Detection",
        "summary": "PyGOD is an open-source Python library for detecting outliers in graph data.\nAs the first comprehensive library of its kind, PyGOD supports a wide array of\nleading graph-based methods for outlier detection under an easy-to-use,\nwell-documented API designed for use by both researchers and practitioners.\nPyGOD provides modularized components of the different detectors implemented so\nthat users can easily customize each detector for their purposes. To ease the\nconstruction of detection workflows, PyGOD offers numerous commonly used\nutility functions. To scale computation to large graphs, PyGOD supports\nfunctionalities for deep models such as sampling and mini-batch processing.\nPyGOD uses best practices in fostering code reliability and maintainability,\nincluding unit testing, continuous integration, and code coverage. To\nfacilitate accessibility, PyGOD is released under a BSD 2-Clause license at\nhttps://pygod.org and at the Python Package Index (PyPI).",
        "updated": "2024-03-17T19:37:05Z",
        "published": "2022-04-26T06:15:21Z",
        "authors": [
            "Kay Liu",
            "Yingtong Dou",
            "Xueying Ding",
            "Xiyang Hu",
            "Ruitong Zhang",
            "Hao Peng",
            "Lichao Sun",
            "Philip S. Yu"
        ],
        "comments": "Accepted by JMLR. Library available at https://pygod.org",
        "categories": [
            "cs.LG",
            "cs.SI"
        ],
        "primary_category": "cs.LG"
    },
    "2204.14100v2": {
        "url": "http://arxiv.org/abs/2204.14100v2",
        "title": "Adversarial Distortion Learning for Medical Image Denoising",
        "summary": "We present a novel adversarial distortion learning (ADL) for denoising two-\nand three-dimensional (2D/3D) biomedical image data. The proposed ADL consists\nof two auto-encoders: a denoiser and a discriminator. The denoiser removes\nnoise from input data and the discriminator compares the denoised result to its\nnoise-free counterpart. This process is repeated until the discriminator cannot\ndifferentiate the denoised data from the reference. Both the denoiser and the\ndiscriminator are built upon a proposed auto-encoder called Efficient-Unet.\nEfficient-Unet has a light architecture that uses the residual blocks and a\nnovel pyramidal approach in the backbone to efficiently extract and re-use\nfeature maps. During training, the textural information and contrast are\ncontrolled by two novel loss functions. The architecture of Efficient-Unet\nallows generalizing the proposed method to any sort of biomedical data. The 2D\nversion of our network was trained on ImageNet and tested on biomedical\ndatasets whose distribution is completely different from ImageNet; so, there is\nno need for re-training. Experimental results carried out on magnetic resonance\nimaging (MRI), dermatoscopy, electron microscopy and X-ray datasets show that\nthe proposed method achieved the best on each benchmark. Our implementation and\npre-trained models are available at https://github.com/mogvision/ADL.",
        "updated": "2024-03-12T16:36:06Z",
        "published": "2022-04-29T13:47:39Z",
        "authors": [
            "Morteza Ghahremani",
            "Mohammad Khateri",
            "Alejandra Sierra",
            "Jussi Tohka"
        ],
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "primary_category": "eess.IV"
    },
    "2205.00386v3": {
        "url": "http://arxiv.org/abs/2205.00386v3",
        "title": "Internal sums for synthetic fibered $(\\infty,1)$-categories",
        "summary": "We give structural results about bifibrations of (internal)\n$(\\infty,1)$-categories with internal sums. This includes a higher version of\nMoens' Theorem, characterizing cartesian bifibrations with extensive aka stable\nand disjoint internal sums over lex bases as Artin gluings of lex functors.\n  We also treat a generalized version of Moens' Theorem due to Streicher which\ndoes not require the Beck--Chevalley condition.\n  Furthermore, we show that also in this setting the Moens fibrations can be\ncharacterized via a condition due to Zawadowski.\n  Our account overall follows Streicher's presentation of fibered category\ntheory \\`{a} la B\\'{e}nabou, generalizing the results to the internal,\nhigher-categorical case, formulated in a synthetic setting.\n  Namely, we work inside simplicial homotopy type theory, which has been\nintroduced by Riehl and Shulman as a logical system to reason about internal\n$(\\infty,1)$-categories, interpreted as Rezk objects in any given\nGrothendieck--Rezk--Lurie $(\\infty,1)$-topos.",
        "updated": "2024-03-11T17:52:07Z",
        "published": "2022-05-01T03:01:55Z",
        "authors": [
            "Jonathan Weinberger"
        ],
        "comments": "59 pages. This text is based on Section 3.4 and Chapter 5 from\n  author's PhD thesis arXiv:2202.13132, with varioius additions and\n  improvements. Revised version, accepted for publication at the Journal of\n  Pure and Applied Algebra",
        "categories": [
            "math.CT",
            "cs.LO",
            "math.AT",
            "math.LO",
            "03B38, 18N60, 18D30, 18B50, 18N45, 55U35, 18N50",
            "F.4.1"
        ],
        "primary_category": "math.CT"
    },
    "2205.00415v3": {
        "url": "http://arxiv.org/abs/2205.00415v3",
        "title": "Don't Blame the Annotator: Bias Already Starts in the Annotation\n  Instructions",
        "summary": "In recent years, progress in NLU has been driven by benchmarks. These\nbenchmarks are typically collected by crowdsourcing, where annotators write\nexamples based on annotation instructions crafted by dataset creators. In this\nwork, we hypothesize that annotators pick up on patterns in the crowdsourcing\ninstructions, which bias them to write many similar examples that are then\nover-represented in the collected data. We study this form of bias, termed\ninstruction bias, in 14 recent NLU benchmarks, showing that instruction\nexamples often exhibit concrete patterns, which are propagated by crowdworkers\nto the collected data. This extends previous work (Geva et al., 2019) and\nraises a new concern of whether we are modeling the dataset creator's\ninstructions, rather than the task. Through a series of experiments, we show\nthat, indeed, instruction bias can lead to overestimation of model performance,\nand that models struggle to generalize beyond biases originating in the\ncrowdsourcing instructions. We further analyze the influence of instruction\nbias in terms of pattern frequency and model size, and derive concrete\nrecommendations for creating future NLU benchmarks.",
        "updated": "2024-03-20T03:23:11Z",
        "published": "2022-05-01T07:51:22Z",
        "authors": [
            "Mihir Parmar",
            "Swaroop Mishra",
            "Mor Geva",
            "Chitta Baral"
        ],
        "comments": "EACL 2023 (Outstanding Paper Award)",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "primary_category": "cs.CL"
    },
    "2205.00769v1": {
        "url": "http://arxiv.org/abs/2205.00769v1",
        "title": "A CAD Framework for Simulation of Network Level Attack on Platoons",
        "summary": "Recent developments in the smart mobility domain have transformed automobiles\ninto networked transportation agents helping realize new age, large-scale\nintelligent transportation systems (ITS). The motivation behind such networked\ntransportation is to improve road safety as well as traffic efficiency. In this\nsetup, vehicles can share information about their speed and/or acceleration\nvalues among themselves and infrastructures can share traffic signal data with\nthem. This enables the connected vehicles (CVs) to stay informed about their\nsurroundings while moving. However, the inter-vehicle communication channels\nsignificantly broaden the attack surface. The inter-vehicle network enables an\nattacker to remotely launch attacks. An attacker can create collision as well\nas hamper performance by reducing the traffic efficiency. Thus, security\nvulnerabilities must be taken into consideration in the early phase of the\ndevelopment cycle of CVs. To the best of our knowledge, there exists no such\nautomated simulation tool using which engineers can verify the performance of\nCV prototypes in the presence of an attacker. In this work, we present an\nautomated tool flow that facilitates false data injection attack synthesis and\nsimulation on customizable platoon structure and vehicle dynamics. This tool\ncan be used to simulate as well as design and verify control-theoretic\nlight-weight attack detection and mitigation algorithms for CVs.",
        "updated": "2022-05-02T09:29:44Z",
        "published": "2022-05-02T09:29:44Z",
        "authors": [
            "Ipsita Koley",
            "Sunandan Adhikary",
            "Rohit Rohit",
            "Soumyajit Dey"
        ],
        "categories": [
            "cs.CR",
            "cs.SY",
            "eess.SY"
        ],
        "primary_category": "cs.CR",
        "doi": "10.1109/DSD57027.2022.00128"
    },
    "2205.00889v2": {
        "url": "http://arxiv.org/abs/2205.00889v2",
        "title": "Vehicle Routing with Time-Dependent Travel Times: Theory, Practice, and\n  Benchmarks",
        "summary": "We develop theoretical foundations and practical algorithms for vehicle\nrouting with time-dependent travel times. We also provide new benchmark\ninstances and experimental results. First, we study basic operations on\npiecewise linear arrival time functions. In particular, we devise a faster\nalgorithm to compute the pointwise minimum of a set of piecewise linear\nfunctions and a monotonicity-preserving variant of the Imai-Iri algorithm to\napproximate an arrival time function with fewer breakpoints.\n  Next, we show how to evaluate insertion and deletion operations in tours\nefficiently and update the underlying data structure faster than previously\nknown when a tour changes. Evaluating a tour also requires a scheduling step\nwhich is non-trivial in the presence of time windows and time-dependent travel\ntimes. We show how to perform this in linear time.\n  Based on these results, we develop a local search heuristic to solve\nreal-world vehicle routing problems with various constraints efficiently and\nreport experimental results on classical benchmarks. Since most of these do not\nhave time-dependent travel times, we generate and publish new benchmark\ninstances that are based on real-world data. This data also demonstrates the\nimportance of considering time-dependent travel times in instances with tight\ntime windows.",
        "updated": "2024-03-25T16:54:54Z",
        "published": "2022-05-02T13:01:55Z",
        "authors": [
            "Jannis Blauth",
            "Stephan Held",
            "Dirk M\u00fcller",
            "Niklas Schlomberg",
            "Vera Traub",
            "Thorben Tr\u00f6bst",
            "Jens Vygen"
        ],
        "categories": [
            "cs.DS"
        ],
        "primary_category": "cs.DS"
    },
    "2205.01478v3": {
        "url": "http://arxiv.org/abs/2205.01478v3",
        "title": "Eigenvector centrality for multilayer networks with dependent node\n  importance",
        "summary": "We present a novel approach for computing a variant of eigenvector centrality\nfor multilayer networks with inter-layer constraints on node importance.\nSpecifically, we consider a multilayer network defined by multiple\nedge-weighted, potentially directed, graphs over the same set of nodes with\neach graph representing one layer of the network and no inter-layer edges. As\nin the standard eigenvector centrality construction, the importance of each\nnode in a given layer is based on the weighted sum of the importance of\nadjacent nodes in that same layer. Unlike standard eigenvector centrality, we\nassume that the adjacency relationship and the importance of adjacent nodes may\nbe based on distinct layers. Importantly, this type of centrality constraint is\nonly partially supported by existing frameworks for multilayer eigenvector\ncentrality that use edges between nodes in different layers to capture\ninter-layer dependencies. For our model, constrained, layer-specific\neigenvector centrality values are defined by a system of independent eigenvalue\nproblems and dependent pseudo-eigenvalue problems, whose solution can be\nefficiently realized using an interleaved power iteration algorithm.",
        "updated": "2023-05-24T00:43:50Z",
        "published": "2022-05-03T13:17:43Z",
        "authors": [
            "H. Robert Frost"
        ],
        "categories": [
            "physics.soc-ph",
            "cs.NA",
            "math.NA"
        ],
        "primary_category": "physics.soc-ph",
        "doi": "10.1007/978-3-031-53472-0_1"
    },
    "2301.05997v3": {
        "url": "http://arxiv.org/abs/2301.05997v3",
        "title": "Exploiting Auxiliary Caption for Video Grounding",
        "summary": "Video grounding aims to locate a moment of interest matching the given query\nsentence from an untrimmed video. Previous works ignore the {sparsity dilemma}\nin video annotations, which fails to provide the context information between\npotential events and query sentences in the dataset. In this paper, we contend\nthat exploiting easily available captions which describe general actions, i.e.,\nauxiliary captions defined in our paper, will significantly boost the\nperformance. To this end, we propose an Auxiliary Caption Network (ACNet) for\nvideo grounding. Specifically, we first introduce dense video captioning to\ngenerate dense captions and then obtain auxiliary captions by Non-Auxiliary\nCaption Suppression (NACS). To capture the potential information in auxiliary\ncaptions, we propose Caption Guided Attention (CGA) project the semantic\nrelations between auxiliary captions and query sentences into temporal space\nand fuse them into visual representations. Considering the gap between\nauxiliary captions and ground truth, we propose Asymmetric Cross-modal\nContrastive Learning (ACCL) for constructing more negative pairs to maximize\ncross-modal mutual information. Extensive experiments on three public datasets\n(i.e., ActivityNet Captions, TACoS and ActivityNet-CG) demonstrate that our\nmethod significantly outperforms state-of-the-art methods.",
        "updated": "2024-03-24T05:46:10Z",
        "published": "2023-01-15T02:04:02Z",
        "authors": [
            "Hongxiang Li",
            "Meng Cao",
            "Xuxin Cheng",
            "Zhihong Zhu",
            "Yaowei Li",
            "Yuexian Zou"
        ],
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2301.06136v3": {
        "url": "http://arxiv.org/abs/2301.06136v3",
        "title": "Quantitative Verification with Neural Networks",
        "summary": "We present a data-driven approach to the quantitative verification of\nprobabilistic programs and stochastic dynamical models. Our approach leverages\nneural networks to compute tight and sound bounds for the probability that a\nstochastic process hits a target condition within finite time. This problem\nsubsumes a variety of quantitative verification questions, from the\nreachability and safety analysis of discrete-time stochastic dynamical models,\nto the study of assertion-violation and termination analysis of probabilistic\nprograms. We rely on neural networks to represent supermartingale certificates\nthat yield such probability bounds, which we compute using a\ncounterexample-guided inductive synthesis loop: we train the neural certificate\nwhile tightening the probability bound over samples of the state space using\nstochastic optimisation, and then we formally check the certificate's validity\nover every possible state using satisfiability modulo theories; if we receive a\ncounterexample, we add it to our set of samples and repeat the loop until\nvalidity is confirmed. We demonstrate on a diverse set of benchmarks that,\nthanks to the expressive power of neural networks, our method yields smaller or\ncomparable probability bounds than existing symbolic methods in all cases, and\nthat our approach succeeds on models that are entirely beyond the reach of such\nalternative techniques.",
        "updated": "2024-03-11T16:55:37Z",
        "published": "2023-01-15T16:35:36Z",
        "authors": [
            "Alessandro Abate",
            "Alec Edwards",
            "Mirco Giacobbe",
            "Hashan Punchihewa",
            "Diptarko Roy"
        ],
        "comments": "The conference version of this manuscript appeared at CONCUR 2023",
        "categories": [
            "cs.LO",
            "cs.PL",
            "cs.SY",
            "eess.SY",
            "F.3.1; D.2.4"
        ],
        "primary_category": "cs.LO",
        "doi": "10.4230/LIPIcs.CONCUR.2023.22"
    },
    "2301.06626v2": {
        "url": "http://arxiv.org/abs/2301.06626v2",
        "title": "Masked Vector Quantization",
        "summary": "Generative models with discrete latent representations have recently\ndemonstrated an impressive ability to learn complex high-dimensional data\ndistributions. However, their performance relies on a long sequence of tokens\nper instance and a large number of codebook entries, resulting in long sampling\ntimes and considerable computation to fit the categorical posterior. To address\nthese issues, we propose the Masked Vector Quantization (MVQ) framework which\nincreases the representational capacity of each code vector by learning mask\nconfigurations via a stochastic winner-takes-all training regime called\nMultiple Hypothese Dropout (MH-Dropout). On ImageNet 64$\\times$64, MVQ reduces\nFID in existing vector quantization architectures by up to $68\\%$ at 2 tokens\nper instance and $57\\%$ at 5 tokens. These improvements widen as codebook\nentries is reduced and allows for $7\\textit{--}45\\times$ speed-up in token\nsampling during inference. As an additional benefit, we find that smaller\nlatent spaces lead to MVQ identifying transferable visual representations where\nmultiple can be smoothly combined.",
        "updated": "2024-03-25T00:45:30Z",
        "published": "2023-01-16T22:30:53Z",
        "authors": [
            "David D. Nguyen",
            "David Leibowitz",
            "Surya Nepal",
            "Salil S. Kanhere"
        ],
        "comments": "A newer version of this manuscript was archived under 2312.11735",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "primary_category": "cs.LG"
    },
    "2301.06662v3": {
        "url": "http://arxiv.org/abs/2301.06662v3",
        "title": "Graph Learning Across Data Silos",
        "summary": "We consider the problem of inferring graph topology from smooth graph signals\nin a novel but practical scenario where data are located in distributed clients\nand prohibited from leaving local clients due to factors such as privacy\nconcerns. The main difficulty in this task is how to exploit the potentially\nheterogeneous data of all clients under data silos. To this end, we first\npropose an auto-weighted multiple graph learning model to jointly learn a\npersonalized graph for each local client and a single consensus graph for all\nclients. The personalized graphs match local data distributions, thereby\nmitigating data heterogeneity, while the consensus graph captures the global\ninformation. Moreover, the model can automatically assign appropriate\ncontribution weights to local graphs based on their similarity to the consensus\ngraph. We next devise a tailored algorithm to solve the induced problem, where\nall raw data are processed locally without leaving clients. Theoretically, we\nestablish a provable estimation error bound and convergence analysis for the\nproposed model and algorithm. Finally, extensive experiments on synthetic and\nreal data are carried out, and the results illustrate that our approach can\nlearn graphs effectively in the target scenario.",
        "updated": "2024-03-01T06:32:40Z",
        "published": "2023-01-17T02:14:57Z",
        "authors": [
            "Xiang Zhang",
            "Qiao Wang"
        ],
        "comments": "13 pages",
        "categories": [
            "cs.LG",
            "cs.CR",
            "eess.SP"
        ],
        "primary_category": "cs.LG"
    },
    "2301.07260v3": {
        "url": "http://arxiv.org/abs/2301.07260v3",
        "title": "Additive Schwarz methods for fourth-order variational inequalities",
        "summary": "Fourth-order variational inequalities are encountered in various scientific\nand engineering disciplines, including elliptic optimal control problems and\nplate obstacle problems. In this paper, we consider additive Schwarz methods\nfor solving fourth-order variational inequalities. Based on a unified framework\nof various finite element methods for fourth-order variational inequalities, we\ndevelop one- and two-level additive Schwarz methods. We prove that the\ntwo-level method is scalable in the sense that the convergence rate of the\nmethod depends on $H/h$ and $H/\\delta$ only, where $h$ and $H$ are the typical\ndiameters of an element and a subdomain, respectively, and $\\delta$ measures\nthe overlap among the subdomains. This proof relies on a new nonlinear\npositivity-preserving coarse interpolation operator, the construction of which\nwas previously unknown. To the best of our knowledge, this analysis represents\nthe first investigation into the scalability of the two-level additive Schwarz\nmethod for fourth-order variational inequalities. Our theoretical results are\nverified by numerical experiments.",
        "updated": "2024-03-16T19:48:05Z",
        "published": "2023-01-18T01:58:03Z",
        "authors": [
            "Jongho Park"
        ],
        "comments": "22 pages, 2 figures",
        "categories": [
            "math.NA",
            "cs.NA",
            "65N55, 65K15, 65N30, 49M27"
        ],
        "primary_category": "math.NA"
    },
    "2301.07482v3": {
        "url": "http://arxiv.org/abs/2301.07482v3",
        "title": "FreshGNN: Reducing Memory Access via Stable Historical Embeddings for\n  Graph Neural Network Training",
        "summary": "A key performance bottleneck when training graph neural network (GNN) models\non large, real-world graphs is loading node features onto a GPU. Due to limited\nGPU memory, expensive data movement is necessary to facilitate the storage of\nthese features on alternative devices with slower access (e.g. CPU memory).\nMoreover, the irregularity of graph structures contributes to poor data\nlocality which further exacerbates the problem. Consequently, existing\nframeworks capable of efficiently training large GNN models usually incur a\nsignificant accuracy degradation because of the currently-available shortcuts\ninvolved. To address these limitations, we instead propose FreshGNN, a\ngeneral-purpose GNN mini-batch training framework that leverages a historical\ncache for storing and reusing GNN node embeddings instead of re-computing them\nthrough fetching raw features at every iteration. Critical to its success, the\ncorresponding cache policy is designed, using a combination of gradient-based\nand staleness criteria, to selectively screen those embeddings which are\nrelatively stable and can be cached, from those that need to be re-computed to\nreduce estimation errors and subsequent downstream accuracy loss. When paired\nwith complementary system enhancements to support this selective historical\ncache, FreshGNN is able to accelerate the training speed on large graph\ndatasets such as ogbn-papers100M and MAG240M by 3.4x up to 20.5x and reduce the\nmemory access by 59%, with less than 1% influence on test accuracy.",
        "updated": "2024-03-24T14:48:59Z",
        "published": "2023-01-18T12:51:13Z",
        "authors": [
            "Kezhao Huang",
            "Haitian Jiang",
            "Minjie Wang",
            "Guangxuan Xiao",
            "David Wipf",
            "Xiang Song",
            "Quan Gan",
            "Zengfeng Huang",
            "Jidong Zhai",
            "Zheng Zhang"
        ],
        "comments": "Accepted by VLDB 2024",
        "categories": [
            "cs.LG"
        ],
        "primary_category": "cs.LG"
    },
    "2301.07628v5": {
        "url": "http://arxiv.org/abs/2301.07628v5",
        "title": "Universal Neural-Cracking-Machines: Self-Configurable Password Models\n  from Auxiliary Data",
        "summary": "We introduce the concept of \"universal password model\" -- a password model\nthat, once pre-trained, can automatically adapt its guessing strategy based on\nthe target system. To achieve this, the model does not need to access any\nplaintext passwords from the target credentials. Instead, it exploits users'\nauxiliary information, such as email addresses, as a proxy signal to predict\nthe underlying password distribution. Specifically, the model uses deep\nlearning to capture the correlation between the auxiliary data of a group of\nusers (e.g., users of a web application) and their passwords. It then exploits\nthose patterns to create a tailored password model for the target system at\ninference time. No further training steps, targeted data collection, or prior\nknowledge of the community's password distribution is required. Besides\nimproving over current password strength estimation techniques and attacks, the\nmodel enables any end-user (e.g., system administrators) to autonomously\ngenerate tailored password models for their systems without the often\nunworkable requirements of collecting suitable training data and fitting the\nunderlying machine learning model. Ultimately, our framework enables the\ndemocratization of well-calibrated password models to the community, addressing\na major challenge in the deployment of password security solutions at scale.",
        "updated": "2024-03-13T08:02:51Z",
        "published": "2023-01-18T16:12:04Z",
        "authors": [
            "Dario Pasquini",
            "Giuseppe Ateniese",
            "Carmela Troncoso"
        ],
        "comments": "Appearing in the proceedings of the 45th IEEE Symposium on Security\n  and Privacy S&P 2024",
        "categories": [
            "cs.CR",
            "cs.LG"
        ],
        "primary_category": "cs.CR"
    },
    "2301.07703v1": {
        "url": "http://arxiv.org/abs/2301.07703v1",
        "title": "Robust Zero-crossings Detection in Noisy Signals using Topological\n  Signal Processing",
        "summary": "We explore a novel application of zero-dimensional persistent homology from\nTopological Data Analysis (TDA) for bracketing zero-crossings of both\none-dimensional continuous functions, and uniformly sampled time series. We\npresent an algorithm and show its robustness in the presence of noise for a\nrange of sampling frequencies. In comparison to state-of-the-art software-based\nmethods for finding zeros of a time series, our method generally converges\nfaster, provides higher accuracy, and is capable of finding all the roots in a\ngiven interval instead of converging only to one of them. We also present and\ncompare options for automatically setting the persistence threshold parameter\nthat influences the accurate bracketing of the roots.",
        "updated": "2023-01-18T18:50:13Z",
        "published": "2023-01-18T18:50:13Z",
        "authors": [
            "Sunia Tanweer",
            "Firas A. Khasawneh",
            "Elizabeth Munch"
        ],
        "categories": [
            "cs.CG",
            "eess.SP"
        ],
        "primary_category": "cs.CG",
        "doi": "10.3934/fods.2024006"
    },
    "2301.07945v3": {
        "url": "http://arxiv.org/abs/2301.07945v3",
        "title": "PDFormer: Propagation Delay-Aware Dynamic Long-Range Transformer for\n  Traffic Flow Prediction",
        "summary": "As a core technology of Intelligent Transportation System, traffic flow\nprediction has a wide range of applications. The fundamental challenge in\ntraffic flow prediction is to effectively model the complex spatial-temporal\ndependencies in traffic data. Spatial-temporal Graph Neural Network (GNN)\nmodels have emerged as one of the most promising methods to solve this problem.\nHowever, GNN-based models have three major limitations for traffic prediction:\ni) Most methods model spatial dependencies in a static manner, which limits the\nability to learn dynamic urban traffic patterns; ii) Most methods only consider\nshort-range spatial information and are unable to capture long-range spatial\ndependencies; iii) These methods ignore the fact that the propagation of\ntraffic conditions between locations has a time delay in traffic systems. To\nthis end, we propose a novel Propagation Delay-aware dynamic long-range\ntransFormer, namely PDFormer, for accurate traffic flow prediction.\nSpecifically, we design a spatial self-attention module to capture the dynamic\nspatial dependencies. Then, two graph masking matrices are introduced to\nhighlight spatial dependencies from short- and long-range views. Moreover, a\ntraffic delay-aware feature transformation module is proposed to empower\nPDFormer with the capability of explicitly modeling the time delay of spatial\ninformation propagation. Extensive experimental results on six real-world\npublic traffic datasets show that our method can not only achieve\nstate-of-the-art performance but also exhibit competitive computational\nefficiency. Moreover, we visualize the learned spatial-temporal attention map\nto make our model highly interpretable.",
        "updated": "2024-03-07T16:00:47Z",
        "published": "2023-01-19T08:42:40Z",
        "authors": [
            "Jiawei Jiang",
            "Chengkai Han",
            "Wayne Xin Zhao",
            "Jingyuan Wang"
        ],
        "comments": "9 pages, 5 figures, Accepted by AAAI2023",
        "categories": [
            "cs.LG"
        ],
        "primary_category": "cs.LG"
    },
    "2301.08343v1": {
        "url": "http://arxiv.org/abs/2301.08343v1",
        "title": "Tacchi: A Pluggable and Low Computational Cost Elastomer Deformation\n  Simulator for Optical Tactile Sensors",
        "summary": "Simulation is widely applied in robotics research to save time and resources.\nThere have been several works to simulate optical tactile sensors that leverage\neither a smoothing method or Finite Element Method (FEM). However, elastomer\ndeformation physics is not considered in the former method, whereas the latter\nrequires a massive amount of computational resources like a computer cluster.\nIn this work, we propose a pluggable and low computational cost simulator using\nthe Taichi programming language for simulating optical tactile sensors, named\nas Tacchi . It reconstructs elastomer deformation using particles, which allows\ndeformed elastomer surfaces to be rendered into tactile images and reveals\ncontact information without suffering from high computational costs. Tacchi\nfacilitates creating realistic tactile images in simulation, e.g., ones that\ncapture wear-and-tear defects on object surfaces. In addition, the proposed\nTacchi can be integrated with robotics simulators for a robot system\nsimulation. Experiment results showed that Tacchi can produce images with\nbetter similarity to real images and achieved higher Sim2Real accuracy compared\nto the existing methods. Moreover, it can be connected with MuJoCo and Gazebo\nwith only the requirement of 1G memory space in GPU compared to a computer\ncluster applied for FEM. With Tacchi, physical robot simulation with optical\ntactile sensors becomes possible. All the materials in this paper are available\nat https://github.com/zixichen007115/Tacchi .",
        "updated": "2023-01-19T22:35:03Z",
        "published": "2023-01-19T22:35:03Z",
        "authors": [
            "Zixi Chen",
            "Shixin Zhang",
            "Shan Luo",
            "Fuchun Sun",
            "Bin Fang"
        ],
        "comments": "8 pages, 6 figures, accepted by IEEE Robotics and Automation Letters",
        "categories": [
            "cs.RO"
        ],
        "primary_category": "cs.RO",
        "doi": "10.1109/LRA.2023.3237042"
    },
    "2305.01763v1": {
        "url": "http://arxiv.org/abs/2305.01763v1",
        "title": "Achieving Realistic Cyclist Behavior in SUMO using the SimRa Dataset",
        "summary": "Increasing the modal share of bicycle traffic to reduce carbon emissions,\nreduce urban car traffic, and to improve the health of citizens, requires a\nshift away from car-centric city planning. For this, traffic planners often\nrely on simulation tools such as SUMO which allow them to study the effects of\nconstruction changes before implementing them. Similarly, studies of vulnerable\nroad users, here cyclists, also use such models to assess the performance of\ncommunication-based road traffic safety systems. The cyclist model in SUMO,\nhowever, is very imprecise as SUMO cyclists behave either like slow cars or\nfast pedestrians, thus, casting doubt on simulation results for bicycle\ntraffic. In this paper, we analyze acceleration, deceleration, velocity, and\nintersection left-turn behavior of cyclists in a large dataset of real world\ncycle tracks. We use the results to improve the existing cyclist model in SUMO\nand add three more detailed cyclist models and implement them in SUMO.",
        "updated": "2023-05-02T20:03:52Z",
        "published": "2023-05-02T20:03:52Z",
        "authors": [
            "Ahmet-Serdar Karakaya",
            "Ioan-Alexandru Stef",
            "Konstantin K\u00f6hler",
            "Julian Heinovski",
            "Falko Dressler"
        ],
        "comments": "arXiv admin note: substantial text overlap with arXiv:2205.04538",
        "categories": [
            "cs.MA",
            "cs.SY",
            "eess.SY"
        ],
        "primary_category": "cs.MA",
        "doi": "10.1016/j.comcom.2023.04.015",
        "journal_ref": "Elsevier Computer Communications, vol. 205, pp. 97-107, May 2023"
    },
    "2305.01883v2": {
        "url": "http://arxiv.org/abs/2305.01883v2",
        "title": "A Lightweight CNN-Transformer Model for Learning Traveling Salesman\n  Problems",
        "summary": "Several studies have attempted to solve traveling salesman problems (TSPs)\nusing various deep learning techniques. Among them, Transformer-based models\nshow state-of-the-art performance even for large-scale Traveling Salesman\nProblems (TSPs). However, they are based on fully-connected attention models\nand suffer from large computational complexity and GPU memory usage. Our work\nis the first CNN-Transformer model based on a CNN embedding layer and partial\nself-attention for TSP. Our CNN-Transformer model is able to better learn\nspatial features from input data using a CNN embedding layer compared with the\nstandard Transformer-based models. It also removes considerable redundancy in\nfully-connected attention models using the proposed partial self-attention.\nExperimental results show that the proposed CNN embedding layer and partial\nself-attention are very effective in improving performance and computational\ncomplexity. The proposed model exhibits the best performance in real-world\ndatasets and outperforms other existing state-of-the-art (SOTA)\nTransformer-based models in various aspects. Our code is publicly available at\nhttps://github.com/cm8908/CNN_Transformer3.",
        "updated": "2024-03-06T01:45:16Z",
        "published": "2023-05-03T04:28:10Z",
        "authors": [
            "Minseop Jung",
            "Jaeseung Lee",
            "Jibum Kim"
        ],
        "categories": [
            "cs.LG",
            "cs.CG"
        ],
        "primary_category": "cs.LG"
    },
    "2305.02151v2": {
        "url": "http://arxiv.org/abs/2305.02151v2",
        "title": "Identifying the Correlation Between Language Distance and Cross-Lingual\n  Transfer in a Multilingual Representation Space",
        "summary": "Prior research has investigated the impact of various linguistic features on\ncross-lingual transfer performance. In this study, we investigate the manner in\nwhich this effect can be mapped onto the representation space. While past\nstudies have focused on the impact on cross-lingual alignment in multilingual\nlanguage models during fine-tuning, this study examines the absolute evolution\nof the respective language representation spaces produced by MLLMs. We place a\nspecific emphasis on the role of linguistic characteristics and investigate\ntheir inter-correlation with the impact on representation spaces and\ncross-lingual transfer performance. Additionally, this paper provides\npreliminary evidence of how these findings can be leveraged to enhance transfer\nto linguistically distant languages.",
        "updated": "2024-03-27T08:43:28Z",
        "published": "2023-05-03T14:33:23Z",
        "authors": [
            "Fred Philippy",
            "Siwen Guo",
            "Shohreh Haddadan"
        ],
        "comments": "SIGTYP Workshop 2023 (co-located with EACL 2023)",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "primary_category": "cs.CL",
        "doi": "10.18653/v1/2023.sigtyp-1.3"
    },
    "2305.02215v2": {
        "url": "http://arxiv.org/abs/2305.02215v2",
        "title": "Exploring Linguistic Properties of Monolingual BERTs with Typological\n  Classification among Languages",
        "summary": "The impressive achievements of transformers force NLP researchers to delve\ninto how these models represent the underlying structure of natural language.\nIn this paper, we propose a novel standpoint to investigate the above issue:\nusing typological similarities among languages to observe how their respective\nmonolingual models encode structural information. We aim to layer-wise compare\ntransformers for typologically similar languages to observe whether these\nsimilarities emerge for particular layers. For this investigation, we propose\nto use Centered Kernel Alignment to measure similarity among weight matrices.\nWe found that syntactic typological similarity is consistent with the\nsimilarity between the weights in the middle layers, which are the pretrained\nBERT layers to which syntax encoding is generally attributed. Moreover, we\nobserve that a domain adaptation on semantically equivalent texts enhances this\nsimilarity among weight matrices.",
        "updated": "2024-02-29T08:35:05Z",
        "published": "2023-05-03T15:52:17Z",
        "authors": [
            "Elena Sofia Ruzzetti",
            "Federico Ranaldi",
            "Felicia Logozzo",
            "Michele Mastromattei",
            "Leonardo Ranaldi",
            "Fabio Massimo Zanzotto"
        ],
        "categories": [
            "cs.CL",
            "cs.AI"
        ],
        "primary_category": "cs.CL"
    },
    "2305.02337v2": {
        "url": "http://arxiv.org/abs/2305.02337v2",
        "title": "Towards Hamiltonian Simulation with Decision Diagrams",
        "summary": "This paper proposes a novel approach to Hamiltonian simulation using Decision\nDiagrams (DDs), which are an exact representation based on exploiting\nredundancies in representations of quantum states and operations. While the\nsimulation of Hamiltonians has been studied extensively, scaling these\nsimulations to larger or more complex systems is often challenging and may\nrequire approximations or new simulation methods altogether. DDs offer such an\nalternative that has not yet been applied to Hamiltonian simulation. In this\nwork, we investigate the behavior of DDs for this task. To this end, we review\nthe basics of DDs such as their construction and present how the relevant\noperations for Hamiltonian simulation are implemented in this data structure --\nleading to the first DD-based Hamiltonian simulation approach. Based on several\nseries of evaluations and comparisons, we then discuss insights about the\nperformance of this complementary approach. Overall, these studies show that\nDDs indeed may offer a promising new data structure which, for certain\nexamples, can provide orders of magnitudes of improvement compared to the\nstate-of-the-art, yet also comes with its own, fundamentally different,\nlimitations.",
        "updated": "2024-03-01T13:39:12Z",
        "published": "2023-05-03T18:00:00Z",
        "authors": [
            "Aaron Sander",
            "Lukas Burgholzer",
            "Robert Wille"
        ],
        "comments": "12 pages, 4 figures",
        "categories": [
            "quant-ph",
            "cond-mat.other",
            "cs.ET"
        ],
        "primary_category": "quant-ph",
        "journal_ref": "2023 IEEE International Conference on Quantum Computing and\n  Engineering (QCE)"
    },
    "2305.02381v2": {
        "url": "http://arxiv.org/abs/2305.02381v2",
        "title": "Discovering Communication Pattern Shifts in Large-Scale Labeled Networks\n  using Encoder Embedding and Vertex Dynamics",
        "summary": "Analyzing large-scale time-series network data, such as social media and\nemail communications, poses a significant challenge in understanding social\ndynamics, detecting anomalies, and predicting trends. In particular, the\nscalability of graph analysis is a critical hurdle impeding progress in\nlarge-scale downstream inference. To address this challenge, we introduce a\ntemporal encoder embedding method. This approach leverages ground-truth or\nestimated vertex labels, enabling an efficient embedding of large-scale graph\ndata and the processing of billions of edges within minutes. Furthermore, this\nembedding unveils a temporal dynamic statistic capable of detecting\ncommunication pattern shifts across all levels, ranging from individual\nvertices to vertex communities and the overall graph structure. We provide\ntheoretical support to confirm its soundness under random graph models, and\ndemonstrate its numerical advantages in capturing evolving communities and\nidentifying outliers. Finally, we showcase the practical application of our\napproach by analyzing an anonymized time-series communication network from a\nlarge organization spanning 2019-2020, enabling us to assess the impact of\nCovid-19 on workplace communication patterns.",
        "updated": "2023-11-29T21:05:22Z",
        "published": "2023-05-03T18:38:01Z",
        "authors": [
            "Cencheng Shen",
            "Jonathan Larson",
            "Ha Trinh",
            "Xihan Qin",
            "Youngser Park",
            "Carey E. Priebe"
        ],
        "comments": "10 pages + 2 pages appendix, 8 figures",
        "categories": [
            "cs.SI",
            "stat.ML"
        ],
        "primary_category": "cs.SI",
        "doi": "10.1109/TNSE.2023.3337600",
        "journal_ref": "IEEE Transactions on Network Science and Engineering, 11(2):\n  2100-2109, 2024"
    },
    "2305.02483v2": {
        "url": "http://arxiv.org/abs/2305.02483v2",
        "title": "Personalized Abstractive Summarization by Tri-agent Generation Pipeline",
        "summary": "Tailoring outputs from large language models, like ChatGPT, to implicit user\npreferences remains a challenge despite their impressive generative\ncapabilities. In this paper, we propose a tri-agent generation pipeline\ncomprising a generator, an instructor, and an editor to enhance output\npersonalization. The generator produces an initial output, the instructor\nautomatically generates editing instructions based on user preferences, and the\neditor refines the output to align with those preferences. The inference-only\nlarge language model (ChatGPT) serves as both the generator and editor, with a\nsmaller model acting as the instructor to guide output generation. We train the\ninstructor using editor-steered reinforcement learning, leveraging feedback\nfrom a large-scale editor model to optimize instruction generation.\nExperimental results on two abstractive summarization datasets demonstrate the\neffectiveness of our approach in generating outputs that better meet user\nexpectations. Code is available at\n\\url{https://github.com/Wendy-Xiao/chatgpt_editing_summ}",
        "updated": "2024-03-01T23:41:24Z",
        "published": "2023-05-04T01:12:35Z",
        "authors": [
            "Wen Xiao",
            "Yujia Xie",
            "Giuseppe Carenini",
            "Pengcheng He"
        ],
        "comments": "Accepted at EACL 2024 Findings",
        "categories": [
            "cs.CL"
        ],
        "primary_category": "cs.CL"
    },
    "2305.02531v6": {
        "url": "http://arxiv.org/abs/2305.02531v6",
        "title": "Can LLMs Capture Human Preferences?",
        "summary": "We explore the viability of Large Language Models (LLMs), specifically\nOpenAI's GPT-3.5 and GPT-4, in emulating human survey respondents and eliciting\npreferences, with a focus on intertemporal choices. Leveraging the extensive\nliterature on intertemporal discounting for benchmarking, we examine responses\nfrom LLMs across various languages and compare them to human responses,\nexploring preferences between smaller, sooner, and larger, later rewards. Our\nfindings reveal that both GPT models demonstrate less patience than humans,\nwith GPT-3.5 exhibiting a lexicographic preference for earlier rewards, unlike\nhuman decision-makers. Though GPT-4 does not display lexicographic preferences,\nits measured discount rates are still considerably larger than those found in\nhumans. Interestingly, GPT models show greater patience in languages with weak\nfuture tense references, such as German and Mandarin, aligning with existing\nliterature that suggests a correlation between language structure and\nintertemporal preferences. We demonstrate how prompting GPT to explain its\ndecisions, a procedure we term \"chain-of-thought conjoint,\" can mitigate, but\ndoes not eliminate, discrepancies between LLM and human responses. While\ndirectly eliciting preferences using LLMs may yield misleading results,\ncombining chain-of-thought conjoint with topic modeling aids in hypothesis\ngeneration, enabling researchers to explore the underpinnings of preferences.\nChain-of-thought conjoint provides a structured framework for marketers to use\nLLMs to identify potential attributes or factors that can explain preference\nheterogeneity across different customers and contexts.",
        "updated": "2024-02-29T18:20:04Z",
        "published": "2023-05-04T03:51:31Z",
        "authors": [
            "Ali Goli",
            "Amandeep Singh"
        ],
        "categories": [
            "cs.CL",
            "cs.AI"
        ],
        "primary_category": "cs.CL"
    },
    "2305.02804v2": {
        "url": "http://arxiv.org/abs/2305.02804v2",
        "title": "An asymptotic preserving kinetic scheme for the M1 model of linear\n  transport",
        "summary": "Moment models with suitable closure can lead to accurate and computationally\nefficient solvers for particle transport. Hence, we propose a new asymptotic\npreserving scheme for the M1 model of linear transport that works uniformly for\nany Knudsen number. Our idea is to apply the M1 closure at the numerical level\nto an existing asymptotic preserving scheme for the corresponding kinetic\nequation, namely the Unified Gas Kinetic scheme (UGKS) originally proposed in\n[27] and extended to linear transport in [24]. In order to ensure the moments\nrealizability in this new scheme, the UGKS positivity needs to be maintained.\nWe propose a new density reconstruction in time to obtain this property. A\nsecond order extension is also suggested and validated. Several test cases show\nthe performances of this new scheme.",
        "updated": "2024-03-18T10:42:46Z",
        "published": "2023-04-13T15:08:08Z",
        "authors": [
            "Feugeas Jean-Luc",
            "Mathiaud Julien",
            "Mieussens Luc",
            "Vigier Thomas"
        ],
        "categories": [
            "math.NA",
            "cs.NA",
            "math-ph",
            "math.MP"
        ],
        "primary_category": "math.NA"
    },
    "2305.03001v2": {
        "url": "http://arxiv.org/abs/2305.03001v2",
        "title": "OSDaR23: Open Sensor Data for Rail 2023",
        "summary": "To achieve a driverless train operation on mainline railways, actual and\npotential obstacles for the train's driveway must be detected automatically by\nappropriate sensor systems. Machine learning algorithms have proven to be\npowerful tools for this task during the last years. However, these algorithms\nrequire large amounts of high-quality annotated data containing\nrailway-specific objects as training data. Unfortunately, all of the publicly\navailable datasets that tackle this requirement are restricted in some way.\nTherefore, this paper presents OSDaR23, a multi-sensor dataset of 45\nsubsequences acquired in Hamburg, Germany, in September 2021, that was created\nto foster driverless train operation on mainline railways. The sensor setup\nconsists of multiple calibrated and synchronized infrared (IR) and visual (RGB)\ncameras, lidars, a radar, and position and acceleration sensors mounted on the\nfront of a rail vehicle. In addition to the raw data, the dataset contains\n204091 polyline, polygonal, rectangle, and cuboid annotations in total for 20\ndifferent object classes. It is the first publicly available multi-sensor\ndataset annotated with a variety of object classes that are relevant for the\nrailway context. OSDaR23, available at data.fid-move.de/dataset/osdar23, can\nalso be used for tasks beyond collision prediction, which are listed in this\npaper.",
        "updated": "2024-03-19T19:40:44Z",
        "published": "2023-05-04T17:19:47Z",
        "authors": [
            "Rustam Tagiew",
            "Martin K\u00f6ppel",
            "Karsten Schwalbe",
            "Patrick Denzler",
            "Philipp Neumaier",
            "Tobias Klockau",
            "Martin Boekhoff",
            "Pavel Klasek",
            "Roman Tilly"
        ],
        "comments": "7 pages, 11 images, 5 tables",
        "categories": [
            "cs.CV",
            "cs.RO",
            "68T40",
            "I.2.9"
        ],
        "primary_category": "cs.CV",
        "doi": "10.1109/ICRAE59816.2023.10458449",
        "journal_ref": "8th International Conference on Robotics and Automation\n  Engineering (ICRAE), Singapore, Singapore, 2023, pp. 270-276"
    },
    "2306.15470v3": {
        "url": "http://arxiv.org/abs/2306.15470v3",
        "title": "Task-oriented and Semantics-aware Communication Framework for\n  Avatar-centric Augmented Reality",
        "summary": "Upon the advent of the emerging metaverse and its related applications in\nAugmented Reality (AR), the current bit-oriented network struggles to support\nreal-time changes for the vast amount of associated information, hindering its\ndevelopment. Thus, a critical revolution in the Sixth Generation (6G) networks\nis envisioned through the joint exploitation of information context and its\nimportance to the task, leading to a communication paradigm shift towards\nsemantic and effectiveness levels. However, current research has not yet\nproposed any explicit and systematic communication framework for AR\napplications that incorporate these two levels. To fill this research gap, this\npaper presents a task-oriented and semantics-aware communication framework for\naugmented reality (TSAR) to enhance communication efficiency and effectiveness\nin 6G. Specifically, we first analyse the traditional wireless AR point cloud\ncommunication framework and then summarize our proposed semantic information\nalong with the end-to-end wireless communication. We then detail the design\nblocks of the TSAR framework, covering both semantic and effectiveness levels.\nFinally, numerous experiments have been conducted to demonstrate that, compared\nto the traditional point cloud communication framework, our proposed TSAR\nsignificantly reduces wireless AR application transmission latency by 95.6%,\nwhile improving communication effectiveness in geometry and color aspects by up\nto 82.4% and 20.4%, respectively.",
        "updated": "2024-03-26T17:32:47Z",
        "published": "2023-06-27T13:41:54Z",
        "authors": [
            "Zhe Wang",
            "Yansha Deng",
            "A. Hamid Aghvami"
        ],
        "categories": [
            "cs.IT",
            "math.IT"
        ],
        "primary_category": "cs.IT"
    },
    "2306.15612v2": {
        "url": "http://arxiv.org/abs/2306.15612v2",
        "title": "Adaptive Multi-Modal Cross-Entropy Loss for Stereo Matching",
        "summary": "Despite the great success of deep learning in stereo matching, recovering\naccurate disparity maps is still challenging. Currently, L1 and cross-entropy\nare the two most widely used losses for stereo network training. Compared with\nthe former, the latter usually performs better thanks to its probability\nmodeling and direct supervision to the cost volume. However, how to accurately\nmodel the stereo ground-truth for cross-entropy loss remains largely\nunder-explored. Existing works simply assume that the ground-truth\ndistributions are uni-modal, which ignores the fact that most of the edge\npixels can be multi-modal. In this paper, a novel adaptive multi-modal\ncross-entropy loss (ADL) is proposed to guide the networks to learn different\ndistribution patterns for each pixel. Moreover, we optimize the disparity\nestimator to further alleviate the bleeding or misalignment artifacts in\ninference. Extensive experimental results show that our method is generic and\ncan help classic stereo networks regain state-of-the-art performance. In\nparticular, GANet with our method ranks $1^{st}$ on both the KITTI 2015 and\n2012 benchmarks among the published methods. Meanwhile, excellent\nsynthetic-to-realistic generalization performance can be achieved by simply\nreplacing the traditional loss with ours.",
        "updated": "2024-03-15T10:04:38Z",
        "published": "2023-06-27T16:53:35Z",
        "authors": [
            "Peng Xu",
            "Zhiyu Xiang",
            "Chenyu Qiao",
            "Jingyun Fu",
            "Tianyu Pu"
        ],
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2306.15620v3": {
        "url": "http://arxiv.org/abs/2306.15620v3",
        "title": "SCENEREPLICA: Benchmarking Real-World Robot Manipulation by Creating\n  Replicable Scenes",
        "summary": "We present a new reproducible benchmark for evaluating robot manipulation in\nthe real world, specifically focusing on pick-and-place. Our benchmark uses the\nYCB objects, a commonly used dataset in the robotics community, to ensure that\nour results are comparable to other studies. Additionally, the benchmark is\ndesigned to be easily reproducible in the real world, making it accessible to\nresearchers and practitioners. We also provide our experimental results and\nanalyzes for model-based and model-free 6D robotic grasping on the benchmark,\nwhere representative algorithms are evaluated for object perception, grasping\nplanning, and motion planning. We believe that our benchmark will be a valuable\ntool for advancing the field of robot manipulation. By providing a standardized\nevaluation framework, researchers can more easily compare different techniques\nand algorithms, leading to faster progress in developing robot manipulation\nmethods.",
        "updated": "2024-03-11T06:20:07Z",
        "published": "2023-06-27T16:59:15Z",
        "authors": [
            "Ninad Khargonkar",
            "Sai Haneesh Allu",
            "Yangxiao Lu",
            "Jishnu Jaykumar P",
            "Balakrishnan Prabhakaran",
            "Yu Xiang"
        ],
        "comments": "Accepted to ICRA 2024. Project page is available at\n  https://irvlutd.github.io/SceneReplica",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
        ],
        "primary_category": "cs.RO"
    },
    "2306.15802v2": {
        "url": "http://arxiv.org/abs/2306.15802v2",
        "title": "Implicit Boundary Conditions in Partial Differential Equations\n  Discretizations: Identifying Spurious Modes and Model Reduction",
        "summary": "We revisit the problem of spurious modes that are sometimes encountered in\npartial differential equations discretizations. It is generally suspected that\none of the causes for spurious modes is due to how boundary conditions are\ntreated, and we use this as the starting point of our investigations. By\nregarding boundary conditions as algebraic constraints on a differential\nequation, we point out that any differential equation with homogeneous boundary\nconditions also admits a typically infinite number of hidden or implicit\nboundary conditions. In most discretization schemes, these additional implicit\nboundary conditions are violated, and we argue that this is what leads to the\nemergence of spurious modes. These observations motivate two definitions of the\nquality of computed eigenvalues based on violations of derivatives of boundary\nconditions on the one hand, and on the Grassmann distance between subspaces\nassociated with computed eigenspaces on the other. Both of these tests are\nbased on a standardized treatment of boundary conditions and do not require a\npriori knowledge of eigenvalue locations. The effectiveness of these tests is\ndemonstrated on several examples known to have spurious modes. In addition,\nthese quality tests show that in most problems, about half the computed\nspectrum of a differential operator is of low quality. The tests also\nspecifically identify the low accuracy modes, which can then be projected out\nas a type of model reduction scheme.",
        "updated": "2024-03-08T05:46:25Z",
        "published": "2023-06-27T21:07:19Z",
        "authors": [
            "Pascal R Karam",
            "Bassam Bamieh"
        ],
        "comments": "15 pages, 4 figures",
        "categories": [
            "math.NA",
            "cs.NA",
            "cs.SY",
            "eess.SY",
            "math.DS"
        ],
        "primary_category": "math.NA",
        "doi": "10.1016/j.jcp.2023.112743",
        "journal_ref": "J. Comput. Phys. 500 (2024)"
    },
    "2306.15909v4": {
        "url": "http://arxiv.org/abs/2306.15909v4",
        "title": "RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$",
        "summary": "Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as\npromising approaches for learning data-efficient RL algorithms tailored to a\ngiven task distribution. However, they show poor asymptotic performance and\nstruggle with out-of-distribution tasks because they rely on sequence models,\nsuch as recurrent neural networks or transformers, to process experiences\nrather than summarize them using general-purpose RL components such as value\nfunctions. In contrast, traditional RL algorithms are data-inefficient as they\ndo not use domain knowledge, but they do converge to an optimal policy in the\nlimit. We propose RL$^3$, a principled hybrid approach that incorporates\naction-values, learned per task through traditional RL, in the inputs to\nmeta-RL. We show that RL$^3$ earns greater cumulative reward in the long term,\ncompared to RL$^2$, while maintaining data-efficiency in the short term, and\ngeneralizes better to out-of-distribution tasks. Experiments are conducted on\nboth custom and benchmark discrete domains from the meta-RL literature that\nexhibit a range of short-term, long-term, and complex dependencies.",
        "updated": "2024-03-26T15:13:20Z",
        "published": "2023-06-28T04:16:16Z",
        "authors": [
            "Abhinav Bhatia",
            "Samer B. Nashed",
            "Shlomo Zilberstein"
        ],
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "primary_category": "cs.LG"
    },
    "2306.15924v3": {
        "url": "http://arxiv.org/abs/2306.15924v3",
        "title": "The Parametric Complexity of Operator Learning",
        "summary": "Neural operator architectures employ neural networks to approximate operators\nmapping between Banach spaces of functions; they may be used to accelerate\nmodel evaluations via emulation, or to discover models from data. Consequently,\nthe methodology has received increasing attention over recent years, giving\nrise to the rapidly growing field of operator learning. The first contribution\nof this paper is to prove that for general classes of operators which are\ncharacterized only by their $C^r$- or Lipschitz-regularity, operator learning\nsuffers from a ``curse of parametric complexity'', which is an\ninfinite-dimensional analogue of the well-known curse of dimensionality\nencountered in high-dimensional approximation problems. The result is\napplicable to a wide variety of existing neural operators, including PCA-Net,\nDeepONet and the FNO. The second contribution of the paper is to prove that\nthis general curse can be overcome for solution operators defined by the\nHamilton-Jacobi equation; this is achieved by leveraging additional structure\nin the underlying solution operator, going beyond regularity. To this end, a\nnovel neural operator architecture is introduced, termed HJ-Net, which\nexplicitly takes into account characteristic information of the underlying\nHamiltonian system. Error and complexity estimates are derived for HJ-Net which\nshow that this architecture can provably beat the curse of parametric\ncomplexity related to the infinite-dimensional input and output function\nspaces.",
        "updated": "2024-03-01T22:01:50Z",
        "published": "2023-06-28T05:02:03Z",
        "authors": [
            "Samuel Lanthaler",
            "Andrew M. Stuart"
        ],
        "categories": [
            "cs.LG",
            "cs.NA",
            "math.NA"
        ],
        "primary_category": "cs.LG"
    },
    "2306.16001v3": {
        "url": "http://arxiv.org/abs/2306.16001v3",
        "title": "Streamlining Social Media Information Retrieval for COVID-19 Research\n  with Deep Learning",
        "summary": "Objective: Social media-based public health research is crucial for epidemic\nsurveillance, but most studies identify relevant corpora with keyword-matching.\nThis study develops a system to streamline the process of curating colloquial\nmedical dictionaries. We demonstrate the pipeline by curating a UMLS-colloquial\nsymptom dictionary from COVID-19-related tweets as proof of concept. Methods:\nCOVID-19-related tweets from February 1, 2020, to April 30, 2022 were used. The\npipeline includes three modules: a named entity recognition module to detect\nsymptoms in tweets; an entity normalization module to aggregate detected\nentities; and a mapping module that iteratively maps entities to Unified\nMedical Language System concepts. A random 500 entity sample were drawn from\nthe final dictionary for accuracy validation. Additionally, we conducted a\nsymptom frequency distribution analysis to compare our dictionary to a\npre-defined lexicon from previous research. Results: We identified 498,480\nunique symptom entity expressions from the tweets. Pre-processing reduces the\nnumber to 18,226. The final dictionary contains 38,175 unique expressions of\nsymptoms that can be mapped to 966 UMLS concepts (accuracy = 95%). Symptom\ndistribution analysis found that our dictionary detects more symptoms and is\neffective at identifying psychiatric disorders like anxiety and depression,\noften missed by pre-defined lexicons. Conclusions: This study advances public\nhealth research by implementing a novel, systematic pipeline for curating\nsymptom lexicons from social media data. The final lexicon's high accuracy,\nvalidated by medical professionals, underscores the potential of this\nmethodology to reliably interpret and categorize vast amounts of unstructured\nsocial media data into actionable medical insights across diverse linguistic\nand regional landscapes.",
        "updated": "2024-03-18T16:22:16Z",
        "published": "2023-06-28T08:20:35Z",
        "authors": [
            "Yining Hua",
            "Jiageng Wu",
            "Shixu Lin",
            "Minghui Li",
            "Yujie Zhang",
            "Dinah Foer",
            "Siwen Wang",
            "Peilin Zhou",
            "Jie Yang",
            "Li Zhou"
        ],
        "comments": "Updated full paper. Abstract presented at IEEE ICHI 2023 and AMIA\n  Annual Symposium 2023",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.IR"
        ],
        "primary_category": "cs.CL"
    },
    "2306.16175v3": {
        "url": "http://arxiv.org/abs/2306.16175v3",
        "title": "$\\mathbf{C}^2$Former: Calibrated and Complementary Transformer for\n  RGB-Infrared Object Detection",
        "summary": "Object detection on visible (RGB) and infrared (IR) images, as an emerging\nsolution to facilitate robust detection for around-the-clock applications, has\nreceived extensive attention in recent years. With the help of IR images,\nobject detectors have been more reliable and robust in practical applications\nby using RGB-IR combined information. However, existing methods still suffer\nfrom modality miscalibration and fusion imprecision problems. Since transformer\nhas the powerful capability to model the pairwise correlations between\ndifferent features, in this paper, we propose a novel Calibrated and\nComplementary Transformer called $\\mathrm{C}^2$Former to address these two\nproblems simultaneously. In $\\mathrm{C}^2$Former, we design an Inter-modality\nCross-Attention (ICA) module to obtain the calibrated and complementary\nfeatures by learning the cross-attention relationship between the RGB and IR\nmodality. To reduce the computational cost caused by computing the global\nattention in ICA, an Adaptive Feature Sampling (AFS) module is introduced to\ndecrease the dimension of feature maps. Because $\\mathrm{C}^2$Former performs\nin the feature domain, it can be embedded into existed RGB-IR object detectors\nvia the backbone network. Thus, one single-stage and one two-stage object\ndetector both incorporating our $\\mathrm{C}^2$Former are constructed to\nevaluate its effectiveness and versatility. With extensive experiments on the\nDroneVehicle and KAIST RGB-IR datasets, we verify that our method can fully\nutilize the RGB-IR complementary information and achieve robust detection\nresults. The code is available at\nhttps://github.com/yuanmaoxun/Calibrated-and-Complementary-Transformer-for-RGB-Infrared-Object-Detection.git.",
        "updated": "2024-03-13T10:57:24Z",
        "published": "2023-06-28T12:52:48Z",
        "authors": [
            "Maoxun Yuan",
            "Xingxing Wei"
        ],
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "primary_category": "cs.CV"
    },
    "2306.16208v3": {
        "url": "http://arxiv.org/abs/2306.16208v3",
        "title": "Continuous-time q-learning for mean-field control problems",
        "summary": "This paper studies the q-learning, recently coined as the continuous time\ncounterpart of Q-learning by Jia and Zhou (2023), for continuous time\nMckean-Vlasov control problems in the setting of entropy-regularized\nreinforcement learning. In contrast to the single agent's control problem in\nJia and Zhou (2023), the mean-field interaction of agents renders the\ndefinition of the q-function more subtle, for which we reveal that two distinct\nq-functions naturally arise: (i) the integrated q-function (denoted by $q$) as\nthe first-order approximation of the integrated Q-function introduced in Gu,\nGuo, Wei and Xu (2023), which can be learnt by a weak martingale condition\ninvolving test policies; and (ii) the essential q-function (denoted by $q_e$)\nthat is employed in the policy improvement iterations. We show that two\nq-functions are related via an integral representation under all test policies.\nBased on the weak martingale condition and our proposed searching method of\ntest policies, some model-free learning algorithms are devised. In two\nexamples, one in LQ control framework and one beyond LQ control framework, we\ncan obtain the exact parameterization of the optimal value function and\nq-functions and illustrate our algorithms with simulation experiments.",
        "updated": "2024-03-08T02:50:45Z",
        "published": "2023-06-28T13:43:46Z",
        "authors": [
            "Xiaoli Wei",
            "Xiang Yu"
        ],
        "comments": "Keywords: Continuous-time reinforcement learning, continuous-time\n  q-function, Mckean-Vlasov control, weak martingale characterization, test\n  policies",
        "categories": [
            "cs.LG",
            "math.OC",
            "q-fin.CP"
        ],
        "primary_category": "cs.LG"
    },
    "2306.16255v3": {
        "url": "http://arxiv.org/abs/2306.16255v3",
        "title": "Theory and applications of the Sum-Of-Squares technique",
        "summary": "The Sum-of-Squares (SOS) approximation method is a technique used in\noptimization problems to derive lower bounds on the optimal value of an\nobjective function. By representing the objective function as a sum of squares\nin a feature space, the SOS method transforms non-convex global optimization\nproblems into solvable semidefinite programs. This note presents an overview of\nthe SOS method. We start with its application in finite-dimensional feature\nspaces and, subsequently, we extend it to infinite-dimensional feature spaces\nusing reproducing kernels (k-SOS). Additionally, we highlight the utilization\nof SOS for estimating some relevant quantities in information theory, including\nthe log-partition function.",
        "updated": "2024-03-11T13:48:33Z",
        "published": "2023-06-28T14:29:17Z",
        "authors": [
            "Francis Bach",
            "Elisabetta Cornacchia",
            "Luca Pesce",
            "Giovanni Piccioli"
        ],
        "comments": "These are notes from the lecture of Francis Bach given at the summer\n  school \"Statistical Physics & Machine Learning\", that took place in Les\n  Houches School of Physics in France from 4th to 29th July 2022. The school\n  was organized by Florent Krzakala and Lenka Zdeborov\\'a from EPFL. 19 pages,\n  4 figures",
        "categories": [
            "math.OC",
            "cs.IT",
            "math.IT",
            "math.ST",
            "stat.TH"
        ],
        "primary_category": "math.OC"
    },
    "2308.16761v6": {
        "url": "http://arxiv.org/abs/2308.16761v6",
        "title": "Learning Category Trees for ID-Based Recommendation: Exploring the Power\n  of Differentiable Vector Quantization",
        "summary": "Category information plays a crucial role in enhancing the quality and\npersonalization of recommender systems. Nevertheless, the availability of item\ncategory information is not consistently present, particularly in the context\nof ID-based recommendations. In this work, we propose a novel approach to\nautomatically learn and generate entity (i.e., user or item) category trees for\nID-based recommendation. Specifically, we devise a differentiable vector\nquantization framework for automatic category tree generation, namely CAGE,\nwhich enables the simultaneous learning and refinement of categorical code\nrepresentations and entity embeddings in an end-to-end manner, starting from\nthe randomly initialized states. With its high adaptability, CAGE can be easily\nintegrated into both sequential and non-sequential recommender systems. We\nvalidate the effectiveness of CAGE on various recommendation tasks including\nlist completion, collaborative filtering, and click-through rate prediction,\nacross different recommendation models. We release the code and data for others\nto reproduce the reported results.",
        "updated": "2024-03-15T01:54:09Z",
        "published": "2023-08-31T14:29:10Z",
        "authors": [
            "Qijiong Liu",
            "Lu Fan",
            "Jiaren Xiao",
            "Jieming Zhu",
            "Xiao-Ming Wu"
        ],
        "comments": "TheWebConf'24 accepted paper",
        "categories": [
            "cs.IR"
        ],
        "primary_category": "cs.IR"
    },
    "2308.16910v3": {
        "url": "http://arxiv.org/abs/2308.16910v3",
        "title": "Robust Variational Physics-Informed Neural Networks",
        "summary": "We introduce a Robust version of the Variational Physics-Informed Neural\nNetworks method (RVPINNs). As in VPINNs, we define the quadratic loss\nfunctional in terms of a Petrov-Galerkin-type variational formulation of the\nPDE problem: the trial space is a (Deep) Neural Network (DNN) manifold, while\nthe test space is a finite-dimensional vector space. Whereas the VPINN's loss\ndepends upon the selected basis functions of a given test space, herein, we\nminimize a loss based on the discrete dual norm of the residual. The main\nadvantage of such a loss definition is that it provides a reliable and\nefficient estimator of the true error in the energy norm under the assumption\nof the existence of a local Fortin operator. We test the performance and\nrobustness of our algorithm in several advection-diffusion problems. These\nnumerical results perfectly align with our theoretical findings, showing that\nour estimates are sharp.",
        "updated": "2024-03-05T15:48:47Z",
        "published": "2023-08-31T17:59:44Z",
        "authors": [
            "Sergio Rojas",
            "Pawe\u0142 Maczuga",
            "Judit Mu\u00f1oz-Matute",
            "David Pardo",
            "Maciej Paszynski"
        ],
        "categories": [
            "math.NA",
            "cs.NA",
            "65N12, 65N15, 65N22, 65N30, 65N50"
        ],
        "primary_category": "math.NA"
    },
    "2309.00064v1": {
        "url": "http://arxiv.org/abs/2309.00064v1",
        "title": "Ethical Framework for Harnessing the Power of AI in Healthcare and\n  Beyond",
        "summary": "In the past decade, the deployment of deep learning (Artificial Intelligence\n(AI)) methods has become pervasive across a spectrum of real-world\napplications, often in safety-critical contexts. This comprehensive research\narticle rigorously investigates the ethical dimensions intricately linked to\nthe rapid evolution of AI technologies, with a particular focus on the\nhealthcare domain. Delving deeply, it explores a multitude of facets including\ntransparency, adept data management, human oversight, educational imperatives,\nand international collaboration within the realm of AI advancement. Central to\nthis article is the proposition of a conscientious AI framework, meticulously\ncrafted to accentuate values of transparency, equity, answerability, and a\nhuman-centric orientation. The second contribution of the article is the\nin-depth and thorough discussion of the limitations inherent to AI systems. It\nastutely identifies potential biases and the intricate challenges of navigating\nmultifaceted contexts. Lastly, the article unequivocally accentuates the\npressing need for globally standardized AI ethics principles and frameworks.\nSimultaneously, it aptly illustrates the adaptability of the ethical framework\nproposed herein, positioned skillfully to surmount emergent challenges.",
        "updated": "2023-08-31T18:12:12Z",
        "published": "2023-08-31T18:12:12Z",
        "authors": [
            "Sidra Nasir",
            "Rizwan Ahmed Khan",
            "Samita Bai"
        ],
        "categories": [
            "cs.CY",
            "cs.AI"
        ],
        "primary_category": "cs.CY",
        "doi": "10.1109/ACCESS.2024.3369912",
        "journal_ref": "IEEE Access 2024"
    },
    "2309.00125v2": {
        "url": "http://arxiv.org/abs/2309.00125v2",
        "title": "Pure Differential Privacy for Functional Summaries via a Laplace-like\n  Process",
        "summary": "Many existing mechanisms to achieve differential privacy (DP) on\ninfinite-dimensional functional summaries often involve embedding these\nsummaries into finite-dimensional subspaces and applying traditional DP\ntechniques. Such mechanisms generally treat each dimension uniformly and\nstruggle with complex, structured summaries. This work introduces a novel\nmechanism for DP functional summary release: the Independent Component Laplace\nProcess (ICLP) mechanism. This mechanism treats the summaries of interest as\ntruly infinite-dimensional objects, thereby addressing several limitations of\nexisting mechanisms. We establish the feasibility of the proposed mechanism in\nmultiple function spaces. Several statistical estimation problems are\nconsidered, and we demonstrate one can enhance the utility of sanitized\nsummaries by oversmoothing their non-private counterpart. Numerical experiments\non synthetic and real datasets demonstrate the efficacy of the proposed\nmechanism.",
        "updated": "2024-03-03T23:49:48Z",
        "published": "2023-08-31T20:24:51Z",
        "authors": [
            "Haotian Lin",
            "Matthew Reimherr"
        ],
        "categories": [
            "stat.ML",
            "cs.CR",
            "cs.LG"
        ],
        "primary_category": "stat.ML"
    },
    "2309.00255v2": {
        "url": "http://arxiv.org/abs/2309.00255v2",
        "title": "SortedNet, a Place for Every Network and Every Network in its Place:\n  Towards a Generalized Solution for Training Many-in-One Neural Networks",
        "summary": "Deep neural networks (DNNs) must cater to a variety of users with different\nperformance needs and budgets, leading to the costly practice of training,\nstoring, and maintaining numerous specific models. There are solutions in the\nliterature to deal with single dynamic or many-in-one models instead of many\nindividual networks; however, they usually suffer from heavy model search\nrequirements, being architecture-specific, working only on a limited number of\ndimensions (e.g. depth only or width only) or sub-models. To address these\nproblems, we propose SortedNet, a generalized and scalable training solution to\nharness the inherent modularity of DNNs. Thanks to a generalized nested\narchitecture (which we refer to as \\textit{sorted} architecture in this paper)\nwith shared parameters and its novel update scheme combining random sub-model\nsampling and gradient accumulation, SortedNet enables the training of numerous\nsub-models simultaneously, simplifies dynamic model selection and deployment\nduring inference, and reduces the model storage requirement significantly. The\nversatility and scalability of SortedNet are validated through various\narchitectures and tasks including LLaMA, BERT, RoBERTa (NLP tasks), ResNet and\nMobileNet (image classification) demonstrating its superiority over existing\ndynamic training methods. SortedNet is able to train up to 160 sub-models at\nonce, achieving at least 96\\% of the original model's performance.",
        "updated": "2024-03-03T05:26:03Z",
        "published": "2023-09-01T05:12:25Z",
        "authors": [
            "Mojtaba Valipour",
            "Mehdi Rezagholizadeh",
            "Hossein Rajabzadeh",
            "Parsa Kavehzadeh",
            "Marzieh Tahaei",
            "Boxing Chen",
            "Ali Ghodsi"
        ],
        "categories": [
            "cs.LG"
        ],
        "primary_category": "cs.LG"
    },
    "2309.00344v4": {
        "url": "http://arxiv.org/abs/2309.00344v4",
        "title": "A Complete Dependency Pair Framework for Almost-Sure Innermost\n  Termination of Probabilistic Term Rewriting",
        "summary": "Recently, the well-known dependency pair (DP) framework was adapted to a\ndependency tuple framework in order to prove almost-sure innermost termination\n(iAST) of probabilistic term rewrite systems. While this approach was\nincomplete, in this paper, we improve it into a complete criterion for iAST by\npresenting a new, more elegant definition of DPs for probabilistic term\nrewriting. Based on this, we extend the probabilistic DP framework by new\ntransformations. Our implementation in the tool AProVE shows that they increase\nits power considerably.",
        "updated": "2024-02-29T18:29:44Z",
        "published": "2023-09-01T09:03:49Z",
        "authors": [
            "Jan-Christoph Kassing",
            "Stefan Dollase",
            "J\u00fcrgen Giesl"
        ],
        "comments": "arXiv admin note: text overlap with arXiv:2305.11741",
        "categories": [
            "cs.LO"
        ],
        "primary_category": "cs.LO"
    },
    "2309.00359v4": {
        "url": "http://arxiv.org/abs/2309.00359v4",
        "title": "Large Content And Behavior Models To Understand, Simulate, And Optimize\n  Content And Behavior",
        "summary": "Shannon and Weaver's seminal information theory divides communication into\nthree levels: technical, semantic, and effectiveness. While the technical level\ndeals with the accurate reconstruction of transmitted symbols, the semantic and\neffectiveness levels deal with the inferred meaning and its effect on the\nreceiver. Large Language Models (LLMs), with their wide generalizability, make\nsome progress towards the second level. However, LLMs and other communication\nmodels are not conventionally designed for predicting and optimizing\ncommunication for desired receiver behaviors and intents. As a result, the\neffectiveness level remains largely untouched by modern communication systems.\nIn this paper, we introduce the receivers' \"behavior tokens,\" such as shares,\nlikes, clicks, purchases, and retweets, in the LLM's training corpora to\noptimize content for the receivers and predict their behaviors. Other than\nshowing similar performance to LLMs on content understanding tasks, our trained\nmodels show generalization capabilities on the behavior dimension for behavior\nsimulation, content simulation, behavior understanding, and behavior domain\nadaptation. We show results on all these capabilities using a wide range of\ntasks on three corpora. We call these models Large Content and Behavior Models\n(LCBMs). Further, to spur more research on LCBMs, we release our new Content\nBehavior Corpus (CBC), a repository containing communicator, message, and\ncorresponding receiver behavior (https://behavior-in-the-wild.github.io/LCBM).",
        "updated": "2024-03-16T14:02:45Z",
        "published": "2023-09-01T09:34:49Z",
        "authors": [
            "Ashmit Khandelwal",
            "Aditya Agrawal",
            "Aanisha Bhattacharyya",
            "Yaman K Singla",
            "Somesh Singh",
            "Uttaran Bhattacharya",
            "Ishita Dasgupta",
            "Stefano Petrangeli",
            "Rajiv Ratn Shah",
            "Changyou Chen",
            "Balaji Krishnamurthy"
        ],
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "primary_category": "cs.CL"
    },
    "2309.00464v2": {
        "url": "http://arxiv.org/abs/2309.00464v2",
        "title": "A Theoretical and Practical Framework for Evaluating Uncertainty\n  Calibration in Object Detection",
        "summary": "The proliferation of Deep Neural Networks has resulted in machine learning\nsystems becoming increasingly more present in various real-world applications.\nConsequently, there is a growing demand for highly reliable models in many\ndomains, making the problem of uncertainty calibration pivotal when considering\nthe future of deep learning. This is especially true when considering object\ndetection systems, that are commonly present in safety-critical applications\nsuch as autonomous driving, robotics and medical diagnosis. For this reason,\nthis work presents a novel theoretical and practical framework to evaluate\nobject detection systems in the context of uncertainty calibration. This\nencompasses a new comprehensive formulation of this concept through distinct\nformal definitions, and also three novel evaluation metrics derived from such\ntheoretical foundation. The robustness of the proposed uncertainty calibration\nmetrics is shown through a series of representative experiments.",
        "updated": "2024-03-18T14:24:34Z",
        "published": "2023-09-01T14:02:44Z",
        "authors": [
            "Pedro Conde",
            "Rui L. Lopes",
            "Cristiano Premebida"
        ],
        "comments": "Pre-print",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "primary_category": "cs.CV"
    },
    "2309.00475v2": {
        "url": "http://arxiv.org/abs/2309.00475v2",
        "title": "Effective equation solving, constraints and growth in virtually abelian\n  groups",
        "summary": "In this paper we study the satisfiability and solutions of group equations\nwhen combinatorial, algebraic and language-theoretic constraints are imposed on\nthe solutions. We show that the solutions to equations with length,\nlexicographic order, abelianisation or context-free constraints added, can be\neffectively produced in finitely generated virtually abelian groups. Crucially,\nwe translate each of the constraints above into a rational set in an effective\nway, and so reduce each problem to solving equations with rational constraints,\nwhich is decidable and well understood in virtually abelian groups. A byproduct\nof our results is that the growth series of a virtually abelian group, with\nrespect to any generating set and any weight, is effectively computable. This\nseries is known to be rational by a result of Benson, but his proof is\nnon-constructive.",
        "updated": "2024-03-28T14:29:54Z",
        "published": "2023-09-01T14:14:03Z",
        "authors": [
            "Laura Ciobanu",
            "Alex Evetts",
            "Alex Levine"
        ],
        "comments": "28 pages",
        "categories": [
            "math.GR",
            "cs.DM",
            "cs.FL",
            "03D05, 20F10, 20F65, 68Q45"
        ],
        "primary_category": "math.GR"
    },
    "2309.00513v2": {
        "url": "http://arxiv.org/abs/2309.00513v2",
        "title": "A normative approach to radicalization in social networks",
        "summary": "In recent decades, the massification of online social connections has made\ninformation globally accessible in a matter of seconds. Unfortunately, this has\nbeen accompanied by a dramatic surge in extreme opinions, without a clear\nsolution in sight. Using a model performing probabilistic inference in\nlarge-scale loopy graphs through exchange of messages between nodes, we show\nhow circularity in the social graph directly leads to radicalization and the\npolarization of opinions. We demonstrate that these detrimental effects could\nbe avoided if the correlations between incoming messages could be decreased.\nThis approach is based on an extension of Belief Propagation (BP) named\nCircular Belief Propagation (CBP) that can be trained to drastically improve\ninference within a cyclic graph. CBP was benchmarked using data from Facebook\nand Twitter. This approach could inspire new methods for preventing the viral\nspreading and amplification of misinformation online, improving the capacity of\nsocial networks to share knowledge globally without resorting to censorship.",
        "updated": "2024-03-07T17:37:07Z",
        "published": "2023-09-01T15:06:37Z",
        "authors": [
            "Vincent Bouttier",
            "Salom\u00e9 Leclercq",
            "Renaud Jardri",
            "Sophie Deneve"
        ],
        "comments": "22 pages, 8 figures, 1 supplementary material",
        "categories": [
            "cs.SI"
        ],
        "primary_category": "cs.SI"
    },
    "2310.01819v3": {
        "url": "http://arxiv.org/abs/2310.01819v3",
        "title": "TP2O: Creative Text Pair-to-Object Generation using Balance\n  Swap-Sampling",
        "summary": "Generating creative combinatorial objects from two seemingly unrelated object\ntexts is a challenging task in text-to-image synthesis, often hindered by a\nfocus on emulating existing data distributions. In this paper, we develop a\nstraightforward yet highly effective method, called \\textbf{balance\nswap-sampling}. First, we propose a swapping mechanism that generates a novel\ncombinatorial object image set by randomly exchanging intrinsic elements of two\ntext embeddings through a cutting-edge diffusion model. Second, we introduce a\nbalance swapping region to efficiently sample a small subset from the newly\ngenerated image set by balancing CLIP distances between the new images and\ntheir original generations, increasing the likelihood of accepting the\nhigh-quality combinations. Last, we employ a segmentation method to compare\nCLIP distances among the segmented components, ultimately selecting the most\npromising object from the sampled subset. Extensive experiments demonstrate\nthat our approach outperforms recent SOTA T2I methods. Surprisingly, our\nresults even rival those of human artists, such as frog-broccoli.",
        "updated": "2024-03-26T12:59:39Z",
        "published": "2023-10-03T06:16:38Z",
        "authors": [
            "Jun Li",
            "Zedong Zhang",
            "Jian Yang"
        ],
        "comments": "Project page: https://tp2o.github.io/anon/",
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2310.01820v2": {
        "url": "http://arxiv.org/abs/2310.01820v2",
        "title": "Towards Robust Fidelity for Evaluating Explainability of Graph Neural\n  Networks",
        "summary": "Graph Neural Networks (GNNs) are neural models that leverage the dependency\nstructure in graphical data via message passing among the graph nodes. GNNs\nhave emerged as pivotal architectures in analyzing graph-structured data, and\ntheir expansive application in sensitive domains requires a comprehensive\nunderstanding of their decision-making processes -- necessitating a framework\nfor GNN explainability. An explanation function for GNNs takes a pre-trained\nGNN along with a graph as input, to produce a `sufficient statistic' subgraph\nwith respect to the graph label. A main challenge in studying GNN\nexplainability is to provide fidelity measures that evaluate the performance of\nthese explanation functions. This paper studies this foundational challenge,\nspotlighting the inherent limitations of prevailing fidelity metrics, including\n$Fid_+$, $Fid_-$, and $Fid_\\Delta$. Specifically, a formal,\ninformation-theoretic definition of explainability is introduced and it is\nshown that existing metrics often fail to align with this definition across\nvarious statistical scenarios. The reason is due to potential distribution\nshifts when subgraphs are removed in computing these fidelity measures.\nSubsequently, a robust class of fidelity measures are introduced, and it is\nshown analytically that they are resilient to distribution shift issues and are\napplicable in a wide range of scenarios. Extensive empirical analysis on both\nsynthetic and real datasets are provided to illustrate that the proposed\nmetrics are more coherent with gold standard metrics. The source code is\navailable at https://trustai4s-lab.github.io/fidelity.",
        "updated": "2024-03-17T17:35:59Z",
        "published": "2023-10-03T06:25:14Z",
        "authors": [
            "Xu Zheng",
            "Farhad Shirani",
            "Tianchun Wang",
            "Wei Cheng",
            "Zhuomin Chen",
            "Haifeng Chen",
            "Hua Wei",
            "Dongsheng Luo"
        ],
        "comments": "Accepted by International Conference on Learning Representations\n  (ICLR 2024); 26 Pages, 12 figures",
        "categories": [
            "cs.LG"
        ],
        "primary_category": "cs.LG"
    },
    "2310.01968v2": {
        "url": "http://arxiv.org/abs/2310.01968v2",
        "title": "PyHexTop: a compact Python code for topology optimization using\n  hexagonal elements",
        "summary": "Python serves as an open-source and cost-effective alternative to the MATLAB\nprogramming language. This paper introduces a concise topology optimization\nPython code, named ``\\texttt{PyHexTop},\" primarily intended for educational\npurposes. Code employs hexagonal elements to parameterize design domains as\nsuch elements provide checkerboard-free optimized design naturally.\n\\texttt{PyHexTop} is developed based on the ``\\texttt{HoneyTop90}\" MATLAB\ncode~\\cite{kumar2023honeytop90} and uses the \\texttt{NumPy} and \\texttt{SciPy}\nlibraries. Code is straightforward and easily comprehensible, proving a helpful\ntool that can help people new in the topology optimization field to learn and\nexplore. \\texttt{PyHexTop} is specifically tailored to address compliance\nminimization with specified volume constraints. The paper provides a detailed\nexplanation of the code for solving the Messerschmitt-Bolkow-Blohm beam and\nextensions to solve problems different problems. The code is publicly shared\nat: \\url{https://github.com/PrabhatIn/PyHexTop.}",
        "updated": "2024-03-23T06:00:31Z",
        "published": "2023-10-03T11:21:34Z",
        "authors": [
            "Aditi Agarwal",
            "Anupam Saxena",
            "Prabhat Kumar"
        ],
        "comments": "Accepted in NCMDAO 2023 conference",
        "categories": [
            "cs.CE"
        ],
        "primary_category": "cs.CE"
    },
    "2310.01986v2": {
        "url": "http://arxiv.org/abs/2310.01986v2",
        "title": "A Vision-Based Tactile Sensing System for Multimodal Contact Information\n  Perception via Neural Network",
        "summary": "In general, robotic dexterous hands are equipped with various sensors for\nacquiring multimodal contact information such as position, force, and pose of\nthe grasped object. This multi-sensor-based design adds complexity to the\nrobotic system. In contrast, vision-based tactile sensors employ specialized\noptical designs to enable the extraction of tactile information across\ndifferent modalities within a single system. Nonetheless, the decoupling design\nfor different modalities in common systems is often independent. Therefore, as\nthe dimensionality of tactile modalities increases, it poses more complex\nchallenges in data processing and decoupling, thereby limiting its application\nto some extent. Here, we developed a multimodal sensing system based on a\nvision-based tactile sensor, which utilizes visual representations of tactile\ninformation to perceive the multimodal contact information of the grasped\nobject. The visual representations contain extensive content that can be\ndecoupled by a deep neural network to obtain multimodal contact information\nsuch as classification, position, posture, and force of the grasped object. The\nresults show that the tactile sensing system can perceive multimodal tactile\ninformation using only one single sensor and without different data decoupling\ndesigns for different modal tactile information, which reduces the complexity\nof the tactile system and demonstrates the potential for multimodal tactile\nintegration in various fields such as biomedicine, biology, and robotics.",
        "updated": "2024-03-06T03:43:02Z",
        "published": "2023-10-03T11:58:14Z",
        "authors": [
            "Weiliang Xu",
            "Guoyuan Zhou",
            "Yuanzhi Zhou",
            "Zhibin Zou",
            "Jiali Wang",
            "Wenfeng Wu",
            "Xinming Li"
        ],
        "categories": [
            "cs.RO"
        ],
        "primary_category": "cs.RO"
    },
    "2310.02011v2": {
        "url": "http://arxiv.org/abs/2310.02011v2",
        "title": "Decoding Human Activities: Analyzing Wearable Accelerometer and\n  Gyroscope Data for Activity Recognition",
        "summary": "A person's movement or relative positioning effectively generates raw\nelectrical signals that can be read by computing machines to apply various\nmanipulative techniques for the classification of different human activities.\nIn this paper, a stratified multi-structural approach based on a Residual\nnetwork ensembled with Residual MobileNet is proposed, termed as FusionActNet.\nThe proposed method involves using carefully designed Residual blocks for\nclassifying the static and dynamic activities separately because they have\nclear and distinct characteristics that set them apart. These networks are\ntrained independently, resulting in two specialized and highly accurate models.\nThese models excel at recognizing activities within a specific superclass by\ntaking advantage of the unique algorithmic benefits of architectural\nadjustments. Afterward, these two ResNets are passed through a weighted\nensemble-based Residual MobileNet. Subsequently, this ensemble proficiently\ndiscriminates between a specific static and a specific dynamic activity, which\nwere previously identified based on their distinct feature characteristics in\nthe earlier stage. The proposed model is evaluated using two publicly\naccessible datasets; namely, UCI HAR and Motion-Sense. Therein, it successfully\nhandled the highly confusing cases of data overlap. Therefore, the proposed\napproach achieves a state-of-the-art accuracy of 96.71% and 95.35% in the UCI\nHAR and Motion-Sense datasets respectively.",
        "updated": "2024-03-09T19:26:46Z",
        "published": "2023-10-03T12:34:31Z",
        "authors": [
            "Utsab Saha",
            "Sawradip Saha",
            "Tahmid Kabir",
            "Shaikh Anowarul Fattah",
            "Mohammad Saquib"
        ],
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "primary_category": "cs.CV"
    },
    "2310.02025v4": {
        "url": "http://arxiv.org/abs/2310.02025v4",
        "title": "DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training",
        "summary": "Zeroth-order (ZO) optimization has become a popular technique for solving\nmachine learning (ML) problems when first-order (FO) information is difficult\nor impossible to obtain. However, the scalability of ZO optimization remains an\nopen problem: Its use has primarily been limited to relatively small-scale ML\nproblems, such as sample-wise adversarial attack generation. To our best\nknowledge, no prior work has demonstrated the effectiveness of ZO optimization\nin training deep neural networks (DNNs) without a significant decrease in\nperformance. To overcome this roadblock, we develop DeepZero, a principled ZO\ndeep learning (DL) framework that can scale ZO optimization to DNN training\nfrom scratch through three primary innovations. First, we demonstrate the\nadvantages of coordinatewise gradient estimation (CGE) over randomized\nvector-wise gradient estimation in training accuracy and computational\nefficiency. Second, we propose a sparsityinduced ZO training protocol that\nextends the model pruning methodology using only finite differences to explore\nand exploit the sparse DL prior in CGE. Third, we develop the methods of\nfeature reuse and forward parallelization to advance the practical\nimplementations of ZO training. Our extensive experiments show that DeepZero\nachieves state-of-the-art (SOTA) accuracy on ResNet-20 trained on CIFAR-10,\napproaching FO training performance for the first time. Furthermore, we show\nthe practical utility of DeepZero in applications of certified adversarial\ndefense and DL-based partial differential equation error correction, achieving\n10-20% improvement over SOTA. We believe our results will inspire future\nresearch on scalable ZO optimization and contribute to advancing DL with black\nbox. Codes are available at https://github.com/OPTML-Group/DeepZero.",
        "updated": "2024-03-15T15:28:11Z",
        "published": "2023-10-03T13:05:36Z",
        "authors": [
            "Aochuan Chen",
            "Yimeng Zhang",
            "Jinghan Jia",
            "James Diffenderfer",
            "Jiancheng Liu",
            "Konstantinos Parasyris",
            "Yihua Zhang",
            "Zheng Zhang",
            "Bhavya Kailkhura",
            "Sijia Liu"
        ],
        "comments": "Accepted to ICLR'24. Codes are available at\n  https://github.com/OPTML-Group/DeepZero",
        "categories": [
            "cs.LG"
        ],
        "primary_category": "cs.LG"
    },
    "2310.02031v6": {
        "url": "http://arxiv.org/abs/2310.02031v6",
        "title": "OceanGPT: A Large Language Model for Ocean Science Tasks",
        "summary": "Ocean science, which delves into the oceans that are reservoirs of life and\nbiodiversity, is of great significance given that oceans cover over 70% of our\nplanet's surface. Recently, advances in Large Language Models (LLMs) have\ntransformed the paradigm in science. Despite the success in other domains,\ncurrent LLMs often fall short in catering to the needs of domain experts like\noceanographers, and the potential of LLMs for ocean science is under-explored.\nThe intrinsic reason may be the immense and intricate nature of ocean data as\nwell as the necessity for higher granularity and richness in knowledge. To\nalleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean\ndomain, which is expert in various ocean science tasks. We propose DoInstruct,\na novel framework to automatically obtain a large volume of ocean domain\ninstruction data, which generates instructions based on multi-agent\ncollaboration. Additionally, we construct the first oceanography benchmark,\nOceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though\ncomprehensive experiments, OceanGPT not only shows a higher level of knowledge\nexpertise for oceans science tasks but also gains preliminary embodied\nintelligence capabilities in ocean technology. Codes, data and checkpoints will\nsoon be available at https://github.com/zjunlp/KnowLM.",
        "updated": "2024-03-04T12:40:44Z",
        "published": "2023-10-03T13:17:35Z",
        "authors": [
            "Zhen Bi",
            "Ningyu Zhang",
            "Yida Xue",
            "Yixin Ou",
            "Daxiong Ji",
            "Guozhou Zheng",
            "Huajun Chen"
        ],
        "comments": "Work in progress. Project Website:\n  https://zjunlp.github.io/project/OceanGPT/",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CE",
            "cs.LG",
            "cs.RO"
        ],
        "primary_category": "cs.CL"
    },
    "2310.02044v3": {
        "url": "http://arxiv.org/abs/2310.02044v3",
        "title": "How Physics and Background Attributes Impact Video Transformers in\n  Robotic Manipulation: A Case Study on Planar Pushing",
        "summary": "As model and dataset sizes continue to scale in robot learning, the need to\nunderstand what is the specific factor in the dataset that affects model\nperformance becomes increasingly urgent to ensure cost-effective data\ncollection and model performance. In this work, we empirically investigate how\nphysics attributes (color, friction coefficient, shape) and scene background\ncharacteristics, such as the complexity and dynamics of interactions with\nbackground objects, influence the performance of Video Transformers in\npredicting planar pushing trajectories. We aim to investigate three primary\nquestions: How do physics attributes and background scene characteristics\ninfluence model performance? What kind of changes in attributes are most\ndetrimental to model generalization? What proportion of fine-tuning data is\nrequired to adapt models to novel scenarios? To facilitate this research, we\npresent CloudGripper-Push-1K, a large real-world vision-based robot pushing\ndataset comprising 1278 hours and 460,000 videos of planar pushing interactions\nwith objects with different physics and background attributes. We also propose\nVideo Occlusion Transformer (VOT), a generic modular video-transformer-based\ntrajectory prediction framework which features 3 choices of 2D-spatial encoders\nas the subject of our case study. Dataset and codes will be available at\nhttps://cloudgripper.org.",
        "updated": "2024-03-17T10:37:08Z",
        "published": "2023-10-03T13:35:49Z",
        "authors": [
            "Shutong Jin",
            "Ruiyu Wang",
            "Muhammad Zahid",
            "Florian T. Pokorny"
        ],
        "comments": "Under review at IEEE/RSJ IROS 2024",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "primary_category": "cs.RO"
    },
    "2310.02107v3": {
        "url": "http://arxiv.org/abs/2310.02107v3",
        "title": "Instances Need More Care: Rewriting Prompts for Instances with LLMs in\n  the Loop Yields Better Zero-Shot Performance",
        "summary": "Large language models (LLMs) have revolutionized zero-shot task performance,\nmitigating the need for task-specific annotations while enhancing task\ngeneralizability. Despite its advancements, current methods using trigger\nphrases such as ``Let's think step by step'' remain limited. This study\nintroduces PRomPTed, an approach that optimizes the zero-shot prompts for\nindividual task instances following an innovative manner of ``LLMs in the\nloop''. Our comprehensive evaluation across 13 datasets and 10 task types based\non GPT-4 reveals that PRomPTed significantly outperforms both the naive\nzero-shot approaches and a strong baseline (i.e., ``Output Refinement'') which\nrefines the task output instead of the input prompt. Our experimental results\nalso confirmed the generalization of this advantage to the relatively weaker\nGPT-3.5. Even more intriguingly, we found that leveraging GPT-3.5 to rewrite\nprompts for the stronger GPT-4 not only matches but occasionally exceeds the\nefficacy of using GPT-4 as the prompt rewriter. Our research thus presents a\nhuge value in not only enhancing zero-shot LLM performance but also potentially\nenabling supervising LLMs with their weaker counterparts, a capability\nattracting much interest recently.",
        "updated": "2024-03-09T19:07:00Z",
        "published": "2023-10-03T14:51:34Z",
        "authors": [
            "Saurabh Srivastava",
            "Chengyue Huang",
            "Weiguo Fan",
            "Ziyu Yao"
        ],
        "comments": "Work in progress",
        "categories": [
            "cs.CL"
        ],
        "primary_category": "cs.CL"
    },
    "2310.02110v2": {
        "url": "http://arxiv.org/abs/2310.02110v2",
        "title": "Sieve: Multimodal Dataset Pruning Using Image Captioning Models",
        "summary": "Vision-Language Models (VLMs) are pretrained on large, diverse, and noisy\nweb-crawled datasets. This underscores the critical need for dataset pruning,\nas the quality of these datasets is strongly correlated with the performance of\nVLMs on downstream tasks. Using CLIPScore from a pretrained model to only train\nmodels using highly-aligned samples is one of the most successful methods for\npruning. We argue that this approach suffers from multiple limitations\nincluding: false positives and negatives due to CLIP's pretraining on noisy\nlabels. We propose a pruning signal, Sieve, that employs synthetic captions\ngenerated by image-captioning models pretrained on small, diverse, and\nwell-aligned image-text pairs to evaluate the alignment of noisy image-text\npairs. To bridge the gap between the limited diversity of generated captions\nand the high diversity of alternative text (alt-text), we estimate the semantic\ntextual similarity in the embedding space of a language model pretrained on\nunlabeled text corpus. Using DataComp, a multimodal dataset filtering\nbenchmark, when evaluating on 38 downstream tasks, our pruning approach,\nsurpasses CLIPScore by 2.6\\% and 1.7\\% on medium and large scale respectively.\nIn addition, on retrieval tasks, Sieve leads to a significant improvement of\n2.7% and 4.5% on medium and large scale respectively.",
        "updated": "2024-03-10T19:12:52Z",
        "published": "2023-10-03T14:53:53Z",
        "authors": [
            "Anas Mahmoud",
            "Mostafa Elhoushi",
            "Amro Abbas",
            "Yu Yang",
            "Newsha Ardalani",
            "Hugh Leather",
            "Ari Morcos"
        ],
        "comments": "Accepted in CVPR 2024",
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2311.00097v2": {
        "url": "http://arxiv.org/abs/2311.00097v2",
        "title": "Cocoon: Static Information Flow Control in Rust",
        "summary": "Information flow control (IFC) provides confidentiality by enforcing\nnoninterference, which ensures that high-secrecy values cannot affect\nlow-secrecy values. Prior work introduces fine-grained IFC approaches that\nmodify the programming language and use nonstandard compilation tools, impose\nrun-time overhead, or report false secrecy leaks -- all of which hinder\nadoption.\n  This paper presents Cocoon, a Rust library for static type-based IFC that\nuses the unmodified Rust language and compiler. The key insight of Cocoon lies\nin leveraging Rust's type system and procedural macros to establish an effect\nsystem that enforces noninterference. A performance evaluation shows that using\nCocoon increases compile time but has no impact on application performance. To\ndemonstrate Cocoon's utility, we retrofitted two popular Rust programs, the\nSpotify TUI client and Mozilla's Servo browser engine, to use Cocoon to enforce\nlimited confidentiality policies.",
        "updated": "2024-03-18T22:14:15Z",
        "published": "2023-10-31T19:21:31Z",
        "authors": [
            "Ada Lamba",
            "Max Taylor",
            "Vincent Beardsley",
            "Jacob Bambeck",
            "Michael D. Bond",
            "Zhiqiang Lin"
        ],
        "comments": "Will be published in PACMPL(OOPSLA) in October 2024",
        "categories": [
            "cs.PL"
        ],
        "primary_category": "cs.PL",
        "doi": "10.1145/3649817"
    },
    "2311.00112v2": {
        "url": "http://arxiv.org/abs/2311.00112v2",
        "title": "Hierarchical Optimization-based Control for Whole-body Loco-manipulation\n  of Heavy Objects",
        "summary": "In recent years, the field of legged robotics has seen growing interest in\nenhancing the capabilities of these robots through the integration of\narticulated robotic arms. However, achieving successful loco-manipulation,\nespecially involving interaction with heavy objects, is far from\nstraightforward, as object manipulation can introduce substantial disturbances\nthat impact the robot's locomotion. This paper presents a novel framework for\nlegged loco-manipulation that considers whole-body coordination through a\nhierarchical optimization-based control framework. First, an online\nmanipulation planner computes the manipulation forces and manipulated object\ntask-based reference trajectory. Then, pose optimization aligns the robot's\ntrajectory with kinematic constraints. The resultant robot reference trajectory\nis executed via a linear MPC controller incorporating the desired manipulation\nforces into its prediction model. Our approach has been validated in simulation\nand hardware experiments, highlighting the necessity of whole-body optimization\ncompared to the baseline locomotion MPC when interacting with heavy objects.\nExperimental results with Unitree Aliengo, equipped with a custom-made robotic\narm, showcase its ability to lift and carry an 8kg payload and manipulate\ndoors.",
        "updated": "2024-03-19T19:37:22Z",
        "published": "2023-10-31T19:39:44Z",
        "authors": [
            "Alberto Rigo",
            "Muqun Hu",
            "Satyandra K. Gupta",
            "Quan Nguyen"
        ],
        "comments": "7 pages, 7 figures",
        "categories": [
            "cs.RO"
        ],
        "primary_category": "cs.RO"
    },
    "2311.00117v2": {
        "url": "http://arxiv.org/abs/2311.00117v2",
        "title": "BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B",
        "summary": "Llama 2-Chat is a collection of large language models that Meta developed and\nreleased to the public. While Meta fine-tuned Llama 2-Chat to refuse to output\nharmful content, we hypothesize that public access to model weights enables bad\nactors to cheaply circumvent Llama 2-Chat's safeguards and weaponize Llama 2's\ncapabilities for malicious purposes. We demonstrate that it is possible to\neffectively undo the safety fine-tuning from Llama 2-Chat 13B with less than\n$200, while retaining its general capabilities. Our results demonstrate that\nsafety-fine tuning is ineffective at preventing misuse when model weights are\nreleased publicly. Given that future models will likely have much greater\nability to cause harm at scale, it is essential that AI developers address\nthreats from fine-tuning when considering whether to publicly release their\nmodel weights.",
        "updated": "2024-03-21T18:40:32Z",
        "published": "2023-10-31T19:45:15Z",
        "authors": [
            "Pranav Gade",
            "Simon Lermen",
            "Charlie Rogers-Smith",
            "Jeffrey Ladish"
        ],
        "categories": [
            "cs.CL"
        ],
        "primary_category": "cs.CL"
    },
    "2311.00118v2": {
        "url": "http://arxiv.org/abs/2311.00118v2",
        "title": "Extracting the Multiscale Causal Backbone of Brain Dynamics",
        "summary": "The bulk of the research effort on brain connectivity revolves around\nstatistical associations among brain regions, which do not directly relate to\nthe causal mechanisms governing brain dynamics. Here we propose the multiscale\ncausal backbone (MCB) of brain dynamics, shared by a set of individuals across\nmultiple temporal scales, and devise a principled methodology to extract it.\n  Our approach leverages recent advances in multiscale causal structure\nlearning and optimizes the trade-off between the model fit and its complexity.\nEmpirical assessment on synthetic data shows the superiority of our methodology\nover a baseline based on canonical functional connectivity networks. When\napplied to resting-state fMRI data, we find sparse MCBs for both the left and\nright brain hemispheres. Thanks to its multiscale nature, our approach shows\nthat at low-frequency bands, causal dynamics are driven by brain regions\nassociated with high-level cognitive functions; at higher frequencies instead,\nnodes related to sensory processing play a crucial role. Finally, our analysis\nof individual multiscale causal structures confirms the existence of a causal\nfingerprint of brain connectivity, thus supporting the existing extensive\nresearch in brain connectivity fingerprinting from a causal perspective.",
        "updated": "2024-03-19T18:53:38Z",
        "published": "2023-10-31T19:47:11Z",
        "authors": [
            "Gabriele D'Acunto",
            "Francesco Bonchi",
            "Gianmarco De Francisci Morales",
            "Giovanni Petri"
        ],
        "comments": "Accepted at the 3rd conference on Causal Learning and Reasoning\n  (CLeaR 2024)",
        "categories": [
            "cs.LG",
            "q-bio.NC",
            "stat.AP",
            "stat.ME",
            "stat.ML"
        ],
        "primary_category": "cs.LG"
    },
    "2311.00123v2": {
        "url": "http://arxiv.org/abs/2311.00123v2",
        "title": "Q-Learning for Stochastic Control under General Information Structures\n  and Non-Markovian Environments",
        "summary": "As a primary contribution, we present a convergence theorem for stochastic\niterations, and in particular, Q-learning iterates, under a general, possibly\nnon-Markovian, stochastic environment. Our conditions for convergence involve\nan ergodicity and a positivity criterion. We provide a precise characterization\non the limit of the iterates and conditions on the environment and\ninitializations for convergence. As our second contribution, we discuss the\nimplications and applications of this theorem to a variety of stochastic\ncontrol problems with non-Markovian environments involving (i) quantized\napproximations of fully observed Markov Decision Processes (MDPs) with\ncontinuous spaces (where quantization break down the Markovian structure), (ii)\nquantized approximations of belief-MDP reduced partially observable MDPS\n(POMDPs) with weak Feller continuity and a mild version of filter stability\n(which requires the knowledge of the model by the controller), (iii) finite\nwindow approximations of POMDPs under a uniform controlled filter stability\n(which does not require the knowledge of the model), and (iv) for multi-agent\nmodels where convergence of learning dynamics to a new class of equilibria,\nsubjective Q-learning equilibria, will be studied. In addition to the\nconvergence theorem, some implications of the theorem above are new to the\nliterature and others are interpreted as applications of the convergence\ntheorem. Some open problems are noted.",
        "updated": "2024-03-04T15:59:43Z",
        "published": "2023-10-31T19:53:16Z",
        "authors": [
            "Ali Devran Kara",
            "Serdar Yuksel"
        ],
        "comments": "2 figures",
        "categories": [
            "math.OC",
            "cs.AI",
            "cs.SY",
            "eess.SY"
        ],
        "primary_category": "math.OC"
    },
    "2311.00136v4": {
        "url": "http://arxiv.org/abs/2311.00136v4",
        "title": "Neuroformer: Multimodal and Multitask Generative Pretraining for Brain\n  Data",
        "summary": "State-of-the-art systems neuroscience experiments yield large-scale\nmultimodal data, and these data sets require new tools for analysis. Inspired\nby the success of large pretrained models in vision and language domains, we\nreframe the analysis of large-scale, cellular-resolution neuronal spiking data\ninto an autoregressive spatiotemporal generation problem. Neuroformer is a\nmultimodal, multitask generative pretrained transformer (GPT) model that is\nspecifically designed to handle the intricacies of data in systems\nneuroscience. It scales linearly with feature size, can process an arbitrary\nnumber of modalities, and is adaptable to downstream tasks, such as predicting\nbehavior. We first trained Neuroformer on simulated datasets, and found that it\nboth accurately predicted simulated neuronal circuit activity, and also\nintrinsically inferred the underlying neural circuit connectivity, including\ndirection. When pretrained to decode neural responses, the model predicted the\nbehavior of a mouse with only few-shot fine-tuning, suggesting that the model\nbegins learning how to do so directly from the neural representations\nthemselves, without any explicit supervision. We used an ablation study to show\nthat joint training on neuronal responses and behavior boosted performance,\nhighlighting the model's ability to associate behavioral and neural\nrepresentations in an unsupervised manner. These findings show that Neuroformer\ncan analyze neural datasets and their emergent properties, informing the\ndevelopment of models and hypotheses associated with the brain.",
        "updated": "2024-03-15T22:07:06Z",
        "published": "2023-10-31T20:17:32Z",
        "authors": [
            "Antonis Antoniades",
            "Yiyi Yu",
            "Joseph Canzano",
            "William Wang",
            "Spencer LaVere Smith"
        ],
        "comments": "9 pages for main paper. 22 pages in total. 13 figures, 1 table",
        "categories": [
            "q-bio.NC",
            "cs.LG",
            "cs.NE"
        ],
        "primary_category": "q-bio.NC"
    },
    "2311.00181v2": {
        "url": "http://arxiv.org/abs/2311.00181v2",
        "title": "Best of Both Worlds Guarantees for Smoothed Online Quadratic\n  Optimization",
        "summary": "We study the smoothed online quadratic optimization (SOQO) problem where, at\neach round $t$, a player plays an action $x_t$ in response to a quadratic\nhitting cost and an additional squared $\\ell_2$-norm cost for switching\nactions. This problem class has strong connections to a wide range of\napplication domains including smart grid management, adaptive control, and data\ncenter management, where switching-efficient algorithms are highly sought\nafter. We study the SOQO problem in both adversarial and stochastic settings,\nand in this process, perform the first stochastic analysis of this class of\nproblems. We provide the online optimal algorithm when the minimizers of the\nhitting cost function evolve as a general stochastic process, which, for the\ncase of martingale process, takes the form of a distribution-agnostic dynamic\ninterpolation algorithm (LAI). Next, we present the stochastic-adversarial\ntrade-off by proving an $\\Omega(T)$ expected regret for the adversarial optimal\nalgorithm in the literature (ROBD) with respect to LAI and, a sub-optimal\ncompetitive ratio for LAI in the adversarial setting. Finally, we present a\nbest-of-both-worlds algorithm that obtains a robust adversarial performance\nwhile simultaneously achieving a near-optimal stochastic performance.",
        "updated": "2024-03-24T01:13:59Z",
        "published": "2023-10-31T22:59:23Z",
        "authors": [
            "Neelkamal Bhuyan",
            "Debankur Mukherjee",
            "Adam Wierman"
        ],
        "comments": "48 pages, 9 figures",
        "categories": [
            "math.OC",
            "cs.DS",
            "cs.LG",
            "math.PR"
        ],
        "primary_category": "math.OC"
    },
    "2311.00232v3": {
        "url": "http://arxiv.org/abs/2311.00232v3",
        "title": "Mechanically-Inflatable Bio-Inspired Locomotion for Robotic Pipeline\n  Inspection",
        "summary": "Pipelines, vital for fluid transport, pose an important yet challenging\ninspection task, particularly in small, flexible biological systems, that\nrobots have yet to master. In this study, we explored the development of an\ninnovative robot inspired by the ovipositor of parasitic wasps to navigate and\ninspect pipelines. The robot features a flexible locomotion system that adapts\nto different tube sizes and shapes through a mechanical inflation technique.\nThe flexible locomotion system employs a reciprocating motion, in which groups\nof three sliders extend and retract in a cyclic fashion. In a\nproof-of-principle experiment, the robot locomotion efficiency demonstrated\npositive linear correlation (r=0.6434) with the diameter ratio (ratio of robot\ndiameter to tube diameter). The robot showcased a remarkable ability to\ntraverse tubes of different sizes, shapes and payloads with an average of (70%)\nlocomotion efficiency across all testing conditions, at varying diameter ratios\n(0.7-1.5). Furthermore, the mechanical inflation mechanism displayed\nsubstantial load-carrying capacity, producing considerable holding force of (13\nN), equivalent to carrying a payload of approximately (5.8 Kg) inclusive the\nrobot weight. This novel soft robotic system shows promise for inspection and\nnavigation within tubular confined spaces, particularly in scenarios requiring\nadaptability to different tube shapes, sizes, and load-carrying capacities.\nThis novel design serves as a foundation for a new class of pipeline inspection\nrobots that exhibit versatility across various pipeline environments,\npotentially including biological systems.",
        "updated": "2024-03-12T15:30:03Z",
        "published": "2023-11-01T02:29:21Z",
        "authors": [
            "Mostafa A. Atalla",
            "Fabian Trauzettel",
            "Sebastiaan P. van Gelder",
            "Paul Breedveld",
            "Micha\u00ebl Wiertlewski",
            "Aim\u00e9e Sakes"
        ],
        "comments": "Accepted paper for RoboSoft 2024",
        "categories": [
            "cs.RO"
        ],
        "primary_category": "cs.RO"
    },
    "2311.00257v2": {
        "url": "http://arxiv.org/abs/2311.00257v2",
        "title": "AMSP: Reducing Communication Overhead of ZeRO for Efficient LLM Training",
        "summary": "Training large language models (LLMs) encounters challenges in GPU memory\nconsumption due to the high memory requirements of model states. The widely\nused Zero Redundancy Optimizer (ZeRO) addresses this issue through strategic\nsharding but introduces communication challenges at scale. To tackle this\nproblem, we propose AMSP, a system designed to optimize ZeRO for scalable LLM\ntraining. AMSP incorporates three flexible sharding strategies: Full-Replica,\nFull-Sharding, and Partial-Sharding, and allows each component within the model\nstates (Parameters, Gradients, Optimizer States) to independently choose a\nsharding strategy as well as the device mesh. We conduct a thorough analysis of\ncommunication costs, formulating an optimization problem to discover the\noptimal sharding strategy. Additionally, AMSP optimizes distributed LLM\ntraining by efficiently overlapping communication with computation. Evaluations\ndemonstrate up to 52\\% Model FLOPs Utilization (MFU) when training the\nLLaMA-based model on 1024 GPUs, resulting in a 1.56 times improvement in\ntraining throughput compared to newly proposed systems like MiCS and ZeRO++.",
        "updated": "2024-03-13T14:28:03Z",
        "published": "2023-11-01T03:14:48Z",
        "authors": [
            "Qiaoling Chen",
            "Qinghao Hu",
            "Guoteng Wang",
            "Yingtong Xiong",
            "Ting Huang",
            "Xun Chen",
            "Yang Gao",
            "Hang Yan",
            "Yonggang Wen",
            "Tianwei Zhang",
            "Peng Sun"
        ],
        "categories": [
            "cs.DC"
        ],
        "primary_category": "cs.DC"
    },
    "2311.00262v2": {
        "url": "http://arxiv.org/abs/2311.00262v2",
        "title": "Plug-and-Play Policy Planner for Large Language Model Powered Dialogue\n  Agents",
        "summary": "Proactive dialogues serve as a practical yet challenging dialogue problem in\nthe era of large language models (LLMs), where the dialogue policy planning is\nthe key to improving the proactivity of LLMs. Most existing studies enable the\ndialogue policy planning of LLMs using various prompting schemes or iteratively\nenhance this capability in handling the given case with verbal AI feedback.\nHowever, these approaches are either bounded by the policy planning capability\nof the frozen LLMs or hard to be transferred to new cases. In this work, we\nintroduce a new dialogue policy planning paradigm to strategize LLMs for\nproactive dialogue problems with a tunable language model plug-in as a\nplug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a\nnovel training framework to facilitate supervised fine-tuning over available\nhuman-annotated data as well as reinforcement learning from goal-oriented AI\nfeedback with dynamic interaction data collected by the LLM-based self-play\nsimulation. In this manner, the LLM-powered dialogue agent can not only be\ngeneralized to different cases after the training, but also be applicable to\ndifferent applications by just substituting the learned plug-in. In addition,\nwe propose to evaluate the policy planning capability of dialogue systems under\nthe interactive setting. Experimental results demonstrate that PPDPP\nconsistently and substantially outperforms existing approaches on three\ndifferent proactive dialogue applications, including negotiation, emotional\nsupport, and tutoring dialogues.",
        "updated": "2024-03-11T08:30:31Z",
        "published": "2023-11-01T03:20:16Z",
        "authors": [
            "Yang Deng",
            "Wenxuan Zhang",
            "Wai Lam",
            "See-Kiong Ng",
            "Tat-Seng Chua"
        ],
        "comments": "Accepted by ICLR 2024",
        "categories": [
            "cs.CL",
            "cs.AI"
        ],
        "primary_category": "cs.CL"
    },
    "2311.18311v2": {
        "url": "http://arxiv.org/abs/2311.18311v2",
        "title": "Anisotropic Neural Representation Learning for High-Quality Neural\n  Rendering",
        "summary": "Neural radiance fields (NeRFs) have achieved impressive view synthesis\nresults by learning an implicit volumetric representation from multi-view\nimages. To project the implicit representation into an image, NeRF employs\nvolume rendering that approximates the continuous integrals of rays as an\naccumulation of the colors and densities of the sampled points. Although this\napproximation enables efficient rendering, it ignores the direction information\nin point intervals, resulting in ambiguous features and limited reconstruction\nquality. In this paper, we propose an anisotropic neural representation\nlearning method that utilizes learnable view-dependent features to improve\nscene representation and reconstruction. We model the volumetric function as\nspherical harmonic (SH)-guided anisotropic features, parameterized by\nmultilayer perceptrons, facilitating ambiguity elimination while preserving the\nrendering efficiency. To achieve robust scene reconstruction without anisotropy\noverfitting, we regularize the energy of the anisotropic features during\ntraining. Our method is flexiable and can be plugged into NeRF-based\nframeworks. Extensive experiments show that the proposed representation can\nboost the rendering quality of various NeRFs and achieve state-of-the-art\nrendering performance on both synthetic and real-world scenes.",
        "updated": "2024-03-11T02:22:50Z",
        "published": "2023-11-30T07:29:30Z",
        "authors": [
            "Y. Wang",
            "J. Xu",
            "Y. Zeng",
            "Y. Gong"
        ],
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "primary_category": "cs.CV"
    },
    "2311.18331v2": {
        "url": "http://arxiv.org/abs/2311.18331v2",
        "title": "MRFP: Learning Generalizable Semantic Segmentation from Sim-2-Real with\n  Multi-Resolution Feature Perturbation",
        "summary": "Deep neural networks have shown exemplary performance on semantic scene\nunderstanding tasks on source domains, but due to the absence of style\ndiversity during training, enhancing performance on unseen target domains using\nonly single source domain data remains a challenging task. Generation of\nsimulated data is a feasible alternative to retrieving large style-diverse\nreal-world datasets as it is a cumbersome and budget-intensive process.\nHowever, the large domain-specfic inconsistencies between simulated and\nreal-world data pose a significant generalization challenge in semantic\nsegmentation. In this work, to alleviate this problem, we propose a novel\nMultiResolution Feature Perturbation (MRFP) technique to randomize\ndomain-specific fine-grained features and perturb style of coarse features. Our\nexperimental results on various urban-scene segmentation datasets clearly\nindicate that, along with the perturbation of style-information, perturbation\nof fine-feature components is paramount to learn domain invariant robust\nfeature maps for semantic segmentation models. MRFP is a simple and\ncomputationally efficient, transferable module with no additional learnable\nparameters or objective functions, that helps state-of-the-art deep neural\nnetworks to learn robust domain invariant features for simulation-to-real\nsemantic segmentation.",
        "updated": "2024-03-28T13:27:33Z",
        "published": "2023-11-30T08:02:49Z",
        "authors": [
            "Sumanth Udupa",
            "Prajwal Gurunath",
            "Aniruddh Sikdar",
            "Suresh Sundaram"
        ],
        "comments": "Accepted to CVPR 2024",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "primary_category": "cs.CV"
    },
    "2311.18377v2": {
        "url": "http://arxiv.org/abs/2311.18377v2",
        "title": "Transfer Learning across Different Chemical Domains: Virtual Screening\n  of Organic Materials with Deep Learning Models Pretrained on Small Molecule\n  and Chemical Reaction Data",
        "summary": "Machine learning is becoming a preferred method for the virtual screening of\norganic materials due to its cost-effectiveness over traditional\ncomputationally demanding techniques. However, the scarcity of labeled data for\norganic materials poses a significant challenge for training advanced machine\nlearning models. This study showcases the potential of utilizing databases of\ndrug-like small molecules and chemical reactions to pretrain the BERT model,\nenhancing its performance in the virtual screening of organic materials. By\nfine-tuning the BERT models with data from five virtual screening tasks, the\nversion pretrained with the USPTO-SMILES dataset achieved R2 scores exceeding\n0.94 for three tasks and over 0.81 for two others. This performance surpasses\nthat of models pretrained on the small molecule or organic materials databases\nand outperforms three traditional machine learning models trained directly on\nvirtual screening data. The success of the USPTO-SMILES pretrained BERT model\ncan be attributed to the diverse array of organic building blocks in the USPTO\ndatabase, offering a broader exploration of the chemical space. The study\nfurther suggests that accessing a reaction database with a wider range of\nreactions than the USPTO could further enhance model performance. Overall, this\nresearch validates the feasibility of applying transfer learning across\ndifferent chemical domains for the efficient virtual screening of organic\nmaterials.",
        "updated": "2024-03-05T10:23:33Z",
        "published": "2023-11-30T09:20:24Z",
        "authors": [
            "Chengwei Zhang",
            "Yushuang Zhai",
            "Ziyang Gong",
            "Hongliang Duan",
            "Yuan-Bin She",
            "Yun-Fang Yang",
            "An Su"
        ],
        "categories": [
            "physics.chem-ph",
            "cs.LG",
            "q-bio.BM"
        ],
        "primary_category": "physics.chem-ph"
    },
    "2311.18438v2": {
        "url": "http://arxiv.org/abs/2311.18438v2",
        "title": "Solution-Set Geometry and Regularization Path of a Nonconvexly\n  Regularized Convex Sparse Model",
        "summary": "The generalized minimax concave (GMC) penalty is a nonconvex sparse\nregularizer which can preserve the overall-convexity of the regularized\nleast-squares problem. In this paper, we focus on a significant instance of the\nGMC model termed scaled GMC (sGMC), and present various notable findings on its\nsolution-set geometry and regularization path. Our investigation indicates that\nwhile the sGMC penalty is a nonconvex extension of the LASSO penalty (i.e., the\n$\\ell_1$-norm), the sGMC model preserves many celebrated properties of the\nLASSO model, hence can serve as a less biased surrogate of LASSO without losing\nits advantages. Specifically, for a fixed regularization parameter $\\lambda$,\nwe show that the solution-set geometry, solution uniqueness and sparseness of\nthe sGMC model can be characterized in a similar elegant way to the LASSO model\n(see, e.g., Osborne et al. 2000, R. J. Tibshirani 2013). For a varying\n$\\lambda$, we prove that the sGMC solution set is a continuous polytope-valued\nmapping of $\\lambda$. Most noticeably, our study indicates that similar to\nLASSO, the minimum $\\ell_2$-norm regularization path of the sGMC model is\ncontinuous and piecewise linear in $\\lambda$. Based on these theoretical\nresults, an efficient regularization path algorithm is proposed for the sGMC\nmodel, extending the well-known least angle regression (LARS) algorithm for\nLASSO. We prove the correctness and finite termination of the proposed\nalgorithm under a mild assumption, and confirm its\ncorrectness-in-general-situation, efficiency, and practical utility through\nnumerical experiments. Many results in this study also contribute to the\ntheoretical research of LASSO.",
        "updated": "2024-03-22T13:26:52Z",
        "published": "2023-11-30T10:39:47Z",
        "authors": [
            "Yi Zhang",
            "Isao Yamada"
        ],
        "comments": "53 pages, 10 figures. Submitted to journal",
        "categories": [
            "math.OC",
            "cs.LG",
            "eess.SP",
            "math.ST",
            "stat.TH"
        ],
        "primary_category": "math.OC"
    },
    "2311.18496v2": {
        "url": "http://arxiv.org/abs/2311.18496v2",
        "title": "Accurate Segmentation of Optic Disc And Cup from Multiple Pseudo-labels\n  by Noise-aware Learning",
        "summary": "Optic disc and cup segmentation plays a crucial role in automating the\nscreening and diagnosis of optic glaucoma. While data-driven convolutional\nneural networks (CNNs) show promise in this area, the inherent ambiguity of\nsegmenting objects and background boundaries in the task of optic disc and cup\nsegmentation leads to noisy annotations that impact model performance. To\naddress this, we propose an innovative label-denoising method of Multiple\nPseudo-labels Noise-aware Network (MPNN) for accurate optic disc and cup\nsegmentation. Specifically, the Multiple Pseudo-labels Generation and Guided\nDenoising (MPGGD) module generates pseudo-labels by multiple different\ninitialization networks trained on true labels, and the pixel-level consensus\ninformation extracted from these pseudo-labels guides to differentiate clean\npixels from noisy pixels. The training framework of the MPNN is constructed by\na teacher-student architecture to learn segmentation from clean pixels and\nnoisy pixels. Particularly, such a framework adeptly leverages (i) reliable and\nfundamental insight from clean pixels and (ii) the supplementary knowledge\nwithin noisy pixels via multiple perturbation-based unsupervised consistency.\nCompared to other label-denoising methods, comprehensive experimental results\non the RIGA dataset demonstrate our method's excellent performance. The code is\navailable at https://github.com/wwwtttjjj/MPNN",
        "updated": "2024-03-15T08:38:04Z",
        "published": "2023-11-30T12:17:16Z",
        "authors": [
            "Tengjin Weng",
            "Yang Shen",
            "Zhidong Zhao",
            "Zhiming Cheng",
            "Shuai Wang"
        ],
        "comments": "CSCWD 2024",
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2311.18528v2": {
        "url": "http://arxiv.org/abs/2311.18528v2",
        "title": "Bottom-up computation using trees of sublists (Functional Pearl)",
        "summary": "Some top-down problem specifications, if executed directly, may compute\nsub-problems repeatedly. Instead, we may want a bottom-up algorithm that stores\nsolutions of sub-problems in a table to be reused. It can be tricky, however,\nto figure out how the table can be represented and efficiently maintained.\n  We study a special case: computing a function $h$ taking lists as inputs such\nthat $h~xs$ is defined in terms of all immediate sublists of $xs$. Richard Bird\nstudied this problem in 2008, and presented a concise but cryptic algorithm\nwithout much explanation. We give this algorithm a proper derivation, and\ndiscover a key property that allows it to work. The algorithm builds trees that\nhave certain shapes -- the sizes along the left spine is a diagonal in Pascal's\ntriangle. The crucial function we derive transforms one diagonal to the next.",
        "updated": "2024-03-04T06:15:33Z",
        "published": "2023-11-30T13:08:46Z",
        "authors": [
            "Shin-Cheng Mu"
        ],
        "comments": "Submitted to Journal of Functional Programming",
        "categories": [
            "cs.PL",
            "F.3.1; D.2.4"
        ],
        "primary_category": "cs.PL"
    },
    "2311.18531v2": {
        "url": "http://arxiv.org/abs/2311.18531v2",
        "title": "Dataset Distillation via the Wasserstein Metric",
        "summary": "Dataset Distillation (DD) emerges as a powerful strategy to encapsulate the\nexpansive information of large datasets into significantly smaller, synthetic\nequivalents, thereby preserving model performance with reduced computational\noverhead. Pursuing this objective, we introduce the Wasserstein distance, a\nmetric grounded in optimal transport theory, to enhance distribution matching\nin DD. Our approach employs the Wasserstein barycenter to provide a\ngeometrically meaningful method for quantifying distribution differences and\ncapturing the centroid of distribution sets efficiently. By embedding synthetic\ndata in the feature spaces of pretrained classification models, we facilitate\neffective distribution matching that leverages prior knowledge inherent in\nthese models. Our method not only maintains the computational advantages of\ndistribution matching-based techniques but also achieves new state-of-the-art\nperformance across a range of high-resolution datasets. Extensive testing\ndemonstrates the effectiveness and adaptability of our method, underscoring the\nuntapped potential of Wasserstein metrics in dataset distillation.",
        "updated": "2024-03-15T22:14:40Z",
        "published": "2023-11-30T13:15:28Z",
        "authors": [
            "Haoyang Liu",
            "Yijiang Li",
            "Tiancheng Xing",
            "Vibhu Dalal",
            "Luwei Li",
            "Jingrui He",
            "Haohan Wang"
        ],
        "comments": "21 pages, 8 figures",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "primary_category": "cs.CV"
    },
    "2311.18542v1": {
        "url": "http://arxiv.org/abs/2311.18542v1",
        "title": "RIS-Assisted Generalized Receive Quadrature Spatial Modulation",
        "summary": "In this paper, reconfigurable intelligent surface (RIS)-assisted generalized\nreceive quadrature spatial modulation (RIS-GRQSM) is proposed to improve the\nspectral efficiency of RIS-aided quadrature spatial modulation (QSM) systems by\nutilizing the concept of generalized spatial modulation (GSM). That is,\nmultiple antennas are activated at the receiver independently for both the real\nand imaginary parts. We propose a max-min optimization problem to adjust the\nphase shifts of all RIS elements to maximize the relevant signal-to-noise\nratios (SNRs) at all activated receive antennas. Using Lagrange duality, the\nnon-convex optimization problem involving the phase shifts of all RIS elements\nreduces to a convex optimization involving a number of variables equal to the\nnumber of activated receive antennas. A successive greedy detector (GD) can be\nused at the receiver to detect the active antennas, which simplifies the\ndetection process. The numerical results show that the proposed scheme\noutperforms the benchmark schemes in terms of error rate performance,\nespecially in systems with a larger number of receive antennas. In the special\ncase where each receive antenna corresponds to a user and is activated, the\nRIS-GRQSM system becomes a multicast communication system. In this context, in\ncontrast to existing phase shift optimization algorithms which exhibit an\nimpractical level of complexity, our proposed solution offers the advantage of\nlow complexity and practical feasibility of implementation.",
        "updated": "2023-11-30T13:24:58Z",
        "published": "2023-11-30T13:24:58Z",
        "authors": [
            "Mohamad H. Dinan",
            "Mark F. Flanagan"
        ],
        "comments": "6 pages (2-column), 5 figures, 1 table, Prepared for Globcom 2023\n  conference",
        "categories": [
            "cs.IT",
            "eess.SP",
            "math.IT"
        ],
        "primary_category": "cs.IT",
        "doi": "10.1109/GLOBECOM54140.2023.10437077"
    },
    "2311.18561v2": {
        "url": "http://arxiv.org/abs/2311.18561v2",
        "title": "Periodic Vibration Gaussian: Dynamic Urban Scene Reconstruction and\n  Real-time Rendering",
        "summary": "Modeling dynamic, large-scale urban scenes is challenging due to their highly\nintricate geometric structures and unconstrained dynamics in both space and\ntime. Prior methods often employ high-level architectural priors, separating\nstatic and dynamic elements, resulting in suboptimal capture of their\nsynergistic interactions. To address this challenge, we present a unified\nrepresentation model, called Periodic Vibration Gaussian (PVG). PVG builds upon\nthe efficient 3D Gaussian splatting technique, originally designed for static\nscene representation, by introducing periodic vibration-based temporal\ndynamics. This innovation enables PVG to elegantly and uniformly represent the\ncharacteristics of various objects and elements in dynamic urban scenes. To\nenhance temporally coherent and large scene representation learning with sparse\ntraining data, we introduce a novel temporal smoothing mechanism and a\nposition-aware adaptive control strategy respectively. Extensive experiments on\nWaymo Open Dataset and KITTI benchmarks demonstrate that PVG surpasses\nstate-of-the-art alternatives in both reconstruction and novel view synthesis\nfor both dynamic and static scenes. Notably, PVG achieves this without relying\non manually labeled object bounding boxes or expensive optical flow estimation.\nMoreover, PVG exhibits 900-fold acceleration in rendering over the best\nalternative.",
        "updated": "2024-03-20T16:27:53Z",
        "published": "2023-11-30T13:53:50Z",
        "authors": [
            "Yurui Chen",
            "Chun Gu",
            "Junzhe Jiang",
            "Xiatian Zhu",
            "Li Zhang"
        ],
        "comments": "Project page: https://fudan-zvg.github.io/PVG/",
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2311.18598v2": {
        "url": "http://arxiv.org/abs/2311.18598v2",
        "title": "Generalisable Agents for Neural Network Optimisation",
        "summary": "Optimising deep neural networks is a challenging task due to complex training\ndynamics, high computational requirements, and long training times. To address\nthis difficulty, we propose the framework of Generalisable Agents for Neural\nNetwork Optimisation (GANNO) -- a multi-agent reinforcement learning (MARL)\napproach that learns to improve neural network optimisation by dynamically and\nresponsively scheduling hyperparameters during training. GANNO utilises an\nagent per layer that observes localised network dynamics and accordingly takes\nactions to adjust these dynamics at a layerwise level to collectively improve\nglobal performance. In this paper, we use GANNO to control the layerwise\nlearning rate and show that the framework can yield useful and responsive\nschedules that are competitive with handcrafted heuristics. Furthermore, GANNO\nis shown to perform robustly across a wide variety of unseen initial\nconditions, and can successfully generalise to harder problems than it was\ntrained on. Our work presents an overview of the opportunities that this\nparadigm offers for training neural networks, along with key challenges that\nremain to be overcome.",
        "updated": "2024-03-22T08:26:20Z",
        "published": "2023-11-30T14:45:51Z",
        "authors": [
            "Kale-ab Tessera",
            "Callum Rhys Tilbury",
            "Sasha Abramowitz",
            "Ruan de Kock",
            "Omayma Mahjoub",
            "Benjamin Rosman",
            "Sara Hooker",
            "Arnu Pretorius"
        ],
        "comments": "Accepted at the Workshop on Advanced Neural Network Training (WANT)\n  and Optimization for Machine Learning (OPT) at NeurIPS 2023",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.MA"
        ],
        "primary_category": "cs.LG"
    },
    "2401.01523v3": {
        "url": "http://arxiv.org/abs/2401.01523v3",
        "title": "GOAT-Bench: Safety Insights to Large Multimodal Models through\n  Meme-Based Social Abuse",
        "summary": "The exponential growth of social media has profoundly transformed how\ninformation is created, disseminated, and absorbed, exceeding any precedent in\nthe digital age. Regrettably, this explosion has also spawned a significant\nincrease in the online abuse of memes. Evaluating the negative impact of memes\nis notably challenging, owing to their often subtle and implicit meanings,\nwhich are not directly conveyed through the overt text and imagery. In light of\nthis, large multimodal models (LMMs) have emerged as a focal point of interest\ndue to their remarkable capabilities in handling diverse multimodal tasks. In\nresponse to this development, our paper aims to thoroughly examine the capacity\nof various LMMs (e.g., GPT-4V) to discern and respond to the nuanced aspects of\nsocial abuse manifested in memes. We introduce the comprehensive meme\nbenchmark, GOAT-Bench, comprising over 6K varied memes encapsulating themes\nsuch as implicit hate speech, sexism, and cyberbullying, etc. Utilizing\nGOAT-Bench, we delve into the ability of LMMs to accurately assess hatefulness,\nmisogyny, offensiveness, sarcasm, and harmful content. Our extensive\nexperiments across a range of LMMs reveal that current models still exhibit a\ndeficiency in safety awareness, showing insensitivity to various forms of\nimplicit abuse. We posit that this shortfall represents a critical impediment\nto the realization of safe artificial intelligence. The GOAT-Bench and\naccompanying resources are publicly accessible at https://goatlmm.github.io/,\ncontributing to ongoing research in this vital field.",
        "updated": "2024-03-01T05:26:39Z",
        "published": "2024-01-03T03:28:55Z",
        "authors": [
            "Hongzhan Lin",
            "Ziyang Luo",
            "Bo Wang",
            "Ruichao Yang",
            "Jing Ma"
        ],
        "comments": "The first work to benchmark Large Multimodal Models in safety insight\n  on social media",
        "categories": [
            "cs.CL",
            "cs.AI"
        ],
        "primary_category": "cs.CL"
    },
    "2401.01524v2": {
        "url": "http://arxiv.org/abs/2401.01524v2",
        "title": "Multimodal self-supervised learning for lesion localization",
        "summary": "Multimodal deep learning utilizing imaging and diagnostic reports has made\nimpressive progress in the field of medical imaging diagnostics, demonstrating\na particularly strong capability for auxiliary diagnosis in cases where\nsufficient annotation information is lacking. Nonetheless, localizing diseases\naccurately without detailed positional annotations remains a challenge.\nAlthough existing methods have attempted to utilize local information to\nachieve fine-grained semantic alignment, their capability in extracting the\nfine-grained semantics of the comprehensive context within reports is limited.\nTo address this problem, a new method is introduced that takes full sentences\nfrom textual reports as the basic units for local semantic alignment. This\napproach combines chest X-ray images with their corresponding textual reports,\nperforming contrastive learning at both global and local levels. The leading\nresults obtained by this method on multiple datasets confirm its efficacy in\nthe task of lesion localization.",
        "updated": "2024-03-17T08:42:19Z",
        "published": "2024-01-03T03:33:48Z",
        "authors": [
            "Hao Yang",
            "Hong-Yu Zhou",
            "Cheng Li",
            "Weijian Huang",
            "Jiarun Liu",
            "Yong Liang",
            "Shanshan Wang"
        ],
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2401.01545v2": {
        "url": "http://arxiv.org/abs/2401.01545v2",
        "title": "DDN-SLAM: Real-time Dense Dynamic Neural Implicit SLAM",
        "summary": "SLAM systems based on NeRF have demonstrated superior performance in\nrendering quality and scene reconstruction for static environments compared to\ntraditional dense SLAM. However, they encounter tracking drift and mapping\nerrors in real-world scenarios with dynamic interferences. To address these\nissues, we introduce DDN-SLAM, the first real-time dense dynamic neural\nimplicit SLAM system integrating semantic features. To address dynamic tracking\ninterferences, we propose a feature point segmentation method that combines\nsemantic features with a mixed Gaussian distribution model. To avoid incorrect\nbackground removal, we propose a mapping strategy based on sparse point cloud\nsampling and background restoration. We propose a dynamic semantic loss to\neliminate dynamic occlusions. Experimental results demonstrate that DDN-SLAM is\ncapable of robustly tracking and producing high-quality reconstructions in\ndynamic environments, while appropriately preserving potential dynamic objects.\nCompared to existing neural implicit SLAM systems, the tracking results on\ndynamic datasets indicate an average 90% improvement in Average Trajectory\nError (ATE) accuracy.",
        "updated": "2024-03-09T04:47:17Z",
        "published": "2024-01-03T05:42:17Z",
        "authors": [
            "Mingrui Li",
            "Yiming Zhou",
            "Guangan Jiang",
            "Tianchen Deng",
            "Yangyang Wang",
            "Hongyu Wang"
        ],
        "comments": "11pages, 4figures",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "primary_category": "cs.CV"
    },
    "2401.01577v3": {
        "url": "http://arxiv.org/abs/2401.01577v3",
        "title": "Test-Time Personalization with Meta Prompt for Gaze Estimation",
        "summary": "Despite the recent remarkable achievement in gaze estimation, efficient and\naccurate personalization of gaze estimation without labels is a practical\nproblem but rarely touched on in the literature. To achieve efficient\npersonalization, we take inspiration from the recent advances in Natural\nLanguage Processing (NLP) by updating a negligible number of parameters,\n\"prompts\", at the test time. Specifically, the prompt is additionally attached\nwithout perturbing original network and can contain less than 1% of a\nResNet-18's parameters. Our experiments show high efficiency of the prompt\ntuning approach. The proposed one can be 10 times faster in terms of adaptation\nspeed than the methods compared. However, it is non-trivial to update the\nprompt for personalized gaze estimation without labels. At the test time, it is\nessential to ensure that the minimizing of particular unsupervised loss leads\nto the goals of minimizing gaze estimation error. To address this difficulty,\nwe propose to meta-learn the prompt to ensure that its updates align with the\ngoal. Our experiments show that the meta-learned prompt can be effectively\nadapted even with a simple symmetry loss. In addition, we experiment on four\ncross-dataset validations to show the remarkable advantages of the proposed\nmethod. Code is available at https://github.com/hmarkamcan/TPGaze.",
        "updated": "2024-03-12T19:06:06Z",
        "published": "2024-01-03T07:02:35Z",
        "authors": [
            "Huan Liu",
            "Julia Qi",
            "Zhenhao Li",
            "Mohammad Hassanpour",
            "Yang Wang",
            "Konstantinos Plataniotis",
            "Yuanhao Yu"
        ],
        "comments": "Accepted by AAAI 2024",
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2401.01614v2": {
        "url": "http://arxiv.org/abs/2401.01614v2",
        "title": "GPT-4V(ision) is a Generalist Web Agent, if Grounded",
        "summary": "The recent development on large multimodal models (LMMs), especially\nGPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries\nof multimodal models beyond traditional tasks like image captioning and visual\nquestion answering. In this work, we explore the potential of LMMs like GPT-4V\nas a generalist web agent that can follow natural language instructions to\ncomplete tasks on any given website. We propose SEEACT, a generalist web agent\nthat harnesses the power of LMMs for integrated visual understanding and acting\non the web. We evaluate on the recent MIND2WEB benchmark. In addition to\nstandard offline evaluation on cached websites, we enable a new online\nevaluation setting by developing a tool that allows running web agents on live\nwebsites. We show that GPT-4V presents a great potential for web agents -- it\ncan successfully complete 51.1 of the tasks on live websites if we manually\nground its textual plans into actions on the websites. This substantially\noutperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2)\nspecifically fine-tuned for web agents. However, grounding still remains a\nmajor challenge. Existing LMM grounding strategies like set-of-mark prompting\nturns out to be not effective for web agents, and the best grounding strategy\nwe develop in this paper leverages both the HTML structure and visuals. Yet,\nthere is still a substantial gap with oracle grounding, leaving ample room for\nfurther improvement. All code, data, and evaluation tools are available at\nhttps://github.com/OSU-NLP-Group/SeeAct.",
        "updated": "2024-03-12T23:14:33Z",
        "published": "2024-01-03T08:33:09Z",
        "authors": [
            "Boyuan Zheng",
            "Boyu Gou",
            "Jihyung Kil",
            "Huan Sun",
            "Yu Su"
        ],
        "categories": [
            "cs.IR",
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "primary_category": "cs.IR"
    },
    "2401.01626v2": {
        "url": "http://arxiv.org/abs/2401.01626v2",
        "title": "On the Expressive Power of Graph Neural Networks",
        "summary": "The study of Graph Neural Networks has received considerable interest in the\npast few years. By extending deep learning to graph-structured data, GNNs can\nsolve a diverse set of tasks in fields including social science, chemistry, and\nmedicine. The development of GNN architectures has largely been focused on\nimproving empirical performance on tasks like node or graph classification.\nHowever, a line of recent work has instead sought to find GNN architectures\nthat have desirable theoretical properties - by studying their expressive power\nand designing architectures that maximize this expressiveness.\n  While there is no consensus on the best way to define the expressiveness of a\nGNN, it can be viewed from several well-motivated perspectives. Perhaps the\nmost natural approach is to study the universal approximation properties of\nGNNs, much in the way that this has been studied extensively for MLPs. Another\ndirection focuses on the extent to which GNNs can distinguish between different\ngraph structures, relating this to the graph isomorphism test. Besides, a GNN's\nability to compute graph properties such as graph moments has been suggested as\nanother form of expressiveness. All of these different definitions are\ncomplementary and have yielded different recommendations for GNN architecture\nchoices. In this paper, we would like to give an overview of the notion of\n\"expressive power\" of GNNs and provide some valuable insights regarding the\ndesign choices of GNNs.",
        "updated": "2024-03-08T19:57:35Z",
        "published": "2024-01-03T08:54:56Z",
        "authors": [
            "Ashwin Nalwade",
            "Kelly Marshall",
            "Axel Eladi",
            "Umang Sharma"
        ],
        "comments": "We felt that significantly more work was needed to improve the\n  quality before it should be put out in its current state. No replacement is\n  available at the moment or in the near future",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "primary_category": "cs.LG"
    },
    "2401.01642v3": {
        "url": "http://arxiv.org/abs/2401.01642v3",
        "title": "BLADE: Box-Level Supervised Amodal Segmentation through Directed\n  Expansion",
        "summary": "Perceiving the complete shape of occluded objects is essential for human and\nmachine intelligence. While the amodal segmentation task is to predict the\ncomplete mask of partially occluded objects, it is time-consuming and\nlabor-intensive to annotate the pixel-level ground truth amodal masks.\nBox-level supervised amodal segmentation addresses this challenge by relying\nsolely on ground truth bounding boxes and instance classes as supervision,\nthereby alleviating the need for exhaustive pixel-level annotations.\nNevertheless, current box-level methodologies encounter limitations in\ngenerating low-resolution masks and imprecise boundaries, failing to meet the\ndemands of practical real-world applications. We present a novel solution to\ntackle this problem by introducing a directed expansion approach from visible\nmasks to corresponding amodal masks. Our approach involves a hybrid end-to-end\nnetwork based on the overlapping region - the area where different instances\nintersect. Diverse segmentation strategies are applied for overlapping regions\nand non-overlapping regions according to distinct characteristics. To guide the\nexpansion of visible masks, we introduce an elaborately-designed connectivity\nloss for overlapping regions, which leverages correlations with visible masks\nand facilitates accurate amodal segmentation. Experiments are conducted on\nseveral challenging datasets and the results show that our proposed method can\noutperform existing state-of-the-art methods with large margins.",
        "updated": "2024-02-25T09:13:18Z",
        "published": "2024-01-03T09:37:03Z",
        "authors": [
            "Zhaochen Liu",
            "Zhixuan Li",
            "Tingting Jiang"
        ],
        "comments": "Accepted to AAAI 2024",
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV",
        "doi": "10.1609/aaai.v38i4.28176",
        "journal_ref": "Proceedings of the AAAI Conference on Artificial Intelligence. 38,\n  4 (Mar. 2024), 3846-3854"
    },
    "2401.01647v2": {
        "url": "http://arxiv.org/abs/2401.01647v2",
        "title": "SIGNeRF: Scene Integrated Generation for Neural Radiance Fields",
        "summary": "Advances in image diffusion models have recently led to notable improvements\nin the generation of high-quality images. In combination with Neural Radiance\nFields (NeRFs), they enabled new opportunities in 3D generation. However, most\ngenerative 3D approaches are object-centric and applying them to editing\nexisting photorealistic scenes is not trivial. We propose SIGNeRF, a novel\napproach for fast and controllable NeRF scene editing and scene-integrated\nobject generation. A new generative update strategy ensures 3D consistency\nacross the edited images, without requiring iterative optimization. We find\nthat depth-conditioned diffusion models inherently possess the capability to\ngenerate 3D consistent views by requesting a grid of images instead of single\nviews. Based on these insights, we introduce a multi-view reference sheet of\nmodified images. Our method updates an image collection consistently based on\nthe reference sheet and refines the original NeRF with the newly generated\nimage set in one go. By exploiting the depth conditioning mechanism of the\nimage diffusion model, we gain fine control over the spatial location of the\nedit and enforce shape guidance by a selected region or an external mesh.",
        "updated": "2024-03-27T09:39:41Z",
        "published": "2024-01-03T09:46:43Z",
        "authors": [
            "Jan-Niklas Dihlmann",
            "Andreas Engelhardt",
            "Hendrik Lensch"
        ],
        "comments": "Project Page: https://signerf.jdihlmann.com",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "primary_category": "cs.CV"
    },
    "2401.01650v2": {
        "url": "http://arxiv.org/abs/2401.01650v2",
        "title": "De-Confusing Pseudo-Labels in Source-Free Domain Adaptation",
        "summary": "Source-free domain adaptation (SFDA) aims to adapt a source-trained model to\nan unlabeled target domain without access to the source data. SFDA has\nattracted growing attention in recent years, where existing approaches focus on\nself-training that usually includes pseudo-labeling techniques. In this paper,\nwe introduce a novel noise-learning approach tailored to address noise\ndistribution in domain adaptation settings and learn to de-confuse the\npseudo-labels. More specifically, we learn a noise transition matrix of the\npseudo-labels to capture the label corruption of each class and learn the\nunderlying true label distribution. Estimating the noise transition matrix\nenables a better true class-posterior estimation, resulting in better\nprediction accuracy. We demonstrate the effectiveness of our approach when\ncombined with several SFDA methods: SHOT, SHOT++, and AaD. We obtain\nstate-of-the-art results on three domain adaptation datasets: VisDA, DomainNet,\nand OfficeHome.",
        "updated": "2024-03-13T13:13:57Z",
        "published": "2024-01-03T10:07:11Z",
        "authors": [
            "Idit Diamant",
            "Amir Rosenfeld",
            "Idan Achituve",
            "Jacob Goldberger",
            "Arnon Netzer"
        ],
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2401.01657v3": {
        "url": "http://arxiv.org/abs/2401.01657v3",
        "title": "Distributed Pose-graph Optimization with Multi-level Partitioning for\n  Collaborative SLAM",
        "summary": "The back-end module of Distributed Collaborative Simultaneous Localization\nand Mapping (DCSLAM) requires solving a nonlinear Pose Graph Optimization (PGO)\nunder a distributed setting, also known as SE(d)-synchronization. Most existing\ndistributed graph optimization algorithms employ a simple sequential\npartitioning scheme, which may result in unbalanced subgraph dimensions due to\nthe different geographic locations of each robot, and hence imposes extra\ncommunication load. Moreover, the performance of current Riemannian\noptimization algorithms can be further accelerated. In this letter, we propose\na novel distributed pose graph optimization algorithm combining multi-level\npartitioning with an accelerated Riemannian optimization method. Firstly, we\nemploy the multi-level graph partitioning algorithm to preprocess the naive\npose graph to formulate a balanced optimization problem. In addition, inspired\nby the accelerated coordinate descent method, we devise an Improved Riemannian\nBlock Coordinate Descent (IRBCD) algorithm and the critical point obtained is\nglobally optimal. Finally, we evaluate the effects of four common graph\npartitioning approaches on the correlation of the inter-subgraphs, and discover\nthat the Highest scheme has the best partitioning performance. Also, we\nimplement simulations to quantitatively demonstrate that our proposed algorithm\noutperforms the state-of-the-art distributed pose graph optimization protocols.",
        "updated": "2024-03-20T08:15:37Z",
        "published": "2024-01-03T10:31:12Z",
        "authors": [
            "Cunhao Li",
            "Peng Yi",
            "Guanghui Guo",
            "Yiguang Hong"
        ],
        "categories": [
            "cs.RO",
            "cs.MA"
        ],
        "primary_category": "cs.RO"
    },
    "2402.05493v4": {
        "url": "http://arxiv.org/abs/2402.05493v4",
        "title": "Investigating White-Box Attacks for On-Device Models",
        "summary": "Numerous mobile apps have leveraged deep learning capabilities. However,\non-device models are vulnerable to attacks as they can be easily extracted from\ntheir corresponding mobile apps. Existing on-device attacking approaches only\ngenerate black-box attacks, which are far less effective and efficient than\nwhite-box strategies. This is because mobile deep learning frameworks like\nTFLite do not support gradient computing, which is necessary for white-box\nattacking algorithms. Thus, we argue that existing findings may underestimate\nthe harmfulness of on-device attacks. To this end, we conduct a study to answer\nthis research question: Can on-device models be directly attacked via white-box\nstrategies? We first systematically analyze the difficulties of transforming\nthe on-device model to its debuggable version, and propose a Reverse\nEngineering framework for On-device Models (REOM), which automatically reverses\nthe compiled on-device TFLite model to the debuggable model. Specifically, REOM\nfirst transforms compiled on-device models into Open Neural Network Exchange\nformat, then removes the non-debuggable parts, and converts them to the\ndebuggable DL models format that allows attackers to exploit in a white-box\nsetting. Our experimental results show that our approach is effective in\nachieving automated transformation among 244 TFLite models. Compared with\nprevious attacks using surrogate models, REOM enables attackers to achieve\nhigher attack success rates with a hundred times smaller attack perturbations.\nIn addition, because the ONNX platform has plenty of tools for model format\nexchanging, the proposed method based on the ONNX platform can be adapted to\nother model formats. Our findings emphasize the need for developers to\ncarefully consider their model deployment strategies, and use white-box methods\nto evaluate the vulnerability of on-device models.",
        "updated": "2024-03-01T05:22:38Z",
        "published": "2024-02-08T09:03:17Z",
        "authors": [
            "Mingyi Zhou",
            "Xiang Gao",
            "Jing Wu",
            "Kui Liu",
            "Hailong Sun",
            "Li Li"
        ],
        "comments": "Published in The International Conference on Software Engineering\n  2024 (ICSE'24)",
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.CR"
        ],
        "primary_category": "cs.SE"
    },
    "2402.05608v3": {
        "url": "http://arxiv.org/abs/2402.05608v3",
        "title": "Scalable Diffusion Models with State Space Backbone",
        "summary": "This paper presents a new exploration into a category of diffusion models\nbuilt upon state space architecture. We endeavor to train diffusion models for\nimage data, wherein the traditional U-Net backbone is supplanted by a state\nspace backbone, functioning on raw patches or latent space. Given its notable\nefficacy in accommodating long-range dependencies, Diffusion State Space Models\n(DiS) are distinguished by treating all inputs including time, condition, and\nnoisy image patches as tokens. Our assessment of DiS encompasses both\nunconditional and class-conditional image generation scenarios, revealing that\nDiS exhibits comparable, if not superior, performance to CNN-based or\nTransformer-based U-Net architectures of commensurate size. Furthermore, we\nanalyze the scalability of DiS, gauged by the forward pass complexity\nquantified in Gflops. DiS models with higher Gflops, achieved through\naugmentation of depth/width or augmentation of input tokens, consistently\ndemonstrate lower FID. In addition to demonstrating commendable scalability\ncharacteristics, DiS-H/2 models in latent space achieve performance levels akin\nto prior diffusion models on class-conditional ImageNet benchmarks at the\nresolution of 256$\\times$256 and 512$\\times$512, while significantly reducing\nthe computational burden. The code and models are available at:\nhttps://github.com/feizc/DiS.",
        "updated": "2024-03-28T08:28:44Z",
        "published": "2024-02-08T12:08:42Z",
        "authors": [
            "Zhengcong Fei",
            "Mingyuan Fan",
            "Changqian Yu",
            "Junshi Huang"
        ],
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "primary_category": "cs.CV"
    },
    "2402.05663v2": {
        "url": "http://arxiv.org/abs/2402.05663v2",
        "title": "Mesoscale Traffic Forecasting for Real-Time Bottleneck and Shockwave\n  Prediction",
        "summary": "Accurate real-time traffic state forecasting plays a pivotal role in traffic\ncontrol research. In particular, the CIRCLES consortium project necessitates\npredictive techniques to mitigate the impact of data source delays. After the\nsuccess of the MegaVanderTest experiment, this paper aims at overcoming the\ncurrent system limitations and develop a more suited approach to improve the\nreal-time traffic state estimation for the next iterations of the experiment.\nIn this paper, we introduce the SA-LSTM, a deep forecasting method integrating\nSelf-Attention (SA) on the spatial dimension with Long Short-Term Memory (LSTM)\nyielding state-of-the-art results in real-time mesoscale traffic forecasting.\nWe extend this approach to multi-step forecasting with the n-step SA-LSTM,\nwhich outperforms traditional multi-step forecasting methods in the trade-off\nbetween short-term and long-term predictions, all while operating in real-time.",
        "updated": "2024-03-04T12:01:53Z",
        "published": "2024-02-08T13:27:10Z",
        "authors": [
            "Raphael Chekroun",
            "Han Wang",
            "Jonathan Lee",
            "Marin Toromanoff",
            "Sascha Hornauer",
            "Fabien Moutarde",
            "Maria Laura Delle Monache"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.RO"
        ],
        "primary_category": "cs.LG"
    },
    "2402.05667v2": {
        "url": "http://arxiv.org/abs/2402.05667v2",
        "title": "S$\u03a9$I: Score-based O-INFORMATION Estimation",
        "summary": "The analysis of scientific data and complex multivariate systems requires\ninformation quantities that capture relationships among multiple random\nvariables. Recently, new information-theoretic measures have been developed to\novercome the shortcomings of classical ones, such as mutual information, that\nare restricted to considering pairwise interactions. Among them, the concept of\ninformation synergy and redundancy is crucial for understanding the high-order\ndependencies between variables. One of the most prominent and versatile\nmeasures based on this concept is O-information, which provides a clear and\nscalable way to quantify the synergy-redundancy balance in multivariate\nsystems. However, its practical application is limited to simplified cases. In\nthis work, we introduce S$\\Omega$I, which allows for the first time to compute\nO-information without restrictive assumptions about the system. Our experiments\nvalidate our approach on synthetic data, and demonstrate the effectiveness of\nS$\\Omega$I in the context of a real-world use case.",
        "updated": "2024-03-20T15:33:23Z",
        "published": "2024-02-08T13:38:23Z",
        "authors": [
            "Mustapha Bounoua",
            "Giulio Franzese",
            "Pietro Michiardi"
        ],
        "categories": [
            "cs.LG",
            "cs.IT",
            "math.IT"
        ],
        "primary_category": "cs.LG"
    },
    "2402.05699v2": {
        "url": "http://arxiv.org/abs/2402.05699v2",
        "title": "Self-Alignment of Large Language Models via Monopolylogue-based Social\n  Scene Simulation",
        "summary": "Aligning large language models (LLMs) with human values is imperative to\nmitigate potential adverse effects resulting from their misuse. Drawing from\nthe sociological insight that acknowledging all parties' concerns is a key\nfactor in shaping human values, this paper proposes a novel direction to align\nLLMs by themselves: social scene simulation. To achieve this, we present\nMATRIX, a novel social scene simulator that emulates realistic scenes around a\nuser's input query, enabling the LLM to take social consequences into account\nbefore responding. MATRIX serves as a virtual rehearsal space, akin to a\nMonopolylogue, where the LLM performs diverse roles related to the query and\npractice by itself. To inject this alignment, we fine-tune the LLM with\nMATRIX-simulated data, ensuring adherence to human values without compromising\ninference speed. We theoretically show that the LLM with MATRIX outperforms\nConstitutional AI under mild assumptions. Finally, extensive experiments\nvalidate that our method outperforms over 10 baselines across 4 benchmarks. As\nevidenced by 875 user ratings, our tuned 13B-size LLM exceeds GPT-4 in aligning\nwith human values. Our project page is available at\nhttps://shuotang123.github.io/MATRIX.",
        "updated": "2024-02-29T08:46:47Z",
        "published": "2024-02-08T14:21:03Z",
        "authors": [
            "Xianghe Pang",
            "Shuo Tang",
            "Rui Ye",
            "Yuxin Xiong",
            "Bolun Zhang",
            "Yanfeng Wang",
            "Siheng Chen"
        ],
        "comments": "36 pages, 9 figures",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CY"
        ],
        "primary_category": "cs.CL"
    },
    "2402.05746v2": {
        "url": "http://arxiv.org/abs/2402.05746v2",
        "title": "Editable Scene Simulation for Autonomous Driving via Collaborative\n  LLM-Agents",
        "summary": "Scene simulation in autonomous driving has gained significant attention\nbecause of its huge potential for generating customized data. However, existing\neditable scene simulation approaches face limitations in terms of user\ninteraction efficiency, multi-camera photo-realistic rendering and external\ndigital assets integration. To address these challenges, this paper introduces\nChatSim, the first system that enables editable photo-realistic 3D driving\nscene simulations via natural language commands with external digital assets.\nTo enable editing with high command flexibility,~ChatSim leverages a large\nlanguage model (LLM) agent collaboration framework. To generate photo-realistic\noutcomes, ChatSim employs a novel multi-camera neural radiance field method.\nFurthermore, to unleash the potential of extensive high-quality digital assets,\nChatSim employs a novel multi-camera lighting estimation method to achieve\nscene-consistent assets' rendering. Our experiments on Waymo Open Dataset\ndemonstrate that ChatSim can handle complex language commands and generate\ncorresponding photo-realistic scene videos.",
        "updated": "2024-03-11T13:45:48Z",
        "published": "2024-02-08T15:26:28Z",
        "authors": [
            "Yuxi Wei",
            "Zi Wang",
            "Yifan Lu",
            "Chenxin Xu",
            "Changxing Liu",
            "Hao Zhao",
            "Siheng Chen",
            "Yanfeng Wang"
        ],
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2402.05808v2": {
        "url": "http://arxiv.org/abs/2402.05808v2",
        "title": "Training Large Language Models for Reasoning through Reverse Curriculum\n  Reinforcement Learning",
        "summary": "In this paper, we propose R$^3$: Learning Reasoning through Reverse\nCurriculum Reinforcement Learning (RL), a novel method that employs only\noutcome supervision to achieve the benefits of process supervision for large\nlanguage models. The core challenge in applying RL to complex reasoning is to\nidentify a sequence of actions that result in positive rewards and provide\nappropriate supervision for optimization. Outcome supervision provides sparse\nrewards for final results without identifying error locations, whereas process\nsupervision offers step-wise rewards but requires extensive manual annotation.\nR$^3$ overcomes these limitations by learning from correct demonstrations.\nSpecifically, R$^3$ progressively slides the start state of reasoning from a\ndemonstration's end to its beginning, facilitating easier model exploration at\nall stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome\nsupervision to offer step-level signals and precisely pinpoint errors. Using\nLlama2-7B, our method surpasses RL baseline on eight reasoning tasks by $4.1$\npoints on average. Notebaly, in program-based reasoning on GSM8K, it exceeds\nthe baseline by $4.2$ points across three backbone models, and without any\nextra data, Codellama-7B + R$^3$ performs comparable to larger models or\nclosed-source models.",
        "updated": "2024-03-17T09:02:02Z",
        "published": "2024-02-08T16:46:26Z",
        "authors": [
            "Zhiheng Xi",
            "Wenxiang Chen",
            "Boyang Hong",
            "Senjie Jin",
            "Rui Zheng",
            "Wei He",
            "Yiwen Ding",
            "Shichun Liu",
            "Xin Guo",
            "Junzhe Wang",
            "Honglin Guo",
            "Wei Shen",
            "Xiaoran Fan",
            "Yuhao Zhou",
            "Shihan Dou",
            "Xiao Wang",
            "Xinbo Zhang",
            "Peng Sun",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "comments": "Preprint. Codes released:\n  https://github.com/WooooDyy/LLM-Reverse-Curriculum-RL",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "primary_category": "cs.AI"
    },
    "2402.05840v2": {
        "url": "http://arxiv.org/abs/2402.05840v2",
        "title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties",
        "summary": "The availability of a robust map-based localization system is essential for\nthe operation of many autonomously navigating vehicles. Since uncertainty is an\ninevitable part of perception, it is beneficial for the robustness of the robot\nto consider it in typical downstream tasks of navigation stacks. In particular\nlocalization and mapping methods, which in modern systems often employ\nconvolutional neural networks (CNNs) for perception tasks, require proper\nuncertainty estimates. In this work, we present uncertainty-aware Panoptic\nLocalization and Mapping (uPLAM), which employs pixel-wise uncertainty\nestimates for panoptic CNNs as a bridge to fuse modern perception with\nclassical probabilistic localization and mapping approaches. Beyond the\nperception, we introduce an uncertainty-based map aggregation technique to\ncreate accurate panoptic maps, containing surface semantics and landmark\ninstances. Moreover, we provide cell-wise map uncertainties, and present a\nparticle filter-based localization method that employs perception\nuncertainties. Extensive evaluations show that our proposed incorporation of\nuncertainties leads to more accurate maps with reliable uncertainty estimates\nand improved localization accuracy. Additionally, we present the Freiburg\nPanoptic Driving dataset for evaluating panoptic mapping and localization\nmethods. We make our code and dataset available at:\n\\url{http://uplam.cs.uni-freiburg.de}",
        "updated": "2024-03-20T16:18:26Z",
        "published": "2024-02-08T17:17:06Z",
        "authors": [
            "Kshitij Sirohi",
            "Daniel B\u00fcscher",
            "Wolfram Burgard"
        ],
        "categories": [
            "cs.RO"
        ],
        "primary_category": "cs.RO"
    },
    "2402.05892v4": {
        "url": "http://arxiv.org/abs/2402.05892v4",
        "title": "Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data",
        "summary": "In recent years, Transformers have become the de-facto architecture for\nsequence modeling on text and a variety of multi-dimensional data, such as\nimages and video. However, the use of self-attention layers in a Transformer\nincurs prohibitive compute and memory complexity that scales quadratically\nw.r.t. the sequence length. A recent architecture, Mamba, based on state space\nmodels has been shown to achieve comparable performance for modeling text\nsequences, while scaling linearly with the sequence length. In this work, we\npresent Mamba-ND, a generalized design extending the Mamba architecture to\narbitrary multi-dimensional data. Our design alternatively unravels the input\ndata across different dimensions following row-major orderings. We provide a\nsystematic comparison of Mamba-ND with several other alternatives, based on\nprior multi-dimensional extensions such as Bi-directional LSTMs and S4ND.\nEmpirically, we show that Mamba-ND demonstrates performance competitive with\nthe state-of-the-art on a variety of multi-dimensional benchmarks, including\nImageNet-1K classification, HMDB-51 action recognition, and ERA5 weather\nforecasting.",
        "updated": "2024-03-20T00:58:15Z",
        "published": "2024-02-08T18:30:50Z",
        "authors": [
            "Shufan Li",
            "Harkanwar Singh",
            "Aditya Grover"
        ],
        "comments": "22 pages, 7 figures",
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2402.05946v2": {
        "url": "http://arxiv.org/abs/2402.05946v2",
        "title": "Unveiling Latent Causal Rules: A Temporal Point Process Approach for\n  Abnormal Event Explanation",
        "summary": "In high-stakes systems such as healthcare, it is critical to understand the\ncausal reasons behind unusual events, such as sudden changes in patient's\nhealth. Unveiling the causal reasons helps with quick diagnoses and precise\ntreatment planning. In this paper, we propose an automated method for\nuncovering \"if-then\" logic rules to explain observational events. We introduce\ntemporal point processes to model the events of interest, and discover the set\nof latent rules to explain the occurrence of events. To achieve this, we employ\nan Expectation-Maximization (EM) algorithm. In the E-step, we calculate the\nlikelihood of each event being explained by each discovered rule. In the\nM-step, we update both the rule set and model parameters to enhance the\nlikelihood function's lower bound. Notably, we optimize the rule set in a\ndifferential manner. Our approach demonstrates accurate performance in both\ndiscovering rules and identifying root causes. We showcase its promising\nresults using synthetic and real healthcare datasets.",
        "updated": "2024-03-19T08:43:29Z",
        "published": "2024-02-03T06:21:33Z",
        "authors": [
            "Yiling Kuang",
            "Chao Yang",
            "Yang Yang",
            "Shuang Li"
        ],
        "comments": "Accepted by AISTATS 2024",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "primary_category": "cs.LG"
    },
    "2402.18355v2": {
        "url": "http://arxiv.org/abs/2402.18355v2",
        "title": "COPR -- Efficient, large-scale log storage and retrieval",
        "summary": "Modern, large scale monitoring systems have to process and store vast amounts\nof log data in near real-time. At query time the systems have to find relevant\nlogs based on the content of the log message using support structures that can\nscale to these amounts of data while still being efficient to use. We present\nour novel Compressed Probabilistic Retrieval algorithm (COPR), capable of\nanswering Multi-Set Multi-Membership-Queries, that can be used as an\nalternative to existing indexing structures for streamed log data. In our\nexperiments, COPR required up to 93% less storage space than the tested\nstate-of-the-art inverted index and had up to four orders of magnitude less\nfalse-positives than the tested state-of-the-art membership sketch.\nAdditionally, COPR achieved up to 250 times higher query throughput than the\ntested inverted index and up to 240 times higher query throughput than the\ntested membership sketch.",
        "updated": "2024-03-27T07:22:40Z",
        "published": "2024-02-28T14:26:52Z",
        "authors": [
            "Julian Reichinger",
            "Thomas Krismayer",
            "Jan Rellermeyer"
        ],
        "comments": "14 pages, 8 figures",
        "categories": [
            "cs.IR",
            "cs.DB",
            "cs.DS",
            "H.3.1"
        ],
        "primary_category": "cs.IR"
    },
    "2402.18372v2": {
        "url": "http://arxiv.org/abs/2402.18372v2",
        "title": "FedUV: Uniformity and Variance for Heterogeneous Federated Learning",
        "summary": "Federated learning is a promising framework to train neural networks with\nwidely distributed data. However, performance degrades heavily with\nheterogeneously distributed data. Recent work has shown this is due to the\nfinal layer of the network being most prone to local bias, some finding success\nfreezing the final layer as an orthogonal classifier. We investigate the\ntraining dynamics of the classifier by applying SVD to the weights motivated by\nthe observation that freezing weights results in constant singular values. We\nfind that there are differences when training in IID and non-IID settings.\nBased on this finding, we introduce two regularization terms for local training\nto continuously emulate IID settings: (1) variance in the dimension-wise\nprobability distribution of the classifier and (2) hyperspherical uniformity of\nrepresentations of the encoder. These regularizations promote local models to\nact as if it were in an IID setting regardless of the local data distribution,\nthus offsetting proneness to bias while being flexible to the data. On\nextensive experiments in both label-shift and feature-shift settings, we verify\nthat our method achieves highest performance by a large margin especially in\nhighly non-IID cases in addition to being scalable to larger models and\ndatasets.",
        "updated": "2024-03-01T21:53:26Z",
        "published": "2024-02-27T15:53:15Z",
        "authors": [
            "Ha Min Son",
            "Moon-Hyun Kim",
            "Tai-Myoung Chung",
            "Chao Huang",
            "Xin Liu"
        ],
        "comments": "11 pages, 4 figures, 5 tables, to appear at CVPR 2024",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "primary_category": "cs.LG"
    },
    "2402.18377v1": {
        "url": "http://arxiv.org/abs/2402.18377v1",
        "title": "Out-of-Domain Generalization in Dynamical Systems Reconstruction",
        "summary": "In science we are interested in finding the governing equations, the\ndynamical rules, underlying empirical phenomena. While traditionally scientific\nmodels are derived through cycles of human insight and experimentation,\nrecently deep learning (DL) techniques have been advanced to reconstruct\ndynamical systems (DS) directly from time series data. State-of-the-art\ndynamical systems reconstruction (DSR) methods show promise in capturing\ninvariant and long-term properties of observed DS, but their ability to\ngeneralize to unobserved domains remains an open challenge. Yet, this is a\ncrucial property we would expect from any viable scientific theory. In this\nwork, we provide a formal framework that addresses generalization in DSR. We\nexplain why and how out-of-domain (OOD) generalization (OODG) in DSR profoundly\ndiffers from OODG considered elsewhere in machine learning. We introduce\nmathematical notions based on topological concepts and ergodic theory to\nformalize the idea of learnability of a DSR model. We formally prove that\nblack-box DL techniques, without adequate structural priors, generally will not\nbe able to learn a generalizing DSR model. We also show this empirically,\nconsidering major classes of DSR algorithms proposed so far, and illustrate\nwhere and why they fail to generalize across the whole phase space. Our study\nprovides the first comprehensive mathematical treatment of OODG in DSR, and\ngives a deeper conceptual understanding of where the fundamental problems in\nOODG lie and how they could possibly be addressed in practice.",
        "updated": "2024-02-28T14:52:58Z",
        "published": "2024-02-28T14:52:58Z",
        "authors": [
            "Niclas G\u00f6ring",
            "Florian Hess",
            "Manuel Brenner",
            "Zahra Monfared",
            "Daniel Durstewitz"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "math.DS",
            "nlin.CD"
        ],
        "primary_category": "cs.LG"
    },
    "2402.18383v2": {
        "url": "http://arxiv.org/abs/2402.18383v2",
        "title": "Robust Quantification of Percent Emphysema on CT via Domain Attention:\n  the Multi-Ethnic Study of Atherosclerosis (MESA) Lung Study",
        "summary": "Robust quantification of pulmonary emphysema on computed tomography (CT)\nremains challenging for large-scale research studies that involve scans from\ndifferent scanner types and for translation to clinical scans. Existing studies\nhave explored several directions to tackle this challenge, including density\ncorrection, noise filtering, regression, hidden Markov measure field (HMMF)\nmodel-based segmentation, and volume-adjusted lung density. Despite some\npromising results, previous studies either required a tedious workflow or\nlimited opportunities for downstream emphysema subtyping, limiting efficient\nadaptation on a large-scale study. To alleviate this dilemma, we developed an\nend-to-end deep learning framework based on an existing HMMF segmentation\nframework. We first demonstrate that a regular UNet cannot replicate the\nexisting HMMF results because of the lack of scanner priors. We then design a\nnovel domain attention block to fuse image feature with quantitative scanner\npriors which significantly improves the results.",
        "updated": "2024-03-06T14:08:43Z",
        "published": "2024-02-28T15:04:44Z",
        "authors": [
            "Xuzhe Zhang",
            "Elsa D. Angelini",
            "Eric A. Hoffman",
            "Karol E. Watson",
            "Benjamin M. Smith",
            "R. Graham Barr",
            "Andrew F. Laine"
        ],
        "comments": "5 pages, 5 figures. Accepted to IEEE International Symposium on\n  Biomedical Imaging 2024 (ISBI 2024). Camera-ready version",
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2402.18394v3": {
        "url": "http://arxiv.org/abs/2402.18394v3",
        "title": "Dual-IMU State Estimation for Relative Localization of Two Mobile Agents",
        "summary": "In this paper, we address the problem of relative localization of two mobile\nagents. Specifically, we consider the Dual-IMU system, where each agent is\nequipped with one IMU, and employs relative pose observations between them.\nPrevious works, however, typically assumed known ego motion and ignored biases\nof the IMUs. Instead, we study the most general case of unknown biases for both\nIMUs. Besides the derivation of dynamic model equations of the proposed system,\nwe focus on the observability analysis, for the observability under general\nmotion and the unobservable directions arising from various special motions.\nThrough numerical simulations, we validate our key observability findings and\nexamine their impact on the estimation accuracy and consistency. Finally, the\nsystem is implemented to achieve effective relative localization of an HMD with\nrespect to a vehicle moving in the real world.",
        "updated": "2024-03-06T15:10:47Z",
        "published": "2024-02-28T15:13:42Z",
        "authors": [
            "Wenqian Lai",
            "Ruonan Guo",
            "Kejian J. Wu"
        ],
        "categories": [
            "cs.RO"
        ],
        "primary_category": "cs.RO"
    },
    "2402.18402v2": {
        "url": "http://arxiv.org/abs/2402.18402v2",
        "title": "A Modular System for Enhanced Robustness of Multimedia Understanding\n  Networks via Deep Parametric Estimation",
        "summary": "In multimedia understanding tasks, corrupted samples pose a critical\nchallenge, because when fed to machine learning models they lead to performance\ndegradation. In the past, three groups of approaches have been proposed to\nhandle noisy data: i) enhancer and denoiser modules to improve the quality of\nthe noisy data, ii) data augmentation approaches, and iii) domain adaptation\nstrategies. All the aforementioned approaches come with drawbacks that limit\ntheir applicability; the first has high computational costs and requires pairs\nof clean-corrupted data for training, while the others only allow deployment of\nthe same task/network they were trained on (\\ie, when upstream and downstream\ntask/network are the same). In this paper, we propose SyMPIE to solve these\nshortcomings. To this end, we design a small, modular, and efficient (just\n2GFLOPs to process a Full HD image) system to enhance input data for robust\ndownstream multimedia understanding with minimal computational cost. Our SyMPIE\nis pre-trained on an upstream task/network that should not match the downstream\nones and does not need paired clean-corrupted samples. Our key insight is that\nmost input corruptions found in real-world tasks can be modeled through global\noperations on color channels of images or spatial filters with small kernels.\nWe validate our approach on multiple datasets and tasks, such as image\nclassification (on ImageNetC, ImageNetC-Bar, VizWiz, and a newly proposed mixed\ncorruption benchmark named ImageNetC-mixed) and semantic segmentation (on\nCityscapes, ACDC, and DarkZurich) with consistent improvements of about 5\\%\nrelative accuracy gain across the board. The code of our approach and the new\nImageNetC-mixed benchmark will be made available upon publication.",
        "updated": "2024-02-29T09:14:17Z",
        "published": "2024-02-28T15:24:58Z",
        "authors": [
            "Francesco Barbato",
            "Umberto Michieli",
            "Mehmet Kerim Yucel",
            "Pietro Zanuttigh",
            "Mete Ozay"
        ],
        "comments": "Accepted at ACM MMSys'24. 10 pages, 7 figures, 8 tables",
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV",
        "doi": "10.1145/3625468.3647623"
    },
    "2402.18409v2": {
        "url": "http://arxiv.org/abs/2402.18409v2",
        "title": "A Cognitive Evaluation Benchmark of Image Reasoning and Description for\n  Large Vision Language Models",
        "summary": "Large Vision Language Models (LVLMs), despite their recent success, are\nhardly comprehensively tested for their cognitive abilities. Inspired by the\nprevalent use of the \"Cookie Theft\" task in human cognition test, we propose a\nnovel evaluation benchmark to evaluate high-level cognitive ability of LVLMs\nusing images with rich semantics. It defines eight reasoning capabilities and\nconsists of an image description task and a visual question answering task. Our\nevaluation on well-known LVLMs shows that there is still a large gap in\ncognitive ability between LVLMs and humans.",
        "updated": "2024-02-29T13:12:34Z",
        "published": "2024-02-28T15:28:36Z",
        "authors": [
            "Xiujie Song",
            "Mengyue Wu",
            "Kenny Q. Zhu",
            "Chunhao Zhang",
            "Yanyi Chen"
        ],
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "primary_category": "cs.AI"
    },
    "2402.18411v2": {
        "url": "http://arxiv.org/abs/2402.18411v2",
        "title": "Unsupervised Cross-Domain Image Retrieval via Prototypical Optimal\n  Transport",
        "summary": "Unsupervised cross-domain image retrieval (UCIR) aims to retrieve images\nsharing the same category across diverse domains without relying on labeled\ndata. Prior approaches have typically decomposed the UCIR problem into two\ndistinct tasks: intra-domain representation learning and cross-domain feature\nalignment. However, these segregated strategies overlook the potential\nsynergies between these tasks. This paper introduces ProtoOT, a novel Optimal\nTransport formulation explicitly tailored for UCIR, which integrates\nintra-domain feature representation learning and cross-domain alignment into a\nunified framework. ProtoOT leverages the strengths of the K-means clustering\nmethod to effectively manage distribution imbalances inherent in UCIR. By\nutilizing K-means for generating initial prototypes and approximating class\nmarginal distributions, we modify the constraints in Optimal Transport\naccordingly, significantly enhancing its performance in UCIR scenarios.\nFurthermore, we incorporate contrastive learning into the ProtoOT framework to\nfurther improve representation learning. This encourages local semantic\nconsistency among features with similar semantics, while also explicitly\nenforcing separation between features and unmatched prototypes, thereby\nenhancing global discriminativeness. ProtoOT surpasses existing\nstate-of-the-art methods by a notable margin across benchmark datasets.\nNotably, on DomainNet, ProtoOT achieves an average P@200 enhancement of 24.44%,\nand on Office-Home, it demonstrates a P@15 improvement of 12.12%. Code is\navailable at https://github.com/HCVLAB/ProtoOT.",
        "updated": "2024-03-24T12:04:11Z",
        "published": "2024-02-28T15:31:45Z",
        "authors": [
            "Bin Li",
            "Ye Shi",
            "Qian Yu",
            "Jingya Wang"
        ],
        "comments": "Accepted by AAAI2024",
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2402.18412v2": {
        "url": "http://arxiv.org/abs/2402.18412v2",
        "title": "QAOA with random and subgraph phase operators",
        "summary": "The quantum approximate optimization algorithm (QAOA) is a promising quantum\nalgorithm that can be used to approximately solve combinatorial optimization\nproblems. The usual QAOA ansatz consists of an alternating application of the\ncost and mixer Hamiltonians. In this work, we study how using Hamiltonians\nother than the usual cost Hamiltonian, dubbed custom phase operators, can\naffect the performance of QAOA. We derive an expected value formula for QAOA\nwith custom phase operators at p = 1 and show numerically that some of these\ncustom phase operators can achieve higher approximation ratio than the original\nalgorithm implementation. Out of all the graphs tested, 0.036\\% of the random\ncustom phase operators, 75.9\\% of the subgraph custom phase operators, 95.1\\%\nof the triangle-removed custom phase operators, and 93.9\\% of the maximal\ndegree edge-removed custom phase operators have a higher approximation ratio\nthan the original QAOA implementation. This finding opens up the question of\nwhether better phase operators can be designed to further improve the\nperformance of QAOA.",
        "updated": "2024-03-18T15:00:09Z",
        "published": "2024-02-28T15:32:35Z",
        "authors": [
            "Anthony Wilkie",
            "Igor Gaidai",
            "James Ostrowski",
            "Rebekah Herrman"
        ],
        "categories": [
            "quant-ph",
            "cs.ET"
        ],
        "primary_category": "quant-ph"
    },
    "2402.18420v2": {
        "url": "http://arxiv.org/abs/2402.18420v2",
        "title": "CafkNet: GNN-Empowered Forward Kinematic Modeling for Cable-Driven\n  Parallel Robots",
        "summary": "The Cable-Driven Parallel Robots (CDPRs) have gained significant attention\ndue to their high payload capacity and large workspace. When deploying CDPRs in\npractice, one of the challenges is kinematic modeling. Unlike serial\nmechanisms, CDPRs have a simple inverse kinematics problem but a complex\nforward kinematics (FK) issue. Therefore, the development of accurate and\nefficient FK solvers has been a prominent research focus in CDPR applications.\nBy observing the topology within CDPRs, in this paper, we propose a graph-based\nrepresentation to model CDPRs and introduce CafkNet, a fast and general FK\nsolver, leveraging Graph Neural Network (GNN). CafkNet is extensively tested on\n3D and 2D CDPRs in different configurations, both in simulators and real\nscenarios. The results demonstrate its ability to learn CDPRs' internal\ntopology and accurately solve the FK problem. Then, the zero-shot\ngeneralization from one configuration to another is validated. Also, the\nsim2real gap can be bridged by CafkNet using both simulation and real-world\ndata. To the best of our knowledge, it is the first study that employs the GNN\nto solve FK problem for CDPRs.",
        "updated": "2024-03-05T11:55:37Z",
        "published": "2024-02-28T15:41:12Z",
        "authors": [
            "Zeqing Zhang",
            "Linhan Yang",
            "Cong Sun",
            "Weiwei Shang",
            "Jia Pan"
        ],
        "comments": "To the best of authors' knowledge, it is the first study to employ\n  the GNN for the FK problem of CDPRs. First two authors have equal\n  contribution. Videos and codes are available at\n  https://sites.google.com/view/cafknet/site",
        "categories": [
            "cs.RO"
        ],
        "primary_category": "cs.RO"
    },
    "2403.00211v2": {
        "url": "http://arxiv.org/abs/2403.00211v2",
        "title": "Trustworthy Self-Attention: Enabling the Network to Focus Only on the\n  Most Relevant References",
        "summary": "The prediction of optical flow for occluded points is still a difficult\nproblem that has not yet been solved. Recent methods use self-attention to find\nrelevant non-occluded points as references for estimating the optical flow of\noccluded points based on the assumption of self-similarity. However, they rely\non visual features of a single image and weak constraints, which are not\nsufficient to constrain the trained network to focus on erroneous and weakly\nrelevant reference points. We make full use of online occlusion recognition\ninformation to construct occlusion extended visual features and two strong\nconstraints, allowing the network to learn to focus only on the most relevant\nreferences without requiring occlusion ground truth to participate in the\ntraining of the network. Our method adds very few network parameters to the\noriginal framework, making it very lightweight. Extensive experiments show that\nour model has the greatest cross-dataset generalization. Our method achieves\nmuch greater error reduction, 18.6%, 16.2%, and 20.1% for all points,\nnon-occluded points, and occluded points respectively from the state-of-the-art\nGMA-base method, MATCHFlow(GMA), on Sintel Albedo pass. Furthermore, our model\nachieves state-of-the-art performance on the Sintel bench-marks, ranking \\#1\namong all published methods on Sintel clean pass. The code will be open-source.",
        "updated": "2024-03-27T01:50:06Z",
        "published": "2024-03-01T01:07:40Z",
        "authors": [
            "Yu Jing",
            "Tan Yujuan",
            "Ren Ao",
            "Liu Duo"
        ],
        "comments": "Correct Figure 1",
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2403.00212v1": {
        "url": "http://arxiv.org/abs/2403.00212v1",
        "title": "Transcription and translation of videos using fine-tuned XLSR Wav2Vec2\n  on custom dataset and mBART",
        "summary": "This research addresses the challenge of training an ASR model for\npersonalized voices with minimal data. Utilizing just 14 minutes of custom\naudio from a YouTube video, we employ Retrieval-Based Voice Conversion (RVC) to\ncreate a custom Common Voice 16.0 corpus. Subsequently, a Cross-lingual\nSelf-supervised Representations (XLSR) Wav2Vec2 model is fine-tuned on this\ndataset. The developed web-based GUI efficiently transcribes and translates\ninput Hindi videos. By integrating XLSR Wav2Vec2 and mBART, the system aligns\nthe translated text with the video timeline, delivering an accessible solution\nfor multilingual video content transcription and translation for personalized\nvoice.",
        "updated": "2024-03-01T01:15:45Z",
        "published": "2024-03-01T01:15:45Z",
        "authors": [
            "Aniket Tathe",
            "Anand Kamble",
            "Suyash Kumbharkar",
            "Atharva Bhandare",
            "Anirban C. Mitra"
        ],
        "categories": [
            "cs.CL",
            "cs.CV",
            "cs.LG",
            "cs.SD",
            "eess.AS"
        ],
        "primary_category": "cs.CL"
    },
    "2403.00219v1": {
        "url": "http://arxiv.org/abs/2403.00219v1",
        "title": "Multi-modal Attribute Prompting for Vision-Language Models",
        "summary": "Large pre-trained Vision-Language Models (VLMs), like CLIP, exhibit strong\ngeneralization ability to downstream tasks but struggle in few-shot scenarios.\nExisting prompting techniques primarily focus on global text and image\nrepresentations, yet overlooking multi-modal attribute characteristics. This\nlimitation hinders the model's ability to perceive fine-grained visual details\nand restricts its generalization ability to a broader range of unseen classes.\nTo address this issue, we propose a Multi-modal Attribute Prompting method\n(MAP) by jointly exploring textual attribute prompting, visual attribute\nprompting, and attribute-level alignment. The proposed MAP enjoys several\nmerits. First, we introduce learnable visual attribute prompts enhanced by\ntextual attribute semantics to adaptively capture visual attributes for images\nfrom unknown categories, boosting fine-grained visual perception capabilities\nfor CLIP. Second, the proposed attribute-level alignment complements the global\nalignment to enhance the robustness of cross-modal alignment for\nopen-vocabulary objects. To our knowledge, this is the first work to establish\ncross-modal attribute-level alignment for CLIP-based few-shot adaptation.\nExtensive experimental results on 11 datasets demonstrate that our method\nperforms favorably against state-of-the-art approaches.",
        "updated": "2024-03-01T01:28:10Z",
        "published": "2024-03-01T01:28:10Z",
        "authors": [
            "Xin Liu",
            "Jiamin Wu",
            "Tianzhu Zhang"
        ],
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2403.00221v1": {
        "url": "http://arxiv.org/abs/2403.00221v1",
        "title": "Mode Consensus Algorithms With Finite Convergence Time",
        "summary": "This paper studies the distributed mode consensus problem in a multi-agent\nsystem, in which the agents each possess a certain attribute and they aim to\nagree upon the mode (the most frequent attribute owned by the agents) via\ndistributed computation. Three algorithms are proposed. The first one directly\ncalculates the frequency of all attributes at every agent, with protocols based\non blended dynamics, and then returns the most frequent attribute as the mode.\nAssuming knowledge at each agent of a lower bound of the mode frequency as a\npriori information, the second algorithm is able to reduce the number of\nfrequencies to be computed at every agent if the lower bound is large. The\nthird algorithm further eliminates the need for this information by introducing\nan adaptive updating mechanism. The algorithms find the mode in finite time,\nand estimates of convergence time are provided. The proposed first and second\nalgorithms enjoy the plug-and-play property with a dwell time.",
        "updated": "2024-03-01T01:39:52Z",
        "published": "2024-03-01T01:39:52Z",
        "authors": [
            "Chao Huang",
            "Hyungbo Shim",
            "Siliang Yu",
            "Brian D. O. Anderson"
        ],
        "categories": [
            "eess.SY",
            "cs.SY"
        ],
        "primary_category": "eess.SY"
    },
    "2403.00222v1": {
        "url": "http://arxiv.org/abs/2403.00222v1",
        "title": "Efficient Reinforcement Learning for Global Decision Making in the\n  Presence of Local Agents at Scale",
        "summary": "We study reinforcement learning for global decision-making in the presence of\nmany local agents, where the global decision-maker makes decisions affecting\nall local agents, and the objective is to learn a policy that maximizes the\nrewards of both the global and the local agents. Such problems find many\napplications, e.g. demand response, EV charging, queueing, etc. In this\nsetting, scalability has been a long-standing challenge due to the size of the\nstate/action space which can be exponential in the number of agents. This work\nproposes the SUB-SAMPLE-Q algorithm where the global agent subsamples $k\\leq n$\nlocal agents to compute an optimal policy in time that is only exponential in\n$k$, providing an exponential speedup from standard methods that are\nexponential in $n$. We show that the learned policy converges to the optimal\npolicy in the order of $\\tilde{O}(1/\\sqrt{k}+\\epsilon_{k,m})$ as the number of\nsub-sampled agents $k$ increases, where $\\epsilon_{k,m}$ is the Bellman noise.\nWe also conduct numerical simulations in a demand-response setting and a\nqueueing setting.",
        "updated": "2024-03-01T01:49:57Z",
        "published": "2024-03-01T01:49:57Z",
        "authors": [
            "Emile Anand",
            "Guannan Qu"
        ],
        "comments": "30 pages, 6 figures",
        "categories": [
            "cs.LG",
            "cs.MA",
            "I.2.6"
        ],
        "primary_category": "cs.LG"
    },
    "2403.00225v2": {
        "url": "http://arxiv.org/abs/2403.00225v2",
        "title": "Robust Policy Learning via Offline Skill Diffusion",
        "summary": "Skill-based reinforcement learning (RL) approaches have shown considerable\npromise, especially in solving long-horizon tasks via hierarchical structures.\nThese skills, learned task-agnostically from offline datasets, can accelerate\nthe policy learning process for new tasks. Yet, the application of these skills\nin different domains remains restricted due to their inherent dependency on the\ndatasets, which poses a challenge when attempting to learn a skill-based policy\nvia RL for a target domain different from the datasets' domains. In this paper,\nwe present a novel offline skill learning framework DuSkill which employs a\nguided Diffusion model to generate versatile skills extended from the limited\nskills in datasets, thereby enhancing the robustness of policy learning for\ntasks in different domains. Specifically, we devise a guided diffusion-based\nskill decoder in conjunction with the hierarchical encoding to disentangle the\nskill embedding space into two distinct representations, one for encapsulating\ndomain-invariant behaviors and the other for delineating the factors that\ninduce domain variations in the behaviors. Our DuSkill framework enhances the\ndiversity of skills learned offline, thus enabling to accelerate the learning\nprocedure of high-level policies for different domains. Through experiments, we\nshow that DuSkill outperforms other skill-based imitation learning and RL\nalgorithms for several long-horizon tasks, demonstrating its benefits in\nfew-shot imitation and online RL.",
        "updated": "2024-03-05T06:23:41Z",
        "published": "2024-03-01T02:00:44Z",
        "authors": [
            "Woo Kyung Kim",
            "Minjong Yoo",
            "Honguk Woo"
        ],
        "comments": "Accepted for AAAI 2024",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.RO"
        ],
        "primary_category": "cs.LG"
    },
    "2403.00228v1": {
        "url": "http://arxiv.org/abs/2403.00228v1",
        "title": "DISORF: A Distributed Online NeRF Training and Rendering Framework for\n  Mobile Robots",
        "summary": "We present a framework, DISORF, to enable online 3D reconstruction and\nvisualization of scenes captured by resource-constrained mobile robots and edge\ndevices. To address the limited compute capabilities of edge devices and\npotentially limited network availability, we design a framework that\nefficiently distributes computation between the edge device and remote server.\nWe leverage on-device SLAM systems to generate posed keyframes and transmit\nthem to remote servers that can perform high quality 3D reconstruction and\nvisualization at runtime by leveraging NeRF models. We identify a key challenge\nwith online NeRF training where naive image sampling strategies can lead to\nsignificant degradation in rendering quality. We propose a novel shifted\nexponential frame sampling method that addresses this challenge for online NeRF\ntraining. We demonstrate the effectiveness of our framework in enabling\nhigh-quality real-time reconstruction and visualization of unknown scenes as\nthey are captured and streamed from cameras in mobile robots and edge devices.",
        "updated": "2024-03-01T02:19:40Z",
        "published": "2024-03-01T02:19:40Z",
        "authors": [
            "Chunlin Li",
            "Ruofan Liang",
            "Hanrui Fan",
            "Zhengen Zhang",
            "Sankeerth Durvasula",
            "Nandita Vijaykumar"
        ],
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "primary_category": "cs.RO"
    },
    "2403.00229v1": {
        "url": "http://arxiv.org/abs/2403.00229v1",
        "title": "Diffraction and Scattering Aware Radio Map and Environment\n  Reconstruction using Geometry Model-Assisted Deep Learning",
        "summary": "Machine learning (ML) facilitates rapid channel modeling for 5G and beyond\nwireless communication systems. Many existing ML techniques utilize a city map\nto construct the radio map; however, an updated city map may not always be\navailable. This paper proposes to employ the received signal strength (RSS)\ndata to jointly construct the radio map and the virtual environment by\nexploiting the geometry structure of the environment. In contrast to many\nexisting ML approaches that lack of an environment model, we develop a virtual\nobstacle model and characterize the geometry relation between the propagation\npaths and the virtual obstacles. A multi-screen knife-edge model is adopted to\nextract the key diffraction features, and these features are fed into a neural\nnetwork (NN) for diffraction representation. To describe the scattering, as\noppose to most existing methods that directly input an entire city map, our\nmodel focuses on the geometry structure from the local area surrounding the\nTX-RX pair and the spatial invariance of such local geometry structure is\nexploited. Numerical experiments demonstrate that, in addition to\nreconstructing a 3D virtual environment, the proposed model outperforms the\nstate-of-the-art methods in radio map construction with 10%-18% accuracy\nimprovements. It can also reduce 20% data and 50% training epochs when\ntransferred to a new environment.",
        "updated": "2024-03-01T02:20:01Z",
        "published": "2024-03-01T02:20:01Z",
        "authors": [
            "Wangqian Chen",
            "Junting Chen"
        ],
        "comments": "13 pages",
        "categories": [
            "eess.SP",
            "cs.LG"
        ],
        "primary_category": "eess.SP"
    },
    "2403.00231v2": {
        "url": "http://arxiv.org/abs/2403.00231v2",
        "title": "Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of\n  Large Vision-Language Models",
        "summary": "Large vision-language models (LVLMs), exemplified by GPT-4V, excel across\ndiverse tasks involving concrete images from natural scenes. However, their\nability to interpret abstract figures, such as geometry shapes and scientific\nplots, remains limited due to a scarcity of training datasets in scientific\ndomains. To fill this gap, we introduce Multimodal ArXiv, consisting of\nArXivCap and ArXivQA, for enhancing LVLMs scientific comprehension. ArXivCap is\na figure-caption dataset comprising 6.4M images and 3.9M captions sourced from\n572K ArXiv papers spanning various scientific domains. Drawing from ArXivCap,\nwe introduce ArXivQA, a question-answering dataset generated by prompting\nGPT-4V based on scientific figures. ArXivQA greatly enhances LVLMs'\nmathematical reasoning capabilities, achieving a 10.4% absolute accuracy gain\non a multimodal mathematical reasoning benchmark. Furthermore, employing\nArXivCap, we devise four vision-to-text tasks for benchmarking LVLMs.\nEvaluation results with state-of-the-art LVLMs underscore their struggle with\nthe nuanced semantics of academic figures, with domain-specific training\nyielding substantial performance gains. Our error analysis uncovers\nmisinterpretations of visual context, recognition errors, and the production of\noverly simplified captions by current LVLMs, shedding light on future\nimprovements.",
        "updated": "2024-03-04T07:01:59Z",
        "published": "2024-03-01T02:21:30Z",
        "authors": [
            "Lei Li",
            "Yuqi Wang",
            "Runxin Xu",
            "Peiyi Wang",
            "Xiachong Feng",
            "Lingpeng Kong",
            "Qi Liu"
        ],
        "comments": "Project page: https://mm-arxiv.github.io Fix typos",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "primary_category": "cs.CV"
    },
    "2403.00232v1": {
        "url": "http://arxiv.org/abs/2403.00232v1",
        "title": "FTTN: Feature-Targeted Testing for Numerical Properties of NVIDIA & AMD\n  Matrix Accelerators",
        "summary": "NVIDIA Tensor Cores and AMD Matrix Cores (together called Matrix\nAccelerators) are of growing interest in high-performance computing and machine\nlearning owing to their high performance. Unfortunately, their numerical\nbehaviors are not publicly documented, including the number of extra precision\nbits maintained, the accumulation order of addition, and predictable subnormal\nnumber handling during computations. This makes it impossible to reliably port\ncodes across these differing accelerators. This paper contributes a collection\nof {\\em Feature Targeted Tests for Numerical Properties} that that help\ndetermine these features across five floating-point formats, four rounding\nmodes and additional that highlight the rounding behaviors and preservation of\nextra precision bits. To show the practical relevance of FTTN, we design a\nsimple matrix-multiplication test designed with insights gathered from our\nfeature-tests. We executed this very simple test on five platforms, producing\ndifferent answers: V100, A100, and MI250X produced 0, MI100 produced 255.875,\nand Hopper H100 produced 191.875. Our matrix multiplication tests employ\npatterns found in iterative refinement-based algorithms, highlighting the need\nto check for significant result variability when porting code across GPUs.",
        "updated": "2024-03-01T02:22:22Z",
        "published": "2024-03-01T02:22:22Z",
        "authors": [
            "Xinyi Li",
            "Ang Li",
            "Bo Fang",
            "Katarzyna Swirydowicz",
            "Ignacio Laguna",
            "Ganesh Gopalakrishnan"
        ],
        "categories": [
            "cs.AR"
        ],
        "primary_category": "cs.AR"
    },
    "2403.01289v1": {
        "url": "http://arxiv.org/abs/2403.01289v1",
        "title": "Greed is All You Need: An Evaluation of Tokenizer Inference Methods",
        "summary": "While subword tokenizers such as BPE and WordPiece are typically used to\nbuild vocabularies for NLP models, the method of decoding text into a sequence\nof tokens from these vocabularies is often left unspecified, or ill-suited to\nthe method in which they were constructed. We provide a controlled analysis of\nseven tokenizer inference methods across four different algorithms and three\nvocabulary sizes, performed on a novel intrinsic evaluation suite we curated\nfor English, combining measures rooted in morphology, cognition, and\ninformation theory. We show that for the most commonly used tokenizers, greedy\ninference performs surprisingly well; and that SaGe, a recently-introduced\ncontextually-informed tokenizer, outperforms all others on morphological\nalignment.",
        "updated": "2024-03-02T19:01:40Z",
        "published": "2024-03-02T19:01:40Z",
        "authors": [
            "Omri Uzan",
            "Craig W. Schmidt",
            "Chris Tanner",
            "Yuval Pinter"
        ],
        "categories": [
            "cs.CL"
        ],
        "primary_category": "cs.CL"
    },
    "2403.01290v1": {
        "url": "http://arxiv.org/abs/2403.01290v1",
        "title": "Characterizing Ethereum Upgradable Smart Contracts and Their Security\n  Implications",
        "summary": "Upgradeable smart contracts (USCs) have been widely adopted to enable\nmodifying deployed smart contracts. While USCs bring great flexibility to\ndevelopers, improper usage might introduce new security issues, potentially\nallowing attackers to hijack USCs and their users. In this paper, we conduct a\nlarge-scale measurement study to characterize USCs and their security\nimplications in the wild. We summarize six commonly used USC patterns and\ndevelop a tool, USCDetector, to identify USCs without needing source code.\nParticularly, USCDetector collects various information such as bytecode and\ntransaction information to construct upgrade chains for USCs and disclose\npotentially vulnerable ones. We evaluate USCDetector using verified smart\ncontracts (i.e., with source code) as ground truth and show that USCDetector\ncan achieve high accuracy with a precision of 96.26%. We then use USCDetector\nto conduct a large-scale study on Ethereum, covering a total of 60,251,064\nsmart contracts. USCDetecor constructs 10,218 upgrade chains and discloses\nmultiple real-world USCs with potential security issues.",
        "updated": "2024-03-02T19:04:20Z",
        "published": "2024-03-02T19:04:20Z",
        "authors": [
            "Xiaofan Li",
            "Jin Yang",
            "Jiaqi Chen",
            "Yuzhe Tang",
            "Xing Gao"
        ],
        "comments": "12 pages, 5 figures",
        "categories": [
            "cs.CR",
            "cs.CE"
        ],
        "primary_category": "cs.CR"
    },
    "2403.01291v1": {
        "url": "http://arxiv.org/abs/2403.01291v1",
        "title": "A fractional-order trace-dev-div inequality",
        "summary": "The trace-dev-div inequality in $H^s$ controls the trace in the norm of $H^s$\nby that of the deviatoric part plus the $H^{s-1}$ norm of the divergence of a\nquadratic tensor field different from the constant unit matrix. This is well\nknown for $s=0$ and established for orders $0\\le s\\le 1$ and arbitrary space\ndimension in this note. For mixed and least-squares finite element error\nanalysis in linear elasticity, this inequality allows to establish robustness\nwith respect to the Lam\\'e parameter $\\lambda$.",
        "updated": "2024-03-02T19:12:28Z",
        "published": "2024-03-02T19:12:28Z",
        "authors": [
            "Carsten Carstensen",
            "Norbert Heuer"
        ],
        "categories": [
            "math.NA",
            "cs.NA",
            "math.AP",
            "35Q74, 35A23, 46E35, 35Q74, 74B05"
        ],
        "primary_category": "math.NA"
    },
    "2403.01292v2": {
        "url": "http://arxiv.org/abs/2403.01292v2",
        "title": "Autonomous Intelligent Systems: From Illusion of Control to Inescapable\n  Delusion",
        "summary": "Autonomous systems, including generative AI, have been adopted faster than\nprevious digital innovations. Their impact on society might as well be more\nprofound, with a radical restructuring of the economy of knowledge and dramatic\nconsequences for social and institutional balances. Different attitudes to\ncontrol these systems have emerged rooted in the classical pillars of legal\nsystems, proprietary rights, and social responsibility. We show how an illusion\nof control might be guiding governments and regulators, while autonomous\nsystems might be driving us to inescapable delusion.",
        "updated": "2024-03-24T10:18:17Z",
        "published": "2024-03-02T19:19:06Z",
        "authors": [
            "St\u00e9phane Grumbach",
            "Giorgio Resta",
            "Riccardo Torlone"
        ],
        "comments": "11 pages, preliminary version, currently under submission to a\n  conference",
        "categories": [
            "cs.CY",
            "K.4; K.4.1; K.5"
        ],
        "primary_category": "cs.CY"
    },
    "2403.01297v1": {
        "url": "http://arxiv.org/abs/2403.01297v1",
        "title": "Experimental Evaluation of the ETSI DCC Adaptive Approach and Related\n  Algorithms",
        "summary": "Decentralized Congestion Control (DCC) mechanisms have been a core part of\nprotocol stacks for vehicular networks since their inception and\nstandardization. The ETSI ITS-G5 protocol stack for vehicular communications\nconsiders the usage of DCC not only in the network or access layers, but also\nas a part of the cross-layer architecture that influences how often messages\nare generated and transmitted. ETSI DCC mechanisms have evolved from a reactive\napproach based on a finite state machine, to an adaptive approach that relies\non a linear control algorithm. This linear control algorithm, called LIMERIC,\nis the basis of the mechanism used in the ETSI DCC Adaptive Approach. The\nbehavior of this algorithm depends on a set of parameters. Different values for\nthese parameters have been proposed in the literature, including those defined\nin the ETSI specification. A recent proposal is Dual-$\\alpha$, which chooses\nparameters to improve convergence and fairness when the algorithm has to react\nto fast changes in the use of the shared medium (transitory situations). This\narticle evaluates, by means of simulations, the performance of the ETSI DCC\nAdaptive Approach and related algorithms, considering both steady state and\ntransitory situations. Results show that a bad selection of parameters can make\na DCC algorithm ineffective, that the ETSI DCC Adaptive algorithm performs well\nin steady state conditions, and that Dual-$\\alpha$ performs as well in steady\nstate conditions and outperforms the ETSI DCC Adaptive Approach in transitory\nscenarios.",
        "updated": "2024-03-02T19:29:52Z",
        "published": "2024-03-02T19:29:52Z",
        "authors": [
            "Oscar Amador",
            "Ignacio Soto",
            "Maria Calderon",
            "Manuel Urue\u00f1a"
        ],
        "categories": [
            "cs.NI"
        ],
        "primary_category": "cs.NI",
        "doi": "10.1109/ACCESS.2020.2980377",
        "journal_ref": "in IEEE Access, vol. 8, pp. 49798-49811, 2020"
    },
    "2403.01299v1": {
        "url": "http://arxiv.org/abs/2403.01299v1",
        "title": "A Photonic Physically Unclonable Function's Resilience to\n  Multiple-Valued Machine Learning Attacks",
        "summary": "Physically unclonable functions (PUFs) identify integrated circuits using\nnonlinearly-related challenge-response pairs (CRPs). Ideally, the relationship\nbetween challenges and corresponding responses is unpredictable, even if a\nsubset of CRPs is known. Previous work developed a photonic PUF offering\nimproved security compared to non-optical counterparts. Here, we investigate\nthis PUF's susceptibility to Multiple-Valued-Logic-based machine learning\nattacks. We find that approximately 1,000 CRPs are necessary to train models\nthat predict response bits better than random chance. Given the significant\nchallenge of acquiring a vast number of CRPs from a photonic PUF, our results\ndemonstrate photonic PUF resilience against such attacks.",
        "updated": "2024-03-02T19:44:19Z",
        "published": "2024-03-02T19:44:19Z",
        "authors": [
            "Jessie M. Henderson",
            "Elena R. Henderson",
            "Clayton A. Harper",
            "Hiva Shahoei",
            "William V. Oxford",
            "Eric C. Larson",
            "Duncan L. MacFarlane",
            "Mitchell A. Thornton"
        ],
        "comments": "6 pages, 4 figures",
        "categories": [
            "cs.CR",
            "cs.LG"
        ],
        "primary_category": "cs.CR"
    },
    "2403.01301v1": {
        "url": "http://arxiv.org/abs/2403.01301v1",
        "title": "Supplier Recommendation in Online Procurement",
        "summary": "Supply chain optimization is key to a healthy and profitable business. Many\ncompanies use online procurement systems to agree contracts with suppliers. It\nis vital that the most competitive suppliers are invited to bid for such\ncontracts. In this work, we propose a recommender system to assist with\nsupplier discovery in road freight online procurement. Our system is able to\nprovide personalized supplier recommendations, taking into account customer\nneeds and preferences. This is a novel application of recommender systems,\ncalling for design choices that fit the unique requirements of online\nprocurement. Our preliminary results, using real-world data, are promising.",
        "updated": "2024-03-02T19:55:38Z",
        "published": "2024-03-02T19:55:38Z",
        "authors": [
            "Victor Coscrato",
            "Derek Bridge"
        ],
        "categories": [
            "cs.IR",
            "cs.LG"
        ],
        "primary_category": "cs.IR"
    },
    "2403.01304v1": {
        "url": "http://arxiv.org/abs/2403.01304v1",
        "title": "Improving the Validity of Automatically Generated Feedback via\n  Reinforcement Learning",
        "summary": "Automatically generating feedback via large language models (LLMs) in\nintelligent tutoring systems and online learning platforms has the potential to\nimprove the learning outcomes of many students. However, both feedback\ngeneration and evaluation are challenging: feedback content has to be valid\nespecially in subjects like math, which requires models to understand the\nproblem, the solution, and where the student's error lies. Feedback also has to\nbe pedagogically valid to reflect effective tutoring strategies, such as\nexplaining possible misconceptions and encouraging the student, among other\ndesirable features. In this work, we address both problems of automatically\ngenerating and evaluating feedback while considering both correctness and\nalignment. First, we propose a rubric for evaluating math feedback and show\nthat GPT-4 is able to effectively use it to annotate human-written and\nLLM-generated feedback. Second, we propose a framework for feedback generation\nthat optimizes both correctness and alignment using reinforcement learning\n(RL). Specifically, we use GPT-4's annotations to create preferences over\nfeedback pairs in an augmented dataset for training via direct preference\noptimization (DPO). We show that our methods significantly increase the\ncorrectness and alignment of generated feedback with Llama 2, an open-source\nLLM, qualitatively analyze our generation and evaluation systems using case\nstudies, and outline several areas for future work.",
        "updated": "2024-03-02T20:25:50Z",
        "published": "2024-03-02T20:25:50Z",
        "authors": [
            "Alexander Scarlatos",
            "Digory Smith",
            "Simon Woodhead",
            "Andrew Lan"
        ],
        "categories": [
            "cs.CL"
        ],
        "primary_category": "cs.CL"
    },
    "2403.01306v1": {
        "url": "http://arxiv.org/abs/2403.01306v1",
        "title": "ICC: Quantifying Image Caption Concreteness for Multimodal Dataset\n  Curation",
        "summary": "Web-scale training on paired text-image data is becoming increasingly central\nto multimodal learning, but is challenged by the highly noisy nature of\ndatasets in the wild. Standard data filtering approaches succeed in removing\nmismatched text-image pairs, but permit semantically related but highly\nabstract or subjective text. These approaches lack the fine-grained ability to\nisolate the most concrete samples that provide the strongest signal for\nlearning in a noisy dataset. In this work, we propose a new metric, image\ncaption concreteness, that evaluates caption text without an image reference to\nmeasure its concreteness and relevancy for use in multimodal learning. Our\napproach leverages strong foundation models for measuring visual-semantic\ninformation loss in multimodal representations. We demonstrate that this\nstrongly correlates with human evaluation of concreteness in both single-word\nand sentence-level texts. Moreover, we show that curation using ICC complements\nexisting approaches: It succeeds in selecting the highest quality samples from\nmultimodal web-scale datasets to allow for efficient training in\nresource-constrained settings.",
        "updated": "2024-03-02T20:36:10Z",
        "published": "2024-03-02T20:36:10Z",
        "authors": [
            "Moran Yanuka",
            "Morris Alper",
            "Hadar Averbuch-Elor",
            "Raja Giryes"
        ],
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "primary_category": "cs.LG"
    },
    "2403.01308v2": {
        "url": "http://arxiv.org/abs/2403.01308v2",
        "title": "VBART: The Turkish LLM",
        "summary": "We present VBART, the first Turkish sequence-to-sequence Large Language\nModels (LLMs) pre-trained on a large corpus from scratch. VBART are compact\nLLMs based on good ideas leveraged from BART and mBART models and come in two\nsizes, Large and XLarge. Fine-tuned VBART models surpass the prior\nstate-of-the-art results in abstractive text summarization, title generation,\ntext paraphrasing, question answering and question generation tasks. They allow\nfine-tuning for future text generation tasks and datasets, carving a new path\nfor Turkish Natural Language Processing (NLP) research. Our work shows that\nhaving a pre-trained LLM for Turkish outperforms up to 3x multilingual models,\nimproving existing results and providing efficient models for training and\ninference. Moreover, we show that our monolingual tokenizer is up to 11x more\nefficient than multilingual tokenizers. Last but not least, we introduce a\nmethod to enlarge an existing pre-trained LLM and question the relevancy of\nChinchilla Scaling Law to sequence-to-sequence masked language models. Our\nfine-tuned models, tokenizer and cleaned vngrs-web-corpus of 135 GB are\npublicly available at huggingface.co/vngrs-ai.",
        "updated": "2024-03-14T16:37:37Z",
        "published": "2024-03-02T20:40:11Z",
        "authors": [
            "Meliksah Turker",
            "Mehmet Erdi Ari",
            "Aydin Han"
        ],
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "primary_category": "cs.CL"
    },
    "2403.02352v1": {
        "url": "http://arxiv.org/abs/2403.02352v1",
        "title": "ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys",
        "summary": "We propose a new attention mechanism with linear complexity, ATP, that\nfixates \\textbf{A}ttention on \\textbf{T}op \\textbf{P}rincipal keys, rather than\non each individual token. Particularly, ATP is driven by an important\nobservation that input sequences are typically low-rank, i.e., input sequences\ncan be represented by a few principal bases. Therefore, instead of directly\niterating over all the input tokens, ATP transforms inputs into an orthogonal\nspace and computes attention only on the top principal bases (keys). Owing to\nthe observed low-rank structure in input sequences, ATP is able to capture\nsemantic relationships in input sequences with a few principal keys.\nFurthermore, the attention complexity is reduced from \\emph{quadratic} to\n\\emph{linear} without incurring a noticeable performance drop. ATP further\nreduces complexity for other linear layers with low-rank inputs, leading to\nmore speedup compared to prior works that solely target the attention module.\nOur evaluations on various models (e.g., BERT and Llama) demonstrate that ATP\nachieves comparable accuracy with much lower computation and memory complexity\nthan the standard attention mechanism. In particular, ATP barely loses accuracy\nwith only $1/2$ principal keys, and only incurs around $2\\%$ accuracy drops\nwith $1/4$ principal keys.",
        "updated": "2024-03-01T19:24:37Z",
        "published": "2024-03-01T19:24:37Z",
        "authors": [
            "Yue Niu",
            "Saurav Prakash",
            "Salman Avestimehr"
        ],
        "comments": "10 pages, 7 figures, 8 tables",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "primary_category": "cs.LG"
    },
    "2403.02354v1": {
        "url": "http://arxiv.org/abs/2403.02354v1",
        "title": "Spatio-Temporal Field Neural Networks for Air Quality Inference",
        "summary": "The air quality inference problem aims to utilize historical data from a\nlimited number of observation sites to infer the air quality index at an\nunknown location. Considering the sparsity of data due to the high maintenance\ncost of the stations, good inference algorithms can effectively save the cost\nand refine the data granularity. While spatio-temporal graph neural networks\nhave made excellent progress on this problem, their non-Euclidean and discrete\ndata structure modeling of reality limits its potential. In this work, we make\nthe first attempt to combine two different spatio-temporal perspectives, fields\nand graphs, by proposing a new model, Spatio-Temporal Field Neural Network, and\nits corresponding new framework, Pyramidal Inference. Extensive experiments\nvalidate that our model achieves state-of-the-art performance in nationwide air\nquality inference in the Chinese Mainland, demonstrating the superiority of our\nproposed model and framework.",
        "updated": "2024-03-02T10:14:42Z",
        "published": "2024-03-02T10:14:42Z",
        "authors": [
            "Yutong Feng",
            "Qiongyan Wang",
            "Yutong Xia",
            "Junlin Huang",
            "Siru Zhong",
            "Kun Wang",
            "Shifen Cheng",
            "Yuxuan Liang"
        ],
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "primary_category": "cs.LG"
    },
    "2403.02355v1": {
        "url": "http://arxiv.org/abs/2403.02355v1",
        "title": "Temporal Knowledge Graph Completion with Time-sensitive Relations in\n  Hypercomplex Space",
        "summary": "Temporal knowledge graph completion (TKGC) aims to fill in missing facts\nwithin a given temporal knowledge graph at a specific time. Existing methods,\noperating in real or complex spaces, have demonstrated promising performance in\nthis task. This paper advances beyond conventional approaches by introducing\nmore expressive quaternion representations for TKGC within hypercomplex space.\nUnlike existing quaternion-based methods, our study focuses on capturing\ntime-sensitive relations rather than time-aware entities. Specifically, we\nmodel time-sensitive relations through time-aware rotation and periodic time\ntranslation, effectively capturing complex temporal variability. Furthermore,\nwe theoretically demonstrate our method's capability to model symmetric,\nasymmetric, inverse, compositional, and evolutionary relation patterns.\nComprehensive experiments on public datasets validate that our proposed\napproach achieves state-of-the-art performance in the field of TKGC.",
        "updated": "2024-03-02T16:50:48Z",
        "published": "2024-03-02T16:50:48Z",
        "authors": [
            "Li Cai",
            "Xin Mao",
            "Zhihong Wang",
            "Shangqing Zhao",
            "Yuhao Zhou",
            "Changxu Wu",
            "Man Lan"
        ],
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "primary_category": "cs.LG"
    },
    "2403.02360v1": {
        "url": "http://arxiv.org/abs/2403.02360v1",
        "title": "Towards Optimal Customized Architecture for Heterogeneous Federated\n  Learning with Contrastive Cloud-Edge Model Decoupling",
        "summary": "Federated learning, as a promising distributed learning paradigm, enables\ncollaborative training of a global model across multiple network edge clients\nwithout the need for central data collecting. However, the heterogeneity of\nedge data distribution drags the model towards the local minima, which can be\ndistant from the global optimum. Such heterogeneity often leads to slow\nconvergence and substantial communication overhead. To address these issues, we\npropose a novel federated learning framework called FedCMD, a model decoupling\ntailored to the Cloud-edge supported federated learning that separates deep\nneural networks into a body for capturing shared representations in Cloud and a\npersonalized head for migrating data heterogeneity. Our motivation is that, by\nthe deep investigation of the performance of selecting different neural network\nlayers as the personalized head, we found rigidly assigning the last layer as\nthe personalized head in current studies is not always optimal. Instead, it is\nnecessary to dynamically select the personalized layer that maximizes the\ntraining performance by taking the representation difference between neighbor\nlayers into account. To find the optimal personalized layer, we utilize the\nlow-dimensional representation of each layer to contrast feature distribution\ntransfer and introduce a Wasserstein-based layer selection method, aimed at\nidentifying the best-match layer for personalization. Additionally, a weighted\nglobal aggregation algorithm is proposed based on the selected personalized\nlayer for the practical application of FedCMD. Extensive experiments on ten\nbenchmarks demonstrate the efficiency and superior performance of our solution\ncompared with nine state-of-the-art solutions. All code and results are\navailable at https://github.com/elegy112138/FedCMD.",
        "updated": "2024-03-04T05:10:28Z",
        "published": "2024-03-04T05:10:28Z",
        "authors": [
            "Xingyan Chen",
            "Tian Du",
            "Mu Wang",
            "Tiancheng Gu",
            "Yu Zhao",
            "Gang Kou",
            "Changqiao Xu",
            "Dapeng Oliver Wu"
        ],
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "primary_category": "cs.LG"
    },
    "2403.02363v1": {
        "url": "http://arxiv.org/abs/2403.02363v1",
        "title": "Addressing Long-Tail Noisy Label Learning Problems: a Two-Stage Solution\n  with Label Refurbishment Considering Label Rarity",
        "summary": "Real-world datasets commonly exhibit noisy labels and class imbalance, such\nas long-tailed distributions. While previous research addresses this issue by\ndifferentiating noisy and clean samples, reliance on information from\npredictions based on noisy long-tailed data introduces potential errors. To\novercome the limitations of prior works, we introduce an effective two-stage\napproach by combining soft-label refurbishing with multi-expert ensemble\nlearning. In the first stage of robust soft label refurbishing, we acquire\nunbiased features through contrastive learning, making preliminary predictions\nusing a classifier trained with a carefully designed BAlanced Noise-tolerant\nCross-entropy (BANC) loss. In the second stage, our label refurbishment method\nis applied to obtain soft labels for multi-expert ensemble learning, providing\na principled solution to the long-tail noisy label problem. Experiments\nconducted across multiple benchmarks validate the superiority of our approach,\nLabel Refurbishment considering Label Rarity (LR^2), achieving remarkable\naccuracies of 94.19% and 77.05% on simulated noisy CIFAR-10 and CIFAR-100\nlong-tail datasets, as well as 77.74% and 81.40% on real-noise long-tail\ndatasets, Food-101N and Animal-10N, surpassing existing state-of-the-art\nmethods.",
        "updated": "2024-03-04T08:06:57Z",
        "published": "2024-03-04T08:06:57Z",
        "authors": [
            "Ying-Hsuan Wu",
            "Jun-Wei Hsieh",
            "Li Xin",
            "Shin-You Teng",
            "Yi-Kuan Hsieh",
            "Ming-Ching Chang"
        ],
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "primary_category": "cs.LG"
    },
    "2403.02366v1": {
        "url": "http://arxiv.org/abs/2403.02366v1",
        "title": "Human Evaluation of English--Irish Transformer-Based NMT",
        "summary": "In this study, a human evaluation is carried out on how hyperparameter\nsettings impact the quality of Transformer-based Neural Machine Translation\n(NMT) for the low-resourced English--Irish pair. SentencePiece models using\nboth Byte Pair Encoding (BPE) and unigram approaches were appraised. Variations\nin model architectures included modifying the number of layers, evaluating the\noptimal number of heads for attention and testing various regularisation\ntechniques. The greatest performance improvement was recorded for a\nTransformer-optimized model with a 16k BPE subword model. Compared with a\nbaseline Recurrent Neural Network (RNN) model, a Transformer-optimized model\ndemonstrated a BLEU score improvement of 7.8 points. When benchmarked against\nGoogle Translate, our translation engines demonstrated significant\nimprovements. Furthermore, a quantitative fine-grained manual evaluation was\nconducted which compared the performance of machine translation systems. Using\nthe Multidimensional Quality Metrics (MQM) error taxonomy, a human evaluation\nof the error types generated by an RNN-based system and a Transformer-based\nsystem was explored. Our findings show the best-performing Transformer system\nsignificantly reduces both accuracy and fluency errors when compared with an\nRNN-based model.",
        "updated": "2024-03-04T11:45:46Z",
        "published": "2024-03-04T11:45:46Z",
        "authors": [
            "S\u00e9amus Lankford",
            "Haithem Afli",
            "Andy Way"
        ],
        "comments": "arXiv admin note: text overlap with arXiv:2403.01985",
        "categories": [
            "cs.CL",
            "cs.AI"
        ],
        "primary_category": "cs.CL",
        "doi": "10.3390/info13070309",
        "journal_ref": "Information 2022, 13(7), 309"
    },
    "2403.02367v1": {
        "url": "http://arxiv.org/abs/2403.02367v1",
        "title": "adaptNMT: an open-source, language-agnostic development environment for\n  Neural Machine Translation",
        "summary": "adaptNMT streamlines all processes involved in the development and deployment\nof RNN and Transformer neural translation models. As an open-source\napplication, it is designed for both technical and non-technical users who work\nin the field of machine translation. Built upon the widely-adopted OpenNMT\necosystem, the application is particularly useful for new entrants to the field\nsince the setup of the development environment and creation of train,\nvalidation and test splits is greatly simplified. Graphing, embedded within the\napplication, illustrates the progress of model training, and SentencePiece is\nused for creating subword segmentation models. Hyperparameter customization is\nfacilitated through an intuitive user interface, and a single-click model\ndevelopment approach has been implemented. Models developed by adaptNMT can be\nevaluated using a range of metrics, and deployed as a translation service\nwithin the application. To support eco-friendly research in the NLP space, a\ngreen report also flags the power consumption and kgCO$_{2}$ emissions\ngenerated during model development. The application is freely available.",
        "updated": "2024-03-04T12:10:17Z",
        "published": "2024-03-04T12:10:17Z",
        "authors": [
            "S\u00e9amus Lankford",
            "Haithem Afli",
            "Andy Way"
        ],
        "categories": [
            "cs.CL",
            "cs.AI"
        ],
        "primary_category": "cs.CL",
        "doi": "10.1007/s10579-023-09671-2",
        "journal_ref": "Language Resources and Evaluation 57, 1671-1696, (2023)"
    },
    "2403.02368v1": {
        "url": "http://arxiv.org/abs/2403.02368v1",
        "title": "A Novel Hybrid Feature Importance and Feature Interaction Detection\n  Framework for Predictive Optimization in Industry 4.0 Applications",
        "summary": "Advanced machine learning algorithms are increasingly utilized to provide\ndata-based prediction and decision-making support in Industry 4.0. However, the\nprediction accuracy achieved by the existing models is insufficient to warrant\npractical implementation in real-world applications. This is because not all\nfeatures present in real-world datasets possess a direct relevance to the\npredictive analysis being conducted. Consequently, the careful incorporation of\nselect features has the potential to yield a substantial positive impact on the\noutcome. To address the research gap, this paper proposes a novel hybrid\nframework that combines the feature importance detector - local interpretable\nmodel-agnostic explanations (LIME) and the feature interaction detector -\nneural interaction detection (NID), to improve prediction accuracy. By applying\nthe proposed framework, unnecessary features can be eliminated, and\ninteractions are encoded to generate a more conducive dataset for predictive\npurposes. Subsequently, the proposed model is deployed to refine the prediction\nof electricity consumption in foundry processing. The experimental outcomes\nreveal an augmentation of up to 9.56% in the R2 score, and a diminution of up\nto 24.05% in the root mean square error.",
        "updated": "2024-03-04T13:22:53Z",
        "published": "2024-03-04T13:22:53Z",
        "authors": [
            "Zhipeng Ma",
            "Bo N\u00f8rregaard J\u00f8rgensen",
            "Zheng Grace Ma"
        ],
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "primary_category": "cs.LG",
        "doi": "10.1109/IECON51785.2023.10312491",
        "journal_ref": "IECON 2023- 49th Annual Conference of the IEEE Industrial\n  Electronics Society"
    },
    "2403.02369v1": {
        "url": "http://arxiv.org/abs/2403.02369v1",
        "title": "A Multi-agent Reinforcement Learning Study of Evolution of Communication\n  and Teaching under Libertarian and Utilitarian Governing Systems",
        "summary": "Laboratory experiments have shown that communication plays an important role\nin solving social dilemmas. Here, by extending the AI-Economist, a mixed motive\nmulti-agent reinforcement learning environment, I intend to find an answer to\nthe following descriptive question: which governing system does facilitate the\nemergence and evolution of communication and teaching among agents? To answer\nthis question, the AI-Economist is extended by a voting mechanism to simulate\nthree different governing systems across individualistic-collectivistic axis,\nfrom full-libertarian to Full-Utilitarian governing systems. Moreover, the\nAI-Economist is further extended to include communication with possible\nmisalignment, a variant of signalling game, by letting agents to build houses\ntogether if they are able to name mutually complement material resources by the\nsame letter. Moreover, another extension is made to the AI-Economist to include\nteaching with possible misalignment, again a variant of signalling game, by\nletting half the agents as teachers who know how to use mutually complement\nmaterial resources to build houses but are not capable of building actual\nhouses, and the other half as students who do not have this information but are\nable to actually build those houses if teachers teach them. I found a strong\nevidence that collectivistic environment such as Full-Utilitarian system is\nmore favourable for the emergence of communication and teaching, or more\nprecisely, evolution of language alignment. Moreover, I found some evidence\nthat evolution of language alignment through communication and teaching under\ncollectivistic governing systems makes individuals more advantageously inequity\naverse. As a result, there is a positive correlation between evolution of\nlanguage alignment and equality in the society.",
        "updated": "2024-03-04T14:49:14Z",
        "published": "2024-03-04T14:49:14Z",
        "authors": [
            "Aslan S. Dizaji"
        ],
        "comments": "20 pages, 14 figures",
        "categories": [
            "cs.MA"
        ],
        "primary_category": "cs.MA"
    },
    "2403.02370v1": {
        "url": "http://arxiv.org/abs/2403.02370v1",
        "title": "adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource\n  Languages with Integrated LLM Playgrounds",
        "summary": "The advent of Multilingual Language Models (MLLMs) and Large Language Models\nhas spawned innovation in many areas of natural language processing. Despite\nthe exciting potential of this technology, its impact on developing\nhigh-quality Machine Translation (MT) outputs for low-resource languages\nremains relatively under-explored. Furthermore, an open-source application,\ndedicated to both fine-tuning MLLMs and managing the complete MT workflow for\nlow-resources languages, remains unavailable. We aim to address these\nimbalances through the development of adaptMLLM, which streamlines all\nprocesses involved in the fine-tuning of MLLMs for MT. This open-source\napplication is tailored for developers, translators, and users who are engaged\nin MT. An intuitive interface allows for easy customisation of hyperparameters,\nand the application offers a range of metrics for model evaluation and the\ncapability to deploy models as a translation service directly within the\napplication. As a multilingual tool, we used adaptMLLM to fine-tune models for\ntwo low-resource language pairs: English to Irish (EN$\\leftrightarrow$GA) and\nEnglish to Marathi (EN$\\leftrightarrow$MR). Compared with baselines from the\nLoResMT2021 Shared Task, the adaptMLLM system demonstrated significant\nimprovements. In the EN$\\rightarrow$GA direction, an improvement of 5.2 BLEU\npoints was observed and an increase of 40.5 BLEU points was recorded in the\nGA$\\rightarrow$EN direction. Significant improvements in the translation\nperformance of the EN$\\leftrightarrow$MR pair were also observed notably in the\nMR$\\rightarrow$EN direction with an increase of 21.3 BLEU points. Finally, a\nfine-grained human evaluation of the MLLM output on the EN$\\rightarrow$GA pair\nwas conducted using the Multidimensional Quality Metrics and Scalar Quality\nMetrics error taxonomies. The application and models are freely available.",
        "updated": "2024-03-04T14:49:18Z",
        "published": "2024-03-04T14:49:18Z",
        "authors": [
            "S\u00e9amus Lankford",
            "Haithem Afli",
            "Andy Way"
        ],
        "categories": [
            "cs.CL",
            "cs.AI"
        ],
        "primary_category": "cs.CL",
        "doi": "10.3390/info14120638",
        "journal_ref": "Information 2023, 14(12), 638"
    },
    "2403.03517v1": {
        "url": "http://arxiv.org/abs/2403.03517v1",
        "title": "IB-Net: Initial Branch Network for Variable Decision in Boolean\n  Satisfiability",
        "summary": "Boolean Satisfiability problems are vital components in Electronic Design\nAutomation, particularly within the Logic Equivalence Checking process.\nCurrently, SAT solvers are employed for these problems and neural network is\ntried as assistance to solvers. However, as SAT problems in the LEC context are\ndistinctive due to their predominantly unsatisfiability nature and a\nsubstantial proportion of UNSAT-core variables, existing neural network\nassistance has proven unsuccessful in this specialized domain. To tackle this\nchallenge, we propose IB-Net, an innovative framework utilizing graph neural\nnetworks and novel graph encoding techniques to model unsatisfiable problems\nand interact with state-of-the-art solvers. Extensive evaluations across\nsolvers and datasets demonstrate IB-Net's acceleration, achieving an average\nruntime speedup of 5.0% on industrial data and 8.3% on SAT competition data\nempirically. This breakthrough advances efficient solving in LEC workflows.",
        "updated": "2024-03-06T07:54:40Z",
        "published": "2024-03-06T07:54:40Z",
        "authors": [
            "Tsz Ho Chan",
            "Wenyi Xiao",
            "Junhua Huang",
            "Huiling Zhen",
            "Guangji Tian",
            "Mingxuan Yuan"
        ],
        "comments": "7 pages, 12 figures",
        "categories": [
            "cs.AI"
        ],
        "primary_category": "cs.AI"
    },
    "2403.03521v1": {
        "url": "http://arxiv.org/abs/2403.03521v1",
        "title": "BiVert: Bidirectional Vocabulary Evaluation using Relations for Machine\n  Translation",
        "summary": "Neural machine translation (NMT) has progressed rapidly in the past few\nyears, promising improvements and quality translations for different languages.\nEvaluation of this task is crucial to determine the quality of the translation.\nOverall, insufficient emphasis is placed on the actual sense of the translation\nin traditional methods. We propose a bidirectional semantic-based evaluation\nmethod designed to assess the sense distance of the translation from the source\ntext. This approach employs the comprehensive multilingual encyclopedic\ndictionary BabelNet. Through the calculation of the semantic distance between\nthe source and its back translation of the output, our method introduces a\nquantifiable approach that empowers sentence comparison on the same linguistic\nlevel. Factual analysis shows a strong correlation between the average\nevaluation scores generated by our method and the human assessments across\nvarious machine translation systems for English-German language pair. Finally,\nour method proposes a new multilingual approach to rank MT systems without the\nneed for parallel corpora.",
        "updated": "2024-03-06T08:02:21Z",
        "published": "2024-03-06T08:02:21Z",
        "authors": [
            "Carinne Cherf",
            "Yuval Pinter"
        ],
        "comments": "LREC-COLING 2024",
        "categories": [
            "cs.CL"
        ],
        "primary_category": "cs.CL"
    },
    "2403.03522v2": {
        "url": "http://arxiv.org/abs/2403.03522v2",
        "title": "Non-verbal information in spontaneous speech -- towards a new framework\n  of analysis",
        "summary": "Non-verbal signals in speech are encoded by prosody and carry information\nthat ranges from conversation action to attitude and emotion. Despite its\nimportance, the principles that govern prosodic structure are not yet\nadequately understood. This paper offers an analytical schema and a\ntechnological proof-of-concept for the categorization of prosodic signals and\ntheir association with meaning. The schema interprets surface-representations\nof multi-layered prosodic events. As a first step towards implementation, we\npresent a classification process that disentangles prosodic phenomena of three\norders. It relies on fine-tuning a pre-trained speech recognition model,\nenabling the simultaneous multi-class/multi-label detection. It generalizes\nover a large variety of spontaneous data, performing on a par with, or superior\nto, human annotation. In addition to a standardized formalization of prosody,\ndisentangling prosodic patterns can direct a theory of communication and speech\norganization. A welcome by-product is an interpretation of prosody that will\nenhance speech- and language-related technologies.",
        "updated": "2024-03-13T09:50:40Z",
        "published": "2024-03-06T08:03:05Z",
        "authors": [
            "Tirza Biron",
            "Moshe Barboy",
            "Eran Ben-Artzy",
            "Alona Golubchik",
            "Yanir Marmor",
            "Smadar Szekely",
            "Yaron Winter",
            "David Harel"
        ],
        "categories": [
            "cs.SD",
            "cs.CL",
            "cs.LG",
            "eess.AS"
        ],
        "primary_category": "cs.SD"
    },
    "2403.03525v1": {
        "url": "http://arxiv.org/abs/2403.03525v1",
        "title": "Exploratory Factory Analysis of the Centrality Metrics for Complex\n  Real-World Networks",
        "summary": "Exploratory factor analysis (EFA) is useful to identify the number and\nmapping of the hidden factors that could dominantly represent the features in\nthe dataset. Principal component analysis (PCA) is the first step as part of\nthe two-step procedure to conduct EFA, with the number of dominant principal\ncomponents being the number of hidden factors and the entries for the features\nin the corresponding Eigenvectors serve as the initial values of the factor\nloadings. In this paper, we conduct EFA on a suite of 80 complex network\ndatasets to identify the number and mapping of the hidden factors (expected to\nbe less than four) that could dominantly represent the values incurred by the\nvertices with respect to the four major centrality metrics (degree: DEG,\neigenvector: EVC, betweenness: BWC and closeness: CLC).",
        "updated": "2024-03-06T08:04:50Z",
        "published": "2024-03-06T08:04:50Z",
        "authors": [
            "Natarajan Meghanathan"
        ],
        "comments": "8 pages, 4 figures",
        "categories": [
            "cs.SI",
            "I.2.6"
        ],
        "primary_category": "cs.SI"
    },
    "2403.03526v1": {
        "url": "http://arxiv.org/abs/2403.03526v1",
        "title": "FingerNet: EEG Decoding of A Fine Motor Imagery with Finger-tapping Task\n  Based on A Deep Neural Network",
        "summary": "Brain-computer interface (BCI) technology facilitates communication between\nthe human brain and computers, primarily utilizing electroencephalography (EEG)\nsignals to discern human intentions. Although EEG-based BCI systems have been\ndeveloped for paralysis individuals, ongoing studies explore systems for speech\nimagery and motor imagery (MI). This study introduces FingerNet, a specialized\nnetwork for fine MI classification, departing from conventional gross MI\nstudies. The proposed FingerNet could extract spatial and temporal features\nfrom EEG signals, improving classification accuracy within the same hand. The\nexperimental results demonstrated that performance showed significantly higher\naccuracy in classifying five finger-tapping tasks, encompassing thumb, index,\nmiddle, ring, and little finger movements. FingerNet demonstrated dominant\nperformance compared to the conventional baseline models, EEGNet and\nDeepConvNet. The average accuracy for FingerNet was 0.3049, whereas EEGNet and\nDeepConvNet exhibited lower accuracies of 0.2196 and 0.2533, respectively.\nStatistical validation also demonstrates the predominance of FingerNet over\nbaseline networks. For biased predictions, particularly for thumb and index\nclasses, we led to the implementation of weighted cross-entropy and also\nadapted the weighted cross-entropy, a method conventionally employed to\nmitigate class imbalance. The proposed FingerNet involves optimizing network\nstructure, improving performance, and exploring applications beyond fine MI.\nMoreover, the weighted Cross Entropy approach employed to address such biased\npredictions appears to have broader applicability and relevance across various\ndomains involving multi-class classification tasks. We believe that effective\nexecution of motor imagery can be achieved not only for fine MI, but also for\nlocal muscle MI",
        "updated": "2024-03-06T08:05:53Z",
        "published": "2024-03-06T08:05:53Z",
        "authors": [
            "Young-Min Go",
            "Seong-Hyun Yu",
            "Hyeong-Yeong Park",
            "Minji Lee",
            "Ji-Hoon Jeong"
        ],
        "comments": "12 pages,5 figures, and 2 tables",
        "categories": [
            "eess.SP",
            "cs.LG",
            "q-bio.NC"
        ],
        "primary_category": "eess.SP"
    },
    "2403.03530v1": {
        "url": "http://arxiv.org/abs/2403.03530v1",
        "title": "Average-case deterministic query complexity of boolean functions with\n  fixed weight",
        "summary": "We explore the $\\textit{average-case deterministic query complexity}$ of\nboolean functions under the $\\textit{uniform distribution}$, denoted by\n$\\mathrm{D}_\\mathrm{ave}(f)$, the minimum average depth of zero-error decision\ntree computing a boolean function $f$. This measure found several applications\nacross diverse fields. We study $\\mathrm{D}_\\mathrm{ave}(f)$ of several common\nfunctions, including penalty shoot-out functions, symmetric functions, linear\nthreshold functions and tribes functions. Let $\\mathrm{wt}(f)$ denote the\nnumber of the inputs on which $f$ outputs $1$. We prove that\n$\\mathrm{D}_\\mathrm{ave}(f) \\le \\log \\frac{\\mathrm{wt}(f)}{\\log n} +\nO\\left(\\log \\log \\frac{\\mathrm{wt}(f)}{\\log n}\\right)$ when $\\mathrm{wt}(f) \\ge\n4 \\log n$ (otherwise, $\\mathrm{D}_\\mathrm{ave}(f) = O(1)$), and that for almost\nall fixed-weight functions, $\\mathrm{D}_\\mathrm{ave}(f) \\geq \\log\n\\frac{\\mathrm{wt}(f)}{\\log n} - O\\left( \\log \\log \\frac{\\mathrm{wt}(f)}{\\log\nn}\\right)$, which implies the tightness of the upper bound up to an additive\nlogarithmic term. We also study $\\mathrm{D}_\\mathrm{ave}(f)$ of circuits. Using\nH\\r{a}stad's switching lemma or Rossman's switching lemma [Comput. Complexity\nConf. 137, 2019], one can derive upper bounds $\\mathrm{D}_\\mathrm{ave}(f) \\leq\nn\\left(1 - \\frac{1}{O(k)}\\right)$ for width-$k$ CNFs/DNFs and\n$\\mathrm{D}_\\mathrm{ave}(f) \\leq n\\left(1 - \\frac{1}{O(\\log s)}\\right)$ for\nsize-$s$ CNFs/DNFs, respectively. For any $w \\ge 1.1 \\log n$, we prove the\nexistence of some width-$w$ size-$(2^w/w)$ DNF formula with\n$\\mathrm{D}_\\mathrm{ave} (f) = n \\left(1 - \\frac{\\log n}{\\Theta(w)}\\right)$,\nproviding evidence on the tightness of the switching lemmas.",
        "updated": "2024-03-06T08:08:02Z",
        "published": "2024-03-06T08:08:02Z",
        "authors": [
            "Yuan Li",
            "Haowei Wu",
            "Yi Yang"
        ],
        "categories": [
            "cs.CC"
        ],
        "primary_category": "cs.CC"
    },
    "2403.03532v2": {
        "url": "http://arxiv.org/abs/2403.03532v2",
        "title": "Extend Your Own Correspondences: Unsupervised Distant Point Cloud\n  Registration by Progressive Distance Extension",
        "summary": "Registration of point clouds collected from a pair of distant vehicles\nprovides a comprehensive and accurate 3D view of the driving scenario, which is\nvital for driving safety related applications, yet existing literature suffers\nfrom the expensive pose label acquisition and the deficiency to generalize to\nnew data distributions. In this paper, we propose EYOC, an unsupervised distant\npoint cloud registration method that adapts to new point cloud distributions on\nthe fly, requiring no global pose labels. The core idea of EYOC is to train a\nfeature extractor in a progressive fashion, where in each round, the feature\nextractor, trained with near point cloud pairs, can label slightly farther\npoint cloud pairs, enabling self-supervision on such far point cloud pairs.\nThis process continues until the derived extractor can be used to register\ndistant point clouds. Particularly, to enable high-fidelity correspondence\nlabel generation, we devise an effective spatial filtering scheme to select the\nmost representative correspondences to register a point cloud pair, and then\nutilize the aligned point clouds to discover more correct correspondences.\nExperiments show that EYOC can achieve comparable performance with\nstate-of-the-art supervised methods at a lower training cost. Moreover, it\noutwits supervised methods regarding generalization performance on new data\ndistributions.",
        "updated": "2024-03-27T05:28:55Z",
        "published": "2024-03-06T08:18:02Z",
        "authors": [
            "Quan Liu",
            "Hongzi Zhu",
            "Zhenxi Wang",
            "Yunsong Zhou",
            "Shan Chang",
            "Minyi Guo"
        ],
        "comments": "In Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2024",
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2403.03535v1": {
        "url": "http://arxiv.org/abs/2403.03535v1",
        "title": "Task Attribute Distance for Few-Shot Learning: Theoretical Analysis and\n  Applications",
        "summary": "Few-shot learning (FSL) aims to learn novel tasks with very few labeled\nsamples by leveraging experience from \\emph{related} training tasks. In this\npaper, we try to understand FSL by delving into two key questions: (1) How to\nquantify the relationship between \\emph{training} and \\emph{novel} tasks? (2)\nHow does the relationship affect the \\emph{adaptation difficulty} on novel\ntasks for different models? To answer the two questions, we introduce Task\nAttribute Distance (TAD) built upon attributes as a metric to quantify the task\nrelatedness. Unlike many existing metrics, TAD is model-agnostic, making it\napplicable to different FSL models. Then, we utilize TAD metric to establish a\ntheoretical connection between task relatedness and task adaptation difficulty.\nBy deriving the generalization error bound on a novel task, we discover how TAD\nmeasures the adaptation difficulty on novel tasks for FSL models. To validate\nour TAD metric and theoretical findings, we conduct experiments on three\nbenchmarks. Our experimental results confirm that TAD metric effectively\nquantifies the task relatedness and reflects the adaptation difficulty on novel\ntasks for various FSL methods, even if some of them do not learn attributes\nexplicitly or human-annotated attributes are not available. Finally, we present\ntwo applications of the proposed TAD metric: data augmentation and test-time\nintervention, which further verify its effectiveness and general applicability.\nThe source code is available at https://github.com/hu-my/TaskAttributeDistance.",
        "updated": "2024-03-06T08:29:45Z",
        "published": "2024-03-06T08:29:45Z",
        "authors": [
            "Minyang Hu",
            "Hong Chang",
            "Zong Guo",
            "Bingpeng Ma",
            "Shiguan Shan",
            "Xilin Chen"
        ],
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "primary_category": "cs.CV"
    },
    "2403.03536v1": {
        "url": "http://arxiv.org/abs/2403.03536v1",
        "title": "Towards Efficient and Effective Unlearning of Large Language Models for\n  Recommendation",
        "summary": "The significant advancements in large language models (LLMs) give rise to a\npromising research direction, i.e., leveraging LLMs as recommenders (LLMRec).\nThe efficacy of LLMRec arises from the open-world knowledge and reasoning\ncapabilities inherent in LLMs. LLMRec acquires the recommendation capabilities\nthrough instruction tuning based on user interaction data. However, in order to\nprotect user privacy and optimize utility, it is also crucial for LLMRec to\nintentionally forget specific user data, which is generally referred to as\nrecommendation unlearning. In the era of LLMs, recommendation unlearning poses\nnew challenges for LLMRec in terms of \\textit{inefficiency} and\n\\textit{ineffectiveness}. Existing unlearning methods require updating billions\nof parameters in LLMRec, which is costly and time-consuming. Besides, they\nalways impact the model utility during the unlearning process. To this end, we\npropose \\textbf{E2URec}, the first \\underline{E}fficient and\n\\underline{E}ffective \\underline{U}nlearning method for LLM\\underline{Rec}. Our\nproposed E2URec enhances the unlearning efficiency by updating only a few\nadditional LoRA parameters, and improves the unlearning effectiveness by\nemploying a teacher-student framework, where we maintain multiple teacher\nnetworks to guide the unlearning process. Extensive experiments show that\nE2URec outperforms state-of-the-art baselines on two real-world datasets.\nSpecifically, E2URec can efficiently forget specific data without affecting\nrecommendation performance. The source code is at\n\\url{https://github.com/justarter/E2URec}.",
        "updated": "2024-03-06T08:31:35Z",
        "published": "2024-03-06T08:31:35Z",
        "authors": [
            "Hangyu Wang",
            "Jianghao Lin",
            "Bo Chen",
            "Yang Yang",
            "Ruiming Tang",
            "Weinan Zhang",
            "Yong Yu"
        ],
        "comments": "12 pages",
        "categories": [
            "cs.IR",
            "cs.AI"
        ],
        "primary_category": "cs.IR"
    },
    "2403.03538v1": {
        "url": "http://arxiv.org/abs/2403.03538v1",
        "title": "RADIA -- Radio Advertisement Detection with Intelligent Analytics",
        "summary": "Radio advertising remains an integral part of modern marketing strategies,\nwith its appeal and potential for targeted reach undeniably effective. However,\nthe dynamic nature of radio airtime and the rising trend of multiple radio\nspots necessitates an efficient system for monitoring advertisement broadcasts.\nThis study investigates a novel automated radio advertisement detection\ntechnique incorporating advanced speech recognition and text classification\nalgorithms. RadIA's approach surpasses traditional methods by eliminating the\nneed for prior knowledge of the broadcast content. This contribution allows for\ndetecting impromptu and newly introduced advertisements, providing a\ncomprehensive solution for advertisement detection in radio broadcasting.\nExperimental results show that the resulting model, trained on carefully\nsegmented and tagged text data, achieves an F1-macro score of 87.76 against a\ntheoretical maximum of 89.33. This paper provides insights into the choice of\nhyperparameters and their impact on the model's performance. This study\ndemonstrates its potential to ensure compliance with advertising broadcast\ncontracts and offer competitive surveillance. This groundbreaking research\ncould fundamentally change how radio advertising is monitored and open new\ndoors for marketing optimization.",
        "updated": "2024-03-06T08:34:28Z",
        "published": "2024-03-06T08:34:28Z",
        "authors": [
            "Jorge \u00c1lvarez",
            "Juan Carlos Armenteros",
            "Camilo Torr\u00f3n",
            "Miguel Ortega-Mart\u00edn",
            "Alfonso Ardoiz",
            "\u00d3scar Garc\u00eda",
            "Ignacio Arranz",
            "\u00cd\u00f1igo Galdeano",
            "Ignacio Garrido",
            "Adri\u00e1n Alonso",
            "Fernando Bay\u00f3n",
            "Oleg Vorontsov"
        ],
        "categories": [
            "cs.SD",
            "cs.AI",
            "cs.CL",
            "eess.AS"
        ],
        "primary_category": "cs.SD"
    },
    "2403.04630v1": {
        "url": "http://arxiv.org/abs/2403.04630v1",
        "title": "Time-Aware Projections: Truly Node-Private Graph Statistics under\n  Continual Observation",
        "summary": "We describe the first algorithms that satisfy the standard notion of\nnode-differential privacy in the continual release setting (i.e., without an\nassumed promise on input streams). Previous work addresses node-private\ncontinual release by assuming an unenforced promise on the maximum degree in a\ngraph; indeed, the algorithms from these works exhibit blatant privacy\nviolations when the degree bound is not met. Our algorithms are accurate on\nsparse graphs, for several fundamental graph problems: counting edges,\ntriangles, other subgraphs, and connected components; and releasing degree\nhistograms. Our unconditionally private algorithms generally have optimal\nerror, up to polylogarithmic factors and lower-order terms.\n  We provide general transformations that take a base algorithm for the\ncontinual release setting, which need only be private for streams satisfying a\npromised degree bound, and produce an algorithm that is unconditionally private\nyet mimics the base algorithm when the stream meets the degree bound (and adds\nonly linear overhead to the time and space complexity of the base algorithm).\nTo do so, we design new projection algorithms for graph streams, based on the\nbatch-model techniques of Day et al. 2016 and Blocki et al. 2013, which modify\nthe stream to limit its degree. Our main technical innovation is to show that\nthe projections are stable -- meaning that similar input graphs have similar\nprojections -- when the input stream satisfies a privately testable safety\ncondition. Our transformation then follows a novel online variant of the\nPropose-Test-Release framework (Dwork and Lei, 2009), privately testing the\nsafety condition before releasing output at each step.",
        "updated": "2024-03-07T16:14:08Z",
        "published": "2024-03-07T16:14:08Z",
        "authors": [
            "Palak Jain",
            "Adam Smith",
            "Connor Wagaman"
        ],
        "categories": [
            "cs.DS",
            "cs.CR"
        ],
        "primary_category": "cs.DS"
    },
    "2403.04633v1": {
        "url": "http://arxiv.org/abs/2403.04633v1",
        "title": "Message-Observing Sessions",
        "summary": "We present Most, a process language with message-observing session types.\nMessage-observing session types extend binary session types with type-level\ncomputation to specify communication protocols that vary based on messages\nobserved on other channels. Hence, Most allows us to express global invariants\nabout processes, rather than just local invariants, in a bottom-up,\ncompositional way. We give Most a semantic foundation using traces with\nbinding, a semantic approach for compositionally reasoning about traces in the\npresence of name generation. We use this semantics to prove type soundness and\ncompositionality for Most processes. We see this as a significant step towards\ncapturing message-dependencies and providing more precise guarantees about\nprocesses.",
        "updated": "2024-03-07T16:16:39Z",
        "published": "2024-03-07T16:16:39Z",
        "authors": [
            "Ryan Kavanagh",
            "Brigitte Pientka"
        ],
        "categories": [
            "cs.PL",
            "D.3.3; F.3.2"
        ],
        "primary_category": "cs.PL",
        "doi": "10.1145/3649859"
    },
    "2403.04634v2": {
        "url": "http://arxiv.org/abs/2403.04634v2",
        "title": "Pix2Gif: Motion-Guided Diffusion for GIF Generation",
        "summary": "We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video)\ngeneration. We tackle this problem differently by formulating the task as an\nimage translation problem steered by text and motion magnitude prompts, as\nshown in teaser fig. To ensure that the model adheres to motion guidance, we\npropose a new motion-guided warping module to spatially transform the features\nof the source image conditioned on the two types of prompts. Furthermore, we\nintroduce a perceptual loss to ensure the transformed feature map remains\nwithin the same space as the target image, ensuring content consistency and\ncoherence. In preparation for the model training, we meticulously curated data\nby extracting coherent image frames from the TGIF video-caption dataset, which\nprovides rich information about the temporal changes of subjects. After\npretraining, we apply our model in a zero-shot manner to a number of video\ndatasets. Extensive qualitative and quantitative experiments demonstrate the\neffectiveness of our model -- it not only captures the semantic prompt from\ntext but also the spatial ones from motion guidance. We train all our models\nusing a single node of 16xV100 GPUs. Code, dataset and models are made public\nat: https://hiteshk03.github.io/Pix2Gif/.",
        "updated": "2024-03-08T18:28:28Z",
        "published": "2024-03-07T16:18:28Z",
        "authors": [
            "Hitesh Kandala",
            "Jianfeng Gao",
            "Jianwei Yang"
        ],
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "primary_category": "cs.CV"
    },
    "2403.04635v1": {
        "url": "http://arxiv.org/abs/2403.04635v1",
        "title": "Virtuoso: An Open-Source, Comprehensive and Modular Simulation Framework\n  for Virtual Memory Research",
        "summary": "Virtual memory is a cornerstone of modern computing systems.Introduced as one\nof the earliest instances of hardware-software co-design, VM facilitates\nprogrammer-transparent memory man agement, data sharing, process isolation and\nmemory protection. Evaluating the efficiency of various virtual memory (VM)\ndesigns is crucial (i) given their significant impact on the system, including\nthe CPU caches, the main memory, and the storage device and (ii) given that\ndifferent system architectures might benefit from various VM techniques. Such\nan evaluation is not straightforward, as it heavily hinges on modeling the\ninterplay between different VM techniques and the interactions of VM with the\nsystem architecture. Modern simulators, however, struggle to keep up with the\nrapid VM research developments, lacking the capability to model a wide range of\ncontemporary VM techniques and their interactions. To this end, we present\nVirtuoso, an open-source, comprehensive and modular simulation framework that\nmodels various VM designs to establish a common ground for virtual memory\nresearch. We demonstrate the versatility and the potential of Virtuoso with\nfour new case studies. Virtuoso is freely open-source and can be found at\nhttps://github.com/CMU-SAFARI/Virtuoso.",
        "updated": "2024-03-07T16:21:02Z",
        "published": "2024-03-07T16:21:02Z",
        "authors": [
            "Konstantinos Kanellopoulos",
            "Konstantinos Sgouras",
            "Onur Mutlu"
        ],
        "categories": [
            "cs.AR",
            "cs.OS"
        ],
        "primary_category": "cs.AR"
    },
    "2403.04636v1": {
        "url": "http://arxiv.org/abs/2403.04636v1",
        "title": "Entropy Aware Message Passing in Graph Neural Networks",
        "summary": "Deep Graph Neural Networks struggle with oversmoothing. This paper introduces\na novel, physics-inspired GNN model designed to mitigate this issue. Our\napproach integrates with existing GNN architectures, introducing an\nentropy-aware message passing term. This term performs gradient ascent on the\nentropy during node aggregation, thereby preserving a certain degree of entropy\nin the embeddings. We conduct a comparative analysis of our model against\nstate-of-the-art GNNs across various common datasets.",
        "updated": "2024-03-07T16:21:09Z",
        "published": "2024-03-07T16:21:09Z",
        "authors": [
            "Philipp Nazari",
            "Oliver Lemke",
            "Davide Guidobene",
            "Artiom Gesp"
        ],
        "comments": "4 pages, 3 figures",
        "categories": [
            "cs.LG",
            "I.2.6; I.5.1"
        ],
        "primary_category": "cs.LG"
    },
    "2403.04638v1": {
        "url": "http://arxiv.org/abs/2403.04638v1",
        "title": "Scalable, Simulation-Guided Compliant Tactile Finger Design",
        "summary": "Compliant grippers enable robots to work with humans in unstructured\nenvironments. In general, these grippers can improve with tactile sensing to\nestimate the state of objects around them to precisely manipulate objects.\nHowever, co-designing compliant structures with high-resolution tactile sensing\nis a challenging task. We propose a simulation framework for the end-to-end\nforward design of GelSight Fin Ray sensors. Our simulation framework consists\nof mechanical simulation using the finite element method (FEM) and optical\nsimulation including physically based rendering (PBR). To simulate the\nfluorescent paint used in these GelSight Fin Rays, we propose an efficient\nmethod that can be directly integrated in PBR. Using the simulation framework,\nwe investigate design choices available in the compliant grippers, namely gel\npad shapes, illumination conditions, Fin Ray gripper sizes, and Fin Ray\nstiffness. This infrastructure enables faster design and prototype time frames\nof new Fin Ray sensors that have various sensing areas, ranging from 48 mm\n$\\times$ \\18 mm to 70 mm $\\times$ 35 mm. Given the parameters we choose, we can\nthus optimize different Fin Ray designs and show their utility in grasping\nday-to-day objects.",
        "updated": "2024-03-07T16:29:12Z",
        "published": "2024-03-07T16:29:12Z",
        "authors": [
            "Yuxiang Ma",
            "Arpit Agarwal",
            "Sandra Q. Liu",
            "Wenzhen Yuan",
            "Edward H. Adelson"
        ],
        "comments": "Yuxiang Ma, Arpit Agarwal, and Sandra Q. Liu contributed equally to\n  this work. Project video: https://youtu.be/CnTUTA5cfMw . 7 pages, 11 figures,\n  2024 IEEE International Conference on Soft Robotics (RoboSoft)",
        "categories": [
            "cs.RO"
        ],
        "primary_category": "cs.RO"
    },
    "2403.04639v2": {
        "url": "http://arxiv.org/abs/2403.04639v2",
        "title": "MaCmS: Magahi Code-mixed Dataset for Sentiment Analysis",
        "summary": "The present paper introduces new sentiment data, MaCMS, for\nMagahi-Hindi-English (MHE) code-mixed language, where Magahi is a\nless-resourced minority language. This dataset is the first\nMagahi-Hindi-English code-mixed dataset for sentiment analysis tasks. Further,\nwe also provide a linguistics analysis of the dataset to understand the\nstructure of code-mixing and a statistical study to understand the language\npreferences of speakers with different polarities. With these analyses, we also\ntrain baseline models to evaluate the dataset's quality.",
        "updated": "2024-03-22T17:28:42Z",
        "published": "2024-03-07T16:29:19Z",
        "authors": [
            "Priya Rani",
            "Gaurav Negi",
            "Theodorus Fransen",
            "John P. McCrae"
        ],
        "comments": "Lrec-Colin 2024",
        "categories": [
            "cs.CL"
        ],
        "primary_category": "cs.CL"
    },
    "2403.04640v1": {
        "url": "http://arxiv.org/abs/2403.04640v1",
        "title": "CAT: Enhancing Multimodal Large Language Model to Answer Questions in\n  Dynamic Audio-Visual Scenarios",
        "summary": "This paper focuses on the challenge of answering questions in scenarios that\nare composed of rich and complex dynamic audio-visual components. Although\nexisting Multimodal Large Language Models (MLLMs) can respond to audio-visual\ncontent, these responses are sometimes ambiguous and fail to describe specific\naudio-visual events. To overcome this limitation, we introduce the CAT, which\nenhances MLLM in three ways: 1) besides straightforwardly bridging audio and\nvideo, we design a clue aggregator that aggregates question-related clues in\ndynamic audio-visual scenarios to enrich the detailed knowledge required for\nlarge language models. 2) CAT is trained on a mixed multimodal dataset,\nallowing direct application in audio-visual scenarios. Notably, we collect an\naudio-visual joint instruction dataset named AVinstruct, to further enhance the\ncapacity of CAT to model cross-semantic correlations. 3) we propose AI-assisted\nambiguity-aware direct preference optimization, a strategy specialized in\nretraining the model to favor the non-ambiguity response and improve the\nability to localize specific audio-visual objects. Extensive experimental\nresults demonstrate that CAT outperforms existing methods on multimodal tasks,\nespecially in Audio-Visual Question Answering (AVQA) tasks. The codes and the\ncollected instructions are released at https://github.com/rikeilong/Bay-CAT.",
        "updated": "2024-03-07T16:31:02Z",
        "published": "2024-03-07T16:31:02Z",
        "authors": [
            "Qilang Ye",
            "Zitong Yu",
            "Rui Shao",
            "Xinyu Xie",
            "Philip Torr",
            "Xiaochun Cao"
        ],
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2403.04641v1": {
        "url": "http://arxiv.org/abs/2403.04641v1",
        "title": "The interdefinability of expansions of Belnap-Dunn logic",
        "summary": "Belnap-Dunn logic, also knows as the logic of First-Degree Entailment, is a\nlogic that can serve as the underlying logic of theories that are inconsistent\nor incomplete. For various reasons, different expansions of Belnap-Dunn logic\nwith non-classical connectives have been studied. This paper investigates the\nquestion whether those expansions are interdefinable with an expansion whose\nconnectives include only classical connectives. This is worth knowing because\nit is difficult to say how close a logic with non-classical connectives is\nrelated to classical logic. The notion of interdefinability of logics used is\nbased on a general notion of definability of a connective in a logic that seems\nto have been forgotten.",
        "updated": "2024-03-07T16:34:02Z",
        "published": "2024-03-07T16:34:02Z",
        "authors": [
            "C. A. Middelburg"
        ],
        "comments": "18 pages. arXiv admin note: text overlap with arXiv:2301.10555",
        "categories": [
            "cs.LO",
            "math.LO",
            "03B50 (Primary)"
        ],
        "primary_category": "cs.LO"
    },
    "2403.04642v1": {
        "url": "http://arxiv.org/abs/2403.04642v1",
        "title": "Teaching Large Language Models to Reason with Reinforcement Learning",
        "summary": "Reinforcement Learning from Human Feedback (\\textbf{RLHF}) has emerged as a\ndominant approach for aligning LLM outputs with human preferences. Inspired by\nthe success of RLHF, we study the performance of multiple algorithms that learn\nfrom feedback (Expert Iteration, Proximal Policy Optimization (\\textbf{PPO}),\nReturn-Conditioned RL) on improving LLM reasoning capabilities. We investigate\nboth sparse and dense rewards provided to the LLM both heuristically and via a\nlearned reward model. We additionally start from multiple model sizes and\ninitializations both with and without supervised fine-tuning (\\textbf{SFT})\ndata. Overall, we find all algorithms perform comparably, with Expert Iteration\nperforming best in most cases. Surprisingly, we find the sample complexity of\nExpert Iteration is similar to that of PPO, requiring at most on the order of\n$10^6$ samples to converge from a pretrained checkpoint. We investigate why\nthis is the case, concluding that during RL training models fail to explore\nsignificantly beyond solutions already produced by SFT models. Additionally, we\ndiscuss a trade off between maj@1 and pass@96 metric performance during SFT\ntraining and how conversely RL training improves both simultaneously. We then\nconclude by discussing the implications of our findings for RLHF and the future\nrole of RL in LLM fine-tuning.",
        "updated": "2024-03-07T16:36:29Z",
        "published": "2024-03-07T16:36:29Z",
        "authors": [
            "Alex Havrilla",
            "Yuqing Du",
            "Sharath Chandra Raparthy",
            "Christoforos Nalmpantis",
            "Jane Dwivedi-Yu",
            "Maksym Zhuravinskyi",
            "Eric Hambro",
            "Sainbayar Sukhbaatar",
            "Roberta Raileanu"
        ],
        "categories": [
            "cs.LG"
        ],
        "primary_category": "cs.LG"
    },
    "2403.05645v1": {
        "url": "http://arxiv.org/abs/2403.05645v1",
        "title": "Geometric Neural Network based on Phase Space for BCI decoding",
        "summary": "The integration of Deep Learning (DL) algorithms on brain signal analysis is\nstill in its nascent stages compared to their success in fields like Computer\nVision, especially in Brain-Computer Interface (BCI), where the brain activity\nis decoded to control external devices without requiring muscle control.\nElectroencephalography (EEG) is a widely adopted choice for designing BCI\nsystems due to its non-invasive and cost-effective nature and excellent\ntemporal resolution. Still, it comes at the expense of limited training data,\npoor signal-to-noise, and a large variability across and within-subject\nrecordings. Finally, setting up a BCI system with many electrodes takes a long\ntime, hindering the widespread adoption of reliable DL architectures in BCIs\noutside research laboratories. To improve adoption, we need to improve user\ncomfort using, for instance, reliable algorithms that operate with few\nelectrodes. \\textbf{Approach:} Our research aims to develop a DL algorithm that\ndelivers effective results with a limited number of electrodes. Taking\nadvantage of the Augmented Covariance Method with SPDNet, we propose the\nSPDNet$_{\\psi}$ architecture and analyze its performance and computational\nimpact, as well as the interpretability of the results. The evaluation is\nconducted on 5-fold cross-validation, using only three electrodes positioned\nabove the Motor Cortex. The methodology was tested on nearly 100 subjects from\nseveral open-source datasets using the Mother Of All BCI Benchmark (MOABB)\nframework. \\textbf{Main results:} The results of our SPDNet$_{\\psi}$\ndemonstrate that the augmented approach combined with the SPDNet significantly\noutperforms all the current state-of-the-art DL architecture in MI decoding.\n\\textbf{Significance:} This new architecture is explainable, with a low number\nof trainable parameters and a reduced carbon footprint.",
        "updated": "2024-03-08T19:36:20Z",
        "published": "2024-03-08T19:36:20Z",
        "authors": [
            "Igor Carrara",
            "Bruno Aristimunha",
            "Marie-Constance Corsi",
            "Raphael Y. de Camargo",
            "Sylvain Chevallier",
            "Th\u00e9odore Papadopoulo"
        ],
        "categories": [
            "eess.SP",
            "cs.AI",
            "cs.LG",
            "q-bio.NC",
            "I.5.1; I.6.3; I.2.6"
        ],
        "primary_category": "eess.SP"
    },
    "2403.05652v1": {
        "url": "http://arxiv.org/abs/2403.05652v1",
        "title": "What is different between these datasets?",
        "summary": "The performance of machine learning models heavily depends on the quality of\ninput data, yet real-world applications often encounter various data-related\nchallenges. One such challenge could arise when curating training data or\ndeploying the model in the real world - two comparable datasets in the same\ndomain may have different distributions. While numerous techniques exist for\ndetecting distribution shifts, the literature lacks comprehensive approaches\nfor explaining dataset differences in a human-understandable manner. To address\nthis gap, we propose a suite of interpretable methods (toolbox) for comparing\ntwo datasets. We demonstrate the versatility of our approach across diverse\ndata modalities, including tabular data, language, images, and signals in both\nlow and high-dimensional settings. Our methods not only outperform comparable\nand related approaches in terms of explanation quality and correctness, but\nalso provide actionable, complementary insights to understand and mitigate\ndataset differences effectively.",
        "updated": "2024-03-08T19:52:39Z",
        "published": "2024-03-08T19:52:39Z",
        "authors": [
            "Varun Babbar",
            "Zhicheng Guo",
            "Cynthia Rudin"
        ],
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "primary_category": "cs.LG"
    },
    "2403.05653v1": {
        "url": "http://arxiv.org/abs/2403.05653v1",
        "title": "Q-CHOP: Quantum constrained Hamiltonian optimization",
        "summary": "Combinatorial optimization problems that arise in science and industry\ntypically have constraints. Yet the presence of constraints makes them\nchallenging to tackle using both classical and quantum optimization algorithms.\nWe propose a new quantum algorithm for constrained optimization, which we call\nquantum constrained Hamiltonian optimization (Q-CHOP). Our algorithm leverages\nthe observation that for many problems, while the best solution is difficult to\nfind, the worst feasible (constraint-satisfying) solution is known. The basic\nidea is to to enforce a Hamiltonian constraint at all times, thereby\nrestricting evolution to the subspace of feasible states, and slowly \"rotate\"\nan objective Hamiltonian to trace an adiabatic path from the worst feasible\nstate to the best feasible state. We additionally propose a version of Q-CHOP\nthat can start in any feasible state. Finally, we benchmark Q-CHOP against the\ncommonly-used adiabatic algorithm with constraints enforced using a penalty\nterm and find that Q-CHOP performs consistently better on a wide range of\nproblems, including textbook problems on graphs, knapsack, combinatorial\nauction, as well as a real-world financial use case, namely bond\nexchange-traded fund basket optimization.",
        "updated": "2024-03-08T20:03:59Z",
        "published": "2024-03-08T20:03:59Z",
        "authors": [
            "Michael A. Perlin",
            "Ruslan Shaydulin",
            "Benjamin P. Hall",
            "Pierre Minssen",
            "Changhao Li",
            "Kabir Dubey",
            "Rich Rines",
            "Eric R. Anschuetz",
            "Marco Pistoia",
            "Pranav Gokhale"
        ],
        "categories": [
            "quant-ph",
            "cs.ET"
        ],
        "primary_category": "quant-ph"
    },
    "2403.05658v1": {
        "url": "http://arxiv.org/abs/2403.05658v1",
        "title": "Feature CAM: Interpretable AI in Image Classification",
        "summary": "Deep Neural Networks have often been called the black box because of the\ncomplex, deep architecture and non-transparency presented by the inner layers.\nThere is a lack of trust to use Artificial Intelligence in critical and\nhigh-precision fields such as security, finance, health, and manufacturing\nindustries. A lot of focused work has been done to provide interpretable\nmodels, intending to deliver meaningful insights into the thoughts and behavior\nof neural networks. In our research, we compare the state-of-the-art methods in\nthe Activation-based methods (ABM) for interpreting predictions of CNN models,\nspecifically in the application of Image Classification. We then extend the\nsame for eight CNN-based architectures to compare the differences in\nvisualization and thus interpretability. We introduced a novel technique\nFeature CAM, which falls in the perturbation-activation combination, to create\nfine-grained, class-discriminative visualizations. The resulting saliency maps\nfrom our experiments proved to be 3-4 times better human interpretable than the\nstate-of-the-art in ABM. At the same time it reserves machine interpretability,\nwhich is the average confidence scores in classification.",
        "updated": "2024-03-08T20:16:00Z",
        "published": "2024-03-08T20:16:00Z",
        "authors": [
            "Frincy Clement",
            "Ji Yang",
            "Irene Cheng"
        ],
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "primary_category": "cs.CV"
    },
    "2403.05659v1": {
        "url": "http://arxiv.org/abs/2403.05659v1",
        "title": "Audio-Synchronized Visual Animation",
        "summary": "Current visual generation methods can produce high quality videos guided by\ntexts. However, effectively controlling object dynamics remains a challenge.\nThis work explores audio as a cue to generate temporally synchronized image\nanimations. We introduce Audio Synchronized Visual Animation (ASVA), a task\nanimating a static image to demonstrate motion dynamics, temporally guided by\naudio clips across multiple classes. To this end, we present AVSync15, a\ndataset curated from VGGSound with videos featuring synchronized audio visual\nevents across 15 categories. We also present a diffusion model, AVSyncD,\ncapable of generating dynamic animations guided by audios. Extensive\nevaluations validate AVSync15 as a reliable benchmark for synchronized\ngeneration and demonstrate our models superior performance. We further explore\nAVSyncDs potential in a variety of audio synchronized generation tasks, from\ngenerating full videos without a base image to controlling object motions with\nvarious sounds. We hope our established benchmark can open new avenues for\ncontrollable visual generation. More videos on project webpage\nhttps://lzhangbj.github.io/projects/asva/asva.html.",
        "updated": "2024-03-08T20:17:34Z",
        "published": "2024-03-08T20:17:34Z",
        "authors": [
            "Lin Zhang",
            "Shentong Mo",
            "Yijing Zhang",
            "Pedro Morgado"
        ],
        "comments": "15 pages",
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2403.05660v1": {
        "url": "http://arxiv.org/abs/2403.05660v1",
        "title": "Decoupling Degradations with Recurrent Network for Video Restoration in\n  Under-Display Camera",
        "summary": "Under-display camera (UDC) systems are the foundation of full-screen display\ndevices in which the lens mounts under the display. The pixel array of\nlight-emitting diodes used for display diffracts and attenuates incident light,\ncausing various degradations as the light intensity changes. Unlike general\nvideo restoration which recovers video by treating different degradation\nfactors equally, video restoration for UDC systems is more challenging that\nconcerns removing diverse degradation over time while preserving temporal\nconsistency. In this paper, we introduce a novel video restoration network,\ncalled D$^2$RNet, specifically designed for UDC systems. It employs a set of\nDecoupling Attention Modules (DAM) that effectively separate the various video\ndegradation factors. More specifically, a soft mask generation function is\nproposed to formulate each frame into flare and haze based on the diffraction\narising from incident light of different intensities, followed by the proposed\nflare and haze removal components that leverage long- and short-term feature\nlearning to handle the respective degradations. Such a design offers an\ntargeted and effective solution to eliminating various types of degradation in\nUDC systems. We further extend our design into multi-scale to overcome the\nscale-changing of degradation that often occur in long-range videos. To\ndemonstrate the superiority of D$^2$RNet, we propose a large-scale UDC video\nbenchmark by gathering HDR videos and generating realistically degraded videos\nusing the point spread function measured by a commercial UDC system. Extensive\nquantitative and qualitative evaluations demonstrate the superiority of\nD$^2$RNet compared to other state-of-the-art video restoration and UDC image\nrestoration methods. Code is available at\nhttps://github.com/ChengxuLiu/DDRNet.git",
        "updated": "2024-03-08T20:21:45Z",
        "published": "2024-03-08T20:21:45Z",
        "authors": [
            "Chengxu Liu",
            "Xuan Wang",
            "Yuanting Fan",
            "Shuai Li",
            "Xueming Qian"
        ],
        "comments": "AAAI 2024",
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2403.05663v1": {
        "url": "http://arxiv.org/abs/2403.05663v1",
        "title": "A Formal Analysis of SCTP: Attack Synthesis and Patch Verification",
        "summary": "SCTP is a transport protocol offering features such as multi-homing,\nmulti-streaming, and message-oriented delivery. Its two main implementations\nwere subjected to conformance tests using the PacketDrill tool. Conformance\ntesting is not exhaustive and a recent vulnerability (CVE-2021-3772) showed\nSCTP is not immune to attacks. Changes addressing the vulnerability were\nimplemented, but the question remains whether other flaws might persist in the\nprotocol design.\n  We study the security of the SCTP design, taking a rigorous approach rooted\nin formal methods. We create a formal Promela model of SCTP, and define 10\nproperties capturing the essential protocol functionality based on its RFC\nspecification and consultation with the lead RFC author. Then we show using the\nSpin model checker that our model satisfies these properties. We define 4\nattacker models - Off-Path, where the attacker is an outsider that can spoof\nthe port and IP of a peer; Evil-Server, where the attacker is a malicious peer;\nReplay, where an attacker can capture and replay, but not modify, packets; and\nOn-Path, where the attacker controls the channel between peers. We modify an\nattack synthesis tool designed for transport protocols, Korg, to support our\nSCTP model and four attacker models.\n  We synthesize 14 unique attacks using the attacker models - including the CVE\nvulnerability in the Off-Path attacker model, 4 attacks in the Evil-Server\nattacker model, an opportunistic ABORT attack in the Replay attacker model, and\neight connection manipulation attacks in the On-Path attacker model. We show\nthat the proposed patch eliminates the vulnerability and does not introduce new\nones according to our model and protocol properties. Finally, we identify and\nanalyze an ambiguity in the RFC, which we show can be interpreted insecurely.\nWe propose an erratum and show that it eliminates the ambiguity.",
        "updated": "2024-03-08T20:38:56Z",
        "published": "2024-03-08T20:38:56Z",
        "authors": [
            "Jacob Ginesin",
            "Max von Hippel",
            "Evan Defloor",
            "Cristina Nita-Rotaru",
            "Michael T\u00fcxen"
        ],
        "categories": [
            "cs.CR"
        ],
        "primary_category": "cs.CR"
    },
    "2403.05666v1": {
        "url": "http://arxiv.org/abs/2403.05666v1",
        "title": "Prepared for the Worst: A Learning-Based Adversarial Attack for\n  Resilience Analysis of the ICP Algorithm",
        "summary": "This paper presents a novel method to assess the resilience of the Iterative\nClosest Point (ICP) algorithm via deep-learning-based attacks on lidar point\nclouds. For safety-critical applications such as autonomous navigation,\nensuring the resilience of algorithms prior to deployments is of utmost\nimportance. The ICP algorithm has become the standard for lidar-based\nlocalization. However, the pose estimate it produces can be greatly affected by\ncorruption in the measurements. Corruption can arise from a variety of\nscenarios such as occlusions, adverse weather, or mechanical issues in the\nsensor. Unfortunately, the complex and iterative nature of ICP makes assessing\nits resilience to corruption challenging. While there have been efforts to\ncreate challenging datasets and develop simulations to evaluate the resilience\nof ICP empirically, our method focuses on finding the maximum possible ICP pose\nerror using perturbation-based adversarial attacks. The proposed attack induces\nsignificant pose errors on ICP and outperforms baselines more than 88% of the\ntime across a wide range of scenarios. As an example application, we\ndemonstrate that our attack can be used to identify areas on a map where ICP is\nparticularly vulnerable to corruption in the measurements.",
        "updated": "2024-03-08T20:43:57Z",
        "published": "2024-03-08T20:43:57Z",
        "authors": [
            "Ziyu Zhang",
            "Johann Laconte",
            "Daniil Lisus",
            "Timothy D. Barfoot"
        ],
        "comments": "8 pages (7 content, 1 reference). 5 figures, submitted to the IEEE\n  Robotics and Automation Letters (RA-L)",
        "categories": [
            "cs.RO",
            "cs.LG"
        ],
        "primary_category": "cs.RO"
    },
    "2403.05668v1": {
        "url": "http://arxiv.org/abs/2403.05668v1",
        "title": "CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model\n  Recommender System",
        "summary": "In the evolving landscape of recommender systems, the integration of Large\nLanguage Models (LLMs) such as ChatGPT marks a new era, introducing the concept\nof Recommendation via LLM (RecLLM). While these advancements promise\nunprecedented personalization and efficiency, they also bring to the fore\ncritical concerns regarding fairness, particularly in how recommendations might\ninadvertently perpetuate or amplify biases associated with sensitive user\nattributes. In order to address these concerns, our study introduces a\ncomprehensive evaluation framework, CFaiRLLM, aimed at evaluating (and thereby\nmitigating) biases on the consumer side within RecLLMs.\n  Our research methodically assesses the fairness of RecLLMs by examining how\nrecommendations might vary with the inclusion of sensitive attributes such as\ngender, age, and their intersections, through both similarity alignment and\ntrue preference alignment. By analyzing recommendations generated under\ndifferent conditions-including the use of sensitive attributes in user\nprompts-our framework identifies potential biases in the recommendations\nprovided. A key part of our study involves exploring how different detailed\nstrategies for constructing user profiles (random, top-rated, recent) impact\nthe alignment between recommendations made without consideration of sensitive\nattributes and those that are sensitive-attribute-aware, highlighting the bias\nmechanisms within RecLLMs.\n  The findings in our study highlight notable disparities in the fairness of\nrecommendations, particularly when sensitive attributes are integrated into the\nrecommendation process, either individually or in combination. The analysis\ndemonstrates that the choice of user profile sampling strategy plays a\nsignificant role in affecting fairness outcomes, highlighting the complexity of\nachieving fair recommendations in the era of LLMs.",
        "updated": "2024-03-08T20:44:59Z",
        "published": "2024-03-08T20:44:59Z",
        "authors": [
            "Yashar Deldjoo",
            "Tommaso di Noia"
        ],
        "categories": [
            "cs.IR"
        ],
        "primary_category": "cs.IR"
    },
    "2403.05669v1": {
        "url": "http://arxiv.org/abs/2403.05669v1",
        "title": "Spectral Clustering of Categorical and Mixed-type Data via Extra Graph\n  Nodes",
        "summary": "Clustering data objects into homogeneous groups is one of the most important\ntasks in data mining. Spectral clustering is arguably one of the most important\nalgorithms for clustering, as it is appealing for its theoretical soundness and\nis adaptable to many real-world data settings. For example, mixed data, where\nthe data is composed of numerical and categorical features, is typically\nhandled via numerical discretization, dummy coding, or similarity computation\nthat takes into account both data types. This paper explores a more natural way\nto incorporate both numerical and categorical information into the spectral\nclustering algorithm, avoiding the need for data preprocessing or the use of\nsophisticated similarity functions. We propose adding extra nodes corresponding\nto the different categories the data may belong to and show that it leads to an\ninterpretable clustering objective function. Furthermore, we demonstrate that\nthis simple framework leads to a linear-time spectral clustering algorithm for\ncategorical-only data. Finally, we compare the performance of our algorithms\nagainst other related methods and show that it provides a competitive\nalternative to them in terms of performance and runtime.",
        "updated": "2024-03-08T20:49:49Z",
        "published": "2024-03-08T20:49:49Z",
        "authors": [
            "Dylan Soemitro",
            "Jeova Farias Sales Rocha Neto"
        ],
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "primary_category": "stat.ML"
    },
    "2403.06625v1": {
        "url": "http://arxiv.org/abs/2403.06625v1",
        "title": "AC/DC optimal power flow and techno-economic assessment for hybrid\n  microgrids: TIGON CEDER demonstrator",
        "summary": "In the recent years, the interest in electric direct current (DC)\ntechnologies (such as converters, batteries, electric vehicles, etc.) is\nincreasing due to its potential on energy efficiency and sustainability.\nHowever, the vast majority of electric systems and networks are based on\nalternating current (AC), as they also have certain advantages regarding\ncost-effective transport and robustness. In this paper, an AC/DC optimal power\nflow method for hybrid microgrids and several key performance indicators (KPIs)\nfor its techno-economic assessment are presented. The combination of both\ncalculations allows users to clearly determine the viability of their hybrid\nmicrogrids. AC/DC networks have been modelled considering their most common\nelements. For the power flow method, a polynomial optimisation is formulated\nconsidering four different objective functions: the minimisation of energy\nlosses, voltage deviation and operational costs, and also the maximisation of\nthe microgrid generation. The power flow method and the techno-economic\nanalysis have been implemented in Python and validated in the Centro de\nDesarrollo de Energ\\'ias Renovables (CEDER) demonstrator for TIGON. The results\nshow that the calculated power flow variables and the ones measured at CEDER\nare practically the same. In addition, the KPIs have been obtained and compared\nfor four operating scenarios: baseline, no battery, battery flexibility and\nvirtual battery (VB) flexibility. The last one result in the most profitable\noption.",
        "updated": "2024-03-11T11:33:53Z",
        "published": "2024-03-11T11:33:53Z",
        "authors": [
            "Alejandro Mart\u00edn-Crespo",
            "Alejandro Hern\u00e1ndez-Serrano",
            "\u00d3scar Izquierdo-Monge",
            "Paula Pe\u00f1a-Carro",
            "\u00c1ngel Hern\u00e1ndez-Jim\u00e9nez",
            "Fernando Frechoso-Escudero",
            "Enrique Baeyens"
        ],
        "categories": [
            "eess.SY",
            "cs.SY"
        ],
        "primary_category": "eess.SY"
    },
    "2403.06631v1": {
        "url": "http://arxiv.org/abs/2403.06631v1",
        "title": "Evaluating the Energy Efficiency of Few-Shot Learning for Object\n  Detection in Industrial Settings",
        "summary": "In the ever-evolving era of Artificial Intelligence (AI), model performance\nhas constituted a key metric driving innovation, leading to an exponential\ngrowth in model size and complexity. However, sustainability and energy\nefficiency have been critical requirements during deployment in contemporary\nindustrial settings, necessitating the use of data-efficient approaches such as\nfew-shot learning. In this paper, to alleviate the burden of lengthy model\ntraining and minimize energy consumption, a finetuning approach to adapt\nstandard object detection models to downstream tasks is examined. Subsequently,\na thorough case study and evaluation of the energy demands of the developed\nmodels, applied in object detection benchmark datasets from volatile industrial\nenvironments is presented. Specifically, different finetuning strategies as\nwell as utilization of ancillary evaluation data during training are examined,\nand the trade-off between performance and efficiency is highlighted in this\nlow-data regime. Finally, this paper introduces a novel way to quantify this\ntrade-off through a customized Efficiency Factor metric.",
        "updated": "2024-03-11T11:41:30Z",
        "published": "2024-03-11T11:41:30Z",
        "authors": [
            "Georgios Tsoumplekas",
            "Vladislav Li",
            "Ilias Siniosoglou",
            "Vasileios Argyriou",
            "Sotirios K. Goudos",
            "Ioannis D. Moscholios",
            "Panagiotis Radoglou-Grammatikis",
            "Panagiotis Sarigiannidis"
        ],
        "comments": "7 pages, 6 figures, 4 tables",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "primary_category": "cs.LG"
    },
    "2403.06632v1": {
        "url": "http://arxiv.org/abs/2403.06632v1",
        "title": "Self-Sovereign Identity for Electric Vehicle Charging",
        "summary": "Electric Vehicles (EVs) are more and more charged at public Charge Points\n(CPs) using Plug-and-Charge (PnC) protocols such as the ISO 15118 standard\nwhich eliminates user interaction for authentication and authorization.\nCurrently, this requires a rather complex Public Key Infrastructure (PKI) and\nenables driver tracking via the included unique identifiers. In this paper, we\npropose an approach for using Self-Sovereign Identities (SSIs) as trusted\ncredentials for EV charging authentication and authorization which overcomes\nthe privacy problems and the issues of a complex centralized PKI. Our\nimplementation shows the feasibility of our approach with ISO 15118. The\nsecurity and privacy of the proposed approach is shown in a formal analysis\nusing the Tamarin prover.",
        "updated": "2024-03-11T11:43:40Z",
        "published": "2024-03-11T11:43:40Z",
        "authors": [
            "Adrian Kailus",
            "Dustin Kern",
            "Christoph Krau\u00df"
        ],
        "comments": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in \"22nd International Conference on Applied Cryptography and\n  Network Security, ACNS 2024, Abu Dhabi, United Arab Emirates, March 5-8,\n  2024, Proceedings, Part III,\" and is available online at\n  https://doi.org/10.1007/978-3-031-54776-8_6",
        "categories": [
            "cs.CR"
        ],
        "primary_category": "cs.CR",
        "doi": "10.1007/978-3-031-54776-8_6",
        "journal_ref": "22nd International Conference on Applied Cryptography and Network\n  Security, ACNS 2024, Abu Dhabi, United Arab Emirates, March 5-8, 2024,\n  Proceedings, Part III"
    },
    "2403.06634v1": {
        "url": "http://arxiv.org/abs/2403.06634v1",
        "title": "Stealing Part of a Production Language Model",
        "summary": "We introduce the first model-stealing attack that extracts precise,\nnontrivial information from black-box production language models like OpenAI's\nChatGPT or Google's PaLM-2. Specifically, our attack recovers the embedding\nprojection layer (up to symmetries) of a transformer model, given typical API\naccess. For under \\$20 USD, our attack extracts the entire projection matrix of\nOpenAI's Ada and Babbage language models. We thereby confirm, for the first\ntime, that these black-box models have a hidden dimension of 1024 and 2048,\nrespectively. We also recover the exact hidden dimension size of the\ngpt-3.5-turbo model, and estimate it would cost under \\$2,000 in queries to\nrecover the entire projection matrix. We conclude with potential defenses and\nmitigations, and discuss the implications of possible future work that could\nextend our attack.",
        "updated": "2024-03-11T11:46:12Z",
        "published": "2024-03-11T11:46:12Z",
        "authors": [
            "Nicholas Carlini",
            "Daniel Paleka",
            "Krishnamurthy Dj Dvijotham",
            "Thomas Steinke",
            "Jonathan Hayase",
            "A. Feder Cooper",
            "Katherine Lee",
            "Matthew Jagielski",
            "Milad Nasr",
            "Arthur Conmy",
            "Eric Wallace",
            "David Rolnick",
            "Florian Tram\u00e8r"
        ],
        "categories": [
            "cs.CR"
        ],
        "primary_category": "cs.CR"
    },
    "2403.06635v1": {
        "url": "http://arxiv.org/abs/2403.06635v1",
        "title": "Aggregated distribution grid flexibilities in subtransmission grid\n  operational management",
        "summary": "Aggregated flexibilities or PQ-capabilities (active and reactive power\ncapabilities) are termed in literature as Feasible Operating Regions (FORs).\nThe FORs from underlying active distribution grids can effectively contribute\nto the operational management at the HV grid level. The HV buses are allocated\naggregated FORs from the underlying MV grids, which are inherently nonlinear\nand non-convex. Therefore, two approaches are proposed in the paper to apply\nthe FOR constraints in the HV grid operational management. First, a mixed\ninteger linear programming (MILP) based optimization approach for alleviating\nthe HV grid constraint violations is proposed, which addresses the\nnon-convexity of the FOR using piecewise segmentation. Furthermore, the MILP\nmethod is enhanced to consider the influence of the HV bus voltage on the\nunderlying MV grid flexibilities resulting in a three dimensional PQ(V)-FOR.\nSecond, a convexification approach is proposed, which uses a convex\napproximation of the non-convex 3D PQ(V)-FOR shape for implementation in a\nlinear optimization method. Results reveal a robust utilization of the\ndistribution flexibilities to maintain grid security and reliability at the HV\ngrid level. Comparisons present increased computation times for the MILP method\nwhich are significantly improved using the convexification based approach.",
        "updated": "2024-03-11T11:47:07Z",
        "published": "2024-03-11T11:47:07Z",
        "authors": [
            "Neelotpal Majumdar",
            "Lutz Hofmann"
        ],
        "categories": [
            "eess.SY",
            "cs.SY"
        ],
        "primary_category": "eess.SY"
    },
    "2403.06636v1": {
        "url": "http://arxiv.org/abs/2403.06636v1",
        "title": "Design and Control of Delta: Deformable Multilinked Multirotor with\n  Rolling Locomotion Ability in Terrestrial Domain",
        "summary": "In recent years, multiple types of locomotion methods for robots have been\ndeveloped and enabled to adapt to multiple domains. In particular, aerial\nrobots are useful for exploration in several situations, taking advantage of\nits three-dimensional mobility. Moreover, some aerial robots have achieved\nmanipulation tasks in the air. However, energy consumption for flight is large\nand thus locomotion ability on the ground is also necessary for aerial robots\nto do tasks for long time. Therefore, in this work, we aim to develop\ndeformable multirotor robot capable of rolling movement with its entire body\nand achieve motions on the ground and in the air. In this paper, we first\ndescribe the design methodology of a deformable multilinked air-ground hybrid\nmultirotor. We also introduce its mechanical design and rotor configuration\nbased on control stability. Then, thrust control method for locomotion in air\nand ground domains is described. Finally, we show the implemented prototype of\nthe proposed robot and evaluate through experiments in air and terrestrial\ndomains. To the best of our knowledge, this is the first time to achieve the\nrolling locomotion by multilink structured mutltrotor.",
        "updated": "2024-03-11T11:51:42Z",
        "published": "2024-03-11T11:51:42Z",
        "authors": [
            "Kazuki Sugihara",
            "Moju Zhao",
            "Takuzumi Nishio",
            "Kei Okada",
            "Masayuki Inaba"
        ],
        "comments": "8 pages, 15 figures",
        "categories": [
            "cs.RO"
        ],
        "primary_category": "cs.RO"
    },
    "2403.06639v1": {
        "url": "http://arxiv.org/abs/2403.06639v1",
        "title": "Robust and fast backbone tracking via phase-locked loops",
        "summary": "Phase-locked loops are commonly used for shaker-based backbone tracking of\nnonlinear structures. The state of the art is to tune the control parameters by\ntrial and error. In the present work, an approach is proposed to make backbone\ntracking much more robust and faster. A simple PI controller is proposed, and\nclosed-form expressions for the gains are provided that lead to an optimal\nsettling of the phase transient. The required input parameters are obtained\nfrom a conventional shaker-based linear modal test, and an open-loop sine test\nat a single frequency and level. For phase detection, an adaptive filter based\non the LMS algorithm is used, which is shown to be superior to the synchronous\ndemodulation commonly used. Once the phase has locked, one can directly take\nthe next step along the backbone, eliminating the hold times. The latter are\ncurrently used for recording the steady state, and to estimate Fourier\ncoefficients in the post-process, which becomes unnecessary since the adaptive\nfilter yields a highly accurate estimation at runtime. The excellent\nperformance of the proposed approach is demonstrated for a doubly clamped beam\nundergoing bending-stretching coupling leading to a 20 percent shift of the\nlowest modal frequency. Even for fixed control parameters, designed for the\nlinear regime, only about 100 periods are needed per backbone point, also in\nthe nonlinear regime. This is much faster than what has been reported in the\nliterature so far. In fact, the nonlinear backbone test becomes faster than the\nlinear modal test, shifting the established paradigm.",
        "updated": "2024-03-11T11:58:27Z",
        "published": "2024-03-11T11:58:27Z",
        "authors": [
            "Patrick Hippold",
            "Maren Scheel",
            "Ludovic Renson",
            "Malte Krack"
        ],
        "categories": [
            "eess.SY",
            "cs.SY"
        ],
        "primary_category": "eess.SY"
    },
    "2403.06640v1": {
        "url": "http://arxiv.org/abs/2403.06640v1",
        "title": "Passive iFIR filters for data-driven control",
        "summary": "We consider the design of a new class of passive iFIR controllers given by\nthe parallel action of an integrator and a finite impulse response filter.\niFIRs are more expressive than PID controllers but retain their features and\nsimplicity. The paper provides a model-free data-driven design for passive iFIR\ncontrollers based on virtual reference feedback tuning. Passivity is enforced\nthrough constrained optimization (three different formulations are discussed).\nThe proposed design does not rely on large datasets or accurate plant models.",
        "updated": "2024-03-11T11:59:24Z",
        "published": "2024-03-11T11:59:24Z",
        "authors": [
            "Zixing Wang",
            "Yongkang Huo",
            "Fulvio Forni"
        ],
        "comments": "6 pages, 8 figures, Submitted to IEEE Control Systems Letters (L-CSS)\n  with the option to present it to 2024 Conference on Decision and Control (CDC\n  2024)",
        "categories": [
            "eess.SY",
            "cs.RO",
            "cs.SY",
            "math.OC"
        ],
        "primary_category": "eess.SY"
    },
    "2403.06641v2": {
        "url": "http://arxiv.org/abs/2403.06641v2",
        "title": "Socio-spatial segregation and human mobility: A review of empirical\n  evidence",
        "summary": "Social segregation, the spatial and social separation between individuals\nfrom different backgrounds, can affect sustainable urban development and social\ncohesion. The literature has traditionally focused on residential segregation,\nexamining how individuals' residential locations are distributed differently\nacross neighborhoods based on income, ethnicity, and education. However, this\napproach overlooks the complexity of spatial segregation because daily\nactivities often extend far beyond residential areas. Over the past one to two\ndecades, emerging mobility data sources have enabled a new understanding of\nsocio-spatial segregation by considering daily activities such as work, school,\nshopping, and leisure visits. From traditional surveys to GPS trajectories,\ndiverse data sources reveal that day-to-day movements can impact segregation by\nreducing or amplifying segregation levels obtained when considering residential\naspects alone. This literature review focuses on three critical questions: (a)\nto what extent do individual mobility patterns contribute to segregation? (b)\nWhich factors explain the role played by mobility in segregation? and (c) What\ninsights are gained by incorporating extensive mobility data into segregation\nresearch? Our literature review contributes to an improved understanding of\nsocio-spatial segregation at the individual level and offers actionable\ninsights into reducing segregation and addressing research gaps in the field.",
        "updated": "2024-03-12T19:53:33Z",
        "published": "2024-03-11T12:02:30Z",
        "authors": [
            "Yuan Liao",
            "Jorge Gil",
            "Sonia Yeh",
            "Rafael H. M. Pereira",
            "Laura Alessandretti"
        ],
        "categories": [
            "cs.SI"
        ],
        "primary_category": "cs.SI"
    },
    "2403.06642v1": {
        "url": "http://arxiv.org/abs/2403.06642v1",
        "title": "KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation",
        "summary": "The utilization of semantic information is an important research problem in\nthe field of recommender systems, which aims to complement the missing parts of\nmainstream ID-based approaches. With the rise of LLM, its ability to act as a\nknowledge base and its reasoning capability have opened up new possibilities\nfor this research area, making LLM-based recommendation an emerging research\ndirection. However, directly using LLM to process semantic information for\nrecommendation scenarios is unreliable and sub-optimal due to several problems\nsuch as hallucination. A promising way to cope with this is to use external\nknowledge to aid LLM in generating truthful and usable text. Inspired by the\nabove motivation, we propose a Knowledge-Enhanced LLMRec method. In addition to\nusing external knowledge in prompts, the proposed method also includes a\nknowledge-based contrastive learning scheme for training. Experiments on public\ndatasets and in-enterprise datasets validate the effectiveness of the proposed\nmethod.",
        "updated": "2024-03-11T12:04:20Z",
        "published": "2024-03-11T12:04:20Z",
        "authors": [
            "Weiqing Luo",
            "Chonggang Song",
            "Lingling Yi",
            "Gong Cheng"
        ],
        "comments": "9 pages, 1 figure",
        "categories": [
            "cs.IR",
            "cs.AI",
            "cs.CL"
        ],
        "primary_category": "cs.IR"
    },
    "2403.07673v3": {
        "url": "http://arxiv.org/abs/2403.07673v3",
        "title": "Towards Model Extraction Attacks in GAN-Based Image Translation via\n  Domain Shift Mitigation",
        "summary": "Model extraction attacks (MEAs) enable an attacker to replicate the\nfunctionality of a victim deep neural network (DNN) model by only querying its\nAPI service remotely, posing a severe threat to the security and integrity of\npay-per-query DNN-based services. Although the majority of current research on\nMEAs has primarily concentrated on neural classifiers, there is a growing\nprevalence of image-to-image translation (I2IT) tasks in our everyday\nactivities. However, techniques developed for MEA of DNN classifiers cannot be\ndirectly transferred to the case of I2IT, rendering the vulnerability of I2IT\nmodels to MEA attacks often underestimated. This paper unveils the threat of\nMEA in I2IT tasks from a new perspective. Diverging from the traditional\napproach of bridging the distribution gap between attacker queries and victim\ntraining samples, we opt to mitigate the effect caused by the different\ndistributions, known as the domain shift. This is achieved by introducing a new\nregularization term that penalizes high-frequency noise, and seeking a flatter\nminimum to avoid overfitting to the shifted distribution. Extensive experiments\non different image translation tasks, including image super-resolution and\nstyle transfer, are performed on different backbone victim models, and the new\ndesign consistently outperforms the baseline by a large margin across all\nmetrics. A few real-life I2IT APIs are also verified to be extremely vulnerable\nto our attack, emphasizing the need for enhanced defenses and potentially\nrevised API publishing policies.",
        "updated": "2024-03-19T09:02:34Z",
        "published": "2024-03-12T14:06:44Z",
        "authors": [
            "Di Mi",
            "Yanjun Zhang",
            "Leo Yu Zhang",
            "Shengshan Hu",
            "Qi Zhong",
            "Haizhuan Yuan",
            "Shirui Pan"
        ],
        "comments": "Accepted by AAAI 2024",
        "categories": [
            "cs.CR"
        ],
        "primary_category": "cs.CR"
    },
    "2403.07675v1": {
        "url": "http://arxiv.org/abs/2403.07675v1",
        "title": "Multichannel Long-Term Streaming Neural Speech Enhancement for Static\n  and Moving Speakers",
        "summary": "In this work, we extend our previously proposed offline SpatialNet for\nlong-term streaming multichannel speech enhancement in both static and moving\nspeaker scenarios. SpatialNet exploits spatial information, such as the\nspatial/steering direction of speech, for discriminating between target speech\nand interferences, and achieved outstanding performance. The core of SpatialNet\nis a narrow-band self-attention module used for learning the temporal dynamic\nof spatial vectors. Towards long-term streaming speech enhancement, we propose\nto replace the offline self-attention network with online networks that have\nlinear inference complexity w.r.t signal length and meanwhile maintain the\ncapability of learning long-term information. Three variants are developed\nbased on (i) masked self-attention, (ii) Retention, a self-attention variant\nwith linear inference complexity, and (iii) Mamba, a\nstructured-state-space-based RNN-like network. Moreover, we investigate the\nlength extrapolation ability of different networks, namely test on signals that\nare much longer than training signals, and propose a short-signal training plus\nlong-signal fine-tuning strategy, which largely improves the length\nextrapolation ability of the networks within limited training time. Overall,\nthe proposed online SpatialNet achieves outstanding speech enhancement\nperformance for long audio streams, and for both static and moving speakers.\nThe proposed method will be open-sourced in\nhttps://github.com/Audio-WestlakeU/NBSS.",
        "updated": "2024-03-12T14:11:29Z",
        "published": "2024-03-12T14:11:29Z",
        "authors": [
            "Changsheng Quan",
            "Xiaofei Li"
        ],
        "categories": [
            "cs.SD",
            "eess.AS"
        ],
        "primary_category": "cs.SD"
    },
    "2403.07678v1": {
        "url": "http://arxiv.org/abs/2403.07678v1",
        "title": "MoralBERT: Detecting Moral Values in Social Discourse",
        "summary": "Morality plays a fundamental role in how we perceive information while\ngreatly influencing our decisions and judgements. Controversial topics,\nincluding vaccination, abortion, racism, and sexuality, often elicit opinions\nand attitudes that are not solely based on evidence but rather reflect moral\nworldviews. Recent advances in natural language processing have demonstrated\nthat moral values can be gauged in human-generated textual content. Here, we\ndesign a range of language representation models fine-tuned to capture exactly\nthe moral nuances in text, called MoralBERT. We leverage annotated moral data\nfrom three distinct sources: Twitter, Reddit, and Facebook user-generated\ncontent covering various socially relevant topics. This approach broadens\nlinguistic diversity and potentially enhances the models' ability to comprehend\nmorality in various contexts. We also explore a domain adaptation technique and\ncompare it to the standard fine-tuned BERT model, using two different\nframeworks for moral prediction: single-label and multi-label. We compare\nin-domain approaches with conventional models relying on lexicon-based\ntechniques, as well as a Machine Learning classifier with Word2Vec\nrepresentation. Our results showed that in-domain prediction models\nsignificantly outperformed traditional models. While the single-label setting\nreaches a higher accuracy than previously achieved for the task when using BERT\npretrained models. Experiments in an out-of-domain setting, instead, suggest\nthat further work is needed for existing domain adaptation techniques to\ngeneralise between different social media platforms, especially for the\nmulti-label task. The investigations and outcomes from this study pave the way\nfor further exploration, enabling a more profound comprehension of moral\nnarratives about controversial social issues.",
        "updated": "2024-03-12T14:12:59Z",
        "published": "2024-03-12T14:12:59Z",
        "authors": [
            "Vjosa Preniqi",
            "Iacopo Ghinassi",
            "Kyriaki Kalimeri",
            "Charalampos Saitis"
        ],
        "categories": [
            "cs.CL",
            "cs.CY"
        ],
        "primary_category": "cs.CL"
    },
    "2403.07680v1": {
        "url": "http://arxiv.org/abs/2403.07680v1",
        "title": "Adapting LoRaWAN to the Open-RAN Architecture",
        "summary": "This article proposes O-LoRaWAN, an adaptation of the LoRaWAN architecture\ninto a modular network architecture based on the Open RAN (O-RAN) principles.\nIn our vision, standardization of the network components and interfaces will\nenable the reuse of network functions, and thus, foster an accelerated\ntailoring of the network functions to the changing application demands. LoRaWAN\nshares similarities to cellular networks and becomes an interesting candidate\nfor a transformation to the O-RAN standard. In the article we draw several\ntransition strategies, these include the reorganization of the LoRa gateway\nfunctions into Radio and Distributed Units; enhancing network performance with\nRAN Intelligent Controllers exploiting the network data; and the\nstandardization of the management and orchestration of network components. Key\nfor that adaptation are the O-RAN interfaces. Along the article, we analyze\nthem and suggest protocol extensions or adjustments for compatibility and\ninteroperability between network components, advocating for the design of\nextensible protocols",
        "updated": "2024-03-12T14:18:19Z",
        "published": "2024-03-12T14:18:19Z",
        "authors": [
            "Sobhi Alfayoumi",
            "Joan Melia-Segui",
            "Xavier Vilajosana"
        ],
        "comments": "8 pages 5 figure",
        "categories": [
            "cs.NI"
        ],
        "primary_category": "cs.NI"
    },
    "2403.07684v1": {
        "url": "http://arxiv.org/abs/2403.07684v1",
        "title": "Genuine Knowledge from Practice: Diffusion Test-Time Adaptation for\n  Video Adverse Weather Removal",
        "summary": "Real-world vision tasks frequently suffer from the appearance of unexpected\nadverse weather conditions, including rain, haze, snow, and raindrops. In the\nlast decade, convolutional neural networks and vision transformers have yielded\noutstanding results in single-weather video removal. However, due to the\nabsence of appropriate adaptation, most of them fail to generalize to other\nweather conditions. Although ViWS-Net is proposed to remove adverse weather\nconditions in videos with a single set of pre-trained weights, it is seriously\nblinded by seen weather at train-time and degenerates when coming to unseen\nweather during test-time. In this work, we introduce test-time adaptation into\nadverse weather removal in videos, and propose the first framework that\nintegrates test-time adaptation into the iterative diffusion reverse process.\nSpecifically, we devise a diffusion-based network with a novel temporal noise\nmodel to efficiently explore frame-correlated information in degraded video\nclips at training stage. During inference stage, we introduce a proxy task\nnamed Diffusion Tubelet Self-Calibration to learn the primer distribution of\ntest video stream and optimize the model by approximating the temporal noise\nmodel for online adaptation. Experimental results, on benchmark datasets,\ndemonstrate that our Test-Time Adaptation method with Diffusion-based\nnetwork(Diff-TTA) outperforms state-of-the-art methods in terms of restoring\nvideos degraded by seen weather conditions. Its generalizable capability is\nalso validated with unseen weather conditions in both synthesized and\nreal-world videos.",
        "updated": "2024-03-12T14:21:30Z",
        "published": "2024-03-12T14:21:30Z",
        "authors": [
            "Yijun Yang",
            "Hongtao Wu",
            "Angelica I. Aviles-Rivero",
            "Yulun Zhang",
            "Jing Qin",
            "Lei Zhu"
        ],
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2403.07687v1": {
        "url": "http://arxiv.org/abs/2403.07687v1",
        "title": "Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model\n  Performance and Annotation Cost",
        "summary": "Current foundation models have shown impressive performance across various\ntasks. However, several studies have revealed that these models are not\neffective for everyone due to the imbalanced geographical and economic\nrepresentation of the data used in the training process. Most of this data\ncomes from Western countries, leading to poor results for underrepresented\ncountries. To address this issue, more data needs to be collected from these\ncountries, but the cost of annotation can be a significant bottleneck. In this\npaper, we propose methods to identify the data to be annotated to balance model\nperformance and annotation costs. Our approach first involves finding the\ncountries with images of topics (objects and actions) most visually distinct\nfrom those already in the training datasets used by current large\nvision-language foundation models. Next, we identify countries with higher\nvisual similarity for these topics and show that using data from these\ncountries to supplement the training data improves model performance and\nreduces annotation costs. The resulting lists of countries and corresponding\ntopics are made available at\nhttps://github.com/MichiganNLP/visual_diversity_budget.",
        "updated": "2024-03-12T14:27:17Z",
        "published": "2024-03-12T14:27:17Z",
        "authors": [
            "Oana Ignat",
            "Longju Bai",
            "Joan Nwatu",
            "Rada Mihalcea"
        ],
        "comments": "accepted at COLING 2024",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "primary_category": "cs.CV"
    },
    "2403.07688v1": {
        "url": "http://arxiv.org/abs/2403.07688v1",
        "title": "Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of\n  Neurons",
        "summary": "When training deep neural networks, the phenomenon of $\\textit{dying\nneurons}$ $\\unicode{x2013}$units that become inactive or saturated, output zero\nduring training$\\unicode{x2013}$ has traditionally been viewed as undesirable,\nlinked with optimization challenges, and contributing to plasticity loss in\ncontinual learning scenarios. In this paper, we reassess this phenomenon,\nfocusing on sparsity and pruning. By systematically exploring the impact of\nvarious hyperparameter configurations on dying neurons, we unveil their\npotential to facilitate simple yet effective structured pruning algorithms. We\nintroduce $\\textit{Demon Pruning}$ (DemP), a method that controls the\nproliferation of dead neurons, dynamically leading to network sparsity.\nAchieved through a combination of noise injection on active units and a\none-cycled schedule regularization strategy, DemP stands out for its simplicity\nand broad applicability. Experiments on CIFAR10 and ImageNet datasets\ndemonstrate that DemP surpasses existing structured pruning techniques,\nshowcasing superior accuracy-sparsity tradeoffs and training speedups. These\nfindings suggest a novel perspective on dying neurons as a valuable resource\nfor efficient model compression and optimization.",
        "updated": "2024-03-12T14:28:06Z",
        "published": "2024-03-12T14:28:06Z",
        "authors": [
            "Simon Dufort-Labb\u00e9",
            "Pierluca D'Oro",
            "Evgenii Nikishin",
            "Razvan Pascanu",
            "Pierre-Luc Bacon",
            "Aristide Baratin"
        ],
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "primary_category": "cs.LG"
    },
    "2403.07690v1": {
        "url": "http://arxiv.org/abs/2403.07690v1",
        "title": "SATDAUG -- A Balanced and Augmented Dataset for Detecting Self-Admitted\n  Technical Debt",
        "summary": "Self-admitted technical debt (SATD) refers to a form of technical debt in\nwhich developers explicitly acknowledge and document the existence of technical\nshortcuts, workarounds, or temporary solutions within the codebase. Over recent\nyears, researchers have manually labeled datasets derived from various software\ndevelopment artifacts: source code comments, messages from the issue tracker\nand pull request sections, and commit messages. These datasets are designed for\ntraining, evaluation, performance validation, and improvement of machine\nlearning and deep learning models to accurately identify SATD instances.\nHowever, class imbalance poses a serious challenge across all the existing\ndatasets, particularly when researchers are interested in categorizing the\nspecific types of SATD. In order to address the scarcity of labeled data for\nSATD \\textit{identification} (i.e., whether an instance is SATD or not) and\n\\textit{categorization} (i.e., which type of SATD is being classified) in\nexisting datasets, we share the \\textit{SATDAUG} dataset, an augmented version\nof existing SATD datasets, including source code comments, issue tracker, pull\nrequests, and commit messages. These augmented datasets have been balanced in\nrelation to the available artifacts and provide a much richer source of labeled\ndata for training machine learning or deep learning models.",
        "updated": "2024-03-12T14:33:53Z",
        "published": "2024-03-12T14:33:53Z",
        "authors": [
            "Edi Sutoyo",
            "Andrea Capiluppi"
        ],
        "comments": "Accepted to be published at the 21st IEEE/ACM International\n  Conference on Mining Software Repositories (MSR 2024)",
        "categories": [
            "cs.SE",
            "cs.CL"
        ],
        "primary_category": "cs.SE"
    },
    "2403.07691v2": {
        "url": "http://arxiv.org/abs/2403.07691v2",
        "title": "ORPO: Monolithic Preference Optimization without Reference Model",
        "summary": "While recent preference alignment algorithms for language models have\ndemonstrated promising results, supervised fine-tuning (SFT) remains imperative\nfor achieving successful convergence. In this paper, we study the crucial role\nof SFT within the context of preference alignment, emphasizing that a minor\npenalty for the disfavored generation style is sufficient for\npreference-aligned SFT. Building on this foundation, we introduce a\nstraightforward and innovative reference model-free monolithic odds ratio\npreference optimization algorithm, ORPO, eliminating the necessity for an\nadditional preference alignment phase. We demonstrate, both empirically and\ntheoretically, that the odds ratio is a sensible choice for contrasting favored\nand disfavored styles during SFT across the diverse sizes from 125M to 7B.\nSpecifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with\nORPO on the UltraFeedback alone surpasses the performance of state-of-the-art\nlanguage models with more than 7B and 13B parameters: achieving up to 12.20% on\n$\\text{AlpacaEval}_{2.0}$ (Figure 1), 66.19% on IFEval (instruction-level\nloose, Table 6), and 7.32 in MT-Bench (Figure 12). We release code and model\ncheckpoints for Mistral-ORPO-$\\alpha$ (7B) and Mistral-ORPO-$\\beta$ (7B).",
        "updated": "2024-03-14T07:47:08Z",
        "published": "2024-03-12T14:34:08Z",
        "authors": [
            "Jiwoo Hong",
            "Noah Lee",
            "James Thorne"
        ],
        "comments": "Preprint",
        "categories": [
            "cs.CL",
            "cs.AI"
        ],
        "primary_category": "cs.CL"
    },
    "2403.07692v2": {
        "url": "http://arxiv.org/abs/2403.07692v2",
        "title": "Masked AutoDecoder is Effective Multi-Task Vision Generalist",
        "summary": "Inspired by the success of general-purpose models in NLP, recent studies\nattempt to unify different vision tasks in the same sequence format and employ\nautoregressive Transformers for sequence prediction. They apply uni-directional\nattention to capture sequential dependencies and generate task sequences\nrecursively. However, such autoregressive Transformers may not fit vision tasks\nwell, as vision task sequences usually lack the sequential dependencies\ntypically observed in natural languages. In this work, we design Masked\nAutoDecoder~(MAD), an effective multi-task vision generalist. MAD consists of\ntwo core designs. First, we develop a parallel decoding framework that\nintroduces bi-directional attention to capture contextual dependencies\ncomprehensively and decode vision task sequences in parallel. Second, we design\na masked sequence modeling approach that learns rich task contexts by masking\nand reconstructing task sequences. In this way, MAD handles all the tasks by a\nsingle network branch and a simple cross-entropy loss with minimal\ntask-specific designs. Extensive experiments demonstrate the great potential of\nMAD as a new paradigm for unifying various vision tasks. MAD achieves superior\nperformance and inference efficiency compared to autoregressive counterparts\nwhile obtaining competitive accuracy with task-specific models. Code will be\nreleased.",
        "updated": "2024-03-14T18:54:46Z",
        "published": "2024-03-12T14:36:52Z",
        "authors": [
            "Han Qiu",
            "Jiaxing Huang",
            "Peng Gao",
            "Lewei Lu",
            "Xiaoqin Zhang",
            "Shijian Lu"
        ],
        "comments": "Accepted by CVPR 2024",
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2403.08682v1": {
        "url": "http://arxiv.org/abs/2403.08682v1",
        "title": "OneVOS: Unifying Video Object Segmentation with All-in-One Transformer\n  Framework",
        "summary": "Contemporary Video Object Segmentation (VOS) approaches typically consist\nstages of feature extraction, matching, memory management, and multiple objects\naggregation. Recent advanced models either employ a discrete modeling for these\ncomponents in a sequential manner, or optimize a combined pipeline through\nsubstructure aggregation. However, these existing explicit staged approaches\nprevent the VOS framework from being optimized as a unified whole, leading to\nthe limited capacity and suboptimal performance in tackling complex videos. In\nthis paper, we propose OneVOS, a novel framework that unifies the core\ncomponents of VOS with All-in-One Transformer. Specifically, to unify all\naforementioned modules into a vision transformer, we model all the features of\nframes, masks and memory for multiple objects as transformer tokens, and\nintegrally accomplish feature extraction, matching and memory management of\nmultiple objects through the flexible attention mechanism. Furthermore, a\nUnidirectional Hybrid Attention is proposed through a double decoupling of the\noriginal attention operation, to rectify semantic errors and ambiguities of\nstored tokens in OneVOS framework. Finally, to alleviate the storage burden and\nexpedite inference, we propose the Dynamic Token Selector, which unveils the\nworking mechanism of OneVOS and naturally leads to a more efficient version of\nOneVOS. Extensive experiments demonstrate the superiority of OneVOS, achieving\nstate-of-the-art performance across 7 datasets, particularly excelling in\ncomplex LVOS and MOSE datasets with 70.1% and 66.4% $J \\& F$ scores, surpassing\nprevious state-of-the-art methods by 4.2% and 7.0%, respectively. And our code\nwill be available for reproducibility and further research.",
        "updated": "2024-03-13T16:38:26Z",
        "published": "2024-03-13T16:38:26Z",
        "authors": [
            "Wanyun Li",
            "Pinxue Guo",
            "Xinyu Zhou",
            "Lingyi Hong",
            "Yangji He",
            "Xiangyu Zheng",
            "Wei Zhang",
            "Wenqiang Zhang"
        ],
        "comments": "19 pages, 7 figures",
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2403.08683v1": {
        "url": "http://arxiv.org/abs/2403.08683v1",
        "title": "Single file motion of robot swarms",
        "summary": "We present experimental results on the single file motion of a group of\nrobots interacting with each other through position sensors. We successfully\nreplicate the fundamental diagram typical of these systems, with a transition\nfrom free flow to congested traffic as the density of the system increases. In\nthe latter scenario we also observe the characteristic stop-and-go waves. The\nunique advantages of this novel system, such as experimental stability and\nrepeatability, allow for extended experimental runs, facilitating a\ncomprehensive statistical analysis of the global dynamics. Above a certain\ndensity, we observe a divergence of the average jam duration and the average\nnumber of robots involved in it. This discovery enables us to precisely\nidentify another transition: from congested intermittent flow (for intermediate\ndensities) to a totally congested scenario for high densities. Beyond this\nfinding, the present work demonstrates the suitability of robot swarms to model\ncomplex behaviors in many particle systems.",
        "updated": "2024-03-13T16:39:32Z",
        "published": "2024-03-13T16:39:32Z",
        "authors": [
            "Laciel Alonso-Llanes",
            "Angel Garcimart\u00edn",
            "Iker Zuriguel"
        ],
        "comments": "5 pages, 4 figures plus supplemental material",
        "categories": [
            "nlin.AO",
            "cond-mat.soft",
            "cs.RO"
        ],
        "primary_category": "nlin.AO"
    },
    "2403.08687v1": {
        "url": "http://arxiv.org/abs/2403.08687v1",
        "title": "Digital Twin-assisted Reinforcement Learning for Resource-aware\n  Microservice Offloading in Edge Computing",
        "summary": "Collaborative edge computing (CEC) has emerged as a promising paradigm,\nenabling edge nodes to collaborate and execute microservices from end devices.\nMicroservice offloading, a fundamentally important problem, decides when and\nwhere microservices are executed upon the arrival of services. However, the\ndynamic nature of the real-world CEC environment often leads to inefficient\nmicroservice offloading strategies, resulting in underutilized resources and\nnetwork congestion. To address this challenge, we formulate an online joint\nmicroservice offloading and bandwidth allocation problem, JMOBA, to minimize\nthe average completion time of services. In this paper, we introduce a novel\nmicroservice offloading algorithm, DTDRLMO, which leverages deep reinforcement\nlearning (DRL) and digital twin technology. Specifically, we employ digital\ntwin techniques to predict and adapt to changing edge node loads and network\nconditions of CEC in real-time. Furthermore, this approach enables the\ngeneration of an efficient offloading plan, selecting the most suitable edge\nnode for each microservice. Simulation results on real-world and synthetic\ndatasets demonstrate that DTDRLMO outperforms heuristic and learning-based\nmethods in average service completion time.",
        "updated": "2024-03-13T16:44:36Z",
        "published": "2024-03-13T16:44:36Z",
        "authors": [
            "Xiangchun Chen",
            "Jiannong Cao",
            "Zhixuan Liang",
            "Yuvraj Sahni",
            "Mingjin Zhang"
        ],
        "comments": "9 pages, 5 figures",
        "categories": [
            "cs.NI",
            "cs.LG"
        ],
        "primary_category": "cs.NI",
        "doi": "10.1109/MASS58611.2023.00012",
        "journal_ref": "2023 IEEE 20th International Conference on Mobile Ad Hoc and Smart\n  Systems (MASS), Toronto, ON, Canada, 2023, pp. 28-36"
    },
    "2403.08688v1": {
        "url": "http://arxiv.org/abs/2403.08688v1",
        "title": "Token Alignment via Character Matching for Subword Completion",
        "summary": "Generative models, widely utilized in various applications, can often\nstruggle with prompts corresponding to partial tokens. This struggle stems from\ntokenization, where partial tokens fall out of distribution during inference,\nleading to incorrect or nonsensical outputs. This paper examines a technique to\nalleviate the tokenization artifact on text completion in generative models,\nmaintaining performance even in regular non-subword cases. The method, termed\ntoken alignment, involves backtracking to the last complete tokens and ensuring\nthe model's generation aligns with the prompt. This approach showcases marked\nimprovement across many partial token scenarios, including nuanced cases like\nspace-prefix and partial indentation, with only a minor time increase. The\ntechnique and analysis detailed in this paper contribute to the continuous\nadvancement of generative models in handling partial inputs, bearing relevance\nfor applications like code completion and text autocompletion.",
        "updated": "2024-03-13T16:44:39Z",
        "published": "2024-03-13T16:44:39Z",
        "authors": [
            "Ben Athiwaratkun",
            "Shiqi Wang",
            "Mingyue Shang",
            "Yuchen Tian",
            "Zijian Wang",
            "Sujan Kumar Gonugondla",
            "Sanjay Krishna Gouda",
            "Rob Kwiatowski",
            "Ramesh Nallapati",
            "Bing Xiang"
        ],
        "categories": [
            "cs.CL",
            "cs.AI"
        ],
        "primary_category": "cs.CL"
    },
    "2403.08689v1": {
        "url": "http://arxiv.org/abs/2403.08689v1",
        "title": "Exploiting Structural Consistency of Chest Anatomy for Unsupervised\n  Anomaly Detection in Radiography Images",
        "summary": "Radiography imaging protocols focus on particular body regions, therefore\nproducing images of great similarity and yielding recurrent anatomical\nstructures across patients. Exploiting this structured information could\npotentially ease the detection of anomalies from radiography images. To this\nend, we propose a Simple Space-Aware Memory Matrix for In-painting and\nDetecting anomalies from radiography images (abbreviated as SimSID). We\nformulate anomaly detection as an image reconstruction task, consisting of a\nspace-aware memory matrix and an in-painting block in the feature space. During\nthe training, SimSID can taxonomize the ingrained anatomical structures into\nrecurrent visual patterns, and in the inference, it can identify anomalies\n(unseen/modified visual patterns) from the test image. Our SimSID surpasses the\nstate of the arts in unsupervised anomaly detection by +8.0%, +5.0%, and +9.9%\nAUC scores on ZhangLab, COVIDx, and CheXpert benchmark datasets, respectively.\nCode: https://github.com/MrGiovanni/SimSID",
        "updated": "2024-03-13T16:44:49Z",
        "published": "2024-03-13T16:44:49Z",
        "authors": [
            "Tiange Xiang",
            "Yixiao Zhang",
            "Yongyi Lu",
            "Alan Yuille",
            "Chaoyi Zhang",
            "Weidong Cai",
            "Zongwei Zhou"
        ],
        "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI). arXiv admin note: substantial text overlap with arXiv:2111.13495",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "primary_category": "eess.IV"
    },
    "2403.08693v1": {
        "url": "http://arxiv.org/abs/2403.08693v1",
        "title": "Do Language Models Care About Text Quality? Evaluating Web-Crawled\n  Corpora Across 11 Languages",
        "summary": "Large, curated, web-crawled corpora play a vital role in training language\nmodels (LMs). They form the lion's share of the training data in virtually all\nrecent LMs, such as the well-known GPT, LLaMA and XLM-RoBERTa models. However,\ndespite this importance, relatively little attention has been given to the\nquality of these corpora. In this paper, we compare four of the currently most\nrelevant large, web-crawled corpora (CC100, MaCoCu, mC4 and OSCAR) across\neleven lower-resourced European languages. Our approach is two-fold: first, we\nperform an intrinsic evaluation by performing a human evaluation of the quality\nof samples taken from different corpora; then, we assess the practical impact\nof the qualitative differences by training specific LMs on each of the corpora\nand evaluating their performance on downstream tasks. We find that there are\nclear differences in quality of the corpora, with MaCoCu and OSCAR obtaining\nthe best results. However, during the extrinsic evaluation, we actually find\nthat the CC100 corpus achieves the highest scores. We conclude that, in our\nexperiments, the quality of the web-crawled corpora does not seem to play a\nsignificant role when training LMs.",
        "updated": "2024-03-13T16:56:33Z",
        "published": "2024-03-13T16:56:33Z",
        "authors": [
            "Rik van Noord",
            "Taja Kuzman",
            "Peter Rupnik",
            "Nikola Ljube\u0161i\u0107",
            "Miquel Espl\u00e0-Gomis",
            "Gema Ram\u00edrez-S\u00e1nchez",
            "Antonio Toral"
        ],
        "comments": "Accepted to LREC-COLING 2024 (long)",
        "categories": [
            "cs.CL"
        ],
        "primary_category": "cs.CL"
    },
    "2403.08695v1": {
        "url": "http://arxiv.org/abs/2403.08695v1",
        "title": "Deep Learning for In-Orbit Cloud Segmentation and Classification in\n  Hyperspectral Satellite Data",
        "summary": "This article explores the latest Convolutional Neural Networks (CNNs) for\ncloud detection aboard hyperspectral satellites. The performance of the latest\n1D CNN (1D-Justo-LiuNet) and two recent 2D CNNs (nnU-net and\n2D-Justo-UNet-Simple) for cloud segmentation and classification is assessed.\nEvaluation criteria include precision and computational efficiency for in-orbit\ndeployment. Experiments utilize NASA's EO-1 Hyperion data, with varying\nspectral channel numbers after Principal Component Analysis. Results indicate\nthat 1D-Justo-LiuNet achieves the highest accuracy, outperforming 2D CNNs,\nwhile maintaining compactness with larger spectral channel sets, albeit with\nincreased inference times. However, the performance of 1D CNN degrades with\nsignificant channel reduction. In this context, the 2D-Justo-UNet-Simple offers\nthe best balance for in-orbit deployment, considering precision, memory, and\ntime costs. While nnU-net is suitable for on-ground processing, deployment of\nlightweight 1D-Justo-LiuNet is recommended for high-precision applications.\nAlternatively, lightweight 2D-Justo-UNet-Simple is recommended for balanced\ncosts between timing and precision in orbit.",
        "updated": "2024-03-13T16:58:37Z",
        "published": "2024-03-13T16:58:37Z",
        "authors": [
            "Daniel Kovac",
            "Jan Mucha",
            "Jon Alvarez Justo",
            "Jiri Mekyska",
            "Zoltan Galaz",
            "Krystof Novotny",
            "Radoslav Pitonak",
            "Jan Knezik",
            "Jonas Herec",
            "Tor Arne Johansen"
        ],
        "comments": "Hyperspectral Satellite Data, Cloud Segmentation, Classification,\n  Convolutional Neural Networks, Principal Component Analysis",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "primary_category": "cs.CV"
    },
    "2403.08699v1": {
        "url": "http://arxiv.org/abs/2403.08699v1",
        "title": "Implicit Regularization of Gradient Flow on One-Layer Softmax Attention",
        "summary": "We study gradient flow on the exponential loss for a classification problem\nwith a one-layer softmax attention model, where the key and query weight\nmatrices are trained separately. Under a separability assumption on the data,\nwe show that when gradient flow achieves the minimal loss value, it further\nimplicitly minimizes the nuclear norm of the product of the key and query\nweight matrices. Such implicit regularization can be described by a Support\nVector Machine (SVM) problem with respect to the attention weights. This\nfinding contrasts with prior results showing that the gradient descent induces\nan implicit regularization on the Frobenius norm on the product weight matrix\nwhen the key and query matrices are combined into a single weight matrix for\ntraining. For diagonal key and query matrices, our analysis builds upon the\nreparameterization technique and exploits approximate KKT conditions of the SVM\nassociated with the classification data. Moreover, the results are extended to\ngeneral weights configurations given proper alignment of the weight matrices'\nsingular spaces with the data features at initialization.",
        "updated": "2024-03-13T17:02:27Z",
        "published": "2024-03-13T17:02:27Z",
        "authors": [
            "Heejune Sheen",
            "Siyu Chen",
            "Tianhao Wang",
            "Harrison H. Zhou"
        ],
        "comments": "34 pages",
        "categories": [
            "cs.LG",
            "cs.AI",
            "math.OC",
            "stat.ML"
        ],
        "primary_category": "cs.LG"
    },
    "2403.08700v1": {
        "url": "http://arxiv.org/abs/2403.08700v1",
        "title": "Diffusion-based Iterative Counterfactual Explanations for Fetal\n  Ultrasound Image Quality Assessment",
        "summary": "Obstetric ultrasound image quality is crucial for accurate diagnosis and\nmonitoring of fetal health. However, producing high-quality standard planes is\ndifficult, influenced by the sonographer's expertise and factors like the\nmaternal BMI or the fetus dynamics. In this work, we propose using\ndiffusion-based counterfactual explainable AI to generate realistic\nhigh-quality standard planes from low-quality non-standard ones. Through\nquantitative and qualitative evaluation, we demonstrate the effectiveness of\nour method in producing plausible counterfactuals of increased quality. This\nshows future promise both for enhancing training of clinicians by providing\nvisual feedback, as well as for improving image quality and, consequently,\ndownstream diagnosis and monitoring.",
        "updated": "2024-03-13T17:04:56Z",
        "published": "2024-03-13T17:04:56Z",
        "authors": [
            "Paraskevas Pegios",
            "Manxi Lin",
            "Nina Weng",
            "Morten Bo S\u00f8ndergaard Svendsen",
            "Zahra Bashir",
            "Siavash Bigdeli",
            "Anders Nymark Christensen",
            "Martin Tolsgaard",
            "Aasa Feragen"
        ],
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.HC",
            "cs.LG"
        ],
        "primary_category": "eess.IV"
    },
    "2403.08701v2": {
        "url": "http://arxiv.org/abs/2403.08701v2",
        "title": "Review of Generative AI Methods in Cybersecurity",
        "summary": "Over the last decade, Artificial Intelligence (AI) has become increasingly\npopular, especially with the use of chatbots such as ChatGPT, Gemini, and\nDALL-E. With this rise, large language models (LLMs) and Generative AI (GenAI)\nhave also become more prevalent in everyday use. These advancements strengthen\ncybersecurity's defensive posture and open up new attack avenues for\nadversaries as well. This paper provides a comprehensive overview of the\ncurrent state-of-the-art deployments of GenAI, covering assaults, jailbreaking,\nand applications of prompt injection and reverse psychology. This paper also\nprovides the various applications of GenAI in cybercrimes, such as automated\nhacking, phishing emails, social engineering, reverse cryptography, creating\nattack payloads, and creating malware. GenAI can significantly improve the\nautomation of defensive cyber security processes through strategies such as\ndataset construction, safe code development, threat intelligence, defensive\nmeasures, reporting, and cyberattack detection. In this study, we suggest that\nfuture research should focus on developing robust ethical norms and innovative\ndefense mechanisms to address the current issues that GenAI creates and to also\nfurther encourage an impartial approach to its future application in\ncybersecurity. Moreover, we underscore the importance of interdisciplinary\napproaches further to bridge the gap between scientific developments and\nethical considerations.",
        "updated": "2024-03-19T15:21:20Z",
        "published": "2024-03-13T17:05:05Z",
        "authors": [
            "Yagmur Yigit",
            "William J Buchanan",
            "Madjid G Tehrani",
            "Leandros Maglaras"
        ],
        "comments": "40 pages",
        "categories": [
            "cs.CR"
        ],
        "primary_category": "cs.CR"
    },
    "2403.09728v1": {
        "url": "http://arxiv.org/abs/2403.09728v1",
        "title": "Simulating Weighted Automata over Sequences and Trees with Transformers",
        "summary": "Transformers are ubiquitous models in the natural language processing (NLP)\ncommunity and have shown impressive empirical successes in the past few years.\nHowever, little is understood about how they reason and the limits of their\ncomputational capabilities. These models do not process data sequentially, and\nyet outperform sequential neural models such as RNNs. Recent work has shown\nthat these models can compactly simulate the sequential reasoning abilities of\ndeterministic finite automata (DFAs). This leads to the following question: can\ntransformers simulate the reasoning of more complex finite state machines? In\nthis work, we show that transformers can simulate weighted finite automata\n(WFAs), a class of models which subsumes DFAs, as well as weighted tree\nautomata (WTA), a generalization of weighted automata to tree structured\ninputs. We prove these claims formally and provide upper bounds on the sizes of\nthe transformer models needed as a function of the number of states the target\nautomata. Empirically, we perform synthetic experiments showing that\ntransformers are able to learn these compact solutions via standard\ngradient-based training.",
        "updated": "2024-03-12T21:54:34Z",
        "published": "2024-03-12T21:54:34Z",
        "authors": [
            "Michael Rizvi",
            "Maude Lizaire",
            "Clara Lacroce",
            "Guillaume Rabusseau"
        ],
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CC"
        ],
        "primary_category": "cs.CL"
    },
    "2403.09732v3": {
        "url": "http://arxiv.org/abs/2403.09732v3",
        "title": "PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with\n  Cross-consistency",
        "summary": "Recent advancements in Text-to-SQL (Text2SQL) emphasize stimulating the large\nlanguage models (LLM) on in-context learning, achieving significant results.\nNevertheless, they face challenges when dealing with verbose database\ninformation and complex user intentions. This paper presents a two-stage\nframework to enhance the performance of current LLM-based natural language to\nSQL systems. We first introduce a novel prompt representation, called\nreference-enhanced representation, which includes schema information and\nrandomly sampled cell values from tables to instruct LLMs in generating SQL\nqueries. Then, in the first stage, question-SQL pairs are retrieved as few-shot\ndemonstrations, prompting the LLM to generate a preliminary SQL (PreSQL). After\nthat, the mentioned entities in PreSQL are parsed to conduct schema linking,\nwhich can significantly compact the useful information. In the second stage,\nwith the linked schema, we simplify the prompt's schema information and\ninstruct the LLM to produce the final SQL. Finally, as the post-refinement\nmodule, we propose using cross-consistency across different LLMs rather than\nself-consistency within a particular LLM. Our methods achieve new SOTA results\non the Spider benchmark, with an execution accuracy of 87.6%.",
        "updated": "2024-03-29T03:21:01Z",
        "published": "2024-03-13T02:32:41Z",
        "authors": [
            "Zhishuai Li",
            "Xiang Wang",
            "Jingjing Zhao",
            "Sun Yang",
            "Guoqing Du",
            "Xiaoru Hu",
            "Bin Zhang",
            "Yuxiao Ye",
            "Ziyue Li",
            "Rui Zhao",
            "Hangyu Mao"
        ],
        "categories": [
            "cs.CL",
            "cs.AI"
        ],
        "primary_category": "cs.CL"
    },
    "2403.09733v1": {
        "url": "http://arxiv.org/abs/2403.09733v1",
        "title": "OverleafCopilot: Empowering Academic Writing in Overleaf with Large\n  Language Models",
        "summary": "The rapid development of Large Language Models (LLMs) has facilitated a\nvariety of applications from different domains. In this technical report, we\nexplore the integration of LLMs and the popular academic writing tool,\nOverleaf, to enhance the efficiency and quality of academic writing. To achieve\nthe above goal, there are three challenges: i) including seamless interaction\nbetween Overleaf and LLMs, ii) establishing reliable communication with the LLM\nprovider, and iii) ensuring user privacy. To address these challenges, we\npresent OverleafCopilot, the first-ever tool (i.e., a browser extension) that\nseamlessly integrates LLMs and Overleaf, enabling researchers to leverage the\npower of LLMs while writing papers. Specifically, we first propose an effective\nframework to bridge LLMs and Overleaf. Then, we developed PromptGenius, a\nwebsite for researchers to easily find and share high-quality up-to-date\nprompts. Thirdly, we propose an agent command system to help researchers\nquickly build their customizable agents. OverleafCopilot\n(https://chromewebstore.google.com/detail/overleaf-copilot/eoadabdpninlhkkbhngoddfjianhlghb\n) has been on the Chrome Extension Store, which now serves thousands of\nresearchers. Additionally, the code of PromptGenius is released at\nhttps://github.com/wenhaomin/ChatGPT-PromptGenius. We believe our work has the\npotential to revolutionize academic writing practices, empowering researchers\nto produce higher-quality papers in less time.",
        "updated": "2024-03-13T07:52:31Z",
        "published": "2024-03-13T07:52:31Z",
        "authors": [
            "Haomin Wen",
            "Zhenjie Wei",
            "Yan Lin",
            "Jiyuan Wang",
            "Yuxuan Liang",
            "Huaiyu Wan"
        ],
        "categories": [
            "cs.CL",
            "cs.AI"
        ],
        "primary_category": "cs.CL"
    },
    "2403.09735v1": {
        "url": "http://arxiv.org/abs/2403.09735v1",
        "title": "A Sophisticated Framework for the Accurate Detection of Phishing\n  Websites",
        "summary": "Phishing is an increasingly sophisticated form of cyberattack that is\ninflicting huge financial damage to corporations throughout the globe while\nalso jeopardizing individuals' privacy. Attackers are constantly devising new\nmethods of launching such assaults and detecting them has become a daunting\ntask. Many different techniques have been suggested, each with its own pros and\ncons. While machine learning-based techniques have been most successful in\nidentifying such attacks, they continue to fall short in terms of performance\nand generalizability. This paper proposes a comprehensive methodology for\ndetecting phishing websites. The goal is to design a system that is capable of\naccurately distinguishing phishing websites from legitimate ones and provides\ngeneralized performance over a broad variety of datasets. A combination of\nfeature selection, greedy algorithm, cross-validation, and deep learning\nmethods have been utilized to construct a sophisticated stacking ensemble\nclassifier. Extensive experimentation on four different phishing datasets was\nconducted to evaluate the performance of the proposed technique. The proposed\nalgorithm outperformed the other existing phishing detection models obtaining\naccuracy of 97.49%, 98.23%, 97.48%, and 98.20% on dataset-1 (UCI Phishing\nWebsites Dataset), dataset-2 (Phishing Dataset for Machine Learning: Feature\nEvaluation), dataset-3 (Phishing Websites Dataset), and dataset-4 (Web page\nphishing detection), respectively. The high accuracy values obtained across all\ndatasets imply the models' generalizability and effectiveness in the accurate\nidentification of phishing websites.",
        "updated": "2024-03-13T14:26:25Z",
        "published": "2024-03-13T14:26:25Z",
        "authors": [
            "Asif Newaz",
            "Farhan Shahriyar Haq",
            "Nadim Ahmed"
        ],
        "categories": [
            "cs.CR",
            "cs.AI"
        ],
        "primary_category": "cs.CR"
    },
    "2403.09738v4": {
        "url": "http://arxiv.org/abs/2403.09738v4",
        "title": "Evaluating Large Language Models as Generative User Simulators for\n  Conversational Recommendation",
        "summary": "Synthetic users are cost-effective proxies for real users in the evaluation\nof conversational recommender systems. Large language models show promise in\nsimulating human-like behavior, raising the question of their ability to\nrepresent a diverse population of users. We introduce a new protocol to measure\nthe degree to which language models can accurately emulate human behavior in\nconversational recommendation. This protocol is comprised of five tasks, each\ndesigned to evaluate a key property that a synthetic user should exhibit:\nchoosing which items to talk about, expressing binary preferences, expressing\nopen-ended preferences, requesting recommendations, and giving feedback.\nThrough evaluation of baseline simulators, we demonstrate these tasks\neffectively reveal deviations of language models from human behavior, and offer\ninsights on how to reduce the deviations with model selection and prompting\nstrategies.",
        "updated": "2024-03-25T23:53:01Z",
        "published": "2024-03-13T18:16:21Z",
        "authors": [
            "Se-eun Yoon",
            "Zhankui He",
            "Jessica Maria Echterhoff",
            "Julian McAuley"
        ],
        "comments": "NAACL 2024",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.IR"
        ],
        "primary_category": "cs.CL"
    },
    "2403.09740v1": {
        "url": "http://arxiv.org/abs/2403.09740v1",
        "title": "Teaching Machines to Code: Smart Contract Translation with LLMs",
        "summary": "The advent of large language models (LLMs) has marked a significant milestone\nin the realm of artificial intelligence, with their capabilities often matching\nor surpassing human expertise in various domains. Among these achievements,\ntheir adeptness in translation tasks stands out, closely mimicking the\nintricate and preliminary processes undertaken by human translators to ensure\nthe fidelity and quality of the translated content. Despite the advancements in\nutilizing LLMs for translating programming code across different languages, the\ndomain of smart contract translation, particularly into languages not\npreviously encountered by the LLM, remains largely unexplored. In our research,\nwe present a pioneering approach, SolMover, which harnesses the synergy of two\ndistinct LLMs within a unified framework. This framework is designed to grasp\ncoding principles and apply this understanding to the translation of code into\nan unfamiliar language. Our study delves into the capacity of LLMs to mimic\nhuman learning processes, offering an in-depth evaluation of our methodology\nfor converting smart contracts written in Solidity to Move, a language with\nlimited resources. The framework employs one LLM to decipher coding conventions\nfor the new language, creating a blueprint for the second LLM, which, lacking\nplanning abilities, possesses coding expertise. The empirical evidence from our\nexperiments suggests that SolMover substantially enhances performance compared\nto gpt-3.5-turbo-1106, and achieves superior results over competitors such as\nPalm2 and Mixtral-8x7B-Instruct. Additionally, our analysis highlights the\nefficacy of our bug mitigation strategy in elevating code quality across all\nmodels, even outside the SolMover framework.",
        "updated": "2024-03-13T18:55:20Z",
        "published": "2024-03-13T18:55:20Z",
        "authors": [
            "Rabimba Karanjai",
            "Lei Xu",
            "Weidong Shi"
        ],
        "categories": [
            "cs.SE",
            "cs.AI"
        ],
        "primary_category": "cs.SE"
    },
    "2403.09742v1": {
        "url": "http://arxiv.org/abs/2403.09742v1",
        "title": "A Short Review on Novel Approaches for Maximum Clique Problem: from\n  Classical algorithms to Graph Neural Networks and Quantum algorithms",
        "summary": "This manuscript provides a comprehensive review of the Maximum Clique\nProblem, a computational problem that involves finding subsets of vertices in a\ngraph that are all pairwise adjacent to each other. The manuscript covers in a\nsimple way classical algorithms for solving the problem and includes a review\nof recent developments in graph neural networks and quantum algorithms. The\nreview concludes with benchmarks for testing classical as well as new learning,\nand quantum algorithms.",
        "updated": "2024-03-13T20:12:05Z",
        "published": "2024-03-13T20:12:05Z",
        "authors": [
            "Raffaele Marino",
            "Lorenzo Buffoni",
            "Bogdan Zavalnij"
        ],
        "comments": "24 pages",
        "categories": [
            "cs.AI",
            "cond-mat.dis-nn",
            "cs.DS",
            "cs.LG",
            "math.OC",
            "quant-ph"
        ],
        "primary_category": "cs.AI"
    },
    "2403.09743v1": {
        "url": "http://arxiv.org/abs/2403.09743v1",
        "title": "The Human Factor in Detecting Errors of Large Language Models: A\n  Systematic Literature Review and Future Research Directions",
        "summary": "The launch of ChatGPT by OpenAI in November 2022 marked a pivotal moment for\nArtificial Intelligence, introducing Large Language Models (LLMs) to the\nmainstream and setting new records in user adoption. LLMs, particularly\nChatGPT, trained on extensive internet data, demonstrate remarkable\nconversational capabilities across various domains, suggesting a significant\nimpact on the workforce. However, these models are susceptible to errors -\n\"hallucinations\" and omissions, generating incorrect or incomplete information.\nThis poses risks especially in contexts where accuracy is crucial, such as\nlegal compliance, medicine or fine-grained process frameworks.\n  There are both technical and human solutions to cope with this isse. This\npaper explores the human factors that enable users to detect errors in LLM\noutputs, a critical component in mitigating risks associated with their use in\nprofessional settings. Understanding these factors is essential for\norganizations aiming to leverage LLM technology efficiently, guiding targeted\ntraining and deployment strategies to enhance error detection by users. This\napproach not only aims to optimize the use of LLMs but also to prevent\npotential downstream issues stemming from reliance on inaccurate model\nresponses. The research emphasizes the balance between technological\nadvancement and human insight in maximizing the benefits of LLMs while\nminimizing the risks, particularly in areas where precision is paramount.\n  This paper performs a systematic literature research on this research topic,\nanalyses and synthesizes the findings, and outlines future research directions.\nLiterature selection cut-off date is January 11th 2024.",
        "updated": "2024-03-13T21:39:39Z",
        "published": "2024-03-13T21:39:39Z",
        "authors": [
            "Christian A. Schiller"
        ],
        "comments": "21 papers analysed and synthesized in detail from a total search\n  result size of 594 (raw results) / 61 (scanned) / 28 (selected)",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "primary_category": "cs.CL"
    },
    "2403.09744v1": {
        "url": "http://arxiv.org/abs/2403.09744v1",
        "title": "Evaluating the Application of Large Language Models to Generate Feedback\n  in Programming Education",
        "summary": "This study investigates the application of large language models,\nspecifically GPT-4, to enhance programming education. The research outlines the\ndesign of a web application that uses GPT-4 to provide feedback on programming\ntasks, without giving away the solution. A web application for working on\nprogramming tasks was developed for the study and evaluated with 51 students\nover the course of one semester. The results show that most of the feedback\ngenerated by GPT-4 effectively addressed code errors. However, challenges with\nincorrect suggestions and hallucinated issues indicate the need for further\nimprovements.",
        "updated": "2024-03-13T23:14:35Z",
        "published": "2024-03-13T23:14:35Z",
        "authors": [
            "Sven Jacobs",
            "Steffen Jaschke"
        ],
        "comments": "accepted at IEEE Global Engineering Education Conference 2024, Kos,\n  Greece",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CY",
            "cs.HC"
        ],
        "primary_category": "cs.CL"
    },
    "2403.09746v1": {
        "url": "http://arxiv.org/abs/2403.09746v1",
        "title": "PICNIQ: Pairwise Comparisons for Natural Image Quality Assessment",
        "summary": "Blind image quality assessment (BIQA) approaches, while promising for\nautomating image quality evaluation, often fall short in real-world scenarios\ndue to their reliance on a generic quality standard applied uniformly across\ndiverse images. This one-size-fits-all approach overlooks the crucial\nperceptual relationship between image content and quality, leading to a 'domain\nshift' challenge where a single quality metric inadequately represents various\ncontent types. Furthermore, BIQA techniques typically overlook the inherent\ndifferences in the human visual system among different observers. In response\nto these challenges, this paper introduces PICNIQ, an innovative pairwise\ncomparison framework designed to bypass the limitations of conventional BIQA by\nemphasizing relative, rather than absolute, quality assessment. PICNIQ is\nspecifically designed to assess the quality differences between image pairs.\nThe proposed framework implements a carefully crafted deep learning\narchitecture, a specialized loss function, and a training strategy optimized\nfor sparse comparison settings. By employing psychometric scaling algorithms\nlike TrueSkill, PICNIQ transforms pairwise comparisons into\njust-objectionable-difference (JOD) quality scores, offering a granular and\ninterpretable measure of image quality. We conduct our research using\ncomparison matrices from the PIQ23 dataset, which are published in this paper.\nOur extensive experimental analysis showcases PICNIQ's broad applicability and\nsuperior performance over existing models, highlighting its potential to set\nnew standards in the field of BIQA.",
        "updated": "2024-03-13T23:43:36Z",
        "published": "2024-03-13T23:43:36Z",
        "authors": [
            "Nicolas Chahine",
            "Sira Ferradans",
            "Jean Ponce"
        ],
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2403.10781v1": {
        "url": "http://arxiv.org/abs/2403.10781v1",
        "title": "Exploring Chinese Humor Generation: A Study on Two-Part Allegorical\n  Sayings",
        "summary": "Humor, a culturally nuanced aspect of human language, poses challenges for\ncomputational understanding and generation, especially in Chinese humor, which\nremains relatively unexplored in the NLP community. This paper investigates the\ncapability of state-of-the-art language models to comprehend and generate\nChinese humor, specifically focusing on training them to create allegorical\nsayings. We employ two prominent training methods: fine-tuning a medium-sized\nlanguage model and prompting a large one. Our novel fine-tuning approach\nincorporates fused Pinyin embeddings to consider homophones and employs\ncontrastive learning with synthetic hard negatives to distinguish humor\nelements. Human-annotated results show that these models can generate humorous\nallegorical sayings, with prompting proving to be a practical and effective\nmethod. However, there is still room for improvement in generating allegorical\nsayings that match human creativity.",
        "updated": "2024-03-16T02:58:57Z",
        "published": "2024-03-16T02:58:57Z",
        "authors": [
            "Rongwu Xu"
        ],
        "categories": [
            "cs.CL",
            "cs.AI"
        ],
        "primary_category": "cs.CL"
    },
    "2403.10782v1": {
        "url": "http://arxiv.org/abs/2403.10782v1",
        "title": "Bidirectional Multi-Step Domain Generalization for Visible-Infrared\n  Person Re-Identification",
        "summary": "A key challenge in visible-infrared person re-identification (V-I ReID) is\ntraining a backbone model capable of effectively addressing the significant\ndiscrepancies across modalities. State-of-the-art methods that generate a\nsingle intermediate bridging domain are often less effective, as this generated\ndomain may not adequately capture sufficient common discriminant information.\nThis paper introduces the Bidirectional Multi-step Domain Generalization\n(BMDG), a novel approach for unifying feature representations across diverse\nmodalities. BMDG creates multiple virtual intermediate domains by finding and\naligning body part features extracted from both I and V modalities. Indeed,\nBMDG aims to reduce the modality gaps in two steps. First, it aligns modalities\nin feature space by learning shared and modality-invariant body part prototypes\nfrom V and I images. Then, it generalizes the feature representation by\napplying bidirectional multi-step learning, which progressively refines feature\nrepresentations in each step and incorporates more prototypes from both\nmodalities. In particular, our method minimizes the cross-modal gap by\nidentifying and aligning shared prototypes that capture key discriminative\nfeatures across modalities, then uses multiple bridging steps based on this\ninformation to enhance the feature representation. Experiments conducted on\nchallenging V-I ReID datasets indicate that our BMDG approach outperforms\nstate-of-the-art part-based models or methods that generate an intermediate\ndomain from V-I person ReID.",
        "updated": "2024-03-16T03:03:27Z",
        "published": "2024-03-16T03:03:27Z",
        "authors": [
            "Mahdi Alehdaghi",
            "Pourya Shamsolmoali",
            "Rafael M. O. Cruz",
            "Eric Granger"
        ],
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2403.10783v1": {
        "url": "http://arxiv.org/abs/2403.10783v1",
        "title": "StableGarment: Garment-Centric Generation via Stable Diffusion",
        "summary": "In this paper, we introduce StableGarment, a unified framework to tackle\ngarment-centric(GC) generation tasks, including GC text-to-image, controllable\nGC text-to-image, stylized GC text-to-image, and robust virtual try-on. The\nmain challenge lies in retaining the intricate textures of the garment while\nmaintaining the flexibility of pre-trained Stable Diffusion. Our solution\ninvolves the development of a garment encoder, a trainable copy of the\ndenoising UNet equipped with additive self-attention (ASA) layers. These ASA\nlayers are specifically devised to transfer detailed garment textures, also\nfacilitating the integration of stylized base models for the creation of\nstylized images. Furthermore, the incorporation of a dedicated try-on\nControlNet enables StableGarment to execute virtual try-on tasks with\nprecision. We also build a novel data engine that produces high-quality\nsynthesized data to preserve the model's ability to follow prompts. Extensive\nexperiments demonstrate that our approach delivers state-of-the-art (SOTA)\nresults among existing virtual try-on methods and exhibits high flexibility\nwith broad potential applications in various garment-centric image generation.",
        "updated": "2024-03-16T03:05:07Z",
        "published": "2024-03-16T03:05:07Z",
        "authors": [
            "Rui Wang",
            "Hailong Guo",
            "Jiaming Liu",
            "Huaxia Li",
            "Haibo Zhao",
            "Xu Tang",
            "Yao Hu",
            "Hao Tang",
            "Peipei Li"
        ],
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2403.10784v1": {
        "url": "http://arxiv.org/abs/2403.10784v1",
        "title": "Identifying Optimal Launch Sites of High-Altitude Latex-Balloons using\n  Bayesian Optimisation for the Task of Station-Keeping",
        "summary": "Station-keeping tasks for high-altitude balloons show promise in areas such\nas ecological surveys, atmospheric analysis, and communication relays. However,\nidentifying the optimal time and position to launch a latex high-altitude\nballoon is still a challenging and multifaceted problem. For example, tasks\nsuch as forest fire tracking place geometric constraints on the launch location\nof the balloon. Furthermore, identifying the most optimal location also heavily\ndepends on atmospheric conditions. We first illustrate how reinforcement\nlearning-based controllers, frequently used for station-keeping tasks, can\nexploit the environment. This exploitation can degrade performance on unseen\nweather patterns and affect station-keeping performance when identifying an\noptimal launch configuration. Valuing all states equally in the region, the\nagent exploits the region's geometry by flying near the edge, leading to risky\nbehaviours. We propose a modification which compensates for this exploitation\nand finds this leads to, on average, higher steps within the target region on\nunseen data. Then, we illustrate how Bayesian Optimisation (BO) can identify\nthe optimal launch location to perform station-keeping tasks, maximising the\nexpected undiscounted return from a given rollout. We show BO can find this\nlaunch location in fewer steps compared to other optimisation methods. Results\nindicate that, surprisingly, the most optimal location to launch from is not\ncommonly within the target region. Please find further information about our\nproject at https://sites.google.com/view/bo-lauch-balloon/.",
        "updated": "2024-03-16T03:07:28Z",
        "published": "2024-03-16T03:07:28Z",
        "authors": [
            "Jack Saunders",
            "Sajad Saeedi",
            "Adam Hartshorne",
            "Binbin Xu",
            "\u00d6zgur \u015eim\u015fek",
            "Alan Hunter",
            "Wenbin Li"
        ],
        "categories": [
            "cs.RO"
        ],
        "primary_category": "cs.RO"
    },
    "2403.10786v1": {
        "url": "http://arxiv.org/abs/2403.10786v1",
        "title": "ContourDiff: Unpaired Image Translation with Contour-Guided Diffusion\n  Models",
        "summary": "Accurately translating medical images across different modalities (e.g., CT\nto MRI) has numerous downstream clinical and machine learning applications.\nWhile several methods have been proposed to achieve this, they often prioritize\nperceptual quality with respect to output domain features over preserving\nanatomical fidelity. However, maintaining anatomy during translation is\nessential for many tasks, e.g., when leveraging masks from the input domain to\ndevelop a segmentation model with images translated to the output domain. To\naddress these challenges, we propose ContourDiff, a novel framework that\nleverages domain-invariant anatomical contour representations of images. These\nrepresentations are simple to extract from images, yet form precise spatial\nconstraints on their anatomical content. We introduce a diffusion model that\nconverts contour representations of images from arbitrary input domains into\nimages in the output domain of interest. By applying the contour as a\nconstraint at every diffusion sampling step, we ensure the preservation of\nanatomical content. We evaluate our method by training a segmentation model on\nimages translated from CT to MRI with their original CT masks and testing its\nperformance on real MRIs. Our method outperforms other unpaired image\ntranslation methods by a significant margin, furthermore without the need to\naccess any input domain information during training.",
        "updated": "2024-03-16T03:33:52Z",
        "published": "2024-03-16T03:33:52Z",
        "authors": [
            "Yuwen Chen",
            "Nicholas Konz",
            "Hanxue Gu",
            "Haoyu Dong",
            "Yaqian Chen",
            "Lin Li",
            "Jisoo Lee",
            "Maciej A. Mazurowski"
        ],
        "comments": "Code will be released on GitHub",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "primary_category": "eess.IV"
    },
    "2403.10787v1": {
        "url": "http://arxiv.org/abs/2403.10787v1",
        "title": "Time Series Representation Learning with Supervised Contrastive Temporal\n  Transformer",
        "summary": "Finding effective representations for time series data is a useful but\nchallenging task. Several works utilize self-supervised or unsupervised\nlearning methods to address this. However, there still remains the open\nquestion of how to leverage available label information for better\nrepresentations. To answer this question, we exploit pre-existing techniques in\ntime series and representation learning domains and develop a simple, yet novel\nfusion model, called: \\textbf{S}upervised \\textbf{CO}ntrastive\n\\textbf{T}emporal \\textbf{T}ransformer (SCOTT). We first investigate suitable\naugmentation methods for various types of time series data to assist with\nlearning change-invariant representations. Secondly, we combine Transformer and\nTemporal Convolutional Networks in a simple way to efficiently learn both\nglobal and local features. Finally, we simplify Supervised Contrastive Loss for\nrepresentation learning of labelled time series data. We preliminarily evaluate\nSCOTT on a downstream task, Time Series Classification, using 45 datasets from\nthe UCR archive. The results show that with the representations learnt by\nSCOTT, even a weak classifier can perform similar to or better than existing\nstate-of-the-art models (best performance on 23/45 datasets and highest rank\nagainst 9 baseline models). Afterwards, we investigate SCOTT's ability to\naddress a real-world task, online Change Point Detection (CPD), on two\ndatasets: a human activity dataset and a surgical patient dataset. We show that\nthe model performs with high reliability and efficiency on the online CPD\nproblem ($\\sim$98\\% and $\\sim$97\\% area under precision-recall curve\nrespectively). Furthermore, we demonstrate the model's potential in tackling\nearly detection and show it performs best compared to other candidates.",
        "updated": "2024-03-16T03:37:19Z",
        "published": "2024-03-16T03:37:19Z",
        "authors": [
            "Yuansan Liu",
            "Sudanthi Wijewickrema",
            "Christofer Bester",
            "Stephen O'Leary",
            "James Bailey"
        ],
        "comments": "8 pages, 8 figures, IJCNN 2024",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "primary_category": "cs.LG"
    },
    "2403.10789v1": {
        "url": "http://arxiv.org/abs/2403.10789v1",
        "title": "Adversarial Knapsack and Secondary Effects of Common Information for\n  Cyber Operations",
        "summary": "Variations of the Flip-It game have been applied to model network cyber\noperations. While Flip-It can accurately express uncertainty and loss of\ncontrol, it imposes no essential resource constraints for operations. Capture\nthe flag (CTF) style competitive games, such as Flip-It , entail uncertainties\nand loss of control, but also impose realistic constraints on resource use. As\nsuch, they bear a closer resemblance to actual cyber operations. We formalize a\ndynamical network control game for CTF competitions and detail the static game\nfor each time step. The static game can be reformulated as instances of a novel\noptimization problem called Adversarial Knapsack (AK) or Dueling Knapsack (DK)\nwhen there are only two players. We define the Adversarial Knapsack\noptimization problems as a system of interacting Weighted Knapsack problems,\nand illustrate its applications to general scenarios involving multiple agents\nwith conflicting optimization goals, e.g., cyber operations and CTF games in\nparticular. Common awareness of the scenario, rewards, and costs will set the\nstage for a non-cooperative game. Critically, rational players may second guess\nthat their AK solution -- with a better response and higher reward -- is\npossible if opponents predictably play their AK optimal solutions. Thus,\nsecondary reasoning which such as belief modeling of opponents play can be\nanticipated for rational players and will introduce a type of non-stability\nwhere players maneuver for slight reward differentials. To analyze this, we\nprovide the best-response algorithms and simulation software to consider how\nrational agents may heuristically search for maneuvers. We further summarize\ninsights offered by the game model by predicting that metrics such as Common\nVulnerability Scoring System (CVSS) may intensify the secondary reasoning in\ncyber operations.",
        "updated": "2024-03-16T03:41:12Z",
        "published": "2024-03-16T03:41:12Z",
        "authors": [
            "Jon Goohs",
            "Georgel Savin",
            "Lucas Starks",
            "Josiah Dykstra",
            "William Casey"
        ],
        "comments": "26 pages",
        "categories": [
            "cs.CR"
        ],
        "primary_category": "cs.CR"
    },
    "2403.10790v1": {
        "url": "http://arxiv.org/abs/2403.10790v1",
        "title": "QuantumLeak: Stealing Quantum Neural Networks from Cloud-based NISQ\n  Machines",
        "summary": "Variational quantum circuits (VQCs) have become a powerful tool for\nimplementing Quantum Neural Networks (QNNs), addressing a wide range of complex\nproblems. Well-trained VQCs serve as valuable intellectual assets hosted on\ncloud-based Noisy Intermediate Scale Quantum (NISQ) computers, making them\nsusceptible to malicious VQC stealing attacks. However, traditional model\nextraction techniques designed for classical machine learning models encounter\nchallenges when applied to NISQ computers due to significant noise in current\ndevices. In this paper, we introduce QuantumLeak, an effective and accurate QNN\nmodel extraction technique from cloud-based NISQ machines. Compared to existing\nclassical model stealing techniques, QuantumLeak improves local VQC accuracy by\n4.99\\%$\\sim$7.35\\% across diverse datasets and VQC architectures.",
        "updated": "2024-03-16T03:42:29Z",
        "published": "2024-03-16T03:42:29Z",
        "authors": [
            "Zhenxiao Fu",
            "Min Yang",
            "Cheng Chu",
            "Yilun Xu",
            "Gang Huang",
            "Fan Chen"
        ],
        "categories": [
            "quant-ph",
            "cs.CR",
            "cs.LG"
        ],
        "primary_category": "quant-ph",
        "journal_ref": "published in IJCNN 2024"
    },
    "2403.10792v1": {
        "url": "http://arxiv.org/abs/2403.10792v1",
        "title": "\"It's Kind of Context Dependent\": Understanding Blind and Low Vision\n  People's Video Accessibility Preferences Across Viewing Scenarios",
        "summary": "While audio description (AD) is the standard approach for making videos\naccessible to blind and low vision (BLV) people, existing AD guidelines do not\nconsider BLV users' varied preferences across viewing scenarios. These\nscenarios range from how-to videos on YouTube, where users seek to learn new\nskills, to historical dramas on Netflix, where a user's goal is entertainment.\nAdditionally, the increase in video watching on mobile devices provides an\nopportunity to integrate nonverbal output modalities (e.g., audio cues, tactile\nelements, and visual enhancements). Through a formative survey and 15\nsemi-structured interviews, we identified BLV people's video accessibility\npreferences across diverse scenarios. For example, participants valued action\nand equipment details for how-to videos, tactile graphics for learning\nscenarios, and 3D models for fantastical content. We define a six-dimensional\nvideo accessibility design space to guide future innovation and discuss how to\nmove from \"one-size-fits-all\" paradigms to scenario-specific approaches.",
        "updated": "2024-03-16T03:53:41Z",
        "published": "2024-03-16T03:53:41Z",
        "authors": [
            "Lucy Jiang",
            "Crescentia Jung",
            "Mahika Phutane",
            "Abigale Stangl",
            "Shiri Azenkot"
        ],
        "comments": "To appear at CHI 2024",
        "categories": [
            "cs.HC"
        ],
        "primary_category": "cs.HC",
        "doi": "10.1145/3613904.3642238"
    },
    "2403.10794v1": {
        "url": "http://arxiv.org/abs/2403.10794v1",
        "title": "Diffusion-Reinforcement Learning Hierarchical Motion Planning in\n  Adversarial Multi-agent Games",
        "summary": "Reinforcement Learning- (RL-)based motion planning has recently shown the\npotential to outperform traditional approaches from autonomous navigation to\nrobot manipulation. In this work, we focus on a motion planning task for an\nevasive target in a partially observable multi-agent adversarial\npursuit-evasion games (PEG). These pursuit-evasion problems are relevant to\nvarious applications, such as search and rescue operations and surveillance\nrobots, where robots must effectively plan their actions to gather intelligence\nor accomplish mission tasks while avoiding detection or capture themselves. We\npropose a hierarchical architecture that integrates a high-level diffusion\nmodel to plan global paths responsive to environment data while a low-level RL\nalgorithm reasons about evasive versus global path-following behavior. Our\napproach outperforms baselines by 51.2% by leveraging the diffusion model to\nguide the RL algorithm for more efficient exploration and improves the\nexplanability and predictability.",
        "updated": "2024-03-16T03:53:55Z",
        "published": "2024-03-16T03:53:55Z",
        "authors": [
            "Zixuan Wu",
            "Sean Ye",
            "Manisha Natarajan",
            "Matthew C. Gombolay"
        ],
        "comments": "This work has been submitted to the IEEE Robotics and Automation\n  Letters (RA-L) for possible publication. Copyright may be transferred without\n  notice, after which this version may no longer be accessible",
        "categories": [
            "cs.RO",
            "cs.LG",
            "cs.MA"
        ],
        "primary_category": "cs.RO"
    },
    "2403.11667v1": {
        "url": "http://arxiv.org/abs/2403.11667v1",
        "title": "Binary Noise for Binary Tasks: Masked Bernoulli Diffusion for\n  Unsupervised Anomaly Detection",
        "summary": "The high performance of denoising diffusion models for image generation has\npaved the way for their application in unsupervised medical anomaly detection.\nAs diffusion-based methods require a lot of GPU memory and have long sampling\ntimes, we present a novel and fast unsupervised anomaly detection approach\nbased on latent Bernoulli diffusion models. We first apply an autoencoder to\ncompress the input images into a binary latent representation. Next, a\ndiffusion model that follows a Bernoulli noise schedule is employed to this\nlatent space and trained to restore binary latent representations from\nperturbed ones. The binary nature of this diffusion model allows us to identify\nentries in the latent space that have a high probability of flipping their\nbinary code during the denoising process, which indicates out-of-distribution\ndata. We propose a masking algorithm based on these probabilities, which\nimproves the anomaly detection scores. We achieve state-of-the-art performance\ncompared to other diffusion-based unsupervised anomaly detection algorithms\nwhile significantly reducing sampling time and memory consumption. The code is\navailable at https://github.com/JuliaWolleb/Anomaly_berdiff.",
        "updated": "2024-03-18T11:15:03Z",
        "published": "2024-03-18T11:15:03Z",
        "authors": [
            "Julia Wolleb",
            "Florentin Bieder",
            "Paul Friedrich",
            "Peter Zhang",
            "Alicia Durrer",
            "Philippe C. Cattin"
        ],
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "primary_category": "cs.CV"
    },
    "2403.11669v1": {
        "url": "http://arxiv.org/abs/2403.11669v1",
        "title": "Semantic Data Representation for Explainable Windows Malware Detection\n  Models",
        "summary": "Ontologies are a standard tool for creating semantic schemata in many\nknowledge intensive domains of human interest. They are becoming increasingly\nimportant also in the areas that have been until very recently dominated by\nsubsymbolic knowledge representation and machine-learning (ML) based data\nprocessing. One such area is information security, and specifically, malware\ndetection. We thus propose PE Malware Ontology that offers a reusable semantic\nschema for Portable Executable (PE - the Windows binary format) malware files.\nThis ontology is inspired by the structure of the EMBER dataset, which focuses\non the static malware analysis of PE files. With this proposal, we hope to\nprovide a unified semantic representation for the existing and future\nPE-malware datasets and facilitate the application of symbolic, neuro-symbolic,\nor otherwise explainable approaches in the PE-malware-detection domain, which\nmay produce interpretable results described by the terms defined in our\nontology. In addition, we also publish semantically treated EMBER data,\nincluding fractional datasets, to support the reproducibility of experiments on\nEMBER. We supplement our work with a preliminary case study, conducted using\nconcept learning, to show the general feasibility of our approach. While we\nwere not able to match the precision of the state-of-the-art ML tools, the\nlearned malware discriminators were interesting and highly interpretable.",
        "updated": "2024-03-18T11:17:27Z",
        "published": "2024-03-18T11:17:27Z",
        "authors": [
            "Peter \u0160vec",
            "\u0160tefan Balogh",
            "Martin Homola",
            "J\u00e1n K\u013euka",
            "Tom\u00e1\u0161 Bist\u00e1k"
        ],
        "comments": "arXiv admin note: substantial text overlap with arXiv:2301.00153",
        "categories": [
            "cs.CR"
        ],
        "primary_category": "cs.CR"
    },
    "2403.11670v1": {
        "url": "http://arxiv.org/abs/2403.11670v1",
        "title": "Advancing Quantum Software Engineering: A Vision of Hybrid Full-Stack\n  Iterative Model",
        "summary": "This paper introduces a vision for Quantum Software Development lifecycle,\nproposing a hybrid full-stack iterative model that integrates quantum and\nclassical computing. Addressing the current challenges in Quantum Computing\n(QC) such as the need for integrating diverse programming languages and\nmanaging the complexities of quantum-classical systems, this model is rooted in\nthe principles of DevOps and continuous software engineering. It presents a\ncomprehensive lifecycle for quantum software development, encompassing\nquantum-agnostic coding, testing, deployment, cloud computing services,\norchestration, translation, execution, and interpretation phases. Each phase is\ndesigned to accommodate the unique demands of QC, enabling traditional software\ndevelopers to engage with QC environments without needing in-depth QC\nexpertise. The paper presents a detailed implementation roadmap, utilizing a\nrange of existing tools and frameworks, thereby making quantum software\ndevelopment more accessible and efficient. The proposed model not only\naddresses current challenges in quantum software development but also makes a\nsubstantial contribution to the field of Quantum Software Engineering (QSE). By\nproposing a structured and accessible model, it sets the stage for further\nadvancements and research in QSE, enhancing its practicality and relevance in a\nwide range of applications.",
        "updated": "2024-03-18T11:18:33Z",
        "published": "2024-03-18T11:18:33Z",
        "authors": [
            "Arif Ali Khan",
            "Davide Taibi",
            "C\u00e9cile M. Perrault",
            "Asif Ali Khan"
        ],
        "categories": [
            "cs.SE"
        ],
        "primary_category": "cs.SE"
    },
    "2403.11671v1": {
        "url": "http://arxiv.org/abs/2403.11671v1",
        "title": "HDLdebugger: Streamlining HDL debugging with Large Language Models",
        "summary": "In the domain of chip design, Hardware Description Languages (HDLs) play a\npivotal role. However, due to the complex syntax of HDLs and the limited\navailability of online resources, debugging HDL codes remains a difficult and\ntime-intensive task, even for seasoned engineers. Consequently, there is a\npressing need to develop automated HDL code debugging models, which can\nalleviate the burden on hardware engineers. Despite the strong capabilities of\nLarge Language Models (LLMs) in generating, completing, and debugging software\ncode, their utilization in the specialized field of HDL debugging has been\nlimited and, to date, has not yielded satisfactory results. In this paper, we\npropose an LLM-assisted HDL debugging framework, namely HDLdebugger, which\nconsists of HDL debugging data generation via a reverse engineering approach, a\nsearch engine for retrieval-augmented generation, and a retrieval-augmented LLM\nfine-tuning approach. Through the integration of these components, HDLdebugger\ncan automate and streamline HDL debugging for chip design. Our comprehensive\nexperiments, conducted on an HDL code dataset sourced from Huawei, reveal that\nHDLdebugger outperforms 13 cutting-edge LLM baselines, displaying exceptional\neffectiveness in HDL code debugging.",
        "updated": "2024-03-18T11:19:37Z",
        "published": "2024-03-18T11:19:37Z",
        "authors": [
            "Xufeng Yao",
            "Haoyang Li",
            "Tsz Ho Chan",
            "Wenyi Xiao",
            "Mingxuan Yuan",
            "Yu Huang",
            "Lei Chen",
            "Bei Yu"
        ],
        "comments": "13 pages,5 figures",
        "categories": [
            "cs.AR",
            "cs.AI",
            "cs.CE",
            "cs.LG",
            "cs.SE"
        ],
        "primary_category": "cs.AR"
    },
    "2403.11672v2": {
        "url": "http://arxiv.org/abs/2403.11672v2",
        "title": "WIA-LD2ND: Wavelet-based Image Alignment for Self-supervised Low-Dose CT\n  Denoising",
        "summary": "In clinical examinations and diagnoses, low-dose computed tomography (LDCT)\nis crucial for minimizing health risks compared with normal-dose computed\ntomography (NDCT). However, reducing the radiation dose compromises the\nsignal-to-noise ratio, leading to degraded quality of CT images. To address\nthis, we analyze LDCT denoising task based on experimental results from the\nfrequency perspective, and then introduce a novel self-supervised CT image\ndenoising method called WIA-LD2ND, only using NDCT data. The proposed WIA-LD2ND\ncomprises two modules: Wavelet-based Image Alignment (WIA) and Frequency-Aware\nMulti-scale Loss (FAM). First, WIA is introduced to align NDCT with LDCT by\nmainly adding noise to the high-frequency components, which is the main\ndifference between LDCT and NDCT. Second, to better capture high-frequency\ncomponents and detailed information, Frequency-Aware Multi-scale Loss (FAM) is\nproposed by effectively utilizing multi-scale feature space. Extensive\nexperiments on two public LDCT denoising datasets demonstrate that our\nWIA-LD2ND, only uses NDCT, outperforms existing several state-of-the-art\nweakly-supervised and self-supervised methods.",
        "updated": "2024-03-19T02:07:11Z",
        "published": "2024-03-18T11:20:11Z",
        "authors": [
            "Haoyu Zhao",
            "Yuliang Gu",
            "Zhou Zhao",
            "Bo Du",
            "Yongchao Xu",
            "Rui Yu"
        ],
        "comments": "12 pages, 5 figures",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "primary_category": "eess.IV"
    },
    "2403.11675v1": {
        "url": "http://arxiv.org/abs/2403.11675v1",
        "title": "Better (pseudo-)labels for semi-supervised instance segmentation",
        "summary": "Despite the availability of large datasets for tasks like image\nclassification and image-text alignment, labeled data for more complex\nrecognition tasks, such as detection and segmentation, is less abundant. In\nparticular, for instance segmentation annotations are time-consuming to\nproduce, and the distribution of instances is often highly skewed across\nclasses. While semi-supervised teacher-student distillation methods show\npromise in leveraging vast amounts of unlabeled data, they suffer from\nmiscalibration, resulting in overconfidence in frequently represented classes\nand underconfidence in rarer ones. Additionally, these methods encounter\ndifficulties in efficiently learning from a limited set of examples. We\nintroduce a dual-strategy to enhance the teacher model's training process,\nsubstantially improving the performance on few-shot learning. Secondly, we\npropose a calibration correction mechanism that that enables the student model\nto correct the teacher's calibration errors. Using our approach, we observed\nmarked improvements over a state-of-the-art supervised baseline performance on\nthe LVIS dataset, with an increase of 2.8% in average precision (AP) and 10.3%\ngain in AP for rare classes.",
        "updated": "2024-03-18T11:23:02Z",
        "published": "2024-03-18T11:23:02Z",
        "authors": [
            "Fran\u00e7ois Porcher",
            "Camille Couprie",
            "Marc Szafraniec",
            "Jakob Verbeek"
        ],
        "comments": "Appeared at the Practical ML for Low Resource Settings workshop at\n  ICLR 2024",
        "categories": [
            "cs.CV"
        ],
        "primary_category": "cs.CV"
    },
    "2403.11681v1": {
        "url": "http://arxiv.org/abs/2403.11681v1",
        "title": "MASSTAR: A Multi-Modal and Large-Scale Scene Dataset with a Versatile\n  Toolchain for Surface Prediction and Completion",
        "summary": "Surface prediction and completion have been widely studied in various\napplications. Recently, research in surface completion has evolved from small\nobjects to complex large-scale scenes. As a result, researchers have begun\nincreasing the volume of data and leveraging a greater variety of data\nmodalities including rendered RGB images, descriptive texts, depth images, etc,\nto enhance algorithm performance. However, existing datasets suffer from a\ndeficiency in the amounts of scene-level models along with the corresponding\nmulti-modal information. Therefore, a method to scale the datasets and generate\nmulti-modal information in them efficiently is essential. To bridge this\nresearch gap, we propose MASSTAR: a Multi-modal lArge-scale Scene dataset with\na verSatile Toolchain for surfAce pRediction and completion. We develop a\nversatile and efficient toolchain for processing the raw 3D data from the\nenvironments. It screens out a set of fine-grained scene models and generates\nthe corresponding multi-modal data. Utilizing the toolchain, we then generate\nan example dataset composed of over a thousand scene-level models with partial\nreal-world data added. We compare MASSTAR with the existing datasets, which\nvalidates its superiority: the ability to efficiently extract high-quality\nmodels from complex scenarios to expand the dataset. Additionally, several\nrepresentative surface completion algorithms are benchmarked on MASSTAR, which\nreveals that existing algorithms can hardly deal with scene-level completion.\nWe will release the source code of our toolchain and the dataset. For more\ndetails, please see our project page at https://sysu-star.github.io/MASSTAR.",
        "updated": "2024-03-18T11:35:18Z",
        "published": "2024-03-18T11:35:18Z",
        "authors": [
            "Guiyong Zheng",
            "Jinqi Jiang",
            "Chen Feng",
            "Shaojie Shen",
            "Boyu Zhou"
        ],
        "comments": "Submitted to IROS2024. Code: https://github.com/SYSU-STAR/MASSTAR.\n  Project Page: https://github.com/SYSU-STAR/MASSTAR",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "primary_category": "cs.RO"
    },
    "2403.11684v1": {
        "url": "http://arxiv.org/abs/2403.11684v1",
        "title": "Primal-dual interior-point algorithm for linearly constrained convex\n  optimization based on a parametric algebraic transformation",
        "summary": "In this paper, we present an interior point algorithm with a full-Newton step\nfor solving a linearly constrained convex optimization problem, in which we\npropose a generalization of the work of Kheirfam and Nasrollahi\n\\cite{kheirfam2018full}, that consists in determining the descent directions\nthrough a parametric algebraic transformation. The work concludes with a\ncomplete study of the convergence of the algorithm and its complexity, where we\nshow that the obtained algorithm achieves a polynomial complexity bounds.",
        "updated": "2024-03-18T11:36:49Z",
        "published": "2024-03-18T11:36:49Z",
        "authors": [
            "Aicha Kraria",
            "Bachir Merikhi",
            "Djamel Benterki"
        ],
        "categories": [
            "math.NA",
            "cs.NA",
            "math.OC"
        ],
        "primary_category": "math.NA"
    },
    "2403.11686v1": {
        "url": "http://arxiv.org/abs/2403.11686v1",
        "title": "Crystalformer: Infinitely Connected Attention for Periodic Structure\n  Encoding",
        "summary": "Predicting physical properties of materials from their crystal structures is\na fundamental problem in materials science. In peripheral areas such as the\nprediction of molecular properties, fully connected attention networks have\nbeen shown to be successful. However, unlike these finite atom arrangements,\ncrystal structures are infinitely repeating, periodic arrangements of atoms,\nwhose fully connected attention results in infinitely connected attention. In\nthis work, we show that this infinitely connected attention can lead to a\ncomputationally tractable formulation, interpreted as neural potential\nsummation, that performs infinite interatomic potential summations in a deeply\nlearned feature space. We then propose a simple yet effective Transformer-based\nencoder architecture for crystal structures called Crystalformer. Compared to\nan existing Transformer-based model, the proposed model requires only 29.4% of\nthe number of parameters, with minimal modifications to the original\nTransformer architecture. Despite the architectural simplicity, the proposed\nmethod outperforms state-of-the-art methods for various property regression\ntasks on the Materials Project and JARVIS-DFT datasets.",
        "updated": "2024-03-18T11:37:42Z",
        "published": "2024-03-18T11:37:42Z",
        "authors": [
            "Tatsunori Taniai",
            "Ryo Igarashi",
            "Yuta Suzuki",
            "Naoya Chiba",
            "Kotaro Saito",
            "Yoshitaka Ushiku",
            "Kanta Ono"
        ],
        "comments": "13 main pages, 3 figures, 4 tables, 10 appendix pages. Published as a\n  conference paper at ICLR 2024. For more information, see\n  https://omron-sinicx.github.io/crystalformer/",
        "categories": [
            "cs.LG",
            "cond-mat.mtrl-sci",
            "physics.comp-ph"
        ],
        "primary_category": "cs.LG"
    },
    "2403.11687v2": {
        "url": "http://arxiv.org/abs/2403.11687v2",
        "title": "Nonsmooth Implicit Differentiation: Deterministic and Stochastic\n  Convergence Rates",
        "summary": "We study the problem of efficiently computing the derivative of the\nfixed-point of a parametric nondifferentiable contraction map. This problem has\nwide applications in machine learning, including hyperparameter optimization,\nmeta-learning and data poisoning attacks. We analyze two popular approaches:\niterative differentiation (ITD) and approximate implicit differentiation (AID).\nA key challenge behind the nonsmooth setting is that the chain rule does not\nhold anymore. Building upon the recent work by Bolte et al. (2022), who proved\nlinear convergence of nondifferentiable ITD, we provide an improved linear rate\nfor ITD and a slightly better rate for AID, both in the deterministic case. We\nfurther introduce NSID, a new stochastic method to compute the implicit\nderivative when the fixed point is defined as the composition of an outer map\nand an inner map which is accessible only through a stochastic unbiased\nestimator. We establish rates for the convergence of NSID, encompassing the\nbest available rates in the smooth setting. We present illustrative experiments\nconfirming our analysis.",
        "updated": "2024-03-28T17:56:05Z",
        "published": "2024-03-18T11:37:53Z",
        "authors": [
            "Riccardo Grazzi",
            "Massimiliano Pontil",
            "Saverio Salzo"
        ],
        "comments": "Removed the assumption on the conservative derivative of the fixed\n  point map having a product structure: the product of partial conservative\n  derivatives is not conservative in general",
        "categories": [
            "stat.ML",
            "cs.LG",
            "math.OC"
        ],
        "primary_category": "stat.ML"
    }
}