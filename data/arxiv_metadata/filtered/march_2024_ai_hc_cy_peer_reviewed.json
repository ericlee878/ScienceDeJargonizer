{"0":{"arxiv_id":"2107.04771v2","url":"http:\/\/arxiv.org\/abs\/2107.04771v2","title":"Similar Cases Recommendation using Legal Knowledge Graphs","summary":"A legal knowledge graph constructed from court cases, judgments, laws and\nother legal documents can enable a number of applications like question\nanswering, document similarity, and search. While the use of knowledge graphs\nfor distant supervision in NLP tasks is well researched, using knowledge graphs\nfor applications like case similarity presents challenges. In this work, we\ndescribe our solution for predicting similar cases in Indian court judgements.\nWe present our results and also discuss the impact of large language models on\nthis task.","updated":1709369211000,"published":1625899056000,"authors":["Jaspreet Singh Dhani","Ruchika Bhatt","Balaji Ganesan","Parikshet Sirohi","Vasudha Bhatnagar"],"comments":"10 pages. 6 figures. 3rd Symposium on Artificial Intelligence and\n  Law. SAIL 2023","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"1":{"arxiv_id":"2301.13755v3","url":"http:\/\/arxiv.org\/abs\/2301.13755v3","title":"Retrosynthetic Planning with Dual Value Networks","summary":"Retrosynthesis, which aims to find a route to synthesize a target molecule\nfrom commercially available starting materials, is a critical task in drug\ndiscovery and materials design. Recently, the combination of ML-based\nsingle-step reaction predictors with multi-step planners has led to promising\nresults. However, the single-step predictors are mostly trained offline to\noptimize the single-step accuracy, without considering complete routes. Here,\nwe leverage reinforcement learning (RL) to improve the single-step predictor,\nby using a tree-shaped MDP to optimize complete routes. Specifically, we\npropose a novel online training algorithm, called Planning with Dual Value\nNetworks (PDVN), which alternates between the planning phase and updating\nphase. In PDVN, we construct two separate value networks to predict the\nsynthesizability and cost of molecules, respectively. To maintain the\nsingle-step accuracy, we design a two-branch network structure for the\nsingle-step predictor. On the widely-used USPTO dataset, our PDVN algorithm\nimproves the search success rate of existing multi-step planners (e.g.,\nincreasing the success rate from 85.79% to 98.95% for Retro*, and reducing the\nnumber of model calls by half while solving 99.47% molecules for RetroGraph).\nAdditionally, PDVN helps find shorter synthesis routes (e.g., reducing the\naverage route length from 5.76 to 4.83 for Retro*, and from 5.63 to 4.78 for\nRetroGraph). Our code is available at \\url{https:\/\/github.com\/DiXue98\/PDVN}.","updated":1709475801000,"published":1675183433000,"authors":["Guoqing Liu","Di Xue","Shufang Xie","Yingce Xia","Austin Tripp","Krzysztof Maziarz","Marwin Segler","Tao Qin","Zongzhang Zhang","Tie-Yan Liu"],"comments":"Accepted to ICML 2023","categories":["cs.AI","cs.LG"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"2":{"arxiv_id":"2306.00292v4","url":"http:\/\/arxiv.org\/abs\/2306.00292v4","title":"Sustainable AI Regulation","summary":"Current proposals for AI regulation, in the EU and beyond, aim to spur AI\nthat is trustworthy (e.g., AI Act) and accountable (e.g., AI Liability) What is\nmissing, however, is a robust regulatory discourse and roadmap to make AI, and\ntechnology more broadly, environmentally sustainable. This paper aims to take\nfirst steps to fill this gap. The ICT sector contributes up to 3.9 percent of\nglobal greenhouse gas (GHG) emissions-more than global air travel at 2.5\npercent. The carbon footprint and water consumption of AI, especially\nlarge-scale generative models like GPT-4, raise significant sustainability\nconcerns. The paper is the first to assess how current and proposed technology\nregulations, including EU environmental law, the General Data Protection\nRegulation (GDPR), and the AI Act, could be adjusted to better account for\nenvironmental sustainability. The GDPR, for instance, could be interpreted to\nlimit certain individual rights like the right to erasure if these rights\nsignificantly conflict with broader sustainability goals. In a second step, the\npaper suggests a multi-faceted approach to achieve sustainable AI regulation.\nIt advocates for transparency mechanisms, such as disclosing the GHG footprint\nof AI systems, as laid out in the proposed EU AI Act. However, sustainable AI\nregulation must go beyond mere transparency. The paper proposes a regulatory\ntoolkit comprising co-regulation, sustainability-by-design principles,\nrestrictions on training data, and consumption caps, including integration into\nthe EU Emissions Trading Scheme. Finally, the paper argues that this regulatory\ntoolkit could serve as a blueprint for regulating other high-emission\ntechnologies and infrastructures like blockchain, Metaverse applications, and\ndata centers. The framework aims to cohesively address the crucial dual\nchallenges of our era: digital transformation and climate change mitigation.","updated":1709744245000,"published":1685586048000,"authors":["Philipp Hacker"],"comments":"Privacy Law Scholars Conference 2023; Common Market Law Review\n  (forthcoming)","categories":["cs.CY","I.2"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"3":{"arxiv_id":"2306.00919v5","url":"http:\/\/arxiv.org\/abs\/2306.00919v5","title":"Learning About Social Context from Smartphone Data: Generalization\n  Across Countries and Daily Life Moments","summary":"Understanding how social situations unfold in people's daily lives is\nrelevant to designing mobile systems that can support users in their personal\ngoals, well-being, and activities. As an alternative to questionnaires, some\nstudies have used passively collected smartphone sensor data to infer social\ncontext (i.e., being alone or not) with machine learning models. However, the\nfew existing studies have focused on specific daily life occasions and limited\ngeographic cohorts in one or two countries. This limits the understanding of\nhow inference models work in terms of generalization to everyday life occasions\nand multiple countries. In this paper, we used a novel, large-scale, and\nmultimodal smartphone sensing dataset with over 216K self-reports collected\nfrom 581 young adults in five countries (Mongolia, Italy, Denmark, UK,\nParaguay), first to understand whether social context inference is feasible\nwith sensor data, and then, to know how behavioral and country-level diversity\naffects inferences. We found that several sensors are informative of social\ncontext, that partially personalized multi-country models (trained and tested\nwith data from all countries) and country-specific models (trained and tested\nwithin countries) can achieve similar performance above 90% AUC, and that\nmodels do not generalize well to unseen countries regardless of geographic\nproximity. These findings confirm the importance of the diversity of mobile\ndata, to better understand social context inference models in different\ncountries.","updated":1709300928000,"published":1685640056000,"authors":["Aurel Ruben Mader","Lakmal Meegahapola","Daniel Gatica-Perez"],"comments":"Accepted at ACM CHI 2024","categories":["cs.HC","cs.CY"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642444","journal_ref":null,"peer_reviewed":true},"4":{"arxiv_id":"2309.03685v2","url":"http:\/\/arxiv.org\/abs\/2309.03685v2","title":"PyGraft: Configurable Generation of Synthetic Schemas and Knowledge\n  Graphs at Your Fingertips","summary":"Knowledge graphs (KGs) have emerged as a prominent data representation and\nmanagement paradigm. Being usually underpinned by a schema (e.g., an ontology),\nKGs capture not only factual information but also contextual knowledge. In some\ntasks, a few KGs established themselves as standard benchmarks. However, recent\nworks outline that relying on a limited collection of datasets is not\nsufficient to assess the generalization capability of an approach. In some\ndata-sensitive fields such as education or medicine, access to public datasets\nis even more limited. To remedy the aforementioned issues, we release PyGraft,\na Python-based tool that generates highly customized, domain-agnostic schemas\nand KGs. The synthesized schemas encompass various RDFS and OWL constructs,\nwhile the synthesized KGs emulate the characteristics and scale of real-world\nKGs. Logical consistency of the generated resources is ultimately ensured by\nrunning a description logic (DL) reasoner. By providing a way of generating\nboth a schema and KG in a single pipeline, PyGraft's aim is to empower the\ngeneration of a more diverse array of KGs for benchmarking novel approaches in\nareas such as graph-based machine learning (ML), or more generally KG\nprocessing. In graph-based ML in particular, this should foster a more holistic\nevaluation of model performance and generalization capability, thereby going\nbeyond the limited collection of available benchmarks. PyGraft is available at:\nhttps:\/\/github.com\/nicolas-hbt\/pygraft.","updated":1709675803000,"published":1694091609000,"authors":["Nicolas Hubert","Pierre Monnin","Mathieu d'Aquin","Davy Monticolo","Armelle Brun"],"comments":"Accepted in ESWC 2024","categories":["cs.AI","cs.SE"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"5":{"arxiv_id":"2309.10108v2","url":"http:\/\/arxiv.org\/abs\/2309.10108v2","title":"How Do Data Analysts Respond to AI Assistance? A Wizard-of-Oz Study","summary":"Data analysis is challenging as analysts must navigate nuanced decisions that\nmay yield divergent conclusions. AI assistants have the potential to support\nanalysts in planning their analyses, enabling more robust decision making.\nThough AI-based assistants that target code execution (e.g., Github Copilot)\nhave received significant attention, limited research addresses assistance for\nboth analysis execution and planning. In this work, we characterize helpful\nplanning suggestions and their impacts on analysts' workflows. We first review\nthe analysis planning literature and crowd-sourced analysis studies to\ncategorize suggestion content. We then conduct a Wizard-of-Oz study (n=13) to\nobserve analysts' preferences and reactions to planning assistance in a\nrealistic scenario. Our findings highlight subtleties in contextual factors\nthat impact suggestion helpfulness, emphasizing design implications for\nsupporting different abstractions of assistance, forms of initiative, increased\nengagement, and alignment of goals between analysts and assistants.","updated":1709570897000,"published":1695065616000,"authors":["Ken Gu","Madeleine Grunde-McLaughlin","Andrew M. McNutt","Jeffrey Heer","Tim Althoff"],"comments":"Accepted to CHI 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"6":{"arxiv_id":"2309.10947v2","url":"http:\/\/arxiv.org\/abs\/2309.10947v2","title":"How Do Analysts Understand and Verify AI-Assisted Data Analyses?","summary":"Data analysis is challenging as it requires synthesizing domain knowledge,\nstatistical expertise, and programming skills. Assistants powered by large\nlanguage models (LLMs), such as ChatGPT, can assist analysts by translating\nnatural language instructions into code. However, AI-assistant responses and\nanalysis code can be misaligned with the analyst's intent or be seemingly\ncorrect but lead to incorrect conclusions. Therefore, validating AI assistance\nis crucial and challenging. Here, we explore how analysts understand and verify\nthe correctness of AI-generated analyses. To observe analysts in diverse\nverification approaches, we develop a design probe equipped with natural\nlanguage explanations, code, visualizations, and interactive data tables with\ncommon data operations. Through a qualitative user study (n=22) using this\nprobe, we uncover common behaviors within verification workflows and how\nanalysts' programming, analysis, and tool backgrounds reflect these behaviors.\nAdditionally, we provide recommendations for analysts and highlight\nopportunities for designers to improve future AI-assistant experiences.","updated":1709571502000,"published":1695160954000,"authors":["Ken Gu","Ruoxi Shang","Tim Althoff","Chenglong Wang","Steven M. Drucker"],"comments":"Accepted to CHI 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"7":{"arxiv_id":"2310.02424v3","url":"http:\/\/arxiv.org\/abs\/2310.02424v3","title":"AXNav: Replaying Accessibility Tests from Natural Language","summary":"Developers and quality assurance testers often rely on manual testing to test\naccessibility features throughout the product lifecycle. Unfortunately, manual\ntesting can be tedious, often has an overwhelming scope, and can be difficult\nto schedule amongst other development milestones. Recently, Large Language\nModels (LLMs) have been used for a variety of tasks including automation of\nUIs, however to our knowledge no one has yet explored their use in controlling\nassistive technologies for the purposes of supporting accessibility testing. In\nthis paper, we explore the requirements of a natural language based\naccessibility testing workflow, starting with a formative study. From this we\nbuild a system that takes as input a manual accessibility test (e.g., ``Search\nfor a show in VoiceOver'') and uses an LLM combined with pixel-based UI\nUnderstanding models to execute the test and produce a chaptered, navigable\nvideo. In each video, to help QA testers we apply heuristics to detect and flag\naccessibility issues (e.g., Text size not increasing with Large Text enabled,\nVoiceOver navigation loops). We evaluate this system through a 10 participant\nuser study with accessibility QA professionals who indicated that the tool\nwould be very useful in their current work and performed tests similarly to how\nthey would manually test the features. The study also reveals insights for\nfuture work on using LLMs for accessibility testing.","updated":1709602105000,"published":1696365478000,"authors":["Maryam Taeb","Amanda Swearngin","Eldon Schoop","Ruijia Cheng","Yue Jiang","Jeffrey Nichols"],"comments":"Accepted into Conference on Human Factors in Computing Systems (CHI)\n  2024, 22 pages, 7 figures","categories":["cs.HC","cs.AI","I.2"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642777","journal_ref":null,"peer_reviewed":true},"8":{"arxiv_id":"2310.02432v2","url":"http:\/\/arxiv.org\/abs\/2310.02432v2","title":"Beyond Dark Patterns: A Concept-Based Framework for Ethical Software\n  Design","summary":"Current dark pattern research tells designers what not to do, but how do they\nknow what to do? In contrast to prior approaches that focus on patterns to\navoid and their underlying principles, we present a framework grounded in\npositive expected behavior against which deviations can be judged. To\narticulate this expected behavior, we use concepts -- abstract units of\nfunctionality that compose applications. We define a design as dark when its\nconcepts violate users' expectations, and benefit the application provider at\nthe user's expense. Though user expectations can differ, users tend to develop\ncommon expectations as they encounter the same concepts across multiple\napplications, which we can record in a concept catalog as standard concepts. We\nevaluate our framework and concept catalog through three studies, illustrating\ntheir ability to describe existing dark patterns, evaluate nuanced designs, and\ndocument common application functionality.","updated":1709528383000,"published":1696366682000,"authors":["Evan Caragay","Katherine Xiong","Jonathan Zong","Daniel Jackson"],"comments":"ACM CHI 2024","categories":["cs.HC","cs.SE"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642781","journal_ref":null,"peer_reviewed":true},"9":{"arxiv_id":"2310.09235v3","url":"http:\/\/arxiv.org\/abs\/2310.09235v3","title":"CoPrompt: Supporting Prompt Sharing and Referring in Collaborative\n  Natural Language Programming","summary":"Natural language (NL) programming has become more approachable due to the\npowerful code-generation capability of large language models (LLMs). This shift\nto using NL to program enhances collaborative programming by reducing\ncommunication barriers and context-switching among programmers from varying\nbackgrounds. However, programmers may face challenges during prompt engineering\nin a collaborative setting as they need to actively keep aware of their\ncollaborators' progress and intents. In this paper, we aim to investigate ways\nto assist programmers' prompt engineering in a collaborative context. We first\nconducted a formative study to understand the workflows and challenges of\nprogrammers when using NL for collaborative programming. Based on our findings,\nwe implemented a prototype, CoPrompt, to support collaborative prompt\nengineering by providing referring, requesting, sharing, and linking\nmechanisms. Our user study indicates that CoPrompt assists programmers in\ncomprehending collaborators' prompts and building on their collaborators' work,\nreducing repetitive updates and communication costs.","updated":1709281335000,"published":1697215095000,"authors":["Li Feng","Ryan Yen","Yuzhe You","Mingming Fan","Jian Zhao","Zhicong Lu"],"comments":"Proceedings of the CHI Conference on Human Factors in Computing\n  Systems (CHI '24), May 11--16, 2024, Honolulu, HI, USA","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"10":{"arxiv_id":"2402.09565v2","url":"http:\/\/arxiv.org\/abs\/2402.09565v2","title":"Graph-Skeleton: ~1% Nodes are Sufficient to Represent Billion-Scale\n  Graph","summary":"Due to the ubiquity of graph data on the web, web graph mining has become a\nhot research spot. Nonetheless, the prevalence of large-scale web graphs in\nreal applications poses significant challenges to storage, computational\ncapacity and graph model design. Despite numerous studies to enhance the\nscalability of graph models, a noticeable gap remains between academic research\nand practical web graph mining applications. One major cause is that in most\nindustrial scenarios, only a small part of nodes in a web graph are actually\nrequired to be analyzed, where we term these nodes as target nodes, while\nothers as background nodes. In this paper, we argue that properly fetching and\ncondensing the background nodes from massive web graph data might be a more\neconomical shortcut to tackle the obstacles fundamentally. To this end, we make\nthe first attempt to study the problem of massive background nodes compression\nfor target nodes classification. Through extensive experiments, we reveal two\ncritical roles played by the background nodes in target node classification:\nenhancing structural connectivity between target nodes, and feature correlation\nwith target nodes. Followingthis, we propose a novel Graph-Skeleton1 model,\nwhich properly fetches the background nodes, and further condenses the semantic\nand topological information of background nodes within similar\ntarget-background local structures. Extensive experiments on various web graph\ndatasets demonstrate the effectiveness and efficiency of the proposed method.\nIn particular, for MAG240M dataset with 0.24 billion nodes, our generated\nskeleton graph achieves highly comparable performance while only containing\n1.8% nodes of the original graph.","updated":1709763753000,"published":1707942791000,"authors":["Linfeng Cao","Haoran Deng","Yang Yang","Chunping Wang","Lei Chen"],"comments":"21 pages, 11 figures, In Proceedings of the ACM Web Conference 2024\n  (WWW'24)","categories":["cs.AI"],"primary_category":"cs.AI","doi":"10.1145\/3589334.3645452","journal_ref":null,"peer_reviewed":true},"11":{"arxiv_id":"2403.00106v2","url":"http:\/\/arxiv.org\/abs\/2403.00106v2","title":"Umwelt: Accessible Structured Editing of Multimodal Data Representations","summary":"We present Umwelt, an authoring environment for interactive multimodal data\nrepresentations. In contrast to prior approaches, which center the visual\nmodality, Umwelt treats visualization, sonification, and textual description as\ncoequal representations: they are all derived from a shared abstract data\nmodel, such that no modality is prioritized over the others. To simplify\nspecification, Umwelt evaluates a set of heuristics to generate default\nmultimodal representations that express a dataset's functional relationships.\nTo support smoothly moving between representations, Umwelt maintains a shared\nquery predicated that is reified across all modalities -- for instance,\nnavigating the textual description also highlights the visualization and\nfilters the sonification. In a study with 5 blind \/ low-vision expert users, we\nfound that Umwelt's multimodal representations afforded complementary overview\nand detailed perspectives on a dataset, allowing participants to fluidly shift\nbetween task- and representation-oriented ways of thinking.","updated":1709527356000,"published":1709238018000,"authors":["Jonathan Zong","Isabella Pedraza Pineros","Mengzhu Katie Chen","Daniel Hajas","Arvind Satyanarayan"],"comments":"ACM CHI 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3641996.","journal_ref":null,"peer_reviewed":true},"12":{"arxiv_id":"2403.00265v1","url":"http:\/\/arxiv.org\/abs\/2403.00265v1","title":"Designing for Harm Reduction: Communication Repair for Multicultural\n  Users' Voice Interactions","summary":"Voice assistants' inability to serve people-of-color and non-native English\nspeakers has largely been documented as a quality-of-service harm. However,\nlittle work has investigated what downstream harms propagate from this poor\nservice. How does poor usability materially manifest and affect users' lives?\nAnd what interaction designs might help users recover from these effects? We\nidentify 6 downstream harms that propagate from quality-of-service harms in\nvoice assistants. Through interviews and design activities with 16\nmulticultural participants, we unveil these 6 harms, outline how multicultural\nusers uniquely personify their voice assistant, and suggest how these harms and\npersonifications may affect their interactions. Lastly, we employ techniques\nfrom psychology on communication repair to contribute suggestions for\nharm-reducing repair that may be implemented in voice technologies. Our\ncommunication repair strategies include: identity affirmations (intermittent\nfrequency), cultural sensitivity, and blame redirection. This work shows\npotential for a harm-repair framework to positively influence voice\ninteractions.","updated":1709265424000,"published":1709265424000,"authors":["Kimi Wenzel","Geoff Kaufman"],"comments":"2024 CHI Conference on Human Factors in Computing Systems (CHI '24)","categories":["cs.HC","cs.CY"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"13":{"arxiv_id":"2403.00323v1","url":"http:\/\/arxiv.org\/abs\/2403.00323v1","title":"Softened Symbol Grounding for Neuro-symbolic Systems","summary":"Neuro-symbolic learning generally consists of two separated worlds, i.e.,\nneural network training and symbolic constraint solving, whose success hinges\non symbol grounding, a fundamental problem in AI. This paper presents a novel,\nsoftened symbol grounding process, bridging the gap between the two worlds, and\nresulting in an effective and efficient neuro-symbolic learning framework.\nTechnically, the framework features (1) modeling of symbol solution states as a\nBoltzmann distribution, which avoids expensive state searching and facilitates\nmutually beneficial interactions between network training and symbolic\nreasoning;(2) a new MCMC technique leveraging projection and SMT solvers, which\nefficiently samples from disconnected symbol solution spaces; (3) an annealing\nmechanism that can escape from %being trapped into sub-optimal symbol\ngroundings. Experiments with three representative neuro symbolic learning tasks\ndemonstrate that, owining to its superior symbol grounding capability, our\nframework successfully solves problems well beyond the frontier of the existing\nproposals.","updated":1709276229000,"published":1709276229000,"authors":["Zenan Li","Yuan Yao","Taolue Chen","Jingwei Xu","Chun Cao","Xiaoxing Ma","Jian L\u00fc"],"comments":"Published as a conference paper at ICLR 2023. Code is available at\n  https:\/\/github.com\/SoftWiser-group\/Soften-NeSy-learning","categories":["cs.AI","cs.LG"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"14":{"arxiv_id":"2403.00329v1","url":"http:\/\/arxiv.org\/abs\/2403.00329v1","title":"Learning with Logical Constraints but without Shortcut Satisfaction","summary":"Recent studies in neuro-symbolic learning have explored the integration of\nlogical knowledge into deep learning via encoding logical constraints as an\nadditional loss function. However, existing approaches tend to vacuously\nsatisfy logical constraints through shortcuts, failing to fully exploit the\nknowledge. In this paper, we present a new framework for learning with logical\nconstraints. Specifically, we address the shortcut satisfaction issue by\nintroducing dual variables for logical connectives, encoding how the constraint\nis satisfied. We further propose a variational framework where the encoded\nlogical constraint is expressed as a distributional loss that is compatible\nwith the model's original training loss. The theoretical analysis shows that\nthe proposed approach bears salient properties, and the experimental\nevaluations demonstrate its superior performance in both model generalizability\nand constraint satisfaction.","updated":1709277440000,"published":1709277440000,"authors":["Zenan Li","Zehua Liu","Yuan Yao","Jingwei Xu","Taolue Chen","Xiaoxing Ma","Jian L\u00fc"],"comments":"Published as a conference paper at ICLR 2023, and code is available\n  at https:\/\/github.com\/SoftWiser-group\/NeSy-without-Shortcuts","categories":["cs.AI","cs.LG"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"15":{"arxiv_id":"2403.00431v1","url":"http:\/\/arxiv.org\/abs\/2403.00431v1","title":"Robotic Process Automation as a Driver for Sustainable Innovation and\n  Entrepreneurship","summary":"Technological innovation plays a crucial role in driving economic growth and\ndevelopment. In this study, we investigate the extent to which technological\ninnovation contributes to a more sustainable future and fosters\nentrepreneurship. To examine this, we focus on robotic process automation (RPA)\nhighly relevant technology. We conducted a comprehensive analysis by examining\nthe usage of RPA and its impact on environmental, social, and governance (ESG)\nfactors. Our research involved gathering data from the 300 largest companies in\nterms of market capitalization. We assessed whether these companies used RPA\nand obtained their corresponding ESG ratings. To investigate the relationship\nbetween RPA and ESG, we employed a contingency table analysis, which involved\ncategorizing the data based on ESG ratings. We further used Pearson's\nChi-square Test of Independence to assess the impact of RPA on ESG. Our\nfindings revealed a statistically significant association between RPA and ESG\nratings, indicating their interconnection. The calculated value for Pearson's\nChi-square Test of Independence was 6.54, with a corresponding p-value of\n0.0381. This indicates that at a significance level of five percent, the RPA\nand ESG variables depend on each other. These results suggest that RPA,\nrepresentative of modern technologies, likely influences the achievement of a\nsustainable future and the promotion of entrepreneurship. In conclusion, our\nstudy provides empirical evidence supporting the notion that technological\ninnovations such as RPA have the potential to positively shape sustainability\nefforts and entrepreneurial endeavours.","updated":1709289168000,"published":1709289168000,"authors":["Petr Prucha"],"comments":"XB-CON International Conference 2023, Zelezna Ruda, Czechia","categories":["cs.CY"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"16":{"arxiv_id":"2403.00439v1","url":"http:\/\/arxiv.org\/abs\/2403.00439v1","title":"Authors' Values and Attitudes Towards AI-bridged Scalable\n  Personalization of Creative Language Arts","summary":"Generative AI has the potential to create a new form of interactive media:\nAI-bridged creative language arts (CLA), which bridge the author and audience\nby personalizing the author's vision to the audience's context and taste at\nscale. However, it is unclear what the authors' values and attitudes would be\nregarding AI-bridged CLA. To identify these values and attitudes, we conducted\nan interview study with 18 authors across eight genres (e.g., poetry, comics)\nby presenting speculative but realistic AI-bridged CLA scenarios. We identified\nthree benefits derived from the dynamics between author, artifact, and\naudience: those that 1) authors get from the process, 2) audiences get from the\nartifact, and 3) authors get from the audience. We found how AI-bridged CLA\nwould either promote or reduce these benefits, along with authors' concerns. We\nhope our investigation hints at how AI can provide intriguing experiences to\nCLA audiences while promoting authors' values.","updated":1709290390000,"published":1709290390000,"authors":["Taewook Kim","Hyomin Han","Eytan Adar","Matthew Kay","John Joon Young Chung"],"comments":"16 pages, 6 figures, 2 tables. Accepted to ACM CHI 2024","categories":["cs.HC","cs.AI"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642529","journal_ref":null,"peer_reviewed":true},"17":{"arxiv_id":"2403.00527v1","url":"http:\/\/arxiv.org\/abs\/2403.00527v1","title":"\"There is a Job Prepared for Me Here\": Understanding How Short Video and\n  Live-streaming Platforms Empower Ageing Job Seekers in China","summary":"In recent years, the global unemployment rate has remained persistently high.\nCompounding this issue, the ageing population in China often encounters\nadditional challenges in finding employment due to prevalent age discrimination\nin daily life. However, with the advent of social media, there has been a rise\nin the popularity of short videos and live-streams for recruiting ageing\nworkers. To better understand the motivations of ageing job seekers to engage\nwith these video-based recruitment methods and to explore the extent to which\nsuch platforms can empower them, we conducted an interview-based study with\nageing job seekers who have had exposure to these short recruitment videos and\nlive-streaming channels. Our findings reveal that these platforms can provide a\njob-seeking choice that is particularly friendly to ageing job seekers,\neffectively improving their disadvantaged situation.","updated":1709300134000,"published":1709300134000,"authors":["PiaoHong Wang","Siying Hu","Bo Wen","Zhicong Lu"],"comments":"14 pages, 3 figures; Accepted to ACM CHI 2024. In Proceedings of the\n  2024 CHI Conference on Human Factors in Computing Systems (CHI'24)","categories":["cs.HC","cs.CY","cs.SI","H.5.m; K.4.0"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642959","journal_ref":null,"peer_reviewed":true},"18":{"arxiv_id":"2403.00632v1","url":"http:\/\/arxiv.org\/abs\/2403.00632v1","title":"Metamorpheus: Interactive, Affective, and Creative Dream Narration\n  Through Metaphorical Visual Storytelling","summary":"Human emotions are essentially molded by lived experiences, from which we\nconstruct personalised meaning. The engagement in such meaning-making process\nhas been practiced as an intervention in various psychotherapies to promote\nwellness. Nevertheless, to support recollecting and recounting lived\nexperiences in everyday life remains under explored in HCI. It also remains\nunknown how technologies such as generative AI models can facilitate the\nmeaning making process, and ultimately support affective mindfulness. In this\npaper we present Metamorpheus, an affective interface that engages users in a\ncreative visual storytelling of emotional experiences during dreams.\nMetamorpheus arranges the storyline based on a dream's emotional arc, and\nprovokes self-reflection through the creation of metaphorical images and text\ndepictions. The system provides metaphor suggestions, and generates visual\nmetaphors and text depictions using generative AI models, while users can apply\ngenerations to recolour and re-arrange the interface to be visually affective.\nOur experience-centred evaluation manifests that, by interacting with\nMetamorpheus, users can recall their dreams in vivid detail, through which they\nrelive and reflect upon their experiences in a meaningful way.","updated":1709309372000,"published":1709309372000,"authors":["Qian Wan","Xin Feng","Yining Bei","Zhiqi Gao","Zhicong Lu"],"comments":"Accepted by CHI 2024","categories":["cs.HC","cs.AI","cs.CL","cs.CY"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"19":{"arxiv_id":"2403.00717v1","url":"http:\/\/arxiv.org\/abs\/2403.00717v1","title":"MAIDR: Making Statistical Visualizations Accessible with Multimodal Data\n  Representation","summary":"This paper investigates new data exploration experiences that enable blind\nusers to interact with statistical data visualizations$-$bar plots, heat maps,\nbox plots, and scatter plots$-$leveraging multimodal data representations. In\naddition to sonification and textual descriptions that are commonly employed by\nexisting accessible visualizations, our MAIDR (multimodal access and\ninteractive data representation) system incorporates two additional modalities\n(braille and review) that offer complementary benefits. It also provides blind\nusers with the autonomy and control to interactively access and understand data\nvisualizations. In a user study involving 11 blind participants, we found the\nMAIDR system facilitated the accurate interpretation of statistical\nvisualizations. Participants exhibited a range of strategies in combining\nmultiple modalities, influenced by their past interactions and experiences with\ndata visualizations. This work accentuates the overlooked potential of\ncombining refreshable tactile representation with other modalities and elevates\nthe discussion on the importance of user autonomy when designing accessible\ndata visualizations.","updated":1709316365000,"published":1709316365000,"authors":["JooYoung Seo","Yilin Xia","Bongshin Lee","Sean McCurry","Yu Jun Yam"],"comments":"Accepted to CHI 2024. Source code is available at\n  https:\/\/github.com\/xability\/maidr","categories":["cs.HC","cs.GR"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642730","journal_ref":null,"peer_reviewed":true},"20":{"arxiv_id":"2403.01055v1","url":"http:\/\/arxiv.org\/abs\/2403.01055v1","title":"Towards Full Authorship with AI: Supporting Revision with AI-Generated\n  Views","summary":"Large language models (LLMs) are shaping a new user interface (UI) paradigm\nin writing tools by enabling users to generate text through prompts. This\nparadigm shifts some creative control from the user to the system, thereby\ndiminishing the user's authorship and autonomy in the writing process. To\nrestore autonomy, we introduce Textfocals, a UI prototype designed to\ninvestigate a human-centered approach that emphasizes the user's role in\nwriting. Textfocals supports the writing process by providing LLM-generated\nsummaries, questions, and advice (i.e., LLM views) in a sidebar of a text\neditor, encouraging reflection and self-driven revision in writing without\ndirect text generation. Textfocals' UI affordances, including contextually\nadaptive views and scaffolding for prompt selection and customization, offer a\nnovel way to interact with LLMs where users maintain full authorship of their\nwriting. A formative user study with Textfocals showed promising evidence that\nthis approach might help users develop underdeveloped ideas, cater to the\nrhetorical audience, and clarify their writing. However, the study also showed\ninteraction design challenges related to document navigation and scoping,\nprompt engineering, and context management. Our work highlights the breadth of\nthe design space of writing support interfaces powered by generative AI that\nmaintain authorship integrity.","updated":1709341895000,"published":1709341895000,"authors":["Jiho Kim","Ray C. Flanagan","Noelle E. Haviland","ZeAi Sun","Souad N. Yakubu","Edom A. Maru","Kenneth C. Arnold"],"comments":"15 pages, 2 figures; Accepted to 5th Workshop on Human-AI Co-Creation\n  with Generative Models (HAI-GEN) at ACM IUI 2024","categories":["cs.HC","cs.AI","cs.CY","H.5.2; I.7.1; I.2.7"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"21":{"arxiv_id":"2403.01090v1","url":"http:\/\/arxiv.org\/abs\/2403.01090v1","title":"Sharing Frissons among Online Video Viewers: Exploring the Design of\n  Affective Communication for Aesthetic Chills","summary":"On online video platforms, viewers often lack a channel to sense others' and\nexpress their affective state on the fly compared to co-located group-viewing.\nThis study explored the design of complementary affective communication\nspecifically for effortless, spontaneous sharing of frissons during video\nwatching. Also known as aesthetic chills, frissons are instant\npsycho-physiological reactions like goosebumps and shivers to arousing stimuli.\nWe proposed an approach that unobtrusively detects viewers' frissons using skin\nelectrodermal activity sensors and presents the aggregated data alongside\nonline videos. Following a design process of brainstorming, focus group\ninterview (N=7), and design iterations, we proposed three different designs to\nencode viewers' frisson experiences, namely, ambient light, icon, and\nvibration. A mixed-methods within-subject study (N=48) suggested that our\napproach offers a non-intrusive and efficient way to share viewers' frisson\nmoments, increases the social presence of others as if watching together, and\ncan create affective contagion among viewers.","updated":1709353281000,"published":1709353281000,"authors":["Zeyu Huang","Xinyi Cao","Yuanhao Zhang","Xiaojuan Ma"],"comments":"Accepted by CHI24","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642818","journal_ref":null,"peer_reviewed":true},"22":{"arxiv_id":"2403.01199v1","url":"http:\/\/arxiv.org\/abs\/2403.01199v1","title":"The Case for Animal-Friendly AI","summary":"Artificial intelligence is seen as increasingly important, and potentially\nprofoundly so, but the fields of AI ethics and AI engineering have not fully\nrecognized that these technologies, including large language models (LLMs),\nwill have massive impacts on animals. We argue that this impact matters,\nbecause animals matter morally.\n  As a first experiment in evaluating animal consideration in LLMs, we\nconstructed a proof-of-concept Evaluation System, which assesses LLM responses\nand biases from multiple perspectives. This system evaluates LLM outputs by two\ncriteria: their truthfulness, and the degree of consideration they give to the\ninterests of animals. We tested OpenAI ChatGPT 4 and Anthropic Claude 2.1 using\na set of structured queries and predefined normative perspectives. Preliminary\nresults suggest that the outcomes of the tested models can be benchmarked\nregarding the consideration they give to animals, and that generated positions\nand biases might be addressed and mitigated with more developed and validated\nsystems.\n  Our research contributes one possible approach to integrating animal ethics\nin AI, opening pathways for future studies and practical applications in\nvarious fields, including education, public policy, and regulation, that\ninvolve or relate to animals and society. Overall, this study serves as a step\ntowards more useful and responsible AI systems that better recognize and\nrespect the vital interests and perspectives of all sentient beings.","updated":1709383271000,"published":1709383271000,"authors":["Sankalpa Ghose","Yip Fai Tse","Kasra Rasaee","Jeff Sebo","Peter Singer"],"comments":"AAAI 2024 Workshop on Public Sector LLMs: Algorithmic and\n  Sociotechnical Design. 12 pages, 11 figures","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"23":{"arxiv_id":"2403.01365v1","url":"http:\/\/arxiv.org\/abs\/2403.01365v1","title":"AI-Powered Reminders for Collaborative Tasks: Experiences and Futures","summary":"Email continues to serve as a central medium for managing collaborations.\nWhile unstructured email messaging is lightweight and conducive to\ncoordination, it is easy to overlook commitments and requests for\ncollaborations that are embedded in the text of free-flowing communications.\nTwenty-one years ago, Bellotti et al. proposed TaskMaster with the goal of\nredesigning the email interface to have explicit task management capabilities.\nRecently, AI-based task recognition and reminder services have been introduced\nin major email systems as one approach to managing asynchronous collaborations.\nWhile these services have been provided to millions of people around the world,\nthere is little understanding of how people interact with and benefit from\nthem. We explore knowledge workers' experiences with Microsoft's Viva Daily\nBriefing Email to better understand how AI-powered reminders can support\nasynchronous collaborations. Through semi-structured interviews and surveys, we\nshed light on how AI-powered reminders are incorporated into workflows to\nsupport asynchronous collaborations. We identify what knowledge workers prefer\nAI-powered reminders to remind them about and how they would like to interact\nwith these reminders. Using mixed methods and a self-assessment methodology, we\ninvestigate the relationship between information workers' work styles and the\nperceived value of the Viva Daily Briefing Email to identify users who are more\nlikely to benefit from AI-powered reminders for asynchronous collaborations. We\nconclude by discussing the experiences and futures of AI-powered reminders for\ncollaborative tasks and asynchronous collaborations.","updated":1709430481000,"published":1709430481000,"authors":["Katelyn Morrison","Shamsi Iqbal","Eric Horvitz"],"comments":"18 pages, 3 figures, 3 tables, accepted to ACM CSCW 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"24":{"arxiv_id":"2403.01594v1","url":"http:\/\/arxiv.org\/abs\/2403.01594v1","title":"Never Tell the Trick: Covert Interactive Mixed Reality System for\n  Immersive Storytelling","summary":"This study explores the integration of Ultra-Wideband (UWB) technology into\nMixed Reality (MR) Systems for immersive storytelling. Addressing the\nlimitations of existing technologies like Microsoft Kinect and HTC Vive, the\nresearch focuses on overcoming challenges in robustness to occlusion, tracking\nvolume, and cost efficiency in props tracking. Utilizing UWB technology, the\ninteractive MR system enhances the scope of performance art by enabling larger\ntracking areas, more reliable and cheaper multi-prop tracking, and reducing\nocclusion issues. Preliminary user tests suggest meaningful improvements in\nimmersive experience, promising a new possibility in Extended Reality (XR)\ntheater, performance art and immersive game.","updated":1709493434000,"published":1709493434000,"authors":["Chanwoo Lee","Kyubeom Shim","Sanggyo Seo","Gwonu Ryu","Yongsoon Choi"],"comments":"To be presented in IEEE VR 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"25":{"arxiv_id":"2403.01697v1","url":"http:\/\/arxiv.org\/abs\/2403.01697v1","title":"Dismantling Gender Blindness in Online Discussion of a Crime\/Gender\n  Dichotomy","summary":"Contemporary feminists utilize social media for activism, while backlashes\ncome along. The gender-related discourses are often diminished when addressing\npublic events regarding sexism and gender inequality on social media platforms.\nThe dichotomized debate around the Tangshan beating incident in China\nepitomized how criminal interpretations of gender-related violence became a\nbacklash against feminist expressions. By analyzing posts on Weibo using mixed\nmethods, we describe the emerging discursive patterns around crime and gender,\nuncovering the inherent gender-blind sexism that refutes feminist discourses on\nthe social platform. We also highlight the critical restrictions facing\ngrassroots feminist activism in Chinese cyberspace and propose implications for\nthe design and research related to digital feminist activism.","updated":1709522274000,"published":1709522274000,"authors":["Yigang Qin","Weilun Duan","Qunfang Wu","Zhicong Lu"],"comments":"31 pages, 3 figures, Accepted for publication in Proceedings of the\n  ACM on Human-Computer Interaction (CSCW 2024)","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"26":{"arxiv_id":"2403.01783v1","url":"http:\/\/arxiv.org\/abs\/2403.01783v1","title":"Towards A Diffractive Analysis of Prompt-Based Generative AI","summary":"Recent developments in prompt-based generative AI has given rise to discourse\nsurrounding the perceived ethical concerns, economic implications, and\nconsequences for the future of cultural production. As generative imagery\nbecomes pervasive in mainstream society, dominated primarily by emerging\nindustry leaders, we encourage that the role of the CHI community be one of\ninquiry; to investigate the numerous ways in which generative AI has the\npotential to, and already is, augmenting human creativity. In this paper, we\nconducted a diffractive analysis exploring the potential role of prompt-based\ninterfaces in artists' creative practice. Over a two week period, seven visual\nartists were given access to a personalised instance of Stable Diffusion,\nfine-tuned on a dataset of their work. In the following diffractive analysis,\nwe identified two dominant modes adopted by participants, AI for ideation, and\nAI for production. We furthermore present a number of ethical design\nconsiderations for the future development of generative AI interfaces.","updated":1709537029000,"published":1709537029000,"authors":["Nina Rajcic","Maria Teresa Llano","Jon McCormack"],"comments":"Preprint of paper accepted for CHI 2024","categories":["cs.HC","J.5; H.1.2; H.5"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3641971","journal_ref":null,"peer_reviewed":true},"27":{"arxiv_id":"2403.01832v1","url":"http:\/\/arxiv.org\/abs\/2403.01832v1","title":"Model-Based Data-Centric AI: Bridging the Divide Between Academic Ideals\n  and Industrial Pragmatism","summary":"This paper delves into the contrasting roles of data within academic and\nindustrial spheres, highlighting the divergence between Data-Centric AI and\nModel-Agnostic AI approaches. We argue that while Data-Centric AI focuses on\nthe primacy of high-quality data for model performance, Model-Agnostic AI\nprioritizes algorithmic flexibility, often at the expense of data quality\nconsiderations. This distinction reveals that academic standards for data\nquality frequently do not meet the rigorous demands of industrial applications,\nleading to potential pitfalls in deploying academic models in real-world\nsettings. Through a comprehensive analysis, we address these disparities,\npresenting both the challenges they pose and strategies for bridging the gap.\nFurthermore, we propose a novel paradigm: Model-Based Data-Centric AI, which\naims to reconcile these differences by integrating model considerations into\ndata optimization processes. This approach underscores the necessity for\nevolving data requirements that are sensitive to the nuances of both academic\nresearch and industrial deployment. By exploring these discrepancies, we aim to\nfoster a more nuanced understanding of data's role in AI development and\nencourage a convergence of academic and industrial standards to enhance AI's\nreal-world applicability.","updated":1709540955000,"published":1709540955000,"authors":["Chanjun Park","Minsoo Khang","Dahyun Kim"],"comments":"Accepted for Data-centric Machine Learning Research (DMLR) Workshop\n  at ICLR 2024","categories":["cs.AI","cs.CL"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"28":{"arxiv_id":"2403.02145v1","url":"http:\/\/arxiv.org\/abs\/2403.02145v1","title":"'SSL?! What on earth is that?': Towards Designing Age-Inclusive Secure\n  Smartphone Browsing","summary":"Owing to the increase in 'certified' phishing websites, there is a steady\nincrease in the number of phishing cases and general susceptibility to\nphishing. Trust mechanisms (e.g., HTTPS Lock Indicators, SSL Certificates) that\nhelp differentiate genuine and phishing websites should therefore be evaluated\nfor their effectiveness in preventing vulnerable users from accessing phishing\nwebsites. In this article, we present a study involving 18 adults (male-6;\nfemale-12) and 12 older adults (male-4; female-8) to understand the usability\nof current trust mechanisms and preferred modalities in a conceptualized\nmechanism. In the first part of the study, using Chrome browser on Android, we\nasked the participants to browse a banking website and a government website for\ndigital particulars. We asked them to identify which one of the two was a\nphishing website, rate the usability of both websites and provide qualitative\nfeedback on the trust mechanisms. In the second part, we conceptualized an\nalternative trust mechanism, which allows seeking social, community and\nAI-based support to make website trust-related decisions. Herein, we asked the\nparticipants as to which modality (social, community or AI) they prefer to seek\nsupport from and why it is preferred. Using the current trust mechanisms, none\nof the participants were able to identify the phishing website. As the\nparticipants rated the current mechanisms poorly in terms of usability, they\nexpressed various difficulties that largely did not differ between adults and\nolder adults. In the conceptualized mechanism, we observed a notable difference\nin the preferred modalities, in that, older adults primarily preferred social\nsupport. In addition to these overall findings, specific observations suggest\nthat future trust mechanisms should not only consider age-specific needs but\nalso incorporate substantial improvement in terms of usability.","updated":1709567673000,"published":1709567673000,"authors":["Pavithren V. S. Pakianathan","L. Siddharth","Sujithra Raviselvam","Kristin L. Wood","Hyowon Lee","Pin Sym Foong","Jianying Zhou","Simon Tangi Perrault"],"comments":"This version was last submitted to EuroUSEC 2023 - European Symposium\n  on Usable Security. It was later invited for poster submission at the same\n  conference","categories":["cs.HC","cs.CY"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"29":{"arxiv_id":"2403.02199v1","url":"http:\/\/arxiv.org\/abs\/2403.02199v1","title":"Piet: Facilitating Color Authoring for Motion Graphics Video","summary":"Motion graphic (MG) videos are effective and compelling for presenting\ncomplex concepts through animated visuals; and colors are important to convey\ndesired emotions, maintain visual continuity, and signal narrative transitions.\nHowever, current video color authoring workflows are fragmented, lacking\ncontextual previews, hindering rapid theme adjustments, and not aligning with\nprogressive authoring flows of designers. To bridge this gap, we introduce\nPiet, the first tool tailored for MG video color authoring. Piet features an\ninteractive palette to visually represent color distributions, support\ncontrollable focus levels, and enable quick theme probing via grouped color\nshifts. We interviewed 6 domain experts to identify the frustrations in current\ntools and inform the design of Piet. An in-lab user study with 13 expert\ndesigners showed that Piet effectively simplified the MG video color authoring\nand reduced the friction in creative color theme exploration.","updated":1709570490000,"published":1709570490000,"authors":["Xinyu Shi","Yinghou Wang","Yun Wang","Jian Zhao"],"comments":"Accepted by CHI 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"30":{"arxiv_id":"2403.02202v1","url":"http:\/\/arxiv.org\/abs\/2403.02202v1","title":"Exploring Interactive Color Palettes for Abstraction-Driven Exploratory\n  Image Colorization","summary":"Color design is essential in areas such as product, graphic, and fashion\ndesign. However, current tools like Photoshop, with their concrete-driven color\nmanipulation approach, often stumble during early ideation, favoring polished\nend results over initial exploration. We introduced Mondrian as a test-bed for\nabstraction-driven approach using interactive color palettes for image\ncolorization. Through a formative study with six design experts, we selected\nthree design options for visual abstractions in color design and developed\nMondrian where humans work with abstractions and AI manages the concrete\naspects. We carried out a user study to understand the benefits and challenges\nof each abstraction format and compare the Mondrian with Photoshop. A survey\ninvolving 100 participants further examined the influence of each abstraction\nformat on color composition perceptions. Findings suggest that interactive\nvisual abstractions encourage a non-linear exploration workflow and an open\nmindset during ideation, thus providing better creative affordance.","updated":1709570876000,"published":1709570876000,"authors":["Xinyu Shi","Mingyu Liu","Ziqi Zhou","Ali Neshati","Ryan Rossi","Jian Zhao"],"comments":"Accepted by CHI 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"31":{"arxiv_id":"2403.02491v1","url":"http:\/\/arxiv.org\/abs\/2403.02491v1","title":"Ivie: Lightweight Anchored Explanations of Just-Generated Code","summary":"Programming assistants have reshaped the experience of programming into one\nwhere programmers spend less time writing and more time critically examining\ncode. In this paper, we explore how programming assistants can be extended to\naccelerate the inspection of generated code. We introduce an extension to the\nprogramming assistant called Ivie, or instantly visible in-situ explanations.\nWhen using Ivie, a programmer's generated code is instantly accompanied by\nexplanations positioned just adjacent to the code. Our design was optimized for\nextremely low-cost invocation and dismissal. Explanations are compact and\ninformative. They describe meaningful expressions, from individual variables to\nentire blocks of code. We present an implementation of Ivie that forks VS Code,\napplying a modern LLM for timely segmentation and explanation of generated\ncode. In a lab study, we compared Ivie to a contemporary baseline tool for code\nunderstanding. Ivie improved understanding of generated code, and was received\nby programmers as a highly useful, low distraction, desirable complement to the\nprogramming assistant.","updated":1709587325000,"published":1709587325000,"authors":["Litao Yan","Alyssa Hwang","Zhiyuan Wu","Andrew Head"],"comments":"15 pages, 10 figures, to be published in the CHI Conference on Human\n  Factors in Computing Systems (CHI 24)","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642239","journal_ref":null,"peer_reviewed":true},"32":{"arxiv_id":"2403.02723v1","url":"http:\/\/arxiv.org\/abs\/2403.02723v1","title":"Minimum Topology Attacks for Graph Neural Networks","summary":"With the great popularity of Graph Neural Networks (GNNs), their robustness\nto adversarial topology attacks has received significant attention. Although\nmany attack methods have been proposed, they mainly focus on fixed-budget\nattacks, aiming at finding the most adversarial perturbations within a fixed\nbudget for target node. However, considering the varied robustness of each\nnode, there is an inevitable dilemma caused by the fixed budget, i.e., no\nsuccessful perturbation is found when the budget is relatively small, while if\nit is too large, the yielding redundant perturbations will hurt the\ninvisibility. To break this dilemma, we propose a new type of topology attack,\nnamed minimum-budget topology attack, aiming to adaptively find the minimum\nperturbation sufficient for a successful attack on each node. To this end, we\npropose an attack model, named MiBTack, based on a dynamic projected gradient\ndescent algorithm, which can effectively solve the involving non-convex\nconstraint optimization on discrete topology. Extensive results on three GNNs\nand four real-world datasets show that MiBTack can successfully lead all target\nnodes misclassified with the minimum perturbation edges. Moreover, the obtained\nminimum budget can be used to measure node robustness, so we can explore the\nrelationships of robustness, topology, and uncertainty for nodes, which is\nbeyond what the current fixed-budget topology attacks can offer.","updated":1709623752000,"published":1709623752000,"authors":["Mengmei Zhang","Xiao Wang","Chuan Shi","Lingjuan Lyu","Tianchi Yang","Junping Du"],"comments":"Published on WWW 2023. Proceedings of the ACM Web Conference 2023","categories":["cs.AI"],"primary_category":"cs.AI","doi":"10.1145\/3543507.3583509","journal_ref":null,"peer_reviewed":true},"33":{"arxiv_id":"2403.02870v1","url":"http:\/\/arxiv.org\/abs\/2403.02870v1","title":"Precise Extraction of Deep Learning Models via Side-Channel Attacks on\n  Edge\/Endpoint Devices","summary":"With growing popularity, deep learning (DL) models are becoming larger-scale,\nand only the companies with vast training datasets and immense computing power\ncan manage their business serving such large models. Most of those DL models\nare proprietary to the companies who thus strive to keep their private models\nsafe from the model extraction attack (MEA), whose aim is to steal the model by\ntraining surrogate models. Nowadays, companies are inclined to offload the\nmodels from central servers to edge\/endpoint devices. As revealed in the latest\nstudies, adversaries exploit this opportunity as new attack vectors to launch\nside-channel attack (SCA) on the device running victim model and obtain various\npieces of the model information, such as the model architecture (MA) and image\ndimension (ID). Our work provides a comprehensive understanding of such a\nrelationship for the first time and would benefit future MEA studies in both\noffensive and defensive sides in that they may learn which pieces of\ninformation exposed by SCA are more important than the others. Our analysis\nadditionally reveals that by grasping the victim model information from SCA,\nMEA can get highly effective and successful even without any prior knowledge of\nthe model. Finally, to evince the practicality of our analysis results, we\nempirically apply SCA, and subsequently, carry out MEA under realistic threat\nassumptions. The results show up to 5.8 times better performance than when the\nadversary has no model information about the victim model.","updated":1709637982000,"published":1709637982000,"authors":["Younghan Lee","Sohee Jun","Yungi Cho","Woorim Han","Hyungon Moon","Yunheung Paek"],"comments":"Accepted by 27th European Symposium on Research in Computer Security\n  (ESORICS 2022)","categories":["cs.AI","cs.CR","cs.LG"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"34":{"arxiv_id":"2403.02906v1","url":"http:\/\/arxiv.org\/abs\/2403.02906v1","title":"Citizen Science and Machine Learning for Research and Nature\n  Conservation: The Case of Eurasian Lynx, Free-ranging Rodents and Insects","summary":"Technology is increasingly used in Nature Reserves and National Parks around\nthe world to support conservation efforts. Endangered species, such as the\nEurasian Lynx (Lynx lynx), are monitored by a network of automatic photo traps.\nYet, this method produces vast amounts of data, which needs to be prepared,\nanalyzed and interpreted. Therefore, researchers working in this area\nincreasingly need support to process this incoming information. One opportunity\nis to seek support from volunteer Citizen Scientists who can help label the\ndata, however, it is challenging to retain their interest. Another way is to\nautomate the process with image recognition using convolutional neural\nnetworks. During the panel, we will discuss considerations related to nature\nresearch and conservation as well as opportunities for the use of Citizen\nScience and Machine Learning to expedite the process of data preparation,\nlabelling and analysis.","updated":1709640807000,"published":1709640807000,"authors":["Kinga Skorupska","Rafa\u0142 Stryjek","Izabela Wierzbowska","Piotr Bebas","Maciej Grzeszczuk","Piotr Gago","Jaros\u0142aw Kowalski","Maciej Krzywicki","Jagoda Lazarek","Wies\u0142aw Kope\u0107"],"comments":"10 pages, 11 figures, MIDI 2023 conference","categories":["cs.HC","cs.CV","cs.CY","cs.LG"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"35":{"arxiv_id":"2403.02908v1","url":"http:\/\/arxiv.org\/abs\/2403.02908v1","title":"Preserving Tangible and Intangible Cultural Heritage: the Cases of\n  Volterra and Atari","summary":"At first glance, the ruins of the Roman Theatre in the Italian town of\nVolterra have little in common with cassette tapes containing Atari games. One\nis certainly considered an important historical landmark, while the consensus\non the importance of the other is partial at best. Still, both are remnants of\ntimes vastly different from the present and are at risk of oblivion. Unearthed\narchitectural structures are exposed to the elements just as the deteriorating\nsignals stored on magnetic tapes. However, the rate of deterioration is much\nfaster with the magnetic media, as their life expectancy is counted in decades,\nwhereas the Roman Theater, which is already in ruin, measures its lifespan in\ncenturies. Hence, both would benefit from some form of digital preservation and\nreconstruction. In this panel, we discuss how to sustainably preserve tangible\nand intangible cultural artifacts for future generations.","updated":1709641088000,"published":1709641088000,"authors":["Maciej Grzeszczuk","Kinga Skorupska","Pawe\u0142 Grabarczyk","W\u0142adys\u0142aw Fuchs","Paul F. Aubin","Mark E. Dietrick","Barbara Karpowicz","Rafa\u0142 Mas\u0142yk","Pavlo Zinevych","Wiktor Stawski","Stanis\u0142aw Knapi\u0144ski","Wies\u0142aw Kope\u0107"],"comments":"8 pages, including 1 page of bibliography, 9 figures. Panel summary\n  to be published in proceedings from 11th Machine Intelligence and Digital\n  Interaction MIDI Conference","categories":["cs.CY","cs.DL","cs.HC"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"36":{"arxiv_id":"2403.02928v1","url":"http:\/\/arxiv.org\/abs\/2403.02928v1","title":"User-Driven Adaptation: Tailoring Autonomous Driving Systems with\n  Dynamic Preferences","summary":"In the realm of autonomous vehicles, dynamic user preferences are critical\nyet challenging to accommodate. Existing methods often misrepresent these\npreferences, either by overlooking their dynamism or overburdening users as\nhumans often find it challenging to express their objectives mathematically.\nThe previously introduced framework, which interprets dynamic preferences as\ninherent uncertainty and includes a ``human-on-the-loop'' mechanism enabling\nusers to give feedback when dissatisfied with system behaviors, addresses this\ngap. In this study, we further enhance the approach with a user study of 20\nparticipants, focusing on aligning system behavior with user expectations\nthrough feedback-driven adaptation. The findings affirm the approach's ability\nto effectively merge algorithm-driven adjustments with user complaints, leading\nto improved participants' subjective satisfaction in autonomous systems.","updated":1709642694000,"published":1709642694000,"authors":["Mingyue Zhang","Jialong Li","Nianyu Li","Eunsuk Kang","Kenji Tei"],"comments":"accepted by CHI LBW 2024","categories":["cs.HC","cs.SE"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"37":{"arxiv_id":"2403.03176v1","url":"http:\/\/arxiv.org\/abs\/2403.03176v1","title":"Unifying and Certifying Top-Quality Planning","summary":"The growing utilization of planning tools in practical scenarios has sparked\nan interest in generating multiple high-quality plans. Consequently, a range of\ncomputational problems under the general umbrella of top-quality planning were\nintroduced over a short time period, each with its own definition. In this\nwork, we show that the existing definitions can be unified into one, based on a\ndominance relation. The different computational problems, therefore, simply\ncorrespond to different dominance relations. Given the unified definition, we\ncan now certify the top-quality of the solutions, leveraging existing\ncertification of unsolvability and optimality. We show that task\ntransformations found in the existing literature can be employed for the\nefficient certification of various top-quality planning problems and propose a\nnovel transformation to efficiently certify loopless top-quality planning.","updated":1709662398000,"published":1709662398000,"authors":["Michael Katz","Junkyu Lee","Shirin Sohrabi"],"comments":"To appear at ICAPS 2024","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"38":{"arxiv_id":"2403.03203v1","url":"http:\/\/arxiv.org\/abs\/2403.03203v1","title":"CLEVR-POC: Reasoning-Intensive Visual Question Answering in Partially\n  Observable Environments","summary":"The integration of learning and reasoning is high on the research agenda in\nAI. Nevertheless, there is only a little attention to use existing background\nknowledge for reasoning about partially observed scenes to answer questions\nabout the scene. Yet, we as humans use such knowledge frequently to infer\nplausible answers to visual questions (by eliminating all inconsistent ones).\nSuch knowledge often comes in the form of constraints about objects and it\ntends to be highly domain or environment-specific. We contribute a novel\nbenchmark called CLEVR-POC for reasoning-intensive visual question answering\n(VQA) in partially observable environments under constraints. In CLEVR-POC,\nknowledge in the form of logical constraints needs to be leveraged to generate\nplausible answers to questions about a hidden object in a given partial scene.\nFor instance, if one has the knowledge that all cups are colored either red,\ngreen or blue and that there is only one green cup, it becomes possible to\ndeduce the color of an occluded cup as either red or blue, provided that all\nother cups, including the green one, are observed. Through experiments, we\nobserve that the low performance of pre-trained vision language models like\nCLIP (~ 22%) and a large language model (LLM) like GPT-4 (~ 46%) on CLEVR-POC\nascertains the necessity for frameworks that can handle reasoning-intensive\ntasks where environment-specific background knowledge is available and crucial.\nFurthermore, our demonstration illustrates that a neuro-symbolic model, which\nintegrates an LLM like GPT-4 with a visual perception network and a formal\nlogical reasoner, exhibits exceptional performance on CLEVR-POC.","updated":1709664097000,"published":1709664097000,"authors":["Savitha Sam Abraham","Marjan Alirezaie","Luc De Raedt"],"comments":"17 pages, 10 images, Accepted at LREC-COLING 2024 - The 2024 Joint\n  International Conference on Computational Linguistics, Language Resources and\n  Evaluation","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"39":{"arxiv_id":"2403.03306v1","url":"http:\/\/arxiv.org\/abs\/2403.03306v1","title":"Wrist-bound Guanxi, Jiazu, and Kuolie: Unpacking Chinese Adolescent\n  Smartwatch-Mediated Socialization","summary":"Adolescent peer relationships, essential for their development, are\nincreasingly mediated by digital technologies. As this trend continues,\nwearable devices, especially smartwatches tailored for adolescents, are\nreshaping their socialization. In China, smartwatches like XTC have gained wide\npopularity, introducing unique features such as \"Bump-to-Connect\" and exclusive\nsocial platforms. Nonetheless, how these devices influence adolescents' peer\nexperience remains unknown. Addressing this, we interviewed 18 Chinese\nadolescents (age: 11 -- 16), discovering a smartwatch-mediated social\necosystem. Our findings highlight the ice-breaking role of smartwatches in\nfriendship initiation and their use for secret messaging with local peers.\nWithin the online smartwatch community, peer status is determined by likes and\nvisibility, leading to diverse pursuit activities (i.e., chu guanxi, jiazu,\nkuolie) and negative social dynamics. We discuss the core affordances of\nsmartwatches and Chinese cultural factors that influence adolescent social\nbehavior and offer implications for designing future wearables that responsibly\nand safely support adolescent socialization.","updated":1709669350000,"published":1709669350000,"authors":["Lanjing Liu","Chao Zhang","Zhicong Lu"],"comments":"Conditionally Accepted at CHI 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642044","journal_ref":null,"peer_reviewed":true},"40":{"arxiv_id":"2403.03382v1","url":"http:\/\/arxiv.org\/abs\/2403.03382v1","title":"Adaptive Discovering and Merging for Incremental Novel Class Discovery","summary":"One important desideratum of lifelong learning aims to discover novel classes\nfrom unlabelled data in a continuous manner. The central challenge is twofold:\ndiscovering and learning novel classes while mitigating the issue of\ncatastrophic forgetting of established knowledge. To this end, we introduce a\nnew paradigm called Adaptive Discovering and Merging (ADM) to discover novel\ncategories adaptively in the incremental stage and integrate novel knowledge\ninto the model without affecting the original knowledge. To discover novel\nclasses adaptively, we decouple representation learning and novel class\ndiscovery, and use Triple Comparison (TC) and Probability Regularization (PR)\nto constrain the probability discrepancy and diversity for adaptive category\nassignment. To merge the learned novel knowledge adaptively, we propose a\nhybrid structure with base and novel branches named Adaptive Model Merging\n(AMM), which reduces the interference of the novel branch on the old classes to\npreserve the previous knowledge, and merges the novel branch to the base model\nwithout performance loss and parameter growth. Extensive experiments on several\ndatasets show that ADM significantly outperforms existing class-incremental\nNovel Class Discovery (class-iNCD) approaches. Moreover, our AMM also benefits\nthe class-incremental Learning (class-IL) task by alleviating the catastrophic\nforgetting problem.","updated":1709684223000,"published":1709684223000,"authors":["Guangyao Chen","Peixi Peng","Yangru Huang","Mengyue Geng","Yonghong Tian"],"comments":"AAAI 2024. arXiv admin note: text overlap with arXiv:2207.08605 by\n  other authors","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"41":{"arxiv_id":"2403.03449v1","url":"http:\/\/arxiv.org\/abs\/2403.03449v1","title":"SalienTime: User-driven Selection of Salient Time Steps for Large-Scale\n  Geospatial Data Visualization","summary":"The voluminous nature of geospatial temporal data from physical monitors and\nsimulation models poses challenges to efficient data access, often resulting in\ncumbersome temporal selection experiences in web-based data portals. Thus,\nselecting a subset of time steps for prioritized visualization and pre-loading\nis highly desirable. Addressing this issue, this paper establishes a\nmultifaceted definition of salient time steps via extensive need-finding\nstudies with domain experts to understand their workflows. Building on this, we\npropose a novel approach that leverages autoencoders and dynamic programming to\nfacilitate user-driven temporal selections. Structural features, statistical\nvariations, and distance penalties are incorporated to make more flexible\nselections. User-specified priorities, spatial regions, and aggregations are\nused to combine different perspectives. We design and implement a web-based\ninterface to enable efficient and context-aware selection of time steps and\nevaluate its efficacy and usability through case studies, quantitative\nevaluations, and expert interviews.","updated":1709699230000,"published":1709699230000,"authors":["Juntong Chen","Haiwen Huang","Huayuan Ye","Zhong Peng","Chenhui Li","Changbo Wang"],"comments":"In Proceedings of the CHI Conference on Human Factors in Computing\n  Systems (CHI'24), May 11-16, 2024, Honolulu, HI, USA","categories":["cs.HC","cs.LG"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642944","journal_ref":null,"peer_reviewed":true},"42":{"arxiv_id":"2403.03594v1","url":"http:\/\/arxiv.org\/abs\/2403.03594v1","title":"Assessing the Aesthetic Evaluation Capabilities of GPT-4 with Vision:\n  Insights from Group and Individual Assessments","summary":"Recently, it has been recognized that large language models demonstrate high\nperformance on various intellectual tasks. However, few studies have\ninvestigated alignment with humans in behaviors that involve sensibility, such\nas aesthetic evaluation. This study investigates the performance of GPT-4 with\nVision, a state-of-the-art language model that can handle image input, on the\ntask of aesthetic evaluation of images. We employ two tasks, prediction of the\naverage evaluation values of a group and an individual's evaluation values. We\ninvestigate the performance of GPT-4 with Vision by exploring prompts and\nanalyzing prediction behaviors. Experimental results reveal GPT-4 with Vision's\nsuperior performance in predicting aesthetic evaluations and the nature of\ndifferent responses to beauty and ugliness. Finally, we discuss developing an\nAI system for aesthetic evaluation based on scientific knowledge of the human\nperception of beauty, employing agent technologies that integrate traditional\ndeep learning models with large language models.","updated":1709720829000,"published":1709720829000,"authors":["Yoshia Abe","Tatsuya Daikoku","Yasuo Kuniyoshi"],"comments":"8 pages, 6 figures, submitted to The 38th Annual Conference of the\n  Japanese Society for Artificial Intelligence, 2024","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"43":{"arxiv_id":"2403.03822v1","url":"http:\/\/arxiv.org\/abs\/2403.03822v1","title":"HoLens: A Visual Analytics Design for Higher-order Movement Modeling and\n  Visualization","summary":"Higher-order patterns reveal sequential multistep state transitions, which\nare usually superior to origin-destination analysis, which depicts only\nfirst-order geospatial movement patterns. Conventional methods for higher-order\nmovement modeling first construct a directed acyclic graph (DAG) of movements,\nthen extract higher-order patterns from the DAG. However, DAG-based methods\nheavily rely on the identification of movement keypoints that are challenging\nfor sparse movements and fail to consider the temporal variants that are\ncritical for movements in urban environments. To overcome the limitations, we\npropose HoLens, a novel approach for modeling and visualizing higher-order\nmovement patterns in the context of an urban environment. HoLens mainly makes\ntwofold contributions: first, we design an auto-adaptive movement aggregation\nalgorithm that self-organizes movements hierarchically by considering spatial\nproximity, contextual information, and temporal variability; second, we develop\nan interactive visual analytics interface consisting of well-established\nvisualization techniques, including the H-Flow for visualizing the higher-order\npatterns on the map and the higher-order state sequence chart for representing\nthe higher-order state transitions. Two real-world case studies manifest that\nthe method can adaptively aggregate the data and exhibit the process of how to\nexplore the higher-order patterns by HoLens. We also demonstrate our approach's\nfeasibility, usability, and effectiveness through an expert interview with\nthree domain experts.","updated":1709741331000,"published":1709741331000,"authors":["Zezheng Feng","Fang Zhu","Hongjun Wang","Jianing Hao","ShuangHua Yang","Wei Zeng","Huamin Qu"],"comments":"20 pages, 18 figures, is accepted by computational visual media\n  journal","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"44":{"arxiv_id":"2403.04014v1","url":"http:\/\/arxiv.org\/abs\/2403.04014v1","title":"PromptCharm: Text-to-Image Generation through Multi-modal Prompting and\n  Refinement","summary":"The recent advancements in Generative AI have significantly advanced the\nfield of text-to-image generation. The state-of-the-art text-to-image model,\nStable Diffusion, is now capable of synthesizing high-quality images with a\nstrong sense of aesthetics. Crafting text prompts that align with the model's\ninterpretation and the user's intent thus becomes crucial. However, prompting\nremains challenging for novice users due to the complexity of the stable\ndiffusion model and the non-trivial efforts required for iteratively editing\nand refining the text prompts. To address these challenges, we propose\nPromptCharm, a mixed-initiative system that facilitates text-to-image creation\nthrough multi-modal prompt engineering and refinement. To assist novice users\nin prompting, PromptCharm first automatically refines and optimizes the user's\ninitial prompt. Furthermore, PromptCharm supports the user in exploring and\nselecting different image styles within a large database. To assist users in\neffectively refining their prompts and images, PromptCharm renders model\nexplanations by visualizing the model's attention values. If the user notices\nany unsatisfactory areas in the generated images, they can further refine the\nimages through model attention adjustment or image inpainting within the rich\nfeedback loop of PromptCharm. To evaluate the effectiveness and usability of\nPromptCharm, we conducted a controlled user study with 12 participants and an\nexploratory user study with another 12 participants. These two studies show\nthat participants using PromptCharm were able to create images with higher\nquality and better aligned with the user's expectations compared with using two\nvariants of PromptCharm that lacked interaction or visualization support.","updated":1709754901000,"published":1709754901000,"authors":["Zhijie Wang","Yuheng Huang","Da Song","Lei Ma","Tianyi Zhang"],"comments":"To appear in the 2024 CHI Conference on Human Factors in Computing\n  Systems (CHI '24), May 11--16, 2024, Honolulu, HI, USA","categories":["cs.HC","cs.AI"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642803","journal_ref":null,"peer_reviewed":true},"45":{"arxiv_id":"2403.04124v1","url":"http:\/\/arxiv.org\/abs\/2403.04124v1","title":"Privacy-preserving Fine-tuning of Large Language Models through Flatness","summary":"The privacy concerns associated with the use of Large Language Models (LLMs)\nhave grown recently with the development of LLMs such as ChatGPT. Differential\nPrivacy (DP) techniques are explored in existing work to mitigate their privacy\nrisks at the cost of generalization degradation. Our paper reveals that the\nflatness of DP-trained models' loss landscape plays an essential role in the\ntrade-off between their privacy and generalization. We further propose a\nholistic framework to enforce appropriate weight flatness, which substantially\nimproves model generalization with competitive privacy preservation. It\ninnovates from three coarse-to-grained levels, including perturbation-aware\nmin-max optimization on model weights within a layer, flatness-guided sparse\nprefix-tuning on weights across layers, and weight knowledge distillation\nbetween DP \\& non-DP weights copies. Comprehensive experiments of both\nblack-box and white-box scenarios are conducted to demonstrate the\neffectiveness of our proposal in enhancing generalization and maintaining DP\ncharacteristics. For instance, on text classification dataset QNLI, DP-Flat\nachieves similar performance with non-private full fine-tuning but with DP\nguarantee under privacy budget $\\epsilon=3$, and even better performance given\nhigher privacy budgets. Codes are provided in the supplement.","updated":1709772251000,"published":1709772251000,"authors":["Tiejin Chen","Longchao Da","Huixue Zhou","Pingzhi Li","Kaixiong Zhou","Tianlong Chen","Hua Wei"],"comments":"Accepted to ICLR 2024 SeT LLM Workshop","categories":["cs.AI","I.2"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"46":{"arxiv_id":"2403.04135v1","url":"http:\/\/arxiv.org\/abs\/2403.04135v1","title":"Unsupervised Learning of Harmonic Analysis Based on Neural HSMM with\n  Code Quality Templates","summary":"This paper presents a method of unsupervised learning of harmonic analysis\nbased on a hidden semi-Markov model (HSMM). We introduce the chord quality\ntemplates, which specify the probability of pitch class emissions given a root\nnote and a chord quality. Other probability distributions that comprise the\nHSMM are automatically learned via unsupervised learning, which has been a\nchallenge in existing research. The results of the harmonic analysis of the\nproposed model were evaluated using existing labeled data. While our proposed\nmethod has yet to perform as well as existing models that used supervised\nlearning and complex rule design, it has the advantage of not requiring\nexpensive labeled data or rule elaboration. Furthermore, we also show how to\nrecognize the tonic without prior knowledge, based on the transition\nprobabilities of the Markov model.","updated":1709774988000,"published":1709774988000,"authors":["Yui Uehara"],"comments":"20 pages, 5 figures, the original edition of this paper will be\n  published in the ICNMC2024 Proceedings and this arXiv publication is a copy","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"47":{"arxiv_id":"2403.04387v1","url":"http:\/\/arxiv.org\/abs\/2403.04387v1","title":"Comparison of Deep Learning Techniques on Human Activity Recognition\n  using Ankle Inertial Signals","summary":"Human Activity Recognition (HAR) is one of the fundamental building blocks of\nhuman assistive devices like orthoses and exoskeletons. There are different\napproaches to HAR depending on the application. Numerous studies have been\nfocused on improving them by optimising input data or classification\nalgorithms. However, most of these studies have been focused on applications\nlike security and monitoring, smart devices, the internet of things, etc. On\nthe other hand, HAR can help adjust and control wearable assistive devices, yet\nthere has not been enough research facilitating its implementation. In this\nstudy, we propose several models to predict four activities from inertial\nsensors located in the ankle area of a lower-leg assistive device user. This\nchoice is because they do not need to be attached to the user's skin and can be\ndirectly implemented inside the control unit of the device. The proposed models\nare based on Artificial Neural Networks and could achieve up to 92.8% average\nclassification accuracy","updated":1709807345000,"published":1709807345000,"authors":["Farhad Nazari","Darius Nahavandi","Navid Mohajer","Abbas Khosravi"],"comments":"This is the accepted version of an article published in the\n  proceedings of the 2022 IEEE International Conference on Systems, Man, and\n  Cybernetics (SMC)","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1109\/SMC53654.2022.9945472","journal_ref":"2022 IEEE International Conference on Systems, Man, and\n  Cybernetics (SMC)","peer_reviewed":true},"48":{"arxiv_id":"2403.04449v1","url":"http:\/\/arxiv.org\/abs\/2403.04449v1","title":"Feedback-Generation for Programming Exercises With GPT-4","summary":"Ever since Large Language Models (LLMs) and related applications have become\nbroadly available, several studies investigated their potential for assisting\neducators and supporting students in higher education. LLMs such as Codex,\nGPT-3.5, and GPT 4 have shown promising results in the context of large\nprogramming courses, where students can benefit from feedback and hints if\nprovided timely and at scale. This paper explores the quality of GPT-4 Turbo's\ngenerated output for prompts containing both the programming task specification\nand a student's submission as input. Two assignments from an introductory\nprogramming course were selected, and GPT-4 was asked to generate feedback for\n55 randomly chosen, authentic student programming submissions. The output was\nqualitatively analyzed regarding correctness, personalization, fault\nlocalization, and other features identified in the material. Compared to prior\nwork and analyses of GPT-3.5, GPT-4 Turbo shows notable improvements. For\nexample, the output is more structured and consistent. GPT-4 Turbo can also\naccurately identify invalid casing in student programs' output. In some cases,\nthe feedback also includes the output of the student program. At the same time,\ninconsistent feedback was noted such as stating that the submission is correct\nbut an error needs to be fixed. The present work increases our understanding of\nLLMs' potential, limitations, and how to integrate them into e-assessment\nsystems, pedagogical scenarios, and instructing students who are using\napplications based on GPT-4.","updated":1709815072000,"published":1709815072000,"authors":["Imen Azaiz","Natalie Kiesler","Sven Strickroth"],"comments":"accepted at ITiCSE 2024, Milan, Italy","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"49":{"arxiv_id":"2403.04511v1","url":"http:\/\/arxiv.org\/abs\/2403.04511v1","title":"Uncovering the Deep Filter Bubble: Narrow Exposure in Short-Video\n  Recommendation","summary":"Filter bubbles have been studied extensively within the context of online\ncontent platforms due to their potential to cause undesirable outcomes such as\nuser dissatisfaction or polarization. With the rise of short-video platforms,\nthe filter bubble has been given extra attention because these platforms rely\non an unprecedented use of the recommender system to provide relevant content.\nIn our work, we investigate the deep filter bubble, which refers to the user\nbeing exposed to narrow content within their broad interests. We accomplish\nthis using one-year interaction data from a top short-video platform in China,\nwhich includes hierarchical data with three levels of categories for each\nvideo. We formalize our definition of a \"deep\" filter bubble within this\ncontext, and then explore various correlations within the data: first\nunderstanding the evolution of the deep filter bubble over time, and later\nrevealing some of the factors that give rise to this phenomenon, such as\nspecific categories, user demographics, and feedback type. We observe that\nwhile the overall proportion of users in a filter bubble remains largely\nconstant over time, the depth composition of their filter bubble changes. In\naddition, we find that some demographic groups that have a higher likelihood of\nseeing narrower content and implicit feedback signals can lead to less bubble\nformation. Finally, we propose some ways in which recommender systems can be\ndesigned to reduce the risk of a user getting caught in a bubble.","updated":1709820880000,"published":1709820880000,"authors":["Nicholas Sukiennik","Chen Gao","Nian Li"],"comments":"accepted to WWW 2024","categories":["cs.AI","H.3.5"],"primary_category":"cs.AI","doi":"10.1145\/3589334.3648159","journal_ref":null,"peer_reviewed":true},"50":{"arxiv_id":"2403.04571v1","url":"http:\/\/arxiv.org\/abs\/2403.04571v1","title":"Machine learning and information theory concepts towards an AI\n  Mathematician","summary":"The current state-of-the-art in artificial intelligence is impressive,\nespecially in terms of mastery of language, but not so much in terms of\nmathematical reasoning. What could be missing? Can we learn something useful\nabout that gap from how the brains of mathematicians go about their craft? This\nessay builds on the idea that current deep learning mostly succeeds at system 1\nabilities -- which correspond to our intuition and habitual behaviors -- but\nstill lacks something important regarding system 2 abilities -- which include\nreasoning and robust uncertainty estimation. It takes an\ninformation-theoretical posture to ask questions about what constitutes an\ninteresting mathematical statement, which could guide future work in crafting\nan AI mathematician. The focus is not on proving a given theorem but on\ndiscovering new and interesting conjectures. The central hypothesis is that a\ndesirable body of theorems better summarizes the set of all provable\nstatements, for example by having a small description length while at the same\ntime being close (in terms of number of derivation steps) to many provable\nstatements.","updated":1709824326000,"published":1709824326000,"authors":["Yoshua Bengio","Nikolay Malkin"],"comments":"To appear in the Bulletin of the AMS, 2024","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"51":{"arxiv_id":"2403.04757v1","url":"http:\/\/arxiv.org\/abs\/2403.04757v1","title":"Preliminary Guidelines For Combining Data Integration and Visual Data\n  Analysis","summary":"Data integration is often performed to consolidate information from multiple\ndisparate data sources during visual data analysis. However, integration\noperations are usually separate from visual analytics operations such as encode\nand filter in both interface design and empirical research. We conducted a\npreliminary user study to investigate whether and how data integration should\nbe incorporated directly into the visual analytics process. We used two\ninterface alternatives featuring contrasting approaches to the data preparation\nand analysis workflow: manual file-based ex-situ integration as a separate step\nfrom visual analytics operations; and automatic UI-based in-situ integration\nmerged with visual analytics operations. Participants were asked to complete\nspecific and free-form tasks with each interface, browsing for patterns,\ngenerating insights, and summarizing relationships between attributes\ndistributed across multiple files. Analyzing participants' interactions and\nfeedback, we found both task completion time and total interactions to be\nsimilar across interfaces and tasks, as well as unique integration strategies\nbetween interfaces and emergent behaviors related to satisficing and cognitive\nbias. Participants' time spent and interactions revealed that in-situ\nintegration enabled users to spend more time on analysis tasks compared with\nex-situ integration. Participants' integration strategies and analytical\nbehaviors revealed differences in interface usage for generating and tracking\nhypotheses and insights. With these results, we synthesized preliminary\nguidelines for designing future visual analytics interfaces that can support\nintegrating attributes throughout an active analysis process.","updated":1709837776000,"published":1709837776000,"authors":["Adam Coscia","Ashley Suh","Remco Chang","Alex Endert"],"comments":"Accepted to IEEE TVCG. 13 pages, 5 figures. For a study breakdown\n  video, see https:\/\/youtu.be\/NzVxHn-OpqQ . The source code, data and analysis\n  are available at https:\/\/github.com\/AdamCoscia\/Integration-Guidelines-VA","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1109\/TVCG.2023.3334513","journal_ref":null,"peer_reviewed":true},"52":{"arxiv_id":"2403.04758v1","url":"http:\/\/arxiv.org\/abs\/2403.04758v1","title":"KnowledgeVIS: Interpreting Language Models by Comparing\n  Fill-in-the-Blank Prompts","summary":"Recent growth in the popularity of large language models has led to their\nincreased usage for summarizing, predicting, and generating text, making it\nvital to help researchers and engineers understand how and why they work. We\npresent KnowledgeVis, a human-in-the-loop visual analytics system for\ninterpreting language models using fill-in-the-blank sentences as prompts. By\ncomparing predictions between sentences, KnowledgeVis reveals learned\nassociations that intuitively connect what language models learn during\ntraining to natural language tasks downstream, helping users create and test\nmultiple prompt variations, analyze predicted words using a novel semantic\nclustering technique, and discover insights using interactive visualizations.\nCollectively, these visualizations help users identify the likelihood and\nuniqueness of individual predictions, compare sets of predictions between\nprompts, and summarize patterns and relationships between predictions across\nall prompts. We demonstrate the capabilities of KnowledgeVis with feedback from\nsix NLP experts as well as three different use cases: (1) probing biomedical\nknowledge in two domain-adapted models; and (2) evaluating harmful identity\nstereotypes and (3) discovering facts and relationships between three\ngeneral-purpose models.","updated":1709837791000,"published":1709837791000,"authors":["Adam Coscia","Alex Endert"],"comments":"Accepted to IEEE TVCG. 20 pages, 10 figures, 1 table. For a demo\n  video, see https:\/\/youtu.be\/hBX4rSUMr_I . For a live demo, visit\n  https:\/\/adamcoscia.com\/papers\/knowledgevis\/demo\/ . The source code is\n  available at https:\/\/github.com\/AdamCoscia\/KnowledgeVIS","categories":["cs.HC","cs.AI","cs.CY","cs.LG"],"primary_category":"cs.HC","doi":"10.1109\/TVCG.2023.3346713","journal_ref":null,"peer_reviewed":true},"53":{"arxiv_id":"2403.04760v1","url":"http:\/\/arxiv.org\/abs\/2403.04760v1","title":"iScore: Visual Analytics for Interpreting How Language Models\n  Automatically Score Summaries","summary":"The recent explosion in popularity of large language models (LLMs) has\ninspired learning engineers to incorporate them into adaptive educational tools\nthat automatically score summary writing. Understanding and evaluating LLMs is\nvital before deploying them in critical learning environments, yet their\nunprecedented size and expanding number of parameters inhibits transparency and\nimpedes trust when they underperform. Through a collaborative user-centered\ndesign process with several learning engineers building and deploying summary\nscoring LLMs, we characterized fundamental design challenges and goals around\ninterpreting their models, including aggregating large text inputs, tracking\nscore provenance, and scaling LLM interpretability methods. To address their\nconcerns, we developed iScore, an interactive visual analytics tool for\nlearning engineers to upload, score, and compare multiple summaries\nsimultaneously. Tightly integrated views allow users to iteratively revise the\nlanguage in summaries, track changes in the resulting LLM scores, and visualize\nmodel weights at multiple levels of abstraction. To validate our approach, we\ndeployed iScore with three learning engineers over the course of a month. We\npresent a case study where interacting with iScore led a learning engineer to\nimprove their LLM's score accuracy by three percentage points. Finally, we\nconducted qualitative interviews with the learning engineers that revealed how\niScore enabled them to understand, evaluate, and build trust in their LLMs\nduring deployment.","updated":1709837799000,"published":1709837799000,"authors":["Adam Coscia","Langdon Holmes","Wesley Morris","Joon Suh Choi","Scott Crossley","Alex Endert"],"comments":"Accepted to IUI 2024. 16 pages, 5 figures, 1 table. For a demo video,\n  see https:\/\/youtu.be\/EYJX-_fQPf0 . For a live demo, visit\n  https:\/\/adamcoscia.com\/papers\/iscore\/demo\/ . The source code is available at\n  https:\/\/github.com\/AdamCoscia\/iScore","categories":["cs.HC","cs.AI","cs.CY","cs.LG"],"primary_category":"cs.HC","doi":"10.1145\/3640543.3645142","journal_ref":null,"peer_reviewed":true},"54":{"arxiv_id":"2403.04761v1","url":"http:\/\/arxiv.org\/abs\/2403.04761v1","title":"DeepSee: Multidimensional Visualizations of Seabed Ecosystems","summary":"Scientists studying deep ocean microbial ecosystems use limited numbers of\nsediment samples collected from the seafloor to characterize important\nlife-sustaining biogeochemical cycles in the environment. Yet conducting\nfieldwork to sample these extreme remote environments is both expensive and\ntime consuming, requiring tools that enable scientists to explore the sampling\nhistory of field sites and predict where taking new samples is likely to\nmaximize scientific return. We conducted a collaborative, user-centered design\nstudy with a team of scientific researchers to develop DeepSee, an interactive\ndata workspace that visualizes 2D and 3D interpolations of biogeochemical and\nmicrobial processes in context together with sediment sampling history overlaid\non 2D seafloor maps. Based on a field deployment and qualitative interviews, we\nfound that DeepSee increased the scientific return from limited sample sizes,\ncatalyzed new research workflows, reduced long-term costs of sharing data, and\nsupported teamwork and communication between team members with diverse research\ngoals.","updated":1709837807000,"published":1709837807000,"authors":["Adam Coscia","Haley M. Sapers","Noah Deutsch","Malika Khurana","John S. Magyar","Sergio A. Parra","Daniel R. Utter","Rebecca L. Wipfler","David W. Caress","Eric J. Martin","Jennifer B. Paduan","Maggie Hendrie","Santiago Lombeyda","Hillary Mushkin","Alex Endert","Scott Davidoff","Victoria J. Orphan"],"comments":"Accepted to CHI 2024. 16 pages, 7 figures, 2 tables. For a demo\n  video, see https:\/\/youtu.be\/HJ4zbueJ9cs . For a live demo, visit\n  https:\/\/www.its.caltech.edu\/~datavis\/deepsee\/ . The source code is available\n  at https:\/\/github.com\/orphanlab\/DeepSee","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642001","journal_ref":null,"peer_reviewed":true},"55":{"arxiv_id":"2007.00714v4","url":"http:\/\/arxiv.org\/abs\/2007.00714v4","title":"Quantifying intrinsic causal contributions via structure preserving\n  interventions","summary":"We propose a notion of causal influence that describes the `intrinsic' part\nof the contribution of a node on a target node in a DAG. By recursively writing\neach node as a function of the upstream noise terms, we separate the intrinsic\ninformation added by each node from the one obtained from its ancestors. To\ninterpret the intrinsic information as a {\\it causal} contribution, we consider\n`structure-preserving interventions' that randomize each node in a way that\nmimics the usual dependence on the parents and does not perturb the observed\njoint distribution. To get a measure that is invariant with respect to\nrelabelling nodes we use Shapley based symmetrization and show that it reduces\nin the linear case to simple ANOVA after resolving the target node into noise\nvariables. We describe our contribution analysis for variance and entropy, but\ncontributions for other target metrics can be defined analogously. The code is\navailable in the package gcm of the open source library DoWhy.","updated":1709904783000,"published":1593632048000,"authors":["Dominik Janzing","Patrick Bl\u00f6baum","Atalanti A. Mastakouri","Philipp M. Faller","Lenon Minorics","Kailash Budhathoki"],"comments":"to appear at AISTATS 2024","categories":["cs.AI","cs.IT","math.IT","stat.ML"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"56":{"arxiv_id":"2201.07381v2","url":"http:\/\/arxiv.org\/abs\/2201.07381v2","title":"Unveiling Project-Specific Bias in Neural Code Models","summary":"Deep learning has introduced significant improvements in many software\nanalysis tasks. Although the Large Language Models (LLMs) based neural code\nmodels demonstrate commendable performance when trained and tested within the\nintra-project independent and identically distributed (IID) setting, they often\nstruggle to generalize effectively to real-world inter-project\nout-of-distribution (OOD) data. In this work, we show that this phenomenon is\ncaused by the heavy reliance on project-specific shortcuts for prediction\ninstead of ground-truth evidence. We propose a Cond-Idf measurement to\ninterpret this behavior, which quantifies the relatedness of a token with a\nlabel and its project-specificness. The strong correlation between model\nbehavior and the proposed measurement indicates that without proper\nregularization, models tend to leverage spurious statistical cues for\nprediction. Equipped with these observations, we propose a novel bias\nmitigation mechanism that regularizes the model's learning behavior by\nleveraging latent logic relations among samples. Experimental results on two\nrepresentative program analysis tasks indicate that our mitigation framework\ncan improve both inter-project OOD generalization and adversarial robustness,\nwhile not sacrificing accuracy on intra-project IID data.","updated":1710143567000,"published":1642558188000,"authors":["Zhiming Li","Yanzhou Li","Tianlin Li","Mengnan Du","Bozhi Wu","Yushi Cao","Junzhe Jiang","Yang Liu"],"comments":"Accepted by LREC-COLING 2024","categories":["cs.AI","cs.SE"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"57":{"arxiv_id":"2209.00626v6","url":"http:\/\/arxiv.org\/abs\/2209.00626v6","title":"The Alignment Problem from a Deep Learning Perspective","summary":"In coming years or decades, artificial general intelligence (AGI) may surpass\nhuman capabilities at many critical tasks. We argue that, without substantial\neffort to prevent it, AGIs could learn to pursue goals that are in conflict\n(i.e. misaligned) with human interests. If trained like today's most capable\nmodels, AGIs could learn to act deceptively to receive higher reward, learn\nmisaligned internally-represented goals which generalize beyond their\nfine-tuning distributions, and pursue those goals using power-seeking\nstrategies. We review emerging evidence for these properties. AGIs with these\nproperties would be difficult to align and may appear aligned even when they\nare not. Finally, we briefly outline how the deployment of misaligned AGIs\nmight irreversibly undermine human control over the world, and we review\nresearch directions aimed at preventing this outcome.","updated":1710868067000,"published":1661825567000,"authors":["Richard Ngo","Lawrence Chan","S\u00f6ren Mindermann"],"comments":"Published in ICLR 2024","categories":["cs.AI","cs.LG"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"58":{"arxiv_id":"2211.09935v3","url":"http:\/\/arxiv.org\/abs\/2211.09935v3","title":"CAPE: Corrective Actions from Precondition Errors using Large Language\n  Models","summary":"Extracting commonsense knowledge from a large language model (LLM) offers a\npath to designing intelligent robots. Existing approaches that leverage LLMs\nfor planning are unable to recover when an action fails and often resort to\nretrying failed actions, without resolving the error's underlying cause. We\npropose a novel approach (CAPE) that attempts to propose corrective actions to\nresolve precondition errors during planning. CAPE improves the quality of\ngenerated plans by leveraging few-shot reasoning from action preconditions. Our\napproach enables embodied agents to execute more tasks than baseline methods\nwhile ensuring semantic correctness and minimizing re-prompting. In\nVirtualHome, CAPE generates executable plans while improving a human-annotated\nplan correctness metric from 28.89% to 49.63% over SayCan. Our improvements\ntransfer to a Boston Dynamics Spot robot initialized with a set of skills\n(specified in language) and associated preconditions, where CAPE improves the\ncorrectness metric of the executed task plans by 76.49% compared to SayCan. Our\napproach enables the robot to follow natural language commands and robustly\nrecover from failures, which baseline approaches largely cannot resolve or\naddress inefficiently.","updated":1709992427000,"published":1668726891000,"authors":["Shreyas Sundara Raman","Vanya Cohen","Ifrah Idrees","Eric Rosen","Ray Mooney","Stefanie Tellex","David Paulius"],"comments":"17 pages, 6 figures, accepted at ICRA 2024","categories":["cs.AI","cs.CL","cs.LG","cs.RO","68T20, 68T50","I.2.7; I.2.8; I.2.2; I.2.4"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"59":{"arxiv_id":"2302.01686v5","url":"http:\/\/arxiv.org\/abs\/2302.01686v5","title":"Playing with Data: An Augmented Reality Approach to Interact with\n  Visualizations of Industrial Process Tomography","summary":"Industrial process tomography (IPT) is a specialized imaging technique widely\nused in industrial scenarios for process supervision and control. Today,\naugmented\/mixed reality (AR\/MR) is increasingly being adopted in many\nindustrial occasions, even though there is still an obvious gap when it comes\nto IPT. To bridge this gap, we propose the first systematic AR approach using\noptical see-through (OST) head mounted displays (HMDs) with comparative\nevaluation for domain users towards IPT visualization analysis. The\nproof-of-concept was demonstrated by a within-subject user study (n=20) with\ncounterbalancing design. Both qualitative and quantitative measurements were\ninvestigated. The results showed that our AR approach outperformed conventional\nsettings for IPT data visualization analysis in bringing higher\nunderstandability, reduced task completion time, lower error rates for domain\ntasks, increased usability with enhanced user experience, and a better\nrecommendation level. We summarize the findings and suggest future research\ndirections for benefiting IPT users with AR\/MR.","updated":1710067201000,"published":1675426741000,"authors":["Yuchong Zhang","Yueming Xuan","Rahul Yadav","Adel Omrani","Morten Fjeld"],"comments":"In IFIP Conference on Human-Computer Interaction 2023","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1007\/978-3-031-42283-6_7","journal_ref":"IFIP Conference on Human-Computer Interaction 2023 Aug 28 vol\n  14143 (pp. 123-144)","peer_reviewed":true},"60":{"arxiv_id":"2303.15027v4","url":"http:\/\/arxiv.org\/abs\/2303.15027v4","title":"A Survey on Causal Discovery Methods for I.I.D. and Time Series Data","summary":"The ability to understand causality from data is one of the major milestones\nof human-level intelligence. Causal Discovery (CD) algorithms can identify the\ncause-effect relationships among the variables of a system from related\nobservational data with certain assumptions. Over the years, several methods\nhave been developed primarily based on the statistical properties of data to\nuncover the underlying causal mechanism. In this study, we present an extensive\ndiscussion on the methods designed to perform causal discovery from both\nindependent and identically distributed (I.I.D.) data and time series data. For\nthis purpose, we first introduce the common terminologies used in causal\ndiscovery literature and then provide a comprehensive discussion of the\nalgorithms designed to identify causal relations in different settings. We\nfurther discuss some of the benchmark datasets available for evaluating the\nalgorithmic performance, off-the-shelf tools or software packages to perform\ncausal discovery readily, and the common metrics used to evaluate these\nmethods. We also evaluate some widely used causal discovery algorithms on\nmultiple benchmark datasets and compare their performances. Finally, we\nconclude by discussing the research challenges and the applications of causal\ndiscovery algorithms in multiple areas of interest.","updated":1710274485000,"published":1679908901000,"authors":["Uzma Hasan","Emam Hossain","Md Osman Gani"],"comments":"Published (05 Sept 2023) in Transactions on Machine Learning Research\n  (TMLR)","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"61":{"arxiv_id":"2304.09572v2","url":"http:\/\/arxiv.org\/abs\/2304.09572v2","title":"An Ecosystem for Personal Knowledge Graphs: A Survey and Research\n  Roadmap","summary":"This paper presents an ecosystem for personal knowledge graphs (PKGs),\ncommonly defined as resources of structured information about entities related\nto an individual, their attributes, and the relations between them. PKGs are a\nkey enabler of secure and sophisticated personal data management and\npersonalized services. However, there are challenges that need to be addressed\nbefore PKGs can achieve widespread adoption. One of the fundamental challenges\nis the very definition of what constitutes a PKG, as there are multiple\ninterpretations of the term. We propose our own definition of a PKG,\nemphasizing the aspects of (1) data ownership by a single individual and (2)\nthe delivery of personalized services as the primary purpose. We further argue\nthat a holistic view of PKGs is needed to unlock their full potential, and\npropose a unified framework for PKGs, where the PKG is a part of a larger\necosystem with clear interfaces towards data services and data sources. A\ncomprehensive survey and synthesis of existing work is conducted, with a\nmapping of the surveyed work into the proposed unified ecosystem. Finally, we\nidentify open challenges and research opportunities for the ecosystem as a\nwhole, as well as for the specific aspects of PKGs, which include population,\nrepresentation and management, and utilization.","updated":1710516052000,"published":1681903281000,"authors":["Martin G. Skj\u00e6veland","Krisztian Balog","Nolwenn Bernard","Weronika \u0141ajewska","Trond Linjordet"],"comments":"Published in AI Open, 2024","categories":["cs.AI","cs.IR"],"primary_category":"cs.AI","doi":"10.1016\/j.aiopen.2024.01.003","journal_ref":"An Ecosystem for Personal Knowledge Graphs: A Survey and Research\n  Roadmap, M. G. Skj{\\ae}veland, K. Balog, N. Bernard, W. {\\L}ajewska, and T.\n  Linjordet. In: AI Open, 5:55-69, 2024","peer_reviewed":true},"62":{"arxiv_id":"2304.10541v2","url":"http:\/\/arxiv.org\/abs\/2304.10541v2","title":"Modular 3D Interface Design for Accessible VR Applications","summary":"Designed with an accessible first design approach, the presented paper\ndescribes how exploiting humans proprioception ability in 3D space can result\nin a more natural interaction experience when using a 3D graphical user\ninterface in a virtual environment. The modularity of the designed interface\nempowers the user to decide where they want to place interface elements in 3D\nspace allowing for a highly customizable experience, both in the context of the\nplayer and the virtual space. Drawing inspiration from todays tangible\ninterfaces used, such as those in aircraft cockpits, a modular interface is\npresented taking advantage of our natural understanding of interacting with 3D\nobjects and exploiting capabilities that otherwise have not been used in 2D\ninteraction. Additionally, the designed interface supports multimodal input\nmechanisms which also demonstrates the opportunity for the design to cross over\nto augmented reality applications. A focus group study was completed to better\nunderstand the usability and constraints of the designed 3D GUI.","updated":1709898655000,"published":1680973666000,"authors":["Corrie Green","Dr Yang Jiang","Dr John Isaacs"],"comments":"This preprint has not undergone peer review or any post-submission\n  corrections. The Version of Record of this contribution will be published in\n  Springer Nature Computer Science book series in Volume HCI International 2023","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1007\/978-3-031-35634-6_2","journal_ref":"Virtual, Augmented and Mixed Reality. HCII 2023","peer_reviewed":true},"63":{"arxiv_id":"2305.11927v2","url":"http:\/\/arxiv.org\/abs\/2305.11927v2","title":"Evaluating how interactive visualizations can assist in finding samples\n  where and how computer vision models make mistakes","summary":"Creating Computer Vision (CV) models remains a complex practice, despite\ntheir ubiquity. Access to data, the requirement for ML expertise, and model\nopacity are just a few points of complexity that limit the ability of end-users\nto build, inspect, and improve these models. Interactive ML perspectives have\nhelped address some of these issues by considering a teacher in the loop where\nplanning, teaching, and evaluating tasks take place. We present and evaluate\ntwo interactive visualizations in the context of Sprite, a system for creating\nCV classification and detection models for images originating from videos. We\nstudy how these visualizations help Sprite's users identify (evaluate) and\nselect (plan) images where a model is struggling and can lead to improved\nperformance, compared to a baseline condition where users used a query\nlanguage. We found that users who had used the visualizations found more images\nacross a wider set of potential types of model errors.","updated":1710526996000,"published":1684507380000,"authors":["Hayeong Song","Gonzalo Ramos","Peter Bodik"],"comments":"Hayeong Song, Gonzalo Ramos, and Peter Bodik. \"Evaluating how\n  interactive visualizations can assist in finding samples where and how\n  computer vision models make mistakes\" 2024 IEEE Pacific Visualization\n  Symposium (PacificVis). Ieee, 2024","categories":["cs.HC","cs.CV","cs.LG"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"64":{"arxiv_id":"2306.09509v2","url":"http:\/\/arxiv.org\/abs\/2306.09509v2","title":"Granger-Causal Hierarchical Skill Discovery","summary":"Reinforcement Learning (RL) has demonstrated promising results in learning\npolicies for complex tasks, but it often suffers from low sample efficiency and\nlimited transferability. Hierarchical RL (HRL) methods aim to address the\ndifficulty of learning long-horizon tasks by decomposing policies into skills,\nabstracting states, and reusing skills in new tasks. However, many HRL methods\nrequire some initial task success to discover useful skills, which\nparadoxically may be very unlikely without access to useful skills. On the\nother hand, reward-free HRL methods often need to learn far too many skills to\nachieve proper coverage in high-dimensional domains. In contrast, we introduce\nthe Chain of Interaction Skills (COInS) algorithm, which focuses on\ncontrollability in factored domains to identify a small number of task-agnostic\nskills that still permit a high degree of control. COInS uses learned detectors\nto identify interactions between state factors and then trains a chain of\nskills to control each of these factors successively. We evaluate COInS on a\nrobotic pushing task with obstacles-a challenging domain where other RL and HRL\nmethods fall short. We also demonstrate the transferability of skills learned\nby COInS, using variants of Breakout, a common RL benchmark, and show 2-3x\nimprovement in both sample efficiency and final performance compared to\nstandard RL baselines.","updated":1710806721000,"published":1686863214000,"authors":["Caleb Chuck","Kevin Black","Aditya Arjun","Yuke Zhu","Scott Niekum"],"comments":"Accepted TMLR 2024","categories":["cs.AI","cs.RO"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"65":{"arxiv_id":"2307.03067v2","url":"http:\/\/arxiv.org\/abs\/2307.03067v2","title":"DeepOnto: A Python Package for Ontology Engineering with Deep Learning","summary":"Integrating deep learning techniques, particularly language models (LMs),\nwith knowledge representation techniques like ontologies has raised widespread\nattention, urging the need of a platform that supports both paradigms. Although\npackages such as OWL API and Jena offer robust support for basic ontology\nprocessing features, they lack the capability to transform various types of\ninformation within ontologies into formats suitable for downstream deep\nlearning-based applications. Moreover, widely-used ontology APIs are primarily\nJava-based while deep learning frameworks like PyTorch and Tensorflow are\nmainly for Python programming. To address the needs, we present DeepOnto, a\nPython package designed for ontology engineering with deep learning. The\npackage encompasses a core ontology processing module founded on the\nwidely-recognised and reliable OWL API, encapsulating its fundamental features\nin a more \"Pythonic\" manner and extending its capabilities to incorporate other\nessential components including reasoning, verbalisation, normalisation,\ntaxonomy, projection, and more. Building on this module, DeepOnto offers a\nsuite of tools, resources, and algorithms that support various ontology\nengineering tasks, such as ontology alignment and completion, by harnessing\ndeep learning methods, primarily pre-trained LMs. In this paper, we also\ndemonstrate the practical utility of DeepOnto through two use-cases: the\nDigital Health Coaching in Samsung Research UK and the Bio-ML track of the\nOntology Alignment Evaluation Initiative (OAEI).","updated":1709950662000,"published":1688657702000,"authors":["Yuan He","Jiaoyan Chen","Hang Dong","Ian Horrocks","Carlo Allocca","Taehun Kim","Brahmananda Sapkota"],"comments":"Accepted by the Semantic Web Journal","categories":["cs.AI","cs.CL","cs.LG","cs.LO"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"66":{"arxiv_id":"2307.07919v2","url":"http:\/\/arxiv.org\/abs\/2307.07919v2","title":"Neural Architecture Retrieval","summary":"With the increasing number of new neural architecture designs and substantial\nexisting neural architectures, it becomes difficult for the researchers to\nsituate their contributions compared with existing neural architectures or\nestablish the connections between their designs and other relevant ones. To\ndiscover similar neural architectures in an efficient and automatic manner, we\ndefine a new problem Neural Architecture Retrieval which retrieves a set of\nexisting neural architectures which have similar designs to the query neural\narchitecture. Existing graph pre-training strategies cannot address the\ncomputational graph in neural architectures due to the graph size and motifs.\nTo fulfill this potential, we propose to divide the graph into motifs which are\nused to rebuild the macro graph to tackle these issues, and introduce\nmulti-level contrastive learning to achieve accurate graph representation\nlearning. Extensive evaluations on both human-designed and synthesized neural\narchitectures demonstrate the superiority of our algorithm. Such a dataset\nwhich contains 12k real-world network architectures, as well as their\nembedding, is built for neural architecture retrieval.","updated":1710724412000,"published":1689472601000,"authors":["Xiaohuan Pei","Yanxi Li","Minjing Dong","Chang Xu"],"comments":"ICLR 2024","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"67":{"arxiv_id":"2308.05984v2","url":"http:\/\/arxiv.org\/abs\/2308.05984v2","title":"Contrastive Explanations of Centralized Multi-agent Optimization\n  Solutions","summary":"In many real-world scenarios, agents are involved in optimization problems.\nSince most of these scenarios are over-constrained, optimal solutions do not\nalways satisfy all agents. Some agents might be unhappy and ask questions of\nthe form ``Why does solution $S$ not satisfy property $P$?''. We propose CMAoE,\na domain-independent approach to obtain contrastive explanations by: (i)\ngenerating a new solution $S^\\prime$ where property $P$ is enforced, while also\nminimizing the differences between $S$ and $S^\\prime$; and (ii) highlighting\nthe differences between the two solutions, with respect to the features of the\nobjective function of the multi-agent system. Such explanations aim to help\nagents understanding why the initial solution is better in the context of the\nmulti-agent system than what they expected. We have carried out a computational\nevaluation that shows that CMAoE can generate contrastive explanations for\nlarge multi-agent optimization problems. We have also performed an extensive\nuser study in four different domains that shows that: (i) after being presented\nwith these explanations, humans' satisfaction with the original solution\nincreases; and (ii) the constrastive explanations generated by CMAoE are\npreferred or equally preferred by humans over the ones generated by state of\nthe art approaches.","updated":1710338165000,"published":1691739737000,"authors":["Parisa Zehtabi","Alberto Pozanco","Ayala Bloch","Daniel Borrajo","Sarit Kraus"],"comments":"Paper accepted at ICAPS 2024. This is a extended version that\n  includes Supplementary Material","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"68":{"arxiv_id":"2308.10148v3","url":"http:\/\/arxiv.org\/abs\/2308.10148v3","title":"Privacy Perceptions and Behaviors of Google Personal Account Holders in\n  Saudi Arabia","summary":"While privacy perceptions and behaviors have been investigated in Western\nsocieties, little is known about these issues in non-Western societies. To\nbridge this gap, we interviewed 30 Google personal account holders in Saudi\nArabia about their privacy perceptions and behaviors regarding the activity\ndata that Google saves about them. Our study focuses on Google's Activity\nControls, which enable users to control whether, and how, Google saves their\nWeb \\& App Activity, Location History, and YouTube History. Our results show\nthat although most participants have some level of awareness about Google's\ndata practices and the Activity Controls, many have only vague awareness, and\nthe majority have not used the available controls. When participants viewed\ntheir saved activity data, many were surprised by what had been saved. While\nmany participants find Google's use of their data to improve the services\nprovided to them acceptable, the majority find the use of their data for ad\npurposes unacceptable. We observe that our Saudi participants exhibit similar\ntrends and patterns in privacy awareness, attitudes, preferences, concerns, and\nbehaviors to what has been found in studies in the US. Our results emphasize\nthe need for: 1) improved techniques to inform users about privacy settings\nduring account sign-up, to remind users about their settings, and to raise\nawareness about privacy settings; 2) improved privacy setting interfaces to\nreduce the costs that deter many users from changing the settings; and 3)\nfurther research to explore privacy concerns in non-Western cultures.","updated":1710788822000,"published":1692501918000,"authors":["Eman Alashwali","Lorrie Faith Cranor"],"comments":"To appear in Proceedings of Human Computer Interaction International\n  (HCII) 2024","categories":["cs.CY","cs.CR","cs.HC"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"69":{"arxiv_id":"2308.13658v2","url":"http:\/\/arxiv.org\/abs\/2308.13658v2","title":"Generating and Explaining Corner Cases Using Learnt Probabilistic Lane\n  Graphs","summary":"Validating the safety of Autonomous Vehicles (AVs) operating in open-ended,\ndynamic environments is challenging as vehicles will eventually encounter\nsafety-critical situations for which there is not representative training data.\nBy increasing the coverage of different road and traffic conditions and by\nincluding corner cases in simulation-based scenario testing, the safety of AVs\ncan be improved. However, the creation of corner case scenarios including\nmultiple agents is non-trivial. Our approach allows engineers to generate\nnovel, realistic corner cases based on historic traffic data and to explain why\nsituations were safety-critical. In this paper, we introduce Probabilistic Lane\nGraphs (PLGs) to describe a finite set of lane positions and directions in\nwhich vehicles might travel. The structure of PLGs is learnt directly from\nspatio-temporal traffic data. The graph model represents the actions of the\ndrivers in response to a given state in the form of a probabilistic policy. We\nuse reinforcement learning techniques to modify this policy and to generate\nrealistic and explainable corner case scenarios which can be used for assessing\nthe safety of AVs.","updated":1710295714000,"published":1692994669000,"authors":["Enrik Maci","Rhys Howard","Lars Kunze"],"comments":"8 Pages, 3 Figures, 1 Table, Published in the Proceedings of the 26th\n  IEEE International Conference on Intelligent Transport Systems (2023), Final\n  submission version with added IEEE copyright notice","categories":["cs.AI","cs.RO","E.1; G.1.1; G.3; I.2.6; I.2.9; I.6.0"],"primary_category":"cs.AI","doi":"10.1109\/ITSC57777.2023.10422229","journal_ref":"2023 IEEE 26th International Conference on Intelligent\n  Transportation Systems (ITSC), Bilbao, Spain, 2023, pp. 4201-4208","peer_reviewed":true},"70":{"arxiv_id":"2308.13812v2","url":"http:\/\/arxiv.org\/abs\/2308.13812v2","title":"Dysen-VDM: Empowering Dynamics-aware Text-to-Video Diffusion with LLMs","summary":"Text-to-video (T2V) synthesis has gained increasing attention in the\ncommunity, in which the recently emerged diffusion models (DMs) have\npromisingly shown stronger performance than the past approaches. While existing\nstate-of-the-art DMs are competent to achieve high-resolution video generation,\nthey may largely suffer from key limitations (e.g., action occurrence\ndisorders, crude video motions) with respect to the intricate temporal dynamics\nmodeling, one of the crux of video synthesis. In this work, we investigate\nstrengthening the awareness of video dynamics for DMs, for high-quality T2V\ngeneration. Inspired by human intuition, we design an innovative dynamic scene\nmanager (dubbed as Dysen) module, which includes (step-1) extracting from input\ntext the key actions with proper time-order arrangement, (step-2) transforming\nthe action schedules into the dynamic scene graph (DSG) representations, and\n(step-3) enriching the scenes in the DSG with sufficient and reasonable\ndetails. Taking advantage of the existing powerful LLMs (e.g., ChatGPT) via\nin-context learning, Dysen realizes (nearly) human-level temporal dynamics\nunderstanding. Finally, the resulting video DSG with rich action scene details\nis encoded as fine-grained spatio-temporal features, integrated into the\nbackbone T2V DM for video generating. Experiments on popular T2V datasets\nsuggest that our Dysen-VDM consistently outperforms prior arts with significant\nmargins, especially in scenarios with complex actions. Codes at\nhttps:\/\/haofei.vip\/Dysen-VDM","updated":1710851394000,"published":1693038708000,"authors":["Hao Fei","Shengqiong Wu","Wei Ji","Hanwang Zhang","Tat-Seng Chua"],"comments":"CVPR 2024","categories":["cs.AI","cs.CV"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"71":{"arxiv_id":"2308.14363v3","url":"http:\/\/arxiv.org\/abs\/2308.14363v3","title":"Mobile Foundation Model as Firmware","summary":"In today's landscape, smartphones have evolved into hubs for hosting a\nmultitude of deep learning models aimed at local execution. A key realization\ndriving this work is the notable fragmentation among these models,\ncharacterized by varied architectures, operators, and implementations. This\nfragmentation imposes a significant burden on the comprehensive optimization of\nhardware, system settings, and algorithms.\n  Buoyed by the recent strides in large foundation models, this work introduces\na pioneering paradigm for mobile AI: a collaborative management approach\nbetween the mobile OS and hardware, overseeing a foundational model capable of\nserving a broad spectrum of mobile AI tasks, if not all. This foundational\nmodel resides within the NPU and remains impervious to app or OS revisions,\nakin to firmware. Concurrently, each app contributes a concise, offline\nfine-tuned \"adapter\" tailored to distinct downstream tasks. From this concept\nemerges a concrete instantiation known as \\sys. It amalgamates a curated\nselection of publicly available Large Language Models (LLMs) and facilitates\ndynamic data flow. This concept's viability is substantiated through the\ncreation of an exhaustive benchmark encompassing 38 mobile AI tasks spanning 50\ndatasets, including domains such as Computer Vision (CV), Natural Language\nProcessing (NLP), audio, sensing, and multimodal inputs. Spanning this\nbenchmark, \\sys unveils its impressive performance. It attains accuracy parity\nin 85\\% of tasks, demonstrates improved scalability in terms of storage and\nmemory, and offers satisfactory inference speed on Commercial Off-The-Shelf\n(COTS) mobile devices fortified with NPU support. This stands in stark contrast\nto task-specific models tailored for individual applications.","updated":1710209823000,"published":1693207286000,"authors":["Jinliang Yuan","Chen Yang","Dongqi Cai","Shihe Wang","Xin Yuan","Zeling Zhang","Xiang Li","Dingge Zhang","Hanzi Mei","Xianqing Jia","Shangguang Wang","Mengwei Xu"],"comments":"17 pages, 15 figures, published to ACM MobiCom'24","categories":["cs.AI"],"primary_category":"cs.AI","doi":"10.1145\/3636534.3649361","journal_ref":"The 30th Annual International Conference on Mobile Computing and\n  Networking, 2024","peer_reviewed":true},"72":{"arxiv_id":"2308.15272v4","url":"http:\/\/arxiv.org\/abs\/2308.15272v4","title":"AutoDroid: LLM-powered Task Automation in Android","summary":"Mobile task automation is an attractive technique that aims to enable\nvoice-based hands-free user interaction with smartphones. However, existing\napproaches suffer from poor scalability due to the limited language\nunderstanding ability and the non-trivial manual efforts required from\ndevelopers or end-users. The recent advance of large language models (LLMs) in\nlanguage understanding and reasoning inspires us to rethink the problem from a\nmodel-centric perspective, where task preparation, comprehension, and execution\nare handled by a unified language model. In this work, we introduce AutoDroid,\na mobile task automation system capable of handling arbitrary tasks on any\nAndroid application without manual efforts. The key insight is to combine the\ncommonsense knowledge of LLMs and domain-specific knowledge of apps through\nautomated dynamic analysis. The main components include a functionality-aware\nUI representation method that bridges the UI with the LLM, exploration-based\nmemory injection techniques that augment the app-specific domain knowledge of\nLLM, and a multi-granularity query optimization module that reduces the cost of\nmodel inference. We integrate AutoDroid with off-the-shelf LLMs including\nonline GPT-4\/GPT-3.5 and on-device Vicuna, and evaluate its performance on a\nnew benchmark for memory-augmented Android task automation with 158 common\ntasks. The results demonstrated that AutoDroid is able to precisely generate\nactions with an accuracy of 90.9%, and complete tasks with a success rate of\n71.3%, outperforming the GPT-4-powered baselines by 36.4% and 39.7%. The demo,\nbenchmark suites, and source code of AutoDroid will be released at\nurl{https:\/\/autodroid-sys.github.io\/}.","updated":1709977131000,"published":1693314150000,"authors":["Hao Wen","Yuanchun Li","Guohong Liu","Shanhui Zhao","Tao Yu","Toby Jia-Jun Li","Shiqi Jiang","Yunhao Liu","Yaqin Zhang","Yunxin Liu"],"comments":"Published in MobiCom 2024; Original title: \"Empowering LLM to use\n  Smartphone for Intelligent Task Automation\"","categories":["cs.AI","cs.SE"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"73":{"arxiv_id":"2309.10444v4","url":"http:\/\/arxiv.org\/abs\/2309.10444v4","title":"Exploring Iterative Enhancement for Improving Learnersourced\n  Multiple-Choice Question Explanations with Large Language Models","summary":"Large language models exhibit superior capabilities in processing and\nunderstanding language, yet their applications in educational contexts remain\nunderexplored. Learnersourcing enhances learning by engaging students in\ncreating their own educational content. When learnersourcing multiple-choice\nquestions, creating explanations for the solution of a question is a crucial\nstep; it helps other students understand the solution and promotes a deeper\nunderstanding of related concepts. However, it is often difficult for students\nto craft effective solution explanations, due to limited subject understanding.\nTo help scaffold the task of automated explanation generation, we present and\nevaluate a framework called \"ILearner-LLM\", that iteratively enhances the\ngenerated explanations for the given questions with large language models.\nComprising an explanation generation model and an explanation evaluation model,\nthe framework generates high-quality student-aligned explanations by\niteratively feeding the quality rating score from the evaluation model back\ninto the instruction prompt of the explanation generation model. Experimental\nresults demonstrate the effectiveness of our ILearner-LLM on LLaMA2-13B and\nGPT-4 to generate higher quality explanations that are closer to those written\nby students on five PeerWise datasets. Our findings represent a promising path\nto enrich the learnersourcing experience for students and to enhance the\ncapabilities of large language models for educational applications.","updated":1710078521000,"published":1695114255000,"authors":["Qiming Bao","Juho Leinonen","Alex Yuxuan Peng","Wanjun Zhong","Ga\u00ebl Gendron","Timothy Pistotti","Alice Huang","Paul Denny","Michael Witbrock","Jiamou Liu"],"comments":"The short version (v4) was accepted as a non-archival workshop paper\n  at AGI@ICLR 2024; the full version is under review","categories":["cs.AI","cs.CL"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"74":{"arxiv_id":"2309.11737v2","url":"http:\/\/arxiv.org\/abs\/2309.11737v2","title":"Choice-75: A Dataset on Decision Branching in Script Learning","summary":"Script learning studies how stereotypical events unfold, enabling machines to\nreason about narratives with implicit information. Previous works mostly\nconsider a script as a linear sequence of events while ignoring the potential\nbranches that arise due to people's circumstantial choices. We hence propose\nChoice-75, the first benchmark that challenges intelligent systems to make\ndecisions given descriptive scenarios, containing 75 scripts and more than 600\nscenarios. We also present preliminary results with current large language\nmodels (LLM). Although they demonstrate overall decent performance, there is\nstill notable headroom in hard scenarios.","updated":1710725748000,"published":1695263024000,"authors":["Zhaoyi Joey Hou","Li Zhang","Chris Callison-Burch"],"comments":"To be published in LREC-COLING-2024","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"75":{"arxiv_id":"2309.15723v2","url":"http:\/\/arxiv.org\/abs\/2309.15723v2","title":"Where Are We So Far? Understanding Data Storytelling Tools from the\n  Perspective of Human-AI Collaboration","summary":"Data storytelling is powerful for communicating data insights, but it\nrequires diverse skills and considerable effort from human creators. Recent\nresearch has widely explored the potential for artificial intelligence (AI) to\nsupport and augment humans in data storytelling. However, there lacks a\nsystematic review to understand data storytelling tools from the perspective of\nhuman-AI collaboration, which hinders researchers from reflecting on the\nexisting collaborative tool designs that promote humans' and AI's advantages\nand mitigate their shortcomings. This paper investigated existing tools with a\nframework from two perspectives: the stages in the storytelling workflow where\na tool serves, including analysis, planning, implementation, and communication,\nand the roles of humans and AI in each stage, such as creators, assistants,\noptimizers, and reviewers. Through our analysis, we recognize the common\ncollaboration patterns in existing tools, summarize lessons learned from these\npatterns, and further illustrate research opportunities for human-AI\ncollaboration in data storytelling.","updated":1710766817000,"published":1695828650000,"authors":["Haotian Li","Yun Wang","Huamin Qu"],"comments":"Accepted by CHI 2024","categories":["cs.HC","cs.AI"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"76":{"arxiv_id":"2309.17167v3","url":"http:\/\/arxiv.org\/abs\/2309.17167v3","title":"DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks","summary":"Large language models (LLMs) have achieved remarkable performance in various\nevaluation benchmarks. However, concerns are raised about potential data\ncontamination in their considerable volume of training corpus. Moreover, the\nstatic nature and fixed complexity of current benchmarks may inadequately gauge\nthe advancing capabilities of LLMs. In this paper, we introduce DyVal, a\ngeneral and flexible protocol for dynamic evaluation of LLMs. Based on our\nframework, we build graph-informed DyVal by leveraging the structural advantage\nof directed acyclic graphs to dynamically generate evaluation samples with\ncontrollable complexities. DyVal generates challenging evaluation sets on\nreasoning tasks including mathematics, logical reasoning, and algorithm\nproblems. We evaluate various LLMs ranging from Flan-T5-large to GPT-3.5-Turbo\nand GPT-4. Experiments show that LLMs perform worse in DyVal-generated\nevaluation samples with different complexities, highlighting the significance\nof dynamic evaluation. We also analyze the failure cases and results of\ndifferent prompting methods. Moreover, DyVal-generated samples are not only\nevaluation sets, but also helpful data for fine-tuning to improve the\nperformance of LLMs on existing benchmarks. We hope that DyVal can shed light\non future evaluation research of LLMs. Code is available at:\nhttps:\/\/github.com\/microsoft\/promptbench.","updated":1710409936000,"published":1695989054000,"authors":["Kaijie Zhu","Jiaao Chen","Jindong Wang","Neil Zhenqiang Gong","Diyi Yang","Xing Xie"],"comments":"ICLR 2024 spotlight; 38 pages; code is at aka.ms\/dyval","categories":["cs.AI","cs.CL","cs.LG"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"77":{"arxiv_id":"2310.00229v4","url":"http:\/\/arxiv.org\/abs\/2310.00229v4","title":"Consciousness-Inspired Spatio-Temporal Abstractions for Better\n  Generalization in Reinforcement Learning","summary":"Inspired by human conscious planning, we propose Skipper, a model-based\nreinforcement learning framework utilizing spatio-temporal abstractions to\ngeneralize better in novel situations. It automatically decomposes the given\ntask into smaller, more manageable subtasks, and thus enables sparse\ndecision-making and focused computation on the relevant parts of the\nenvironment. The decomposition relies on the extraction of an abstracted proxy\nproblem represented as a directed graph, in which vertices and edges are\nlearned end-to-end from hindsight. Our theoretical analyses provide performance\nguarantees under appropriate assumptions and establish where our approach is\nexpected to be helpful. Generalization-focused experiments validate Skipper's\nsignificant advantage in zero-shot generalization, compared to some existing\nstate-of-the-art hierarchical planning methods.","updated":1710615563000,"published":1696040718000,"authors":["Mingde Zhao","Safa Alver","Harm van Seijen","Romain Laroche","Doina Precup","Yoshua Bengio"],"comments":"ICLR 2024 Camera-Ready","categories":["cs.AI","cs.LG"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"78":{"arxiv_id":"2310.05136v5","url":"http:\/\/arxiv.org\/abs\/2310.05136v5","title":"InstructDET: Diversifying Referring Object Detection with Generalized\n  Instructions","summary":"We propose InstructDET, a data-centric method for referring object detection\n(ROD) that localizes target objects based on user instructions. While deriving\nfrom referring expressions (REC), the instructions we leverage are greatly\ndiversified to encompass common user intentions related to object detection.\nFor one image, we produce tremendous instructions that refer to every single\nobject and different combinations of multiple objects. Each instruction and its\ncorresponding object bounding boxes (bbxs) constitute one training data pair.\nIn order to encompass common detection expressions, we involve emerging\nvision-language model (VLM) and large language model (LLM) to generate\ninstructions guided by text prompts and object bbxs, as the generalizations of\nfoundation models are effective to produce human-like expressions (e.g.,\ndescribing object property, category, and relationship). We name our\nconstructed dataset as InDET. It contains images, bbxs and generalized\ninstructions that are from foundation models. Our InDET is developed from\nexisting REC datasets and object detection datasets, with the expanding\npotential that any image with object bbxs can be incorporated through using our\nInstructDET method. By using our InDET dataset, we show that a conventional ROD\nmodel surpasses existing methods on standard REC datasets and our InDET test\nset. Our data-centric method InstructDET, with automatic data expansion by\nleveraging foundation models, directs a promising field that ROD can be greatly\ndiversified to execute common object detection instructions.","updated":1710142351000,"published":1696767044000,"authors":["Ronghao Dang","Jiangyan Feng","Haodong Zhang","Chongjian Ge","Lin Song","Lijun Gong","Chengju Liu","Qijun Chen","Feng Zhu","Rui Zhao","Yibing Song"],"comments":"29 pages (include Appendix) Published in ICLR","categories":["cs.AI","cs.CV"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"79":{"arxiv_id":"2310.08992v3","url":"http:\/\/arxiv.org\/abs\/2310.08992v3","title":"CodeChain: Towards Modular Code Generation Through Chain of\n  Self-revisions with Representative Sub-modules","summary":"Large Language Models (LLMs) have already become quite proficient at solving\nsimpler programming tasks like those in HumanEval or MBPP benchmarks. However,\nsolving more complex and competitive programming tasks is still quite\nchallenging for these models - possibly due to their tendency to generate\nsolutions as monolithic code blocks instead of decomposing them into logical\nsub-tasks and sub-modules. On the other hand, experienced programmers\ninstinctively write modularized code with abstraction for solving complex\ntasks, often reusing previously developed modules. To address this gap, we\npropose CodeChain, a novel framework for inference that elicits modularized\ncode generation through a chain of self-revisions, each being guided by some\nrepresentative sub-modules generated in previous iterations. Concretely,\nCodeChain first instructs the LLM to generate modularized codes through\nchain-of-thought prompting. Then it applies a chain of self-revisions by\niterating the two steps: 1) extracting and clustering the generated sub-modules\nand selecting the cluster representatives as the more generic and re-usable\nimplementations, and 2) augmenting the original chain-of-thought prompt with\nthese selected module-implementations and instructing the LLM to re-generate\nnew modularized solutions. We find that by naturally encouraging the LLM to\nreuse the previously developed and verified sub-modules, CodeChain can\nsignificantly boost both modularity as well as correctness of the generated\nsolutions, achieving relative pass@1 improvements of 35% on APPS and 76% on\nCodeContests. It is shown to be effective on both OpenAI LLMs as well as\nopen-sourced LLMs like WizardCoder. We also conduct comprehensive ablation\nstudies with different methods of prompting, number of clusters, model sizes,\nprogram qualities, etc., to provide useful insights that underpin CodeChain's\nsuccess.","updated":1710386949000,"published":1697192268000,"authors":["Hung Le","Hailin Chen","Amrita Saha","Akash Gokul","Doyen Sahoo","Shafiq Joty"],"comments":"Accepted to ICLR 2024","categories":["cs.AI","cs.CL","cs.PL"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"80":{"arxiv_id":"2311.10112v2","url":"http:\/\/arxiv.org\/abs\/2311.10112v2","title":"zrLLM: Zero-Shot Relational Learning on Temporal Knowledge Graphs with\n  Large Language Models","summary":"Modeling evolving knowledge over temporal knowledge graphs (TKGs) has become\na heated topic. Various methods have been proposed to forecast links on TKGs.\nMost of them are embedding-based, where hidden representations are learned to\nrepresent knowledge graph (KG) entities and relations based on the observed\ngraph contexts. Although these methods show strong performance on traditional\nTKG forecasting (TKGF) benchmarks, they face a strong challenge in modeling the\nunseen zero-shot relations that have no prior graph context. In this paper, we\ntry to mitigate this problem as follows. We first input the text descriptions\nof KG relations into large language models (LLMs) for generating relation\nrepresentations, and then introduce them into embedding-based TKGF methods.\nLLM-empowered representations can capture the semantic information in the\nrelation descriptions. This makes the relations, whether seen or unseen, with\nsimilar semantic meanings stay close in the embedding space, enabling TKGF\nmodels to recognize zero-shot relations even without any observed graph\ncontext. Experimental results show that our approach helps TKGF models to\nachieve much better performance in forecasting the facts with previously unseen\nrelations, while still maintaining their ability in link forecasting regarding\nseen relations.","updated":1710517087000,"published":1700083515000,"authors":["Zifeng Ding","Heling Cai","Jingpei Wu","Yunpu Ma","Ruotong Liao","Bo Xiong","Volker Tresp"],"comments":"Accepted to NAACL 2024 main conference","categories":["cs.AI","cs.CL","cs.LG"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"81":{"arxiv_id":"2311.12996v2","url":"http:\/\/arxiv.org\/abs\/2311.12996v2","title":"RLIF: Interactive Imitation Learning as Reinforcement Learning","summary":"Although reinforcement learning methods offer a powerful framework for\nautomatic skill acquisition, for practical learning-based control problems in\ndomains such as robotics, imitation learning often provides a more convenient\nand accessible alternative. In particular, an interactive imitation learning\nmethod such as DAgger, which queries a near-optimal expert to intervene online\nto collect correction data for addressing the distributional shift challenges\nthat afflict na\\\"ive behavioral cloning, can enjoy good performance both in\ntheory and practice without requiring manually specified reward functions and\nother components of full reinforcement learning methods. In this paper, we\nexplore how off-policy reinforcement learning can enable improved performance\nunder assumptions that are similar but potentially even more practical than\nthose of interactive imitation learning. Our proposed method uses reinforcement\nlearning with user intervention signals themselves as rewards. This relaxes the\nassumption that intervening experts in interactive imitation learning should be\nnear-optimal and enables the algorithm to learn behaviors that improve over the\npotential suboptimal human expert. We also provide a unified framework to\nanalyze our RL method and DAgger; for which we present the asymptotic analysis\nof the suboptimal gap for both methods as well as the non-asymptotic sample\ncomplexity bound of our method. We then evaluate our method on challenging\nhigh-dimensional continuous control simulation benchmarks as well as real-world\nrobotic vision-based manipulation tasks. The results show that it strongly\noutperforms DAgger-like approaches across the different tasks, especially when\nthe intervening experts are suboptimal. Code and videos can be found on the\nproject website: https:\/\/rlif-page.github.io","updated":1710794717000,"published":1700600721000,"authors":["Jianlan Luo","Perry Dong","Yuexiang Zhai","Yi Ma","Sergey Levine"],"comments":"ICLR 2024","categories":["cs.AI","cs.RO"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"82":{"arxiv_id":"2312.11678v4","url":"http:\/\/arxiv.org\/abs\/2312.11678v4","title":"Misinformation as a harm: structured approaches for fact-checking\n  prioritization","summary":"In this work, we examine how fact-checkers prioritize which claims to\nfact-check and what tools may assist them in their efforts. Through a series of\ninterviews with 23 professional fact-checkers from around the world, we\nvalidate that harm assessment is a central component of how fact-checkers\ntriage their work. We also clarify the processes behind fact-checking\nprioritization, finding that they are typically ad hoc, and gather suggestions\nfor tools that could help with these processes.\n  To address the needs articulated by fact-checkers, we present a structured\nframework of questions to help fact-checkers negotiate the priority of claims\nthrough assessing potential harms. Our FABLE Framework of Misinformation Harms\nincorporates five dimensions of magnitude -- (social) Fragmentation,\nActionability, Believability, Likelihood of spread, and Exploitativeness --\nthat can help determine the potential urgency of a specific message or claim\nwhen considering misinformation as harm. The result is a practical and\nconceptual tool to support fact-checkers and others as they make strategic\ndecisions to prioritize their efforts. We conclude with a discussion of\ncomputational approaches to support structured prioritization, as well as\napplications beyond fact-checking to content moderation and curation.","updated":1710775163000,"published":1702928672000,"authors":["Connie Moon Sehat","Ryan Li","Peipei Nie","Tarunima Prabhakar","Amy X. Zhang"],"comments":"Accepted to CSCW 2024, with clean up for typos and figures","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"83":{"arxiv_id":"2312.12403v2","url":"http:\/\/arxiv.org\/abs\/2312.12403v2","title":"On Alternating-Time Temporal Logic, Hyperproperties, and Strategy\n  Sharing","summary":"Alternating-time temporal logic (ATL$^*$) is a well-established framework for\nformal reasoning about multi-agent systems. However, while ATL$^*$ can reason\nabout the strategic ability of agents (e.g., some coalition $A$ can ensure that\na goal is reached eventually), we cannot compare multiple strategic\ninteractions, nor can we require multiple agents to follow the same strategy.\nFor example, we cannot state that coalition $A$ can reach a goal sooner (or\nmore often) than some other coalition $A'$. In this paper, we propose\nHyperATLS$^*_S$, an extension of ATL$^*$ in which we can (1) compare the\noutcome of multiple strategic interactions w.r.t. a hyperproperty, i.e., a\nproperty that refers to multiple paths at the same time, and (2) enforce that\nsome agents share the same strategy. We show that HyperATL$^*_S$ is a rich\nspecification language that captures important AI-related properties that were\nout of reach of existing logics. We prove that model checking of HyperATL$^*_S$\non concurrent game structures is decidable. We implement our model-checking\nalgorithm in a tool we call HyMASMC and evaluate it on a range of benchmarks.","updated":1710238190000,"published":1703011059000,"authors":["Raven Beutner","Bernd Finkbeiner"],"comments":"AAAI 2024","categories":["cs.AI","cs.LO","cs.MA"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"84":{"arxiv_id":"2312.12620v2","url":"http:\/\/arxiv.org\/abs\/2312.12620v2","title":"\"It Can Relate to Real Lives\": Attitudes and Expectations in\n  Justice-Centered Data Structures & Algorithms for Non-Majors","summary":"Prior work has argued for a more justice-centered approach to postsecondary\ncomputing education by emphasizing ethics, identity, and political vision. In\nthis experience report, we examine how postsecondary students of diverse gender\nand racial identities experience a justice-centered Data Structures and\nAlgorithms designed for undergraduate non-computer science majors. Through a\nquantitative and qualitative analysis of two quarters of student survey data\ncollected at the start and end of each quarter, we report on student attitudes\nand expectations.\n  Across the class, we found a significant increase in the following attitudes:\ncomputing confidence and sense of belonging. While women, non-binary, and other\nstudents not identifying as men (WNB+) also increased in these areas, they\nstill reported significantly lower confidence and sense of belonging than men\nat the end of the quarter. Black, Latinx, Middle Eastern and North African,\nNative American, and Pacific Islander (BLMNPI) students had no significant\ndifferences compared to white and Asian students.\n  We also analyzed end-of-quarter student self-reflections on their fulfillment\nof expectations prior to taking the course. While the majority of students\nreported a positive overall sentiment about the course and many students\nspecifically appreciated the justice-centered approach, some desired more\npractice with program implementation and interview preparation. We discuss\nimplications for practice and articulate a political vision for holding both\nappreciation for computing ethics and a desire for professional preparation\ntogether through iterative design.","updated":1710541223000,"published":1703022984000,"authors":["Anna Batra","Iris Zhou","Suh Young Choi","Chongjiu Gao","Yanbing Xiao","Sonia Fereidooni","Kevin Lin"],"comments":"Experience Reports and Tools paper in the Proceedings of the 55th ACM\n  Technical Symposium on Computer Science Education V. 1 (SIGCSE 2024); 7 pages","categories":["cs.CY","K.3.2"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"85":{"arxiv_id":"2312.14106v2","url":"http:\/\/arxiv.org\/abs\/2312.14106v2","title":"Learning Human-like Representations to Enable Learning Human Values","summary":"How can we build AI systems that are aligned with human values to avoid\ncausing harm or violating societal standards for acceptable behavior? We argue\nthat representational alignment between humans and AI agents facilitates value\nalignment. Making AI systems learn human-like representations of the world has\nmany known benefits, including improving generalization, robustness to domain\nshifts, and few-shot learning performance. We propose that this kind of\nrepresentational alignment between machine learning (ML) models and humans can\nalso support value alignment, allowing ML systems to conform to human values\nand societal norms. We focus on ethics as one aspect of value alignment and\ntrain ML agents using a variety of methods in a multi-armed bandit setting,\nwhere rewards reflect the moral acceptability of the chosen action. We use a\nsynthetic experiment to demonstrate that agents' representational alignment\nwith the environment bounds their learning performance. We then repeat this\nprocedure in a realistic setting, using textual action descriptions and\nsimilarity judgments collected from humans and a variety of language models, to\nshow that the results generalize and are model-agnostic when grounded in an\nethically relevant context.","updated":1710293875000,"published":1703183493000,"authors":["Andrea Wynn","Ilia Sucholutsky","Thomas L. Griffiths"],"comments":"Paper accepted in Human-Centric Representation Learning workshop at\n  AAAI 2024 (https:\/\/hcrl-workshop.github.io\/2024\/)","categories":["cs.AI","cs.LG"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"86":{"arxiv_id":"2401.10838v2","url":"http:\/\/arxiv.org\/abs\/2401.10838v2","title":"Rambler: Supporting Writing With Speech via LLM-Assisted Gist\n  Manipulation","summary":"Dictation enables efficient text input on mobile devices. However, writing\nwith speech can produce disfluent, wordy, and incoherent text and thus requires\nheavy post-processing. This paper presents Rambler, an LLM-powered graphical\nuser interface that supports gist-level manipulation of dictated text with two\nmain sets of functions: gist extraction and macro revision. Gist extraction\ngenerates keywords and summaries as anchors to support the review and\ninteraction with spoken text. LLM-assisted macro revisions allow users to\nrespeak, split, merge and transform dictated text without specifying precise\nediting locations. Together they pave the way for interactive dictation and\nrevision that help close gaps between spontaneous spoken words and\nwell-structured writing. In a comparative study with 12 participants performing\nverbal composition tasks, Rambler outperformed the baseline of a speech-to-text\neditor + ChatGPT, as it better facilitates iterative revisions with enhanced\nuser control over the content while supporting surprisingly diverse user\nstrategies.","updated":1709865965000,"published":1705685996000,"authors":["Susan Lin","Jeremy Warner","J. D. Zamfirescu-Pereira","Matthew G. Lee","Sauhard Jain","Michael Xuelin Huang","Piyawat Lertvittayakumjorn","Shanqing Cai","Shumin Zhai","Bj\u00f6rn Hartmann","Can Liu"],"comments":"To appear at ACM CHI 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642217","journal_ref":null,"peer_reviewed":true},"87":{"arxiv_id":"2401.11317v3","url":"http:\/\/arxiv.org\/abs\/2401.11317v3","title":"Third-Party Developers and Tool Development For Community Management on\n  Live Streaming Platform Twitch","summary":"Community management is critical for stakeholders to collaboratively build\nand sustain communities with socio-technical support. However, most of the\nexisting research has mainly focused on the community members and the platform,\nwith little attention given to the developers who act as intermediaries between\nthe platform and community members and develop tools to support community\nmanagement. This study focuses on third-party developers (TPDs) for the live\nstreaming platform Twitch and explores their tool development practices. Using\na mixed method with in-depth qualitative analysis, we found that TPDs maintain\ncomplex relationships with different stakeholders (streamers, viewers,\nplatform, professional developers), and the multi-layered policy restricts\ntheir agency regarding idea innovation and tool development. We argue that HCI\nresearch should shift its focus from tool users to tool developers with regard\nto community management. We propose designs to support closer collaboration\nbetween TPDS and the platform and professional developers and streamline TPDs'\ndevelopment process with unified toolkits and policy documentation.","updated":1710712977000,"published":1705782497000,"authors":["Jie Cai","Ya-Fang Lin","He Zhang","John M. Carroll"],"comments":"Accepted by ACM CHI 2024","categories":["cs.HC","cs.CY"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642787","journal_ref":null,"peer_reviewed":true},"88":{"arxiv_id":"2402.07398v2","url":"http:\/\/arxiv.org\/abs\/2402.07398v2","title":"VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language\n  Models with Autonomous Instruction Optimization","summary":"This paper presents VisLingInstruct, a novel approach to advancing\nMulti-Modal Language Models (MMLMs) in zero-shot learning. Current MMLMs show\nimpressive zero-shot abilities in multi-modal tasks, but their performance\ndepends heavily on the quality of instructions. VisLingInstruct tackles this by\nautonomously evaluating and optimizing instructional texts through In-Context\nLearning, improving the synergy between visual perception and linguistic\nexpression in MMLMs. Alongside this instructional advancement, we have also\noptimized the visual feature extraction modules in MMLMs, further augmenting\ntheir responsiveness to textual cues. Our comprehensive experiments on MMLMs,\nbased on FlanT5 and Vicuna, show that VisLingInstruct significantly improves\nzero-shot performance in visual multi-modal tasks. Notably, it achieves a 13.1%\nand 9% increase in accuracy over the prior state-of-the-art on the TextVQA and\nHatefulMemes datasets.","updated":1710426614000,"published":1707711196000,"authors":["Dongsheng Zhu","Xunzhu Tang","Weidong Han","Jinghui Lu","Yukun Zhao","Guoliang Xing","Junfeng Wang","Dawei Yin"],"comments":"Accepted to NAACL2024 main conference","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"89":{"arxiv_id":"2402.11446v2","url":"http:\/\/arxiv.org\/abs\/2402.11446v2","title":"Penetration Vision through Virtual Reality Headsets: Identifying\n  360-degree Videos from Head Movements","summary":"In this paper, we present the first contactless side-channel attack for\nidentifying 360 videos being viewed in a Virtual Reality (VR) Head Mounted\nDisplay (HMD). Although the video content is displayed inside the HMD without\nany external exposure, we observe that user head movements are driven by the\nvideo content, which creates a unique side channel that does not exist in\ntraditional 2D videos. By recording the user whose vision is blocked by the HMD\nvia a malicious camera, an attacker can analyze the correlation between the\nuser's head movements and the victim video to infer the video title.\n  To exploit this new vulnerability, we present INTRUDE, a system for\nidentifying 360 videos from recordings of user head movements. INTRUDE is\nempowered by an HMD-based head movement estimation scheme to extract a head\nmovement trace from the recording and a video saliency-based trace-fingerprint\nmatching framework to infer the video title. Evaluation results show that\nINTRUDE achieves over 96% of accuracy for video identification and is robust\nunder different recording environments. Moreover, INTRUDE maintains its\neffectiveness in the open-world identification scenario.","updated":1709857193000,"published":1708229202000,"authors":["Anh Nguyen","Xiaokuan Zhang","Zhisheng Yan"],"comments":"Accepted to USENIX Security '24","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"90":{"arxiv_id":"2402.15013v2","url":"http:\/\/arxiv.org\/abs\/2402.15013v2","title":"Filter Bubble or Homogenization? Disentangling the Long-Term Effects of\n  Recommendations on User Consumption Patterns","summary":"Recommendation algorithms play a pivotal role in shaping our media choices,\nwhich makes it crucial to comprehend their long-term impact on user behavior.\nThese algorithms are often linked to two critical outcomes: homogenization,\nwherein users consume similar content despite disparate underlying preferences,\nand the filter bubble effect, wherein individuals with differing preferences\nonly consume content aligned with their preferences (without much overlap with\nother users). Prior research assumes a trade-off between homogenization and\nfilter bubble effects and then shows that personalized recommendations mitigate\nfilter bubbles by fostering homogenization. However, because of this assumption\nof a tradeoff between these two effects, prior work cannot develop a more\nnuanced view of how recommendation systems may independently impact\nhomogenization and filter bubble effects. We develop a more refined definition\nof homogenization and the filter bubble effect by decomposing them into two key\nmetrics: how different the average consumption is between users (inter-user\ndiversity) and how varied an individual's consumption is (intra-user\ndiversity). We then use a novel agent-based simulation framework that enables a\nholistic view of the impact of recommendation systems on homogenization and\nfilter bubble effects. Our simulations show that traditional recommendation\nalgorithms (based on past behavior) mainly reduce filter bubbles by affecting\ninter-user diversity without significantly impacting intra-user diversity.\nBuilding on these findings, we introduce two new recommendation algorithms that\ntake a more nuanced approach by accounting for both types of diversity.","updated":1709851593000,"published":1708643540000,"authors":["Md Sanzeed Anwar","Grant Schoenebeck","Paramveer S. Dhillon"],"comments":"This paper was accepted at the ACM Web Conference 2024 (WWW '24)","categories":["cs.CY","cs.IR"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"91":{"arxiv_id":"2403.04732v2","url":"http:\/\/arxiv.org\/abs\/2403.04732v2","title":"How Far Are We from Intelligent Visual Deductive Reasoning?","summary":"Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated\nincredible strides on diverse vision language tasks. We dig into vision-based\ndeductive reasoning, a more sophisticated but less explored realm, and find\npreviously unexposed blindspots in the current SOTA VLMs. Specifically, we\nleverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to\nperform multi-hop relational and deductive reasoning relying solely on visual\nclues. We perform comprehensive evaluations of several popular VLMs employing\nstandard strategies such as in-context learning, self-consistency, and\nChain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test,\nIntelligenceTest, and RAVEN. The results reveal that despite the impressive\ncapabilities of LLMs in text-based reasoning, we are still far from achieving\ncomparable proficiency in visual deductive reasoning. We found that certain\nstandard strategies that are effective when applied to LLMs do not seamlessly\ntranslate to the challenges presented by visual reasoning tasks. Moreover, a\ndetailed analysis reveals that VLMs struggle to solve these tasks mainly\nbecause they are unable to perceive and comprehend multiple, confounding\nabstract patterns in RPM examples.","updated":1709880428000,"published":1709836554000,"authors":["Yizhe Zhang","He Bai","Ruixiang Zhang","Jiatao Gu","Shuangfei Zhai","Josh Susskind","Navdeep Jaitly"],"comments":"ICLR 2024 AGI workshop. https:\/\/github.com\/apple\/ml-rpm-bench","categories":["cs.AI","cs.CL","cs.CV"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"92":{"arxiv_id":"2403.05083v1","url":"http:\/\/arxiv.org\/abs\/2403.05083v1","title":"Technology-assisted Journal Writing for Improving Student Mental\n  Wellbeing: Humanoid Robot vs. Voice Assistant","summary":"Conversational agents have a potential in improving student mental wellbeing\nwhile assisting them in self-disclosure activities such as journalling. Their\nembodiment might have an effect on what students disclose, and how they\ndisclose this, and students overall adherence to the disclosure activity.\nHowever, the effect of embodiment in the context of agent assisted journal\nwriting has not been studied. Therefore, this study aims to investigate the\nviability of using social robots (SR) and voice assistants (VA) for eliciting\nrich disclosures in journal writing that contributes to mental health status\nimprovement in students over time. Forty two undergraduate and graduate\nstudents participated in the study that assessed the mood changes (via Brief\nMood Introspection Scale, BMIS), level of subjective self-disclosure (via\nSubjective Self-Disclosure Questionnaire, SSDQ), and perceptions toward the\nagents (via Robot Social Attributes Scale, RoSAS) with and without agent (SR or\nVA) assisted journal writing. Results suggest that only in robot condition\nthere are mood improvements, higher levels of disclosure, and positive\nperceptions over time in technology-assisted journal writing. Our results\nsuggest that robot assisted journal writing has some advantages over voice\nassistant one for eliciting rich disclosures that contributes to mental health\nstatus improvement in students over time.","updated":1709878725000,"published":1709878725000,"authors":["Batuhan Sayis","Hatice Gunes"],"comments":"accepted to HRI 2024 (LBR)","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"93":{"arxiv_id":"2403.05098v1","url":"http:\/\/arxiv.org\/abs\/2403.05098v1","title":"Love, Joy, and Autism Robots: A Metareview and Provocatype","summary":"Previous work has observed how Neurodivergence is often harmfully\npathologized in Human-Computer Interaction (HCI) and Human-Robot interaction\n(HRI) research. We conduct a review of autism robot reviews and find the\ndominant research direction is Autistic people's second to lowest (24 of 25)\nresearch priority: interventions and treatments purporting to 'help'\nneurodivergent individuals to conform to neurotypical social norms, become\nbetter behaved, improve social and emotional skills, and otherwise 'fix' us --\nrarely prioritizing the internal experiences that might lead to such\ndifferences. Furthermore, a growing body of evidence indicates many of the most\npopular current approaches risk inflicting lasting trauma and damage on\nAutistic people. We draw on the principles and findings of the latest Autism\nresearch, Feminist HRI, and Robotics to imagine a role reversal, analyze the\nimplications, then conclude with actionable guidance on Autistic-led scientific\nmethods and research directions.","updated":1709881272000,"published":1709881272000,"authors":["Andrew Hundt","Gabrielle Ohlson","Pieter Wolfert","Lux Miranda","Sophia Zhu","Katie Winkle"],"comments":"3 pages; In Assistive Applications, Accessibility, and Disability\n  Ethics (A3DE) workshop at the Human Robot Interaction (HRI) Conference 2024;\n  https:\/\/sites.google.com\/view\/love-joy-and-autism-robots\/home","categories":["cs.HC","cs.CY","cs.RO"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"94":{"arxiv_id":"2403.05104v1","url":"http:\/\/arxiv.org\/abs\/2403.05104v1","title":"How Culture Shapes What People Want From AI","summary":"There is an urgent need to incorporate the perspectives of culturally diverse\ngroups into AI developments. We present a novel conceptual framework for\nresearch that aims to expand, reimagine, and reground mainstream visions of AI\nusing independent and interdependent cultural models of the self and the\nenvironment. Two survey studies support this framework and provide preliminary\nevidence that people apply their cultural models when imagining their ideal AI.\nCompared with European American respondents, Chinese respondents viewed it as\nless important to control AI and more important to connect with AI, and were\nmore likely to prefer AI with capacities to influence. Reflecting both cultural\nmodels, findings from African American respondents resembled both European\nAmerican and Chinese respondents. We discuss study limitations and future\ndirections and highlight the need to develop culturally responsive and relevant\nAI to serve a broader segment of the world population.","updated":1709881699000,"published":1709881699000,"authors":["Xiao Ge","Chunchen Xu","Daigo Misaki","Hazel Rose Markus","Jeanne L Tsai"],"comments":"To appear at CHI 2024","categories":["cs.HC","cs.AI"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642660","journal_ref":null,"peer_reviewed":true},"95":{"arxiv_id":"2403.05112v1","url":"http:\/\/arxiv.org\/abs\/2403.05112v1","title":"RLPeri: Accelerating Visual Perimetry Test with Reinforcement Learning\n  and Convolutional Feature Extraction","summary":"Visual perimetry is an important eye examination that helps detect vision\nproblems caused by ocular or neurological conditions. During the test, a\npatient's gaze is fixed at a specific location while light stimuli of varying\nintensities are presented in central and peripheral vision. Based on the\npatient's responses to the stimuli, the visual field mapping and sensitivity\nare determined. However, maintaining high levels of concentration throughout\nthe test can be challenging for patients, leading to increased examination\ntimes and decreased accuracy.\n  In this work, we present RLPeri, a reinforcement learning-based approach to\noptimize visual perimetry testing. By determining the optimal sequence of\nlocations and initial stimulus values, we aim to reduce the examination time\nwithout compromising accuracy. Additionally, we incorporate reward shaping\ntechniques to further improve the testing performance. To monitor the patient's\nresponses over time during testing, we represent the test's state as a pair of\n3D matrices. We apply two different convolutional kernels to extract spatial\nfeatures across locations as well as features across different stimulus values\nfor each location. Through experiments, we demonstrate that our approach\nresults in a 10-20% reduction in examination time while maintaining the\naccuracy as compared to state-of-the-art methods. With the presented approach,\nwe aim to make visual perimetry testing more efficient and patient-friendly,\nwhile still providing accurate results.","updated":1709882383000,"published":1709882383000,"authors":["Tanvi Verma","Linh Le Dinh","Nicholas Tan","Xinxing Xu","Chingyu Cheng","Yong Liu"],"comments":"Published at AAAI-24","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":"The 38th Annual AAAI Conference on Artificial Intelligence, 2024","peer_reviewed":true},"96":{"arxiv_id":"2403.05213v1","url":"http:\/\/arxiv.org\/abs\/2403.05213v1","title":"AQuA: Automated Question-Answering in Software Tutorial Videos with\n  Visual Anchors","summary":"Tutorial videos are a popular help source for learning feature-rich software.\nHowever, getting quick answers to questions about tutorial videos is difficult.\nWe present an automated approach for responding to tutorial questions. By\nanalyzing 633 questions found in 5,944 video comments, we identified different\nquestion types and observed that users frequently described parts of the video\nin questions. We then asked participants (N=24) to watch tutorial videos and\nask questions while annotating the video with relevant visual anchors. Most\nvisual anchors referred to UI elements and the application workspace. Based on\nthese insights, we built AQuA, a pipeline that generates useful answers to\nquestions with visual anchors. We demonstrate this for Fusion 360, showing that\nwe can recognize UI elements in visual anchors and generate answers using GPT-4\naugmented with that visual information and software documentation. An\nevaluation study (N=16) demonstrates that our approach provides better answers\nthan baseline methods.","updated":1709895343000,"published":1709895343000,"authors":["Saelyne Yang","Jo Vermeulen","George Fitzmaurice","Justin Matejka"],"comments":"19 pages, CHI 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642752","journal_ref":null,"peer_reviewed":true},"97":{"arxiv_id":"2403.05225v1","url":"http:\/\/arxiv.org\/abs\/2403.05225v1","title":"Trust Recognition in Human-Robot Cooperation Using EEG","summary":"Collaboration between humans and robots is becoming increasingly crucial in\nour daily life. In order to accomplish efficient cooperation, trust recognition\nis vital, empowering robots to predict human behaviors and make trust-aware\ndecisions. Consequently, there is an urgent need for a generalized approach to\nrecognize human-robot trust. This study addresses this need by introducing an\nEEG-based method for trust recognition during human-robot cooperation. A\nhuman-robot cooperation game scenario is used to stimulate various human trust\nlevels when working with robots. To enhance recognition performance, the study\nproposes an EEG Vision Transformer model coupled with a 3-D spatial\nrepresentation to capture the spatial information of EEG, taking into account\nthe topological relationship among electrodes. To validate this approach, a\npublic EEG-based human trust dataset called EEGTrust is constructed.\nExperimental results indicate the effectiveness of the proposed approach,\nachieving an accuracy of 74.99% in slice-wise cross-validation and 62.00% in\ntrial-wise cross-validation. This outperforms baseline models in both\nrecognition accuracy and generalization. Furthermore, an ablation study\ndemonstrates a significant improvement in trust recognition performance of the\nspatial representation. The source code and EEGTrust dataset are available at\nhttps:\/\/github.com\/CaiyueXu\/EEGTrust.","updated":1709897331000,"published":1709897331000,"authors":["Caiyue Xu","Changming Zhang","Yanmin Zhou","Zhipeng Wang","Ping Lu","Bin He"],"comments":"Accepted at IEEE International Conference on Robotics and Automation\n  (ICRA) 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"98":{"arxiv_id":"2403.05264v1","url":"http:\/\/arxiv.org\/abs\/2403.05264v1","title":"To Reach the Unreachable: Exploring the Potential of VR Hand Redirection\n  for Upper Limb Rehabilitation","summary":"Rehabilitation therapies are widely employed to assist people with motor\nimpairments in regaining control over their affected body parts. Nevertheless,\nfactors such as fatigue and low self-efficacy can hinder patient compliance\nduring extensive rehabilitation processes. Utilizing hand redirection in\nvirtual reality (VR) enables patients to accomplish seemingly more challenging\ntasks, thereby bolstering their motivation and confidence. While previous\nresearch has investigated user experience and hand redirection among\nable-bodied people, its effects on motor-impaired people remain unexplored. In\nthis paper, we present a VR rehabilitation application that harnesses hand\nredirection. Through a user study and semi-structured interviews, we examine\nthe impact of hand redirection on the rehabilitation experiences of people with\nmotor impairments and its potential to enhance their motivation for upper limb\nrehabilitation. Our findings suggest that patients are not sensitive to hand\nmovement inconsistency, and the majority express interest in incorporating hand\nredirection into future long-term VR rehabilitation programs.","updated":1709901707000,"published":1709901707000,"authors":["Peixuan Xiong","Yukai Zhang","Nandi Zhang","Shihan Fu","Xin Li","Yadan Zheng","Jinni Zhou","Xiquan Hu","Mingming Fan"],"comments":"Proceedings of the CHI Conference on Human Factors in Computing\n  Systems (CHI '24), May 11--16, 2024, Honolulu, HI, USA","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642912","journal_ref":null,"peer_reviewed":true},"99":{"arxiv_id":"2403.05499v1","url":"http:\/\/arxiv.org\/abs\/2403.05499v1","title":"Enabling Developers, Protecting Users: Investigating Harassment and\n  Safety in VR","summary":"Virtual Reality (VR) has witnessed a rising issue of harassment, prompting\nthe integration of safety controls like muting and blocking in VR applications.\nHowever, the lack of standardized safety measures across VR applications\nhinders their universal effectiveness, especially across contexts like\nsocializing, gaming, and streaming. While prior research has studied safety\ncontrols in social VR applications, our user study (n = 27) takes a\nmulti-perspective approach, examining both users' perceptions of safety control\nusability and effectiveness as well as the challenges that developers face in\ndesigning and deploying VR safety controls. We identify challenges VR users\nface while employing safety controls, such as finding users in crowded virtual\nspaces to block them. VR users also find controls ineffective in addressing\nharassment; for instance, they fail to eliminate the harassers' presence from\nthe environment. Further, VR users find the current methods of submitting\nevidence for reports time-consuming and cumbersome. Improvements desired by\nusers include live moderation and behavior tracking across VR apps; however,\ndevelopers cite technological, financial, and legal obstacles to implementing\nsuch solutions, often due to a lack of awareness and high development costs. We\nemphasize the importance of establishing technical and legal guidelines to\nenhance user safety in virtual environments.","updated":1709921753000,"published":1709921753000,"authors":["Abhinaya S. B.","Aafaq Sabir","Anupam Das"],"comments":"Accepted at USENIX Security 2024","categories":["cs.HC","cs.CR","cs.CY","cs.ET"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"100":{"arxiv_id":"2403.05723v2","url":"http:\/\/arxiv.org\/abs\/2403.05723v2","title":"Digital Wellbeing Redefined: Toward User-Centric Approach for Positive\n  Social Media Engagement","summary":"The prevalence of social media and its escalating impact on mental health has\nhighlighted the need for effective digital wellbeing strategies. Current\ndigital wellbeing interventions have primarily focused on reducing screen time\nand social media use, often neglecting the potential benefits of these\nplatforms. This paper introduces a new perspective centered around empowering\npositive social media experiences, instead of limiting users with restrictive\nrules. In line with this perspective, we lay out the key requirements that\nshould be considered in future work, aiming to spark a dialogue in this\nemerging area. We further present our initial effort to address these\nrequirements with PauseNow, an innovative digital wellbeing intervention\ndesigned to align users' digital behaviors with their intentions. PauseNow\nleverages digital nudging and intention-aware recommendations to gently guide\nusers back to their original intentions when they \"get lost\" during their\ndigital usage, promoting a more mindful use of social media.","updated":1710808212000,"published":1709940367000,"authors":["Yixue Zhao","Tianyi Li","Michael Sobolev"],"comments":"MOBILESoft 2024, Lisbon, Portugal","categories":["cs.HC","cs.SE"],"primary_category":"cs.HC","doi":"10.1145\/3647632.3651392","journal_ref":null,"peer_reviewed":true},"101":{"arxiv_id":"2403.05801v1","url":"http:\/\/arxiv.org\/abs\/2403.05801v1","title":"Enhancing Multi-Hop Knowledge Graph Reasoning through Reward Shaping\n  Techniques","summary":"In the realm of computational knowledge representation, Knowledge Graph\nReasoning (KG-R) stands at the forefront of facilitating sophisticated\ninferential capabilities across multifarious domains. The quintessence of this\nresearch elucidates the employment of reinforcement learning (RL) strategies,\nnotably the REINFORCE algorithm, to navigate the intricacies inherent in\nmulti-hop KG-R. This investigation critically addresses the prevalent\nchallenges introduced by the inherent incompleteness of Knowledge Graphs (KGs),\nwhich frequently results in erroneous inferential outcomes, manifesting as both\nfalse negatives and misleading positives. By partitioning the Unified Medical\nLanguage System (UMLS) benchmark dataset into rich and sparse subsets, we\ninvestigate the efficacy of pre-trained BERT embeddings and Prompt Learning\nmethodologies to refine the reward shaping process. This approach not only\nenhances the precision of multi-hop KG-R but also sets a new precedent for\nfuture research in the field, aiming to improve the robustness and accuracy of\nknowledge inference within complex KG frameworks. Our work contributes a novel\nperspective to the discourse on KG reasoning, offering a methodological\nadvancement that aligns with the academic rigor and scholarly aspirations of\nthe Natural journal, promising to invigorate further advancements in the realm\nof computational knowledge representation.","updated":1709962447000,"published":1709962447000,"authors":["Chen Li","Haotian Zheng","Yiping Sun","Cangqing Wang","Liqiang Yu","Che Chang","Xinyu Tian","Bo Liu"],"comments":"This paper has been accepted by the 2024 5th International Seminar on\n  Artificial Intelligence, Networking and Information Technology (AINIT 2024)","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"102":{"arxiv_id":"2403.05816v1","url":"http:\/\/arxiv.org\/abs\/2403.05816v1","title":"LEVA: Using Large Language Models to Enhance Visual Analytics","summary":"Visual analytics supports data analysis tasks within complex domain problems.\nHowever, due to the richness of data types, visual designs, and interaction\ndesigns, users need to recall and process a significant amount of information\nwhen they visually analyze data. These challenges emphasize the need for more\nintelligent visual analytics methods. Large language models have demonstrated\nthe ability to interpret various forms of textual data, offering the potential\nto facilitate intelligent support for visual analytics. We propose LEVA, a\nframework that uses large language models to enhance users' VA workflows at\nmultiple stages: onboarding, exploration, and summarization. To support\nonboarding, we use large language models to interpret visualization designs and\nview relationships based on system specifications. For exploration, we use\nlarge language models to recommend insights based on the analysis of system\nstatus and data to facilitate mixed-initiative exploration. For summarization,\nwe present a selective reporting strategy to retrace analysis history through a\nstream visualization and generate insight reports with the help of large\nlanguage models. We demonstrate how LEVA can be integrated into existing visual\nanalytics systems. Two usage scenarios and a user study suggest that LEVA\neffectively aids users in conducting visual analytics.","updated":1709965808000,"published":1709965808000,"authors":["Yuheng Zhao","Yixing Zhang","Yu Zhang","Xinyi Zhao","Junjie Wang","Zekai Shao","Cagatay Turkay","Siming Chen"],"comments":"Accepted to IEEE TVCG 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1109\/TVCG.2024.3368060","journal_ref":null,"peer_reviewed":true},"103":{"arxiv_id":"2403.06034v1","url":"http:\/\/arxiv.org\/abs\/2403.06034v1","title":"Content Moderation Justice and Fairness on Social Media: Comparisons\n  Across Different Contexts and Platforms","summary":"Social media users may perceive moderation decisions by the platform\ndifferently, which can lead to frustration and dropout. This study investigates\nusers' perceived justice and fairness of online moderation decisions when they\nare exposed to various illegal versus legal scenarios, retributive versus\nrestorative moderation strategies, and user-moderated versus commercially\nmoderated platforms. We conduct an online experiment on 200 American social\nmedia users of Reddit and Twitter. Results show that retributive moderation\ndelivers higher justice and fairness for commercially moderated than for\nuser-moderated platforms in illegal violations; restorative moderation delivers\nhigher fairness for legal violations than illegal ones. We discuss the\nopportunities for platform policymaking to improve moderation system design.","updated":1710024606000,"published":1710024606000,"authors":["Jie Cai","Aashka Patel","Azadeh Naderi","Donghee Yvette Wohn"],"comments":"Accepted by CHI LBW 2024","categories":["cs.HC","cs.CY"],"primary_category":"cs.HC","doi":"10.1145\/3613905.3650882","journal_ref":null,"peer_reviewed":true},"104":{"arxiv_id":"2403.06039v1","url":"http:\/\/arxiv.org\/abs\/2403.06039v1","title":"A Preliminary Exploration of YouTubers' Use of Generative-AI in Content\n  Creation","summary":"Content creators increasingly utilize generative artificial intelligence\n(Gen-AI) on platforms such as YouTube, TikTok, Instagram, and various blogging\nsites to produce imaginative images, AI-generated videos, and articles using\nLarge Language Models (LLMs). Despite its growing popularity, there remains an\nunderexplored area concerning the specific domains where AI-generated content\nis being applied, and the methodologies content creators employ with Gen-AI\ntools during the creation process. This study initially explores this emerging\narea through a qualitative analysis of 68 YouTube videos demonstrating Gen-AI\nusage. Our research focuses on identifying the content domains, the variety of\ntools used, the activities performed, and the nature of the final products\ngenerated by Gen-AI in the context of user-generated content.","updated":1710026576000,"published":1710026576000,"authors":["Yao Lyu","He Zhang","Shuo Niu","Jie Cai"],"comments":"Accepted at CHI LBW 2024","categories":["cs.HC","cs.AI"],"primary_category":"cs.HC","doi":"10.1145\/3613905.3651057","journal_ref":null,"peer_reviewed":true},"105":{"arxiv_id":"2403.06050v1","url":"http:\/\/arxiv.org\/abs\/2403.06050v1","title":"Explaining Code with a Purpose: An Integrated Approach for Developing\n  Code Comprehension and Prompting Skills","summary":"Reading, understanding and explaining code have traditionally been important\nskills for novices learning programming. As large language models (LLMs) become\nprevalent, these foundational skills are more important than ever given the\nincreasing need to understand and evaluate model-generated code. Brand new\nskills are also needed, such as the ability to formulate clear prompts that can\nelicit intended code from an LLM. Thus, there is great interest in integrating\npedagogical approaches for the development of both traditional coding\ncompetencies and the novel skills required to interact with LLMs. One effective\nway to develop and assess code comprehension ability is with ``Explain in plain\nEnglish'' (EiPE) questions, where students succinctly explain the purpose of a\nfragment of code. However, grading EiPE questions has always been difficult\ngiven the subjective nature of evaluating written explanations and this has\nstifled their uptake. In this paper, we explore a natural synergy between EiPE\nquestions and code-generating LLMs to overcome this limitation. We propose\nusing an LLM to generate code based on students' responses to EiPE questions --\nnot only enabling EiPE responses to be assessed automatically, but helping\nstudents develop essential code comprehension and prompt crafting skills in\nparallel. We investigate this idea in an introductory programming course and\nreport student success in creating effective prompts for solving EiPE\nquestions. We also examine student perceptions of this activity and how it\ninfluences their views on the use of LLMs for aiding and assessing learning.","updated":1710030188000,"published":1710030188000,"authors":["Paul Denny","David H. Smith IV","Max Fowler","James Prather","Brett A. Becker","Juho Leinonen"],"comments":"Accepted to ITiCSE 2024","categories":["cs.HC","cs.CY","cs.SE"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"106":{"arxiv_id":"2403.06086v1","url":"http:\/\/arxiv.org\/abs\/2403.06086v1","title":"Towards Generalizable and Interpretable Motion Prediction: A Deep\n  Variational Bayes Approach","summary":"Estimating the potential behavior of the surrounding human-driven vehicles is\ncrucial for the safety of autonomous vehicles in a mixed traffic flow. Recent\nstate-of-the-art achieved accurate prediction using deep neural networks.\nHowever, these end-to-end models are usually black boxes with weak\ninterpretability and generalizability. This paper proposes the Goal-based\nNeural Variational Agent (GNeVA), an interpretable generative model for motion\nprediction with robust generalizability to out-of-distribution cases. For\ninterpretability, the model achieves target-driven motion prediction by\nestimating the spatial distribution of long-term destinations with a\nvariational mixture of Gaussians. We identify a causal structure among maps and\nagents' histories and derive a variational posterior to enhance\ngeneralizability. Experiments on motion prediction datasets validate that the\nfitted model can be interpretable and generalizable and can achieve comparable\nperformance to state-of-the-art results.","updated":1710044164000,"published":1710044164000,"authors":["Juanwu Lu","Wei Zhan","Masayoshi Tomizuka","Yeping Hu"],"comments":"Accepted at AISTATS 2024","categories":["cs.AI","cs.RO"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"107":{"arxiv_id":"2403.06207v1","url":"http:\/\/arxiv.org\/abs\/2403.06207v1","title":"Design and Development of a Multi-Purpose Collaborative Remote\n  Laboratory Platform","summary":"This work-in-progress paper presents the current development of a new\ncollaborative remote laboratory platform. The results are intended to serve as\na foundation for future research on collaborative work in remote laboratories.\nOur platform, standing out with its adaptive and collaborative capabilities,\nintegrates a distributed web-application for streamlined management and\nengagement in diverse remote educational environments.","updated":1710075888000,"published":1710075888000,"authors":["Sven Jacobs","Timo Hardebusch","Esther Franke","Henning Peters","Rashed Al Amin","Veit Wiese","Steffen Jaschke"],"comments":"accepted at IEEE Global Engineering Education Conference 2024, Kos,\n  Greece","categories":["cs.CY","cs.HC"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"108":{"arxiv_id":"2403.06267v1","url":"http:\/\/arxiv.org\/abs\/2403.06267v1","title":"FARPLS: A Feature-Augmented Robot Trajectory Preference Labeling System\n  to Assist Human Labelers' Preference Elicitation","summary":"Preference-based learning aims to align robot task objectives with human\nvalues. One of the most common methods to infer human preferences is by\npairwise comparisons of robot task trajectories. Traditional comparison-based\npreference labeling systems seldom support labelers to digest and identify\ncritical differences between complex trajectories recorded in videos. Our\nformative study (N = 12) suggests that individuals may overlook non-salient\ntask features and establish biased preference criteria during their preference\nelicitation process because of partial observations. In addition, they may\nexperience mental fatigue when given many pairs to compare, causing their label\nquality to deteriorate. To mitigate these issues, we propose FARPLS, a\nFeature-Augmented Robot trajectory Preference Labeling System. FARPLS\nhighlights potential outliers in a wide variety of task features that matter to\nhumans and extracts the corresponding video keyframes for easy review and\ncomparison. It also dynamically adjusts the labeling order according to users'\nfamiliarities, difficulties of the trajectory pair, and level of disagreements.\nAt the same time, the system monitors labelers' consistency and provides\nfeedback on labeling progress to keep labelers engaged. A between-subjects\nstudy (N = 42, 105 pairs of robot pick-and-place trajectories per person) shows\nthat FARPLS can help users establish preference criteria more easily and notice\nmore relevant details in the presented trajectories than the conventional\ninterface. FARPLS also improves labeling consistency and engagement, mitigating\nchallenges in preference elicitation without raising cognitive loads\nsignificantly","updated":1710090440000,"published":1710090440000,"authors":["Hanfang Lyu","Yuanchen Bai","Xin Liang","Ujaan Das","Chuhan Shi","Leiliang Gong","Yingchi Li","Mingfei Sun","Ming Ge","Xiaojuan Ma"],"comments":"Accepted to ACM Conference on Intelligent User Interfaces (IUI) 2024,\n  March 18-21, 2024, Greenville, SC, USA","categories":["cs.HC","cs.AI"],"primary_category":"cs.HC","doi":"10.1145\/3640543.3645145","journal_ref":null,"peer_reviewed":true},"109":{"arxiv_id":"2403.06431v1","url":"http:\/\/arxiv.org\/abs\/2403.06431v1","title":"From Fitting Participation to Forging Relationships: The Art of\n  Participatory ML","summary":"Participatory machine learning (ML) encourages the inclusion of end users and\npeople affected by ML systems in design and development processes. We\ninterviewed 18 participation brokers -- individuals who facilitate such\ninclusion and transform the products of participants' labour into inputs for an\nML artefact or system -- across a range of organisational settings and project\nlocations. Our findings demonstrate the inherent challenges of integrating\nmessy contextual information generated through participation with the\nstructured data formats required by ML workflows and the uneven power dynamics\nin project contexts. We advocate for evolution in the role of brokers to more\nequitably balance value generated in Participatory ML projects for design and\ndevelopment teams with value created for participants. To move beyond `fitting'\nparticipation to existing processes and empower participants to envision\nalternative futures through ML, brokers must become educators and advocates for\nend users, while attending to frustration and dissent from indirect\nstakeholders.","updated":1710132274000,"published":1710132274000,"authors":["Ned Cooper","Alex Zafiroglu"],"comments":"To appear in Proceedings of the 2024 CHI Conference on Human Factors\n  in Computing Systems (CHI '24)","categories":["cs.HC","cs.CY"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"110":{"arxiv_id":"2403.06651v1","url":"http:\/\/arxiv.org\/abs\/2403.06651v1","title":"SoniWeight Shoes: Investigating Effects and Personalization of a\n  Wearable Sound Device for Altering Body Perception and Behavior","summary":"Changes in body perception influence behavior and emotion and can be induced\nthrough multisensory feedback. Auditory feedback to one's actions can trigger\nsuch alterations; however, it is unclear which individual factors modulate\nthese effects. We employ and evaluate SoniWeight Shoes, a wearable device based\non literature for altering one's weight perception through manipulated footstep\nsounds. In a healthy population sample across a spectrum of individuals (n=84)\nwith varying degrees of eating disorder symptomatology, physical activity\nlevels, body concerns, and mental imagery capacities, we explore the effects of\nthree sound conditions (low-frequency, high-frequency and control) on extensive\nbody perception measures (demographic, behavioral, physiological,\npsychological, and subjective). Analyses revealed an impact of individual\ndifferences in each of these dimensions. Besides replicating previous findings,\nwe reveal and highlight the role of individual differences in body perception,\noffering avenues for personalized sonification strategies. Datasets, technical\nrefinements, and novel body map quantification tools are provided.","updated":1710159374000,"published":1710159374000,"authors":["A. D'Adamo","M. Roel-Lesur","L. Turmo-Vidal","M. M. Dehshibi","D. De La Prida","J. R. Diaz-Duran","L. A. Azpicueta-Ruiz","A. V\u00e4ljam\u00e4e","A. Tajadura-Jim\u00e9nez"],"comments":"Conditionally Accepted in CHI '24 Conference","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642651","journal_ref":null,"peer_reviewed":true},"111":{"arxiv_id":"2403.06823v2","url":"http:\/\/arxiv.org\/abs\/2403.06823v2","title":"Transparent AI Disclosure Obligations: Who, What, When, Where, Why, How","summary":"Advances in Generative Artificial Intelligence (AI) are resulting in\nAI-generated media output that is (nearly) indistinguishable from human-created\ncontent. This can drastically impact users and the media sector, especially\ngiven global risks of misinformation. While the currently discussed European AI\nAct aims at addressing these risks through Article 52's AI transparency\nobligations, its interpretation and implications remain unclear. In this early\nwork, we adopt a participatory AI approach to derive key questions based on\nArticle 52's disclosure obligations. We ran two workshops with researchers,\ndesigners, and engineers across disciplines (N=16), where participants\ndeconstructed Article 52's relevant clauses using the 5W1H framework. We\ncontribute a set of 149 questions clustered into five themes and 18 sub-themes.\nWe believe these can not only help inform future legal developments and\ninterpretations of Article 52, but also provide a starting point for\nHuman-Computer Interaction research to (re-)examine disclosure transparency\nfrom a human-centered AI lens.","updated":1710319233000,"published":1710171636000,"authors":["Abdallah El Ali","Karthikeya Puttur Venkatraj","Sophie Morosoli","Laurens Naudts","Natali Helberger","Pablo Cesar"],"comments":"Accepted to CHI 2024 Late-Breaking Work","categories":["cs.HC","cs.CY","H.5.m"],"primary_category":"cs.HC","doi":"10.1145\/3613905.3650750","journal_ref":null,"peer_reviewed":true},"112":{"arxiv_id":"2403.07082v1","url":"http:\/\/arxiv.org\/abs\/2403.07082v1","title":"Exploring the Impact of ChatGPT on Student Interactions in\n  Computer-Supported Collaborative Learning","summary":"The growing popularity of generative AI, particularly ChatGPT, has sparked\nboth enthusiasm and caution among practitioners and researchers in education.\nTo effectively harness the full potential of ChatGPT in educational contexts,\nit is crucial to analyze its impact and suitability for different educational\npurposes. This paper takes an initial step in exploring the applicability of\nChatGPT in a computer-supported collaborative learning (CSCL) environment.\nUsing statistical analysis, we validate the shifts in student interactions\nduring an asynchronous group brainstorming session by introducing ChatGPT as an\ninstantaneous question-answering agent.","updated":1710181098000,"published":1710181098000,"authors":["Han Kyul Kim","Shriniwas Nayak","Aleyeh Roknaldin","Xiaoci Zhang","Marlon Twyman","Stephen Lu"],"comments":"AAAI2024 Workshop on AI for Education (AI4ED)","categories":["cs.CY"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"113":{"arxiv_id":"2403.07131v1","url":"http:\/\/arxiv.org\/abs\/2403.07131v1","title":"Bigraph Matching Weighted with Learnt Incentive Function for Multi-Robot\n  Task Allocation","summary":"Most real-world Multi-Robot Task Allocation (MRTA) problems require fast and\nefficient decision-making, which is often achieved using heuristics-aided\nmethods such as genetic algorithms, auction-based methods, and bipartite graph\nmatching methods. These methods often assume a form that lends better\nexplainability compared to an end-to-end (learnt) neural network based policy\nfor MRTA. However, deriving suitable heuristics can be tedious, risky and in\nsome cases impractical if problems are too complex. This raises the question:\ncan these heuristics be learned? To this end, this paper particularly develops\na Graph Reinforcement Learning (GRL) framework to learn the heuristics or\nincentives for a bipartite graph matching approach to MRTA. Specifically a\nCapsule Attention policy model is used to learn how to weight task\/robot\npairings (edges) in the bipartite graph that connects the set of tasks to the\nset of robots. The original capsule attention network architecture is\nfundamentally modified by adding encoding of robots' state graph, and two\nMultihead Attention based decoders whose output are used to construct a\nLogNormal distribution matrix from which positive bigraph weights can be drawn.\nThe performance of this new bigraph matching approach augmented with a\nGRL-derived incentive is found to be at par with the original bigraph matching\napproach that used expert-specified heuristics, with the former offering\nnotable robustness benefits. During training, the learned incentive policy is\nfound to get initially closer to the expert-specified incentive and then\nslightly deviate from its trend.","updated":1710186908000,"published":1710186908000,"authors":["Steve Paul","Nathan Maurer","Souma Chowdhury"],"comments":"This paper was accepted for presentation in proceedings of IEEE\n  International Conference on Robotics and Automation 2024","categories":["cs.AI","cs.MA"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"114":{"arxiv_id":"2403.07548v2","url":"http:\/\/arxiv.org\/abs\/2403.07548v2","title":"Online Continual Learning For Interactive Instruction Following Agents","summary":"In learning an embodied agent executing daily tasks via language directives,\nthe literature largely assumes that the agent learns all training data at the\nbeginning. We argue that such a learning scenario is less realistic since a\nrobotic agent is supposed to learn the world continuously as it explores and\nperceives it. To take a step towards a more realistic embodied agent learning\nscenario, we propose two continual learning setups for embodied agents;\nlearning new behaviors (Behavior Incremental Learning, Behavior-IL) and new\nenvironments (Environment Incremental Learning, Environment-IL) For the tasks,\nprevious 'data prior' based continual learning methods maintain logits for the\npast tasks. However, the stored information is often insufficiently learned\ninformation and requires task boundary information, which might not always be\navailable. Here, we propose to update them based on confidence scores without\ntask boundary information during training (i.e., task-free) in a moving average\nfashion, named Confidence-Aware Moving Average (CAMA). In the proposed\nBehavior-IL and Environment-IL setups, our simple CAMA outperforms prior state\nof the art in our empirical validations by noticeable margins. The project page\nincluding codes is https:\/\/github.com\/snumprlab\/cl-alfred.","updated":1710297107000,"published":1710243228000,"authors":["Byeonghwi Kim","Minhyuk Seo","Jonghyun Choi"],"comments":"ICLR 2024 (Project page:\n  https:\/\/bhkim94.github.io\/projects\/CL-ALFRED)","categories":["cs.AI","cs.LG","cs.RO"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"115":{"arxiv_id":"2403.07587v1","url":"http:\/\/arxiv.org\/abs\/2403.07587v1","title":"Perennial Semantic Data Terms of Use for Decentralized Web","summary":"In today's digital landscape, the Web has become increasingly centralized,\nraising concerns about user privacy violations. Decentralized Web\narchitectures, such as Solid, offer a promising solution by empowering users\nwith better control over their data in their personal `Pods'. However, a\nsignificant challenge remains: users must navigate numerous applications to\ndecide which application can be trusted with access to their data Pods. This\noften involves reading lengthy and complex Terms of Use agreements, a process\nthat users often find daunting or simply ignore. This compromises user autonomy\nand impedes detection of data misuse. We propose a novel formal description of\nData Terms of Use (DToU), along with a DToU reasoner. Users and applications\nspecify their own parts of the DToU policy with local knowledge, covering\npermissions, requirements, prohibitions and obligations. Automated reasoning\nverifies compliance, and also derives policies for output data. This\nconstitutes a ``perennial'' DToU language, where the policy authoring only\noccurs once, and we can conduct ongoing automated checks across users,\napplications and activity cycles. Our solution is built on Turtle, Notation 3\nand RDF Surfaces, for the language and the reasoning engine. It ensures\nseamless integration with other semantic tools for enhanced interoperability.\nWe have successfully integrated this language into the Solid framework, and\nconducted performance benchmark. We believe this work demonstrates a\npracticality of a perennial DToU language and the potential of a paradigm shift\nto how users interact with data and applications in a decentralized Web,\noffering both improved privacy and usability.","updated":1710245900000,"published":1710245900000,"authors":["Rui Zhao","Jun Zhao"],"comments":"This paper is accepted by International World Wide Web Conference\n  2024 (WWW 2024 \/ The Web Conf 2024)","categories":["cs.AI","cs.CY","cs.LO"],"primary_category":"cs.AI","doi":"10.1145\/3589334.3645631","journal_ref":null,"peer_reviewed":true},"116":{"arxiv_id":"2403.07997v1","url":"http:\/\/arxiv.org\/abs\/2403.07997v1","title":"Fast-Forward Reality: Authoring Error-Free Context-Aware Policies with\n  Real-Time Unit Tests in Extended Reality","summary":"Advances in ubiquitous computing have enabled end-user authoring of\ncontext-aware policies (CAPs) that control smart devices based on specific\ncontexts of the user and environment. However, authoring CAPs accurately and\navoiding run-time errors is challenging for end-users as it is difficult to\nforesee CAP behaviors under complex real-world conditions. We propose\nFast-Forward Reality, an Extended Reality (XR) based authoring workflow that\nenables end-users to iteratively author and refine CAPs by validating their\nbehaviors via simulated unit test cases. We develop a computational approach to\nautomatically generate test cases based on the authored CAP and the user's\ncontext history. Our system delivers each test case with immersive\nvisualizations in XR, facilitating users to verify the CAP behavior and\nidentify necessary refinements. We evaluated Fast-Forward Reality in a user\nstudy (N=12). Our authoring and validation process improved the accuracy of\nCAPs and the users provided positive feedback on the system usability.","updated":1710266738000,"published":1710266738000,"authors":["Xun Qian","Tianyi Wang","Xuhai Xu","Tanya R Jonker","Kashyap Todi"],"comments":"17 pages, 7 figures, ACM CHI 2024 Full Paper","categories":["cs.HC","H.5.2"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642158","journal_ref":null,"peer_reviewed":true},"117":{"arxiv_id":"2403.08041v1","url":"http:\/\/arxiv.org\/abs\/2403.08041v1","title":"What would Plato say? Concepts and notions from Greek philosophy applied\n  to gamification mechanics for a meaningful and ethical gamification","summary":"Gamification, the integration of game mechanics in non-game settings, has\nbecome increasingly prevalent in various digital platforms; however, its\nethical and societal impacts are often overlooked. This paper delves into how\nPlatonic and Aristotelian philosophies can provide a critical framework for\nunderstanding and evaluating the ethical dimensions of gamification. Plato's\nallegory of the cave and theory of forms are used to analyse the perception of\nreality in gamified environments, questioning their authenticity and the value\nof virtual achievements, while Aristotle's virtue ethics, with its emphasis on\nmoderation, virtue, and eudaimonia (true and full happiness), can help assess\nhow gamification influences user behaviour and ethical decision-making. The\npaper critically examines various gamification elements, such as the hero's\njourney, altruistic actions, badge levels, and user autonomy, through these\nphilosophical lenses, and addresses the ethical responsibilities of\ngamification designers, advocating for a balanced approach that prioritizes\nuser well-being and ethical development over commercial interests. By bridging\nancient philosophical insights with modern digital culture, this research\ncontributes to a deeper understanding of the ethical implications of\ngamification, emphasizing the need for responsible and virtuous design in\ndigital applications.","updated":1710271513000,"published":1710271513000,"authors":["Kostas Karpouzis"],"comments":"Accepted for presentation at GamiFIN 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"118":{"arxiv_id":"2403.08049v1","url":"http:\/\/arxiv.org\/abs\/2403.08049v1","title":"TutoAI: A Cross-domain Framework for AI-assisted Mixed-media Tutorial\n  Creation on Physical Tasks","summary":"Mixed-media tutorials, which integrate videos, images, text, and diagrams to\nteach procedural skills, offer more browsable alternatives than timeline-based\nvideos. However, manually creating such tutorials is tedious, and existing\nautomated solutions are often restricted to a particular domain. While AI\nmodels hold promise, it is unclear how to effectively harness their powers,\ngiven the multi-modal data involved and the vast landscape of models. We\npresent TutoAI, a cross-domain framework for AI-assisted mixed-media tutorial\ncreation on physical tasks. First, we distill common tutorial components by\nsurveying existing work; then, we present an approach to identify, assemble,\nand evaluate AI models for component extraction; finally, we propose guidelines\nfor designing user interfaces (UI) that support tutorial creation based on\nAI-generated components. We show that TutoAI has achieved higher or similar\nquality compared to a baseline model in preliminary user studies.","updated":1710272819000,"published":1710272819000,"authors":["Yuexi Chen","Vlad I. Morariu","Anh Truong","Zhicheng Liu"],"comments":"CHI 2024, supplementary materials:\n  https:\/\/hdi.cs.umd.edu\/papers\/TutoAI_CHI24_Supp.pdf","categories":["cs.HC","cs.AI","cs.LG"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"119":{"arxiv_id":"2403.08057v1","url":"http:\/\/arxiv.org\/abs\/2403.08057v1","title":"MineXR: Mining Personalized Extended Reality Interfaces","summary":"Extended Reality (XR) interfaces offer engaging user experiences, but their\neffective design requires a nuanced understanding of user behavior and\npreferences. This knowledge is challenging to obtain without the widespread\nadoption of XR devices. We introduce MineXR, a design mining workflow and data\nanalysis platform for collecting and analyzing personalized XR user interaction\nand experience data. MineXR enables elicitation of personalized interfaces from\nparticipants of a data collection: for any particular context, participants\ncreate interface elements using application screenshots from their own\nsmartphone, place them in the environment, and simultaneously preview the\nresulting XR layout on a headset. Using MineXR, we contribute a dataset of\npersonalized XR interfaces collected from 31 participants, consisting of 695 XR\nwidgets created from 178 unique applications. We provide insights for XR widget\nfunctionalities, categories, clusters, UI element types, and placement. Our\nopen-source tools and data support researchers and designers in developing\nfuture XR interfaces.","updated":1710273914000,"published":1710273914000,"authors":["Hyunsung Cho","Yukang Yan","Kashyap Todi","Mark Parent","Missie Smith","Tanya R. Jonker","Hrvoje Benko","David Lindlbauer"],"comments":"17 pages, 18 figures, Proceedings of the 2024 CHI Conference on Human\n  Factors in Computing Systems","categories":["cs.HC","H.5.2"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642394","journal_ref":null,"peer_reviewed":true},"120":{"arxiv_id":"2403.08221v1","url":"http:\/\/arxiv.org\/abs\/2403.08221v1","title":"Help Supporters: Exploring the Design Space of Assistive Technologies to\n  Support Face-to-Face Help Between Blind and Sighted Strangers","summary":"Blind and low-vision (BLV) people face many challenges when venturing into\npublic environments, often wishing it were easier to get help from people\nnearby. Ironically, while many sighted individuals are willing to help, such\ninteractions are infrequent. Asking for help is socially awkward for BLV\npeople, and sighted people lack experience in helping BLV people. Through a\nmixed-ability research-through-design process, we explore four diverse\napproaches toward how assistive technology can serve as help supporters that\ncollaborate with both BLV and sighted parties throughout the help process.\nThese approaches span two phases: the connection phase (finding someone to\nhelp) and the collaboration phase (facilitating help after finding someone).\nOur findings from a 20-participant mixed-ability study reveal how help\nsupporters can best facilitate connection, which types of information they\nshould present during both phases, and more. We discuss design implications for\nfuture approaches to support face-to-face help.","updated":1710301606000,"published":1710301606000,"authors":["Yuanyang Teng","Connor Courtien","David Angel Rios","Yves M. Tseng","Jacqueline Gibson","Maryam Aziz","Avery Reyna","Rajan Vaish","Brian A. Smith"],"comments":"To Appear In Proceedings of the 2024 CHI Conference on Human Factors\n  in Computing Systems (Honolulu, HI, USA) Association for Computing Machinery,\n  New York, NY, USA. 24 pages","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642816","journal_ref":null,"peer_reviewed":true},"121":{"arxiv_id":"2403.08260v1","url":"http:\/\/arxiv.org\/abs\/2403.08260v1","title":"Understanding Reader Takeaways in Thematic Maps Under Varying Text,\n  Detail, and Spatial Autocorrelation","summary":"Maps are crucial in conveying geospatial data in diverse contexts such as\nnews and scientific reports. This research, utilizing thematic maps, probes\ndeeper into the underexplored intersection of text framing and map types in\ninfluencing map interpretation. In this work, we conducted experiments to\nevaluate how textual detail and semantic content variations affect the quality\nof insights derived from map examination. We also explored the influence of\nexplanatory annotations across different map types (e.g., choropleth, hexbin,\nisarithmic), base map details, and changing levels of spatial autocorrelation\nin the data. From two online experiments with $N=103$ participants, we found\nthat annotations, their specific attributes, and map type used to present the\ndata significantly shape the quality of takeaways. Notably, we found that the\neffectiveness of annotations hinges on their contextual integration. These\nfindings offer valuable guidance to the visualization community for crafting\nimpactful thematic geospatial representations.","updated":1710307400000,"published":1710307400000,"authors":["Arlen Fan","Fan Lei","Michelle Mancenido","Alan MacEachren","Ross Maciejewski"],"comments":"accepted to the ACM (Association of Computing Machinery) CHI\n  Conference on Human Factors in Computing Systems, CHI 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"122":{"arxiv_id":"2403.08363v1","url":"http:\/\/arxiv.org\/abs\/2403.08363v1","title":"ShareYourReality: Investigating Haptic Feedback and Agency in Virtual\n  Avatar Co-embodiment","summary":"Virtual co-embodiment enables two users to share a single avatar in Virtual\nReality (VR). During such experiences, the illusion of shared motion control\ncan break during joint-action activities, highlighting the need for\nposition-aware feedback mechanisms. Drawing on the perceptual crossing\nparadigm, we explore how haptics can enable non-verbal coordination between\nco-embodied participants. In a within-subjects study (20 participant pairs), we\nexamined the effects of vibrotactile haptic feedback (None, Present) and avatar\ncontrol distribution (25-75%, 50-50%, 75-25%) across two VR reaching tasks\n(Targeted, Free-choice) on participants Sense of Agency (SoA), co-presence,\nbody ownership, and motion synchrony. We found (a) lower SoA in the free-choice\nwith haptics than without, (b) higher SoA during the shared targeted task, (c)\nco-presence and body ownership were significantly higher in the free-choice\ntask, (d) players hand motions synchronized more in the targeted task. We\nprovide cautionary considerations when including haptic feedback mechanisms for\navatar co-embodiment experiences.","updated":1710321833000,"published":1710321833000,"authors":["Karthikeya Puttur Venkatraj","Wo Meijer","Monica Perusqu\u00eda-Hern\u00e1ndez","Gijs Huisman","Abdallah El Ali"],"comments":"Accepted to CHI 2024","categories":["cs.HC","cs.CY","H.5.m"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642425","journal_ref":null,"peer_reviewed":true},"123":{"arxiv_id":"2403.08451v1","url":"http:\/\/arxiv.org\/abs\/2403.08451v1","title":"An Integrated Usability Framework for Evaluating Open Government Data\n  Portals: Comparative Analysis of EU and GCC Countries","summary":"This study explores the critical role of open government data (OGD) portals\nin fostering transparency and collaboration between diverse stakeholders.\nRecognizing the challenges of usability, communication with diverse\npopulations, and strategic value creation, this paper develops an integrated\nframework for evaluating OGD portal effectiveness that accommodates user\ndiversity (regardless of their data literacy and language), evaluates\ncollaboration and participation, and the ability of users to explore and\nunderstand the data provided through them. The framework is validated by\napplying it to 33 national portals across European Union and Gulf Cooperation\nCouncil (GCC) countries, as a result of which we rank OGD portals, identify\nsome good practices that lower-performing portals can learn from, and common\nshortcomings. Notably, the study unveils the competitive and innovative nature\nof GCC OGD portals, pinpointing specific improvement areas such as multilingual\nsupport and data understandability. The findings underscore the growing trend\nof exposing data quality metrics and advocate for enhanced two-way\ncommunication channels between users and portal representatives. Overall, the\nstudy contributes to accelerating the development of user-friendly,\ncollaborative, and sustainable OGD portals while addressing gaps identified in\nprevious research.","updated":1710331602000,"published":1710331602000,"authors":["Fillip Molodtsov","Anastasija Nikiforova"],"comments":"This paper has been accepted for publication in Proceedings of the\n  25th Annual International Conference on Digital Government Research and this\n  is a preprint version of the manuscript","categories":["cs.CY","cs.SE"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"124":{"arxiv_id":"2403.08624v1","url":"http:\/\/arxiv.org\/abs\/2403.08624v1","title":"Towards a Privacy and Security-Aware Framework for Ethical AI: Guiding\n  the Development and Assessment of AI Systems","summary":"As artificial intelligence continues its unprecedented global expansion,\naccompanied by a proliferation of benefits, an increasing apprehension about\nthe privacy and security implications of AI-enabled systems emerges. The\npivotal question of effectively controlling AI development at both\njurisdictional and organizational levels has become a prominent theme in\ncontemporary discourse. While the European Parliament and Council have taken a\ndecisive step by reaching a political agreement on the EU AI Act, the first\ncomprehensive AI law, organizations still find it challenging to adapt to the\nfast-evolving AI landscape, lacking a universal tool for evaluating the privacy\nand security dimensions of their AI models and systems. In response to this\ncritical challenge, this study conducts a systematic literature review spanning\nthe years 2020 to 2023, with a primary focus on establishing a unified\ndefinition of key concepts in AI Ethics, particularly emphasizing the domains\nof privacy and security. Through the synthesis of knowledge extracted from the\nSLR, this study presents a conceptual framework tailored for privacy- and\nsecurity-aware AI systems. This framework is designed to assist diverse\nstakeholders, including organizations, academic institutions, and governmental\nbodies, in both the development and critical assessment of AI systems.\nEssentially, the proposed framework serves as a guide for ethical\ndecision-making, fostering an environment wherein AI is developed and utilized\nwith a strong commitment to ethical principles. In addition, the study unravels\nthe key issues and challenges surrounding the privacy and security dimensions,\ndelineating promising avenues for future research, thereby contributing to the\nongoing dialogue on the globalization and democratization of AI ethics.","updated":1710344397000,"published":1710344397000,"authors":["Daria Korobenko","Anastasija Nikiforova","Rajesh Sharma"],"comments":"\\c{opyright} Korobenko, D., Nikiforova, A., Rajesh, S. | ACM 2024.\n  This paper has been accepted for publication in In Proceedings of the 25th\n  Annual International Conference on Digital Government Research. This is the\n  author's version of the work. It is posted here for your personal use. Not\n  for redistribution. The definitive Version of Record will be published in ACM\n  DL","categories":["cs.CY"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"125":{"arxiv_id":"2403.08910v1","url":"http:\/\/arxiv.org\/abs\/2403.08910v1","title":"Meta-operators for Enabling Parallel Planning Using Deep Reinforcement\n  Learning","summary":"There is a growing interest in the application of Reinforcement Learning (RL)\ntechniques to AI planning with the aim to come up with general policies.\nTypically, the mapping of the transition model of AI planning to the state\ntransition system of a Markov Decision Process is established by assuming a\none-to-one correspondence of the respective action spaces. In this paper, we\nintroduce the concept of meta-operator as the result of simultaneously applying\nmultiple planning operators, and we show that including meta-operators in the\nRL action space enables new planning perspectives to be addressed using RL,\nsuch as parallel planning. Our research aims to analyze the performance and\ncomplexity of including meta-operators in the RL process, concretely in domains\nwhere satisfactory outcomes have not been previously achieved using usual\ngeneralized planning models. The main objective of this article is thus to pave\nthe way towards a redefinition of the RL action space in a manner that is more\nclosely aligned with the planning perspective.","updated":1710356436000,"published":1710356436000,"authors":["\u00c1ngel Aso-Mollar","Eva Onaindia"],"comments":"9 pages. Submitted to PRL workshop at ICAPS 2023","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"126":{"arxiv_id":"2403.08940v1","url":"http:\/\/arxiv.org\/abs\/2403.08940v1","title":"A Virtual Environment for Collaborative Inspection in Additive\n  Manufacturing","summary":"Additive manufacturing (AM) techniques have been used to enhance the design\nand fabrication of complex components for various applications in the medical,\naerospace, energy, and consumer products industries. A defining feature for\nmany AM parts is the complex internal geometry enabled by the printing process.\nHowever, inspecting these internal structures requires volumetric imaging,\ni.e., X-ray CT, leading to the well-known challenge of visualizing complex 3D\ngeometries using 2D desktop interfaces. Furthermore, existing tools are limited\nto single-user systems making it difficult to jointly discuss or share findings\nwith a larger team, i.e., the designers, manufacturing experts, and evaluation\nteam. In this work, we present a collaborative virtual reality (VR) for the\nexploration and inspection of AM parts. Geographically separated experts can\nvirtually inspect and jointly discuss data. It also supports VR and non-VR\nusers, who can be spectators in the VR environment. Various features for data\nexploration and inspection are developed and enhanced via real-time\nsynchronization. We followed usability and interface verification guidelines\nusing Nielsen's heuristics approach. Furthermore, we conducted exploratory and\nsemi-structured interviews with domain experts to collect qualitative feedback.\nResults reveal potential benefits, applicability, and current limitations. The\nproposed collaborative VR environment provides a new basis and opens new\nresearch directions for virtual inspection and team collaboration in AM\nsettings.","updated":1710360976000,"published":1710360976000,"authors":["Vuthea Chheang","Brian Thomas Weston","Robert William Cerda","Brian Au","Brian Giera","Peer-Timo Bremer","Haichao Miao"],"comments":"Conditionally Accepted - CHI LBW 2024","categories":["cs.HC","cs.DC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"127":{"arxiv_id":"2403.09043v1","url":"http:\/\/arxiv.org\/abs\/2403.09043v1","title":"How do Older Adults Set Up Voice Assistants? Lessons Learned from a\n  Deployment Experience for Older Adults to Set Up Standalone Voice Assistants","summary":"While standalone Voice Assistants (VAs) are promising to support older\nadults' daily routine and wellbeing management, onboarding and setting up these\ndevices can be challenging. Although some older adults choose to seek\nassistance from technicians and adult children, easy set up processes that\nfacilitate independent use are still critical, especially for those who do not\nhave access to external resources. We aim to understand the older adults'\nexperience while setting up commercially available voice-only and voice-first\nscreen-based VAs. Rooted in participants observations and semi-structured\ninterviews, we designed a within-subject study with 10 older adults using\nAmazon Echo Dot and Echo Show. We identified the values of the built-in\ntouchscreen and the instruction documents, as well as the impact of form\nfactors, and outline important directions to support older adult independence\nwith VAs.","updated":1710383309000,"published":1710383309000,"authors":["Chen Chen","Ella T. Lifset","Yichen Han","Arkajyoti Roy","Michael Hogarth","Alison A. Moore","Emilia Farcas","Nadir Weibel"],"comments":"5 pages, 1 figure, 1 table, Companion Publication of the 2023 ACM\n  Designing Interactive Systems Conference, July 2023, Pages 164-168","categories":["cs.HC","J.0; J.3; J.4"],"primary_category":"cs.HC","doi":"10.1145\/3563703.3596640","journal_ref":null,"peer_reviewed":true},"128":{"arxiv_id":"2403.09121v1","url":"http:\/\/arxiv.org\/abs\/2403.09121v1","title":"OutlineSpark: Igniting AI-powered Presentation Slides Creation from\n  Computational Notebooks through Outlines","summary":"Computational notebooks are widely utilized for exploration and analysis.\nHowever, creating slides to communicate analysis results from these notebooks\nis quite tedious and time-consuming. Researchers have proposed automatic\nsystems for generating slides from notebooks, which, however, often do not\nconsider the process of users conceiving and organizing their messages from\nmassive code cells. Those systems ask users to go directly into the slide\ncreation process, which causes potentially ill-structured slides and burdens in\nfurther refinement. Inspired by the common and widely recommended slide\ncreation practice: drafting outlines first and then adding concrete content, we\nintroduce OutlineSpark, an AI-powered slide creation tool that generates slides\nfrom a slide outline written by the user. The tool automatically retrieves\nrelevant notebook cells based on the outlines and converts them into slide\ncontent. We evaluated OutlineSpark with 12 users. Both the quantitative and\nqualitative feedback from the participants verify its effectiveness and\nusability.","updated":1710396651000,"published":1710396651000,"authors":["Fengjie Wang","Yanna Lin","Leni Yang","Haotian Li","Mingyang Gu","Min Zhu","Huamin Qu"],"comments":"To appear in Proceedings of the CHI Conference on Human Factors in\n  Computing Systems (CHI 2024)","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"129":{"arxiv_id":"2403.09216v1","url":"http:\/\/arxiv.org\/abs\/2403.09216v1","title":"Unlocking the Potential of Open Government Data: Exploring the\n  Strategic, Technical, and Application Perspectives of High-Value Datasets\n  Opening in Taiwan","summary":"Today, data has an unprecedented value as it forms the basis for data-driven\ndecision-making, including serving as an input for AI models, where the latter\nis highly dependent on the availability of the data. However, availability of\ndata in an open data format creates a little added value, where the value of\nthese data, i.e., their relevance to the real needs of the end user, is key.\nThis is where the concept of high-value dataset (HVD) comes into play, which\nhas become popular in recent years. Defining and opening HVD is an ongoing\nprocess consisting of a set of interrelated steps, the implementation of which\nmay vary from one country or region to another. Therefore, there has recently\nbeen a call to conduct research in a country or region setting considered to be\nof greatest national value. So far, only a few studies have been conducted at\nthe regional or national level, most of which consider only one step of the\nprocess, such as identifying HVD or measuring their impact. With this study, we\nanswer this call and examine the national case of Taiwan by exploring the\nentire lifecycle of HVD opening. The aim of the paper is to understand and\nevaluate the lifecycle of high-value dataset publishing in one of the world's\nleading producers of information and communication technology (ICT) products -\nTaiwan. To do this, we conduct a qualitative study with exploratory interviews\nwith representatives from government agencies in Taiwan responsible for HVD\nopening, exploring HVD opening lifecycle. As such, we examine (1) strategic\naspects related to the HVD determination process, (2) technical aspects, and\n(3) application aspects.","updated":1710408680000,"published":1710408680000,"authors":["Hsien-Lee Tseng","Anastasija Nikiforova"],"comments":"This paper has been accepted for publication in Proceedings of the\n  25th Annual International Conference on Digital Government Research and this\n  is a pre-print version of the manuscript. It is posted here for your personal\n  use. Not for redistribution","categories":["cs.CY"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"130":{"arxiv_id":"2403.09308v1","url":"http:\/\/arxiv.org\/abs\/2403.09308v1","title":"Enabling Waypoint Generation for Collaborative Robots using LLMs and\n  Mixed Reality","summary":"Programming a robotic is a complex task, as it demands the user to have a\ngood command of specific programming languages and awareness of the robot's\nphysical constraints. We propose a framework that simplifies robot deployment\nby allowing direct communication using natural language. It uses large language\nmodels (LLM) for prompt processing, workspace understanding, and waypoint\ngeneration. It also employs Augmented Reality (AR) to provide visual feedback\nof the planned outcome. We showcase the effectiveness of our framework with a\nsimple pick-and-place task, which we implement on a real robot. Moreover, we\npresent an early concept of expressive robot behavior and skill generation that\ncan be used to communicate with the user and learn new skills (e.g., object\ngrasping).","updated":1710417547000,"published":1710417547000,"authors":["Cathy Mengying Fang","Krzysztof Zieli\u0144ski","Pattie Maes","Joe Paradiso","Bruce Blumberg","Mikkel Baun Kj\u00e6rgaard"],"comments":"Submitted to VLMNM 2024 - Workshop, ICRA 2024. This work has been\n  submitted to the IEEE for possible publication. Copyright may be transferred\n  without notice, after which this version may no longer be accessible","categories":["cs.HC","cs.RO"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"131":{"arxiv_id":"2403.09405v1","url":"http:\/\/arxiv.org\/abs\/2403.09405v1","title":"Which Artificial Intelligences Do People Care About Most? A Conjoint\n  Experiment on Moral Consideration","summary":"Many studies have identified particular features of artificial intelligences\n(AI), such as their autonomy and emotion expression, that affect the extent to\nwhich they are treated as subjects of moral consideration. However, there has\nnot yet been a comparison of the relative importance of features as is\nnecessary to design and understand increasingly capable, multi-faceted AI\nsystems. We conducted an online conjoint experiment in which 1,163 participants\nevaluated descriptions of AIs that varied on these features. All 11 features\nincreased how morally wrong participants considered it to harm the AIs. The\nlargest effects were from human-like physical bodies and prosociality (i.e.,\nemotion expression, emotion recognition, cooperation, and moral judgment). For\nhuman-computer interaction designers, the importance of prosociality suggests\nthat, because AIs are often seen as threatening, the highest levels of moral\nconsideration may only be granted if the AI has positive intentions.","updated":1710424534000,"published":1710424534000,"authors":["Ali Ladak","Jamie Harris","Jacy Reese Anthis"],"comments":"11 pages, 2 figures. Accepted to 2024 CHI Conference on Human Factors\n  in Computing Systems (CHI '24)","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"132":{"arxiv_id":"2403.09592v1","url":"http:\/\/arxiv.org\/abs\/2403.09592v1","title":"DungeonMaker: Embedding Tangible Creation and Destruction in Hybrid\n  Board Games through Personal Fabrication Technology","summary":"Hybrid board games (HBGs) augment their analog origins digitally (e.g.,\nthrough apps) and are an increasingly popular pastime activity. Continuous\nworld and character development and customization, known to facilitate\nengagement in video games, remain rare in HBGs. If present, they happen\ndigitally or imaginarily, often leaving physical aspects generic. We developed\nDungeonMaker, a fabrication-augmented HBG bridging physical and digital game\nelements: 1) the setup narrates a story and projects a digital game board onto\na laser cutter; 2) DungeonMaker assesses player-crafted artifacts; 3)\nDungeonMaker's modified laser head senses and moves player- and non-player\nfigures, and 4) can physically damage figures. An evaluation (n=4x3) indicated\nthat DungeonMaker provides an engaging experience, may support players'\nconnection to their figures, and potentially spark novices' interest in\nfabrication. DungeonMaker provides a rich constellation to play HBGs by\nblending aspects of craft and automation to couple the physical and digital\nelements of an HBG tightly.","updated":1710437719000,"published":1710437719000,"authors":["Evgeny Stemasov","Tobias Wagner","Ali Askari","Jessica Janek","Omid Rajabi","Anja Schikorr","Julian Frommel","Jan Gugenheimer","Enrico Rukzio"],"comments":"16 pages, 8 figures, 1 table. Accepted to ACM CHI 2024 (ACM CHI\n  conference on Human Factors in Computing Systems)","categories":["cs.HC","H.5.m"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642243","journal_ref":"Proceedings of the CHI Conference on Human Factors in Computing\n  Systems (CHI '24), May 11-16, 2024, Honolulu, HI, USA","peer_reviewed":true},"133":{"arxiv_id":"2403.09607v1","url":"http:\/\/arxiv.org\/abs\/2403.09607v1","title":"pARam: Leveraging Parametric Design in Extended Reality to Support the\n  Personalization of Artifacts for Personal Fabrication","summary":"Extended Reality (XR) allows in-situ previewing of designs to be manufactured\nthrough Personal Fabrication (PF). These in-situ interactions exhibit\nadvantages for PF, like incorporating the environment into the design process.\nHowever, design-for-fabrication in XR often happens through either highly\ncomplex 3D-modeling or is reduced to rudimentary adaptations of crowd-sourced\nmodels. We present pARam, a tool combining parametric designs (PDs) and XR,\nenabling in-situ configuration of artifacts for PF. In contrast to modeling- or\nsearch-focused approaches, pARam supports customization through embodied and\npractical inputs (e.g., gestures, recommendations) and evaluation (e.g.,\nlighting estimation) without demanding complex 3D-modeling skills. We\nimplemented pARam for HoloLens 2 and evaluated it (n=20), comparing XR and\ndesktop conditions. Users succeeded in choosing context-related parameters and\ntook their environment into account for their configuration using pARam. We\nreflect on the prospects and challenges of PDs in XR to streamline complex\ndesign methods for PF while retaining suitable expressivity.","updated":1710438510000,"published":1710438510000,"authors":["Evgeny Stemasov","Simon Demharter","Max R\u00e4dler","Jan Gugenheimer","Enrico Rukzio"],"comments":"17 pages, 17 figures. Accepted to ACM CHI 2024 (ACM CHI conference on\n  Human Factors in Computing Systems)","categories":["cs.HC","cs.GR","H.5.2"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642083","journal_ref":"Proceedings of the CHI Conference on Human Factors in Computing\n  Systems (CHI '24), May 11-16, 2024, Honolulu, HI, USA","peer_reviewed":true},"134":{"arxiv_id":"2403.09925v1","url":"http:\/\/arxiv.org\/abs\/2403.09925v1","title":"Surrogate Assisted Monte Carlo Tree Search in Combinatorial Optimization","summary":"Industries frequently adjust their facilities network by opening new branches\nin promising areas and closing branches in areas where they expect low profits.\nIn this paper, we examine a particular class of facility location problems. Our\nobjective is to minimize the loss of sales resulting from the removal of\nseveral retail stores. However, estimating sales accurately is expensive and\ntime-consuming. To overcome this challenge, we leverage Monte Carlo Tree Search\n(MCTS) assisted by a surrogate model that computes evaluations faster. Results\nsuggest that MCTS supported by a fast surrogate function can generate solutions\nfaster while maintaining a consistent solution compared to MCTS that does not\nbenefit from the surrogate function.","updated":1710460459000,"published":1710460459000,"authors":["Saeid Amiri","Parisa Zehtabi","Danial Dervovic","Michael Cashmore"],"comments":"Accepted to the ICAPS Planning and Scheduling for Financial Services\n  (FINPLAN) 2023 workshop","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"135":{"arxiv_id":"2403.10112v1","url":"http:\/\/arxiv.org\/abs\/2403.10112v1","title":"Single- and Multi-Agent Private Active Sensing: A Deep Neuroevolution\n  Approach","summary":"In this paper, we focus on one centralized and one decentralized problem of\nactive hypothesis testing in the presence of an eavesdropper. For the\ncentralized problem including a single legitimate agent, we present a new\nframework based on NeuroEvolution (NE), whereas, for the decentralized problem,\nwe develop a novel NE-based method for solving collaborative multi-agent tasks,\nwhich interestingly maintains all computational benefits of single-agent NE.\nThe superiority of the proposed EAHT approaches over conventional active\nhypothesis testing policies, as well as learning-based methods, is validated\nthrough numerical investigations in an example use case of anomaly detection\nover wireless sensor networks.","updated":1710492956000,"published":1710492956000,"authors":["George Stamatelis","Angelos-Nikolaos Kanatas","Ioannis Asprogerakas","George C. Alexandropoulos"],"comments":"7 pages, 5 figures, accepted at IEEE ICC 2024 (to be presented)","categories":["cs.AI","cs.CR","cs.MA","cs.NE"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"136":{"arxiv_id":"2403.10184v1","url":"http:\/\/arxiv.org\/abs\/2403.10184v1","title":"Lifted Causal Inference in Relational Domains","summary":"Lifted inference exploits symmetries in probabilistic graphical models by\nusing a representative for indistinguishable objects, thereby speeding up query\nanswering while maintaining exact answers. Even though lifting is a\nwell-established technique for the task of probabilistic inference in\nrelational domains, it has not yet been applied to the task of causal\ninference. In this paper, we show how lifting can be applied to efficiently\ncompute causal effects in relational domains. More specifically, we introduce\nparametric causal factor graphs as an extension of parametric factor graphs\nincorporating causal knowledge and give a formal semantics of interventions\ntherein. We further present the lifted causal inference algorithm to compute\ncausal effects on a lifted level, thereby drastically speeding up causal\ninference compared to propositional inference, e.g., in causal Bayesian\nnetworks. In our empirical evaluation, we demonstrate the effectiveness of our\napproach.","updated":1710499467000,"published":1710499467000,"authors":["Malte Luttermann","Mattis Hartwig","Tanya Braun","Ralf M\u00f6ller","Marcel Gehrke"],"comments":"Accepted to the Proceedings of the 3rd Conference on Causal Learning\n  and Reasoning (CLeaR-24)","categories":["cs.AI","cs.DS"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"137":{"arxiv_id":"2403.10279v1","url":"http:\/\/arxiv.org\/abs\/2403.10279v1","title":"Emotion-Aware Multimodal Fusion for Meme Emotion Detection","summary":"The ever-evolving social media discourse has witnessed an overwhelming use of\nmemes to express opinions or dissent. Besides being misused for spreading\nmalcontent, they are mined by corporations and political parties to glean the\npublic's opinion. Therefore, memes predominantly offer affect-enriched insights\ntowards ascertaining the societal psyche. However, the current approaches are\nyet to model the affective dimensions expressed in memes effectively. They rely\nextensively on large multimodal datasets for pre-training and do not generalize\nwell due to constrained visual-linguistic grounding. In this paper, we\nintroduce MOOD (Meme emOtiOns Dataset), which embodies six basic emotions. We\nthen present ALFRED (emotion-Aware muLtimodal Fusion foR Emotion Detection), a\nnovel multimodal neural framework that (i) explicitly models emotion-enriched\nvisual cues, and (ii) employs an efficient cross-modal fusion via a gating\nmechanism. Our investigation establishes ALFRED's superiority over existing\nbaselines by 4.94% F1. Additionally, ALFRED competes strongly with previous\nbest approaches on the challenging Memotion task. We then discuss ALFRED's\ndomain-agnostic generalizability by demonstrating its dominance on two\nrecently-released datasets - HarMeme and Dank Memes, over other baselines.\nFurther, we analyze ALFRED's interpretability using attention maps. Finally, we\nhighlight the inherent challenges posed by the complex interplay of disparate\nmodality-specific cues toward meme analysis.","updated":1710508838000,"published":1710508838000,"authors":["Shivam Sharma","Ramaneswaran S","Md. Shad Akhtar","Tanmoy Chakraborty"],"comments":"Accepted to IEEE Transactions on Affective Computing","categories":["cs.CY"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"138":{"arxiv_id":"2403.10299v1","url":"http:\/\/arxiv.org\/abs\/2403.10299v1","title":"A Multi-constraint and Multi-objective Allocation Model for Emergency\n  Rescue in IoT Environment","summary":"Emergency relief operations are essential in disaster aftermaths,\nnecessitating effective resource allocation to minimize negative impacts and\nmaximize benefits. In prolonged crises or extensive disasters, a systematic,\nmulti-cycle approach is key for timely and informed decision-making. Leveraging\nadvancements in IoT and spatio-temporal data analytics, we've developed the\nMulti-Objective Shuffled Gray-Wolf Frog Leaping Model (MSGW-FLM). This\nmulti-constraint, multi-objective resource allocation model has been rigorously\ntested against 28 diverse challenges, showing superior performance in\ncomparison to established models such as NSGA-II, IBEA, and MOEA\/D. MSGW-FLM's\neffectiveness is particularly notable in complex, multi-cycle emergency rescue\nscenarios, which involve numerous constraints and objectives. This model\nrepresents a significant step forward in optimizing resource distribution in\nemergency response situations.","updated":1710510120000,"published":1710510120000,"authors":["Xinrun Xu","Zhanbiao Lian","Yurong Wu","Manying Lv","Zhiming Ding","Jian Yan","Shang Jiang"],"comments":"5 pages, 5 figures, ISCAS 2024","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"139":{"arxiv_id":"2403.10356v1","url":"http:\/\/arxiv.org\/abs\/2403.10356v1","title":"Understanding Stress: A Web Interface for Mental Arithmetic Tasks in a\n  Trier Social Stress Test","summary":"Stress is a dynamic process that reflects the responses of the brain.\nTraditional methods for measuring stress are often time-consuming and\nsusceptible to recall bias. To address this, we investigated changes in heart\nrate (HR) during the Trier Social Stress Test (TSST). Our study incorporated\nvarying levels of complexity in mental arithmetic problems. Participants' HR\nincreased during the Mental Arithmetic Task phase compared to baseline and\nresting stages, indicating that stress is reflected in HR.","updated":1710513829000,"published":1710513829000,"authors":["Manjeet Yadav","Nilesh Kumar Sahu"],"comments":"Accepted in Student Design Consortium of India HCI 2023","categories":["cs.CY"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"140":{"arxiv_id":"2403.10438v1","url":"http:\/\/arxiv.org\/abs\/2403.10438v1","title":"Data Ethics Emergency Drill: A Toolbox for Discussing Responsible AI for\n  Industry Teams","summary":"Researchers urge technology practitioners such as data scientists to consider\nthe impacts and ethical implications of algorithmic decisions. However, unlike\nprogramming, statistics, and data management, discussion of ethical\nimplications is rarely included in standard data science training. To begin to\naddress this gap, we designed and tested a toolbox called the data ethics\nemergency drill (DEED) to help data science teams discuss and reflect on the\nethical implications of their work. The DEED is a roleplay of a fictional\nethical emergency scenario that is contextually situated in the team's specific\nworkplace and applications. This paper outlines the DEED toolbox and describes\nthree studies carried out with two different data science teams that\niteratively shaped its design. Our findings show that practitioners can apply\nlessons learnt from the roleplay to real-life situations, and how the DEED\nopened up conversations around ethics and values.","updated":1710519651000,"published":1710519651000,"authors":["Vanessa Aisyahsari Hanschke","Dylan Rees","Merve Alanyali","David Hopkinson","Paul Marshall"],"comments":"accepted to CHI 2024","categories":["cs.HC","cs.AI","cs.CY"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"141":{"arxiv_id":"2403.10720v1","url":"http:\/\/arxiv.org\/abs\/2403.10720v1","title":"Development and Application of a Monte Carlo Tree Search Algorithm for\n  Simulating Da Vinci Code Game Strategies","summary":"In this study, we explore the efficiency of the Monte Carlo Tree Search\n(MCTS), a prominent decision-making algorithm renowned for its effectiveness in\ncomplex decision environments, contingent upon the volume of simulations\nconducted. Notwithstanding its broad applicability, the algorithm's performance\ncan be adversely impacted in certain scenarios, particularly within the domain\nof game strategy development. This research posits that the inherent branch\ndivergence within the Da Vinci Code board game significantly impedes\nparallelism when executed on Graphics Processing Units (GPUs). To investigate\nthis hypothesis, we implemented and meticulously evaluated two variants of the\nMCTS algorithm, specifically designed to assess the impact of branch divergence\non computational performance. Our comparative analysis reveals a linear\nimprovement in performance with the CPU-based implementation, in stark contrast\nto the GPU implementation, which exhibits a non-linear enhancement pattern and\ndiscernible performance troughs. These findings contribute to a deeper\nunderstanding of the MCTS algorithm's behavior in divergent branch scenarios,\nhighlighting critical considerations for optimizing game strategy algorithms on\nparallel computing architectures.","updated":1710542617000,"published":1710542617000,"authors":["Ye Zhang","Mengran Zhu","Kailin Gui","Jiayue Yu","Yong Hao","Haozhan Sun"],"comments":"This paper has been accepted by CVIDL2024","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"142":{"arxiv_id":"2403.10776v1","url":"http:\/\/arxiv.org\/abs\/2403.10776v1","title":"From Melting Pots to Misrepresentations: Exploring Harms in Generative\n  AI","summary":"With the widespread adoption of advanced generative models such as Gemini and\nGPT, there has been a notable increase in the incorporation of such models into\nsociotechnical systems, categorized under AI-as-a-Service (AIaaS). Despite\ntheir versatility across diverse sectors, concerns persist regarding\ndiscriminatory tendencies within these models, particularly favoring selected\n`majority' demographics across various sociodemographic dimensions. Despite\nwidespread calls for diversification of media representations, marginalized\nracial and ethnic groups continue to face persistent distortion, stereotyping,\nand neglect within the AIaaS context. In this work, we provide a critical\nsummary of the state of research in the context of social harms to lead the\nconversation to focus on their implications. We also present open-ended\nresearch questions, guided by our discussion, to help define future research\npathways.","updated":1710556182000,"published":1710556182000,"authors":["Sanjana Gautam","Pranav Narayanan Venkit","Sourojit Ghosh"],"comments":"In CHI 2024: Generative AI and HCI workshop (GenAICHI 24)","categories":["cs.HC","cs.AI","cs.CY","cs.LG"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"143":{"arxiv_id":"2403.10792v1","url":"http:\/\/arxiv.org\/abs\/2403.10792v1","title":"\"It's Kind of Context Dependent\": Understanding Blind and Low Vision\n  People's Video Accessibility Preferences Across Viewing Scenarios","summary":"While audio description (AD) is the standard approach for making videos\naccessible to blind and low vision (BLV) people, existing AD guidelines do not\nconsider BLV users' varied preferences across viewing scenarios. These\nscenarios range from how-to videos on YouTube, where users seek to learn new\nskills, to historical dramas on Netflix, where a user's goal is entertainment.\nAdditionally, the increase in video watching on mobile devices provides an\nopportunity to integrate nonverbal output modalities (e.g., audio cues, tactile\nelements, and visual enhancements). Through a formative survey and 15\nsemi-structured interviews, we identified BLV people's video accessibility\npreferences across diverse scenarios. For example, participants valued action\nand equipment details for how-to videos, tactile graphics for learning\nscenarios, and 3D models for fantastical content. We define a six-dimensional\nvideo accessibility design space to guide future innovation and discuss how to\nmove from \"one-size-fits-all\" paradigms to scenario-specific approaches.","updated":1710561221000,"published":1710561221000,"authors":["Lucy Jiang","Crescentia Jung","Mahika Phutane","Abigale Stangl","Shiri Azenkot"],"comments":"To appear at CHI 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642238","journal_ref":null,"peer_reviewed":true},"144":{"arxiv_id":"2403.10851v1","url":"http:\/\/arxiv.org\/abs\/2403.10851v1","title":"GustosonicSense: Towards understanding the design of playful gustosonic\n  eating experiences","summary":"The pleasure that often comes with eating can be further enhanced with\nintelligent technology, as the field of human-food interaction suggests.\nHowever, knowledge on how to design such pleasure-supporting eating systems is\nlimited. To begin filling this knowledge gap, we designed \"GustosonicSense\", a\nnovel gustosonic eating system that utilizes wireless earbuds for sensing\ndifferent eating and drinking actions with a machine learning algorithm and\ntrigger playful sounds as a way to facilitate pleasurable eating experiences.\nWe present the findings from our design and a study that revealed how we can\nsupport the \"stimulation\", \"hedonism\", and \"reflexivity\" for playful human-food\ninteractions. Ultimately, with our work, we aim to support interaction\ndesigners in facilitating playful experiences with food.","updated":1710576669000,"published":1710576669000,"authors":["Yan Wang","Humphrey O. Obie","Zhuying Li","Flora D. Salim","John Grundy","Florian 'Floyd' Mueller"],"comments":"To appear at CHI'24: The ACM Conference on Human Factors in Computing\n  Systems (CHI), Honolulu, Hawaii, 2024","categories":["cs.HC","cs.MM"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"145":{"arxiv_id":"2403.11164v1","url":"http:\/\/arxiv.org\/abs\/2403.11164v1","title":"The Effects of Generative AI on Design Fixation and Divergent Thinking","summary":"Generative AI systems have been heralded as tools for augmenting human\ncreativity and inspiring divergent thinking, though with little empirical\nevidence for these claims. This paper explores the effects of exposure to\nAI-generated images on measures of design fixation and divergent thinking in a\nvisual ideation task. Through a between-participants experiment (N=60), we\nfound that support from an AI image generator during ideation leads to higher\nfixation on an initial example. Participants who used AI produced fewer ideas,\nwith less variety and lower originality compared to a baseline. Our qualitative\nanalysis suggests that the effectiveness of co-ideation with AI rests on\nparticipants' chosen approach to prompt creation and on the strategies used by\nparticipants to generate ideas in response to the AI's suggestions. We discuss\nopportunities for designing generative AI systems for ideation support and\nincorporating these AI tools into ideation workflows.","updated":1710670311000,"published":1710670311000,"authors":["Samangi Wadinambiarachchi","Ryan M. Kelly","Saumya Pareek","Qiushi Zhou","Eduardo Velloso"],"comments":"Accepted at the CHI Conference on Human Factors in Computing Systems\n  (CHI 24),18 pages, 15 figures,","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642919","journal_ref":null,"peer_reviewed":true},"146":{"arxiv_id":"2403.11734v1","url":"http:\/\/arxiv.org\/abs\/2403.11734v1","title":"Learning General Policies for Classical Planning Domains: Getting Beyond\n  C$_2$","summary":"GNN-based approaches for learning general policies across planning domains\nare limited by the expressive power of $C_2$, namely; first-order logic with\ntwo variables and counting. This limitation can be overcomed by transitioning\nto $k$-GNNs, for $k=3$, wherein object embeddings are substituted with triplet\nembeddings. Yet, while $3$-GNNs have the expressive power of $C_3$, unlike $1$-\nand $2$-GNNs that are confined to $C_2$, they require quartic time for message\nexchange and cubic space for embeddings, rendering them impractical. In this\nwork, we introduce a parameterized version of relational GNNs. When $t$ is\ninfinity, R-GNN[$t$] approximates $3$-GNNs using only quadratic space for\nembeddings. For lower values of $t$, such as $t=1$ and $t=2$, R-GNN[$t$]\nachieves a weaker approximation by exchanging fewer messages, yet\ninterestingly, often yield the $C_3$ features required in several planning\ndomains. Furthermore, the new R-GNN[$t$] architecture is the original R-GNN\narchitecture with a suitable transformation applied to the input states only.\nExperimental results illustrate the clear performance gains of R-GNN[$1$] and\nR-GNN[$2$] over plain R-GNNs, and also over edge transformers that also\napproximate $3$-GNNs.","updated":1710765773000,"published":1710765773000,"authors":["Simon St\u00e5hlberg","Blai Bonet","Hector Geffner"],"comments":"Submitted to IJCAI 2024","categories":["cs.AI","cs.LG"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"147":{"arxiv_id":"2403.11756v1","url":"http:\/\/arxiv.org\/abs\/2403.11756v1","title":"Just Undo It: Exploring Undo Mechanics in Multi-User Virtual Reality","summary":"With the proliferation of VR and a metaverse on the horizon, many multi-user\nactivities are migrating to the VR world, calling for effective collaboration\nsupport. As one key feature, traditional collaborative systems provide users\nwith undo mechanics to reverse errors and other unwanted changes. While undo\nhas been extensively researched in this domain and is now considered industry\nstandard, it is strikingly absent for VR systems in research and industry. This\nwork addresses this research gap by exploring different undo techniques for\nbasic object manipulation in different collaboration modes in VR. We conducted\na study involving 32 participants organized in teams of two. Here, we studied\nusers' performance and preferences in a tower stacking task, varying the\navailable undo techniques and their mode of collaboration. The results suggest\nthat users desire and use undo in VR and that the choice of the undo technique\nimpacts users' performance and social connection.","updated":1710767203000,"published":1710767203000,"authors":["Julian Rasch","Florian Perzl","Yannick Weiss","Florian M\u00fcller"],"comments":"To appear in Proceedings of the CHI Conference on Human Factors in\n  Computing Systems (CHI '24), May 11-16, 2024, Honolulu, HI, USA","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642864","journal_ref":null,"peer_reviewed":true},"148":{"arxiv_id":"2403.11952v1","url":"http:\/\/arxiv.org\/abs\/2403.11952v1","title":"Exploring Estonia's Open Government Data Development as a Journey\n  towards Excellence: Unveiling the Progress of Local Governments in Open Data\n  Provision","summary":"Estonia has a global reputation of a digital state or e-country. However,\ndespite the success in digital governance, the country has faced challenges in\nthe realm of Open Government Data (OGD) area, with significant advancements in\nits OGD ecosystem, as reflected in various open data rankings from 2020 and\nonwards, in the recent years being recognized among trend-setters. This paper\naims to explore the evolution and positioning of Estonia's OGD development,\nencompassing national and local levels, through an integrated analysis of\nvarious indices, primary data from the Estonian OGD portal, and a thorough\nliterature review. The research shows that Estonia has made progress in the\nnational level open data ecosystem, primarily due to improvements in the OGD\nportal usability and legislation amendments. However, the local level is not as\ndeveloped, with local governments lagging behind in OGD provision. The\nliterature review highlights the lack of previous research focusing on Estonian\nand European local open data, emphasizing the need for future studies to\nexplore the barriers and enablers of municipal OGD. This study contributes to a\nnuanced understanding of Estonia's dynamic journey in the OGD landscape,\nshedding light on both achievements and areas warranting further attention for\nestablishing a sustainable open data ecosystem.","updated":1710780605000,"published":1710780605000,"authors":["Katrin Rajam\u00e4e-Soosaar","Anastasija Nikiforova"],"comments":"This paper has been accepted for publication in Proceedings of the\n  25th Annual International Conference on Digital Government Research and this\n  is a pre-print version of the manuscript. It is posted here for your personal\n  use. Not for redistribution","categories":["cs.CY","cs.CE","cs.DB","cs.SE","cs.SI"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"149":{"arxiv_id":"2403.12004v1","url":"http:\/\/arxiv.org\/abs\/2403.12004v1","title":"The Value, Benefits, and Concerns of Generative AI-Powered Assistance in\n  Writing","summary":"Recent advances in generative AI technologies like large language models\nraise both excitement and concerns about the future of human-AI co-creation in\nwriting. To unpack people's attitude towards and experience with generative\nAI-powered writing assistants, in this paper, we conduct an experiment to\nunderstand whether and how much value people attach to AI assistance, and how\nthe incorporation of AI assistance in writing workflows changes people's\nwriting perceptions and performance. Our results suggest that people are\nwilling to forgo financial payments to receive writing assistance from AI,\nespecially if AI can provide direct content generation assistance and the\nwriting task is highly creative. Generative AI-powered assistance is found to\noffer benefits in increasing people's productivity and confidence in writing.\nHowever, direct content generation assistance offered by AI also comes with\nrisks, including decreasing people's sense of accountability and diversity in\nwriting. We conclude by discussing the implications of our findings.","updated":1710783745000,"published":1710783745000,"authors":["Zhuoyan Li","Chen Liang","Jing Peng","Ming Yin"],"comments":"CHI 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"150":{"arxiv_id":"2403.12162v1","url":"http:\/\/arxiv.org\/abs\/2403.12162v1","title":"Intelligent Execution through Plan Analysis","summary":"Intelligent robots need to generate and execute plans. In order to deal with\nthe complexity of real environments, planning makes some assumptions about the\nworld. When executing plans, the assumptions are usually not met. Most works\nhave focused on the negative impact of this fact and the use of replanning\nafter execution failures. Instead, we focus on the positive impact, or\nopportunities to find better plans. When planning, the proposed technique finds\nand stores those opportunities. Later, during execution, the monitoring system\ncan use them to focus perception and repair the plan, instead of replanning\nfrom scratch. Experiments in several paradigmatic robotic tasks show how the\napproach outperforms standard replanning strategies.","updated":1710786216000,"published":1710786216000,"authors":["Daniel Borrajo","Manuela Veloso"],"comments":"Published at IROS 21, 6 pages","categories":["cs.AI","cs.RO"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"151":{"arxiv_id":"2403.12227v1","url":"http:\/\/arxiv.org\/abs\/2403.12227v1","title":"Analyzing-Evaluating-Creating: Assessing Computational Thinking and\n  Problem Solving in Visual Programming Domains","summary":"Computational thinking (CT) and problem-solving skills are increasingly\nintegrated into K-8 school curricula worldwide. Consequently, there is a\ngrowing need to develop reliable assessments for measuring students'\nproficiency in these skills. Recent works have proposed tests for assessing\nthese skills across various CT concepts and practices, in particular, based on\nmulti-choice items enabling psychometric validation and usage in large-scale\nstudies. Despite their practical relevance, these tests are limited in how they\nmeasure students' computational creativity, a crucial ability when applying CT\nand problem solving in real-world settings. In our work, we have developed ACE,\na novel test focusing on the three higher cognitive levels in Bloom's Taxonomy,\ni.e., Analyze, Evaluate, and Create. ACE comprises a diverse set of 7x3\nmulti-choice items spanning these three levels, grounded in elementary\nblock-based visual programming. We evaluate the psychometric properties of ACE\nthrough a study conducted with 371 students in grades 3-7 from 10 schools.\nBased on several psychometric analysis frameworks, our results confirm the\nreliability and validity of ACE. Our study also shows a positive correlation\nbetween students' performance on ACE and performance on Hour of Code: Maze\nChallenge by Code.org.","updated":1710793114000,"published":1710793114000,"authors":["Ahana Ghosh","Liina Malva","Adish Singla"],"comments":"This extended version of the SIGCSE 2024 paper includes all 21 test\n  items from ACE along with their answers in the appendix","categories":["cs.CY"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"152":{"arxiv_id":"2403.12343v1","url":"http:\/\/arxiv.org\/abs\/2403.12343v1","title":"Glanceable Data Visualizations for Older Adults: Establishing Thresholds\n  and Examining Disparities Between Age Groups","summary":"We present results of a replication study on smartwatch visualizations with\nadults aged 65 and older. The older adult population is rising globally,\ncoinciding with their increasing interest in using small wearable devices, such\nas smartwatches, to track and view data. Smartwatches, however, pose challenges\nto this population: fonts and visualizations are often small and meant to be\nseen at a glance. How concise design on smartwatches interacts with\naging-related changes in perception and cognition, however, is not well\nunderstood. We replicate a study that investigated how visualization type and\nnumber of data points affect glanceable perception. We observe strong evidence\nof differences for participants aged 75 and older, sparking interesting\nquestions regarding the study of visualization and older adults. We discuss\nfirst steps toward better understanding and supporting an older population of\nsmartwatch wearers and reflect on our experiences working with this population.\nSupplementary materials are available at https:\/\/osf.io\/7x4hq\/.","updated":1710811256000,"published":1710811256000,"authors":["Zack While","Tanja Blascheck","Yujie Gong","Petra Isenberg","Ali Sarvghad"],"comments":"17 pages, 10 figures, accepted to CHI '24","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"153":{"arxiv_id":"2403.12344v1","url":"http:\/\/arxiv.org\/abs\/2403.12344v1","title":"Human Factors in Space Exploration: Opportunities for International and\n  Interdisciplinary Collaboration","summary":"As humanity pushes the boundaries of space exploration, human factors\nresearch becomes more important. Human factors encompass a broad spectrum of\npsychological, physiological, and ergonomic factors that affect human\nperformance, well-being, and safety in the unique and challenging space\nenvironment. This panel explores the multifaceted field of human factors in\nspace exploration and highlights the opportunities that lie in fostering\ninternational and interdisciplinary cooperation. This exploration delves into\nthe current state of research on human factors in space missions, addressing\nthe physiological and psychological challenges astronauts face during long\nspace flights. It emphasizes the importance of interdisciplinary collaboration,\ncombining knowledge from fields such as psychology, medicine, engineering, and\ndesign to address the complex interaction of factors affecting human\nperformance and adaptation to the space environment","updated":1710811635000,"published":1710811635000,"authors":["Wies\u0142aw Kope\u0107","Grzegorz Pochwatko","Monika Kornacka","Wiktor Stawski","Maciej Grzeszczuk","Kinga Skorupska","Barbara Karpowicz","Rafa\u0142 Mas\u0142yk","Pavlo Zinevych","Stanis\u0142aw Knapi\u0144ski","Steven Barnes","Cezary Biele"],"comments":"13 pages including bibliography, 4 figures. To be published by\n  Springer as MIDI 2023 Conference proceedings","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"154":{"arxiv_id":"2403.12599v1","url":"http:\/\/arxiv.org\/abs\/2403.12599v1","title":"Preventing Eviction-Caused Homelessness through ML-Informed Distribution\n  of Rental Assistance","summary":"Rental assistance programs provide individuals with financial assistance to\nprevent housing instabilities caused by evictions and avert homelessness. Since\nthese programs operate under resource constraints, they must decide who to\nprioritize. Typically, funding is distributed by a reactive or first-come-first\nserve allocation process that does not systematically consider risk of future\nhomelessness. We partnered with Allegheny County, PA to explore a proactive\nallocation approach that prioritizes individuals facing eviction based on their\nrisk of future homelessness. Our ML system that uses state and county\nadministrative data to accurately identify individuals in need of support\noutperforms simpler prioritization approaches by at least 20% while being fair\nand equitable across race and gender. Furthermore, our approach would identify\n28% of individuals who are overlooked by the current process and end up\nhomeless. Beyond improvements to the rental assistance program in Allegheny\nCounty, this study can inform the development of evidence-based decision\nsupport tools in similar contexts, including lessons about data needs, model\ndesign, evaluation, and field validation.","updated":1710842981000,"published":1710842981000,"authors":["Catalina Vajiac","Arun Frey","Joachim Baumann","Abigail Smith","Kasun Amarasinghe","Alice Lai","Kit Rodolfa","Rayid Ghani"],"comments":"Published at AAAI 2024","categories":["cs.CY","cs.LG"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"155":{"arxiv_id":"2403.12730v1","url":"http:\/\/arxiv.org\/abs\/2403.12730v1","title":"What Does Evaluation of Explainable Artificial Intelligence Actually\n  Tell Us? A Case for Compositional and Contextual Validation of XAI Building\n  Blocks","summary":"Despite significant progress, evaluation of explainable artificial\nintelligence remains elusive and challenging. In this paper we propose a\nfine-grained validation framework that is not overly reliant on any one facet\nof these sociotechnical systems, and that recognises their inherent modular\nstructure: technical building blocks, user-facing explanatory artefacts and\nsocial communication protocols. While we concur that user studies are\ninvaluable in assessing the quality and effectiveness of explanation\npresentation and delivery strategies from the explainees' perspective in a\nparticular deployment context, the underlying explanation generation mechanisms\nrequire a separate, predominantly algorithmic validation strategy that accounts\nfor the technical and human-centred desiderata of their (numerical) outputs.\nSuch a comprehensive sociotechnical utility-based evaluation framework could\nallow to systematically reason about the properties and downstream influence of\ndifferent building blocks from which explainable artificial intelligence\nsystems are composed -- accounting for a diverse range of their engineering and\nsocial aspects -- in view of the anticipated use case.","updated":1710855934000,"published":1710855934000,"authors":["Kacper Sokol","Julia E. Vogt"],"comments":"Published in Extended Abstracts of the 2024 CHI Conference on Human\n  Factors in Computing Systems (CHI EA '24)","categories":["cs.HC","cs.AI"],"primary_category":"cs.HC","doi":"10.1145\/3613905.3651047","journal_ref":null,"peer_reviewed":true},"156":{"arxiv_id":"2403.12768v1","url":"http:\/\/arxiv.org\/abs\/2403.12768v1","title":"ContextVis: Envision Contextual Learning and Interaction with Generative\n  Models","summary":"ContextVis introduces a workflow by integrating generative models to create\ncontextual learning materials. It aims to boost knowledge acquisition through\nthe creation of resources with contextual cues. A case study on vocabulary\nlearning demonstrates the effectiveness of generative models in developing\neducational resources that enrich language understanding and aid memory\nretention. The system combines an easy-to-use Dashboard for educators with an\ninteractive Playground for learners, establishing a unified platform for\ncontent creation and interaction. Future work may expand to include a wider\nrange of generative models, media formats, and customization features for\neducators.","updated":1710858748000,"published":1710858748000,"authors":["Bo Shui","Chufan Shi","Yujiu Yang","Xiaomei Nie"],"comments":"Accepted by HCII 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"157":{"arxiv_id":"2403.12774v1","url":"http:\/\/arxiv.org\/abs\/2403.12774v1","title":"Is open source software culture enough to make AI a common ?","summary":"Language models (LM or LLM) are increasingly deployed in the field of\nartificial intelligence (AI) and its applications, but the question arises as\nto whether they can be a common resource managed and maintained by a community\nof users. Indeed, the dominance of private companies with exclusive access to\nmassive data and language processing resources can create inequalities and\nbiases in LM, as well as obstacles to innovation for those who do not have the\nsame resources necessary for their implementation. In this contribution, we\nexamine the concept of the commons and its relevance for thinking about LM. We\nhighlight the potential benefits of treating the data and resources needed to\ncreate LMs as commons, including increased accessibility, equity, and\ntransparency in the development and use of AI technologies. Finally, we present\na case study centered on the Hugging Face platform, an open-source platform for\ndeep learning designed to encourage collaboration and sharing among AI\ndesigners.","updated":1710859432000,"published":1710859432000,"authors":["Robin Quillivic","Salma Mesmoudi"],"comments":"in French language. JSR'23: Rochebrune Scientific Days 2023","categories":["cs.CY"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"158":{"arxiv_id":"2403.12869v1","url":"http:\/\/arxiv.org\/abs\/2403.12869v1","title":"Regularization in Spider-Style Strategy Discovery and Schedule\n  Construction","summary":"To achieve the best performance, automatic theorem provers often rely on\nschedules of diverse proving strategies to be tried out (either sequentially or\nin parallel) on a given problem. In this paper, we report on a large-scale\nexperiment with discovering strategies for the Vampire prover, targeting the\nFOF fragment of the TPTP library and constructing a schedule for it, based on\nthe ideas of Andrei Voronkov's system Spider. We examine the process from\nvarious angles, discuss the difficulty (or ease) of obtaining a strong Vampire\nschedule for the CASC competition, and establish how well a schedule can be\nexpected to generalize to unseen problems and what factors influence this\nproperty.","updated":1710864745000,"published":1710864745000,"authors":["Filip B\u00e1rtek","Karel Chvalovsk\u00fd","Martin Suda"],"comments":"25 pages, 8 figures, submitted to IJCAR 2024","categories":["cs.AI","cs.LO"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"159":{"arxiv_id":"2303.15662v3","url":"http:\/\/arxiv.org\/abs\/2303.15662v3","title":"ChatGPT4PCG Competition: Character-like Level Generation for Science\n  Birds","summary":"This paper presents the first ChatGPT4PCG Competition at the 2023 IEEE\nConference on Games. The objective of this competition is for participants to\ncreate effective prompts for ChatGPT--enabling it to generate Science Birds\nlevels with high stability and character-like qualities--fully using their\ncreativity as well as prompt engineering skills. ChatGPT is a conversational\nagent developed by OpenAI. Science Birds is selected as the competition\nplatform because designing an Angry Birds-like level is not a trivial task due\nto the in-game gravity; the quality of the levels is determined by their\nstability. To lower the entry barrier to the competition, we limit the task to\nthe generation of capitalized English alphabetical characters. We also allow\nonly a single prompt to be used for generating all the characters. Here, the\nquality of the generated levels is determined by their stability and similarity\nto the given characters. A sample prompt is provided to participants for their\nreference. An experiment is conducted to determine the effectiveness of several\nmodified versions of this sample prompt on level stability and similarity by\ntesting them on several characters. To the best of our knowledge, we believe\nthat ChatGPT4PCG is the first competition of its kind and hope to inspire\nenthusiasm for prompt engineering in procedural content generation.","updated":1710985363000,"published":1679965658000,"authors":["Pittawat Taveekitworachai","Febri Abdullah","Mury F. Dewantoro","Ruck Thawonmas","Julian Togelius","Jochen Renz"],"comments":"This paper accepted for presentation at IEEE CoG 2023 is made\n  available for participants of ChatGPT4PCG Competition\n  (https:\/\/chatgpt4pcg.github.io\/) and readers interested in relevant areas. In\n  this PDF version, the affiliation symbol of Julian Togelius has been revised","categories":["cs.AI","cs.CL","I.2.7; I.2.8"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"160":{"arxiv_id":"2305.09820v5","url":"http:\/\/arxiv.org\/abs\/2305.09820v5","title":"Machine-Made Media: Monitoring the Mobilization of Machine-Generated\n  Articles on Misinformation and Mainstream News Websites","summary":"As large language models (LLMs) like ChatGPT have gained traction, an\nincreasing number of news websites have begun utilizing them to generate\narticles. However, not only can these language models produce factually\ninaccurate articles on reputable websites but disreputable news sites can\nutilize LLMs to mass produce misinformation. To begin to understand this\nphenomenon, we present one of the first large-scale studies of the prevalence\nof synthetic articles within online news media. To do this, we train a\nDeBERTa-based synthetic news detector and classify over 15.46 million articles\nfrom 3,074 misinformation and mainstream news websites. We find that between\nJanuary 1, 2022, and May 1, 2023, the relative number of synthetic news\narticles increased by 57.3% on mainstream websites while increasing by 474% on\nmisinformation sites. We find that this increase is largely driven by smaller\nless popular websites. Analyzing the impact of the release of ChatGPT using an\ninterrupted-time-series, we show that while its release resulted in a marked\nincrease in synthetic articles on small sites as well as misinformation news\nwebsites, there was not a corresponding increase on large mainstream news\nwebsites.","updated":1710907114000,"published":1684273861000,"authors":["Hans W. A. Hanley","Zakir Durumeric"],"comments":"Accepted to ICWSM 2024","categories":["cs.CY","cs.LG","cs.SI"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"161":{"arxiv_id":"2309.12276v3","url":"http:\/\/arxiv.org\/abs\/2309.12276v3","title":"LLMR: Real-time Prompting of Interactive Worlds using Large Language\n  Models","summary":"We present Large Language Model for Mixed Reality (LLMR), a framework for the\nreal-time creation and modification of interactive Mixed Reality experiences\nusing LLMs. LLMR leverages novel strategies to tackle difficult cases where\nideal training data is scarce, or where the design goal requires the synthesis\nof internal dynamics, intuitive analysis, or advanced interactivity. Our\nframework relies on text interaction and the Unity game engine. By\nincorporating techniques for scene understanding, task planning,\nself-debugging, and memory management, LLMR outperforms the standard GPT-4 by\n4x in average error rate. We demonstrate LLMR's cross-platform interoperability\nwith several example worlds, and evaluate it on a variety of creation and\nmodification tasks to show that it can produce and edit diverse objects, tools,\nand scenes. Finally, we conducted a usability study (N=11) with a diverse set\nthat revealed participants had positive experiences with the system and would\nuse it again.","updated":1711128497000,"published":1695317821000,"authors":["Fernanda De La Torre","Cathy Mengying Fang","Han Huang","Andrzej Banburski-Fahey","Judith Amores Fernandez","Jaron Lanier"],"comments":"46 pages, 18 figures; Matching version accepted at CHI 2024","categories":["cs.HC","cs.AI","cs.CL","cs.ET"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"162":{"arxiv_id":"2310.06155v3","url":"http:\/\/arxiv.org\/abs\/2310.06155v3","title":"CoQuest: Exploring Research Question Co-Creation with an LLM-based Agent","summary":"Developing novel research questions (RQs) often requires extensive literature\nreviews, especially in interdisciplinary fields. To support RQ development\nthrough human-AI co-creation, we leveraged Large Language Models (LLMs) to\nbuild an LLM-based agent system named CoQuest. We conducted an experiment with\n20 HCI researchers to examine the impact of two interaction designs:\nbreadth-first and depth-first RQ generation. The findings revealed that\nparticipants perceived the breadth-first approach as more creative and\ntrustworthy upon task completion. Conversely, during the task, participants\nconsidered the depth-first generated RQs as more creative. Additionally, we\ndiscovered that AI processing delays allowed users to reflect on multiple RQs\nsimultaneously, leading to a higher quantity of generated RQs and an enhanced\nsense of control. Our work makes both theoretical and practical contributions\nby proposing and evaluating a mental model for human-AI co-creation of RQs. We\nalso address potential ethical issues, such as biases and over-reliance on AI,\nadvocating for using the system to improve human research creativity rather\nthan automating scientific inquiry.","updated":1710967383000,"published":1696885527000,"authors":["Yiren Liu","Si Chen","Haocong Cheng","Mengxia Yu","Xiao Ran","Andrew Mo","Yiliu Tang","Yun Huang"],"comments":"Accepted to SIGCHI 2024","categories":["cs.HC","cs.CE"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"163":{"arxiv_id":"2312.00812v4","url":"http:\/\/arxiv.org\/abs\/2312.00812v4","title":"Empowering Autonomous Driving with Large Language Models: A Safety\n  Perspective","summary":"Autonomous Driving (AD) encounters significant safety hurdles in long-tail\nunforeseen driving scenarios, largely stemming from the non-interpretability\nand poor generalization of the deep neural networks within the AD system,\nparticularly in out-of-distribution and uncertain data. To this end, this paper\nexplores the integration of Large Language Models (LLMs) into AD systems,\nleveraging their robust common-sense knowledge and reasoning abilities. The\nproposed methodologies employ LLMs as intelligent decision-makers in behavioral\nplanning, augmented with a safety verifier shield for contextual safety\nlearning, for enhancing driving performance and safety. We present two key\nstudies in a simulated environment: an adaptive LLM-conditioned Model\nPredictive Control (MPC) and an LLM-enabled interactive behavior planning\nscheme with a state machine. Demonstrating superior performance and safety\nmetrics compared to state-of-the-art approaches, our approach shows the\npromising potential for using LLMs for autonomous vehicles.","updated":1711128541000,"published":1701141189000,"authors":["Yixuan Wang","Ruochen Jiao","Sinong Simon Zhan","Chengtian Lang","Chao Huang","Zhaoran Wang","Zhuoran Yang","Qi Zhu"],"comments":"Accepted to LLMAgent workshop @ICLR2024","categories":["cs.AI","cs.LG","cs.SY","eess.SY"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"164":{"arxiv_id":"2401.07312v3","url":"http:\/\/arxiv.org\/abs\/2401.07312v3","title":"Understanding Nonlinear Collaboration between Human and AI Agents: A\n  Co-design Framework for Creative Design","summary":"Creative design is a nonlinear process where designers generate diverse ideas\nin the pursuit of an open-ended goal and converge towards consensus through\niterative remixing. In contrast, AI-powered design tools often employ a linear\nsequence of incremental and precise instructions to approximate design\nobjectives. Such operations violate customary creative design practices and\nthus hinder AI agents' ability to complete creative design tasks. To explore\nbetter human-AI co-design tools, we first summarize human designers' practices\nthrough a formative study with 12 design experts. Taking graphic design as a\nrepresentative scenario, we formulate a nonlinear human-AI co-design framework\nand develop a proof-of-concept prototype, OptiMuse. We evaluate OptiMuse and\nvalidate the nonlinear framework through a comparative study. We notice a\nsubconscious change in people's attitudes towards AI agents, shifting from\nperceiving them as mere executors to regarding them as opinionated colleagues.\nThis shift effectively fostered the exploration and reflection processes of\nindividual designers.","updated":1710933392000,"published":1705245428000,"authors":["Jiayi Zhou","Renzhong Li","Junxiu Tang","Tan Tang","Haotian Li","Weiwei Cui","Yingcai Wu"],"comments":"to be published in CHI 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"165":{"arxiv_id":"2403.13073v1","url":"http:\/\/arxiv.org\/abs\/2403.13073v1","title":"A Canary in the AI Coal Mine: American Jews May Be Disproportionately\n  Harmed by Intellectual Property Dispossession in Large Language Model\n  Training","summary":"Systemic property dispossession from minority groups has often been carried\nout in the name of technological progress. In this paper, we identify evidence\nthat the current paradigm of large language models (LLMs) likely continues this\nlong history. Examining common LLM training datasets, we find that a\ndisproportionate amount of content authored by Jewish Americans is used for\ntraining without their consent. The degree of over-representation ranges from\naround 2x to around 6.5x. Given that LLMs may substitute for the paid labor of\nthose who produced their training data, they have the potential to cause even\nmore substantial and disproportionate economic harm to Jewish Americans in the\ncoming years. This paper focuses on Jewish Americans as a case study, but it is\nprobable that other minority communities (e.g., Asian Americans, Hindu\nAmericans) may be similarly affected and, most importantly, the results should\nlikely be interpreted as a \"canary in the coal mine\" that highlights deep\nstructural concerns about the current LLM paradigm whose harms could soon\naffect nearly everyone. We discuss the implications of these results for the\npolicymakers thinking about how to regulate LLMs as well as for those in the AI\nfield who are working to advance LLMs. Our findings stress the importance of\nworking together towards alternative LLM paradigms that avoid both disparate\nimpacts and widespread societal harms.","updated":1710871659000,"published":1710871659000,"authors":["Heila Precel","Allison McDonald","Brent Hecht","Nicholas Vincent"],"comments":"Preprint, to appear in CHI 2024 proceedings","categories":["cs.CY"],"primary_category":"cs.CY","doi":"10.1145\/3613904.3642749","journal_ref":null,"peer_reviewed":true},"166":{"arxiv_id":"2403.13139v1","url":"http:\/\/arxiv.org\/abs\/2403.13139v1","title":"Generating Automatic Feedback on UI Mockups with Large Language Models","summary":"Feedback on user interface (UI) mockups is crucial in design. However, human\nfeedback is not always readily available. We explore the potential of using\nlarge language models for automatic feedback. Specifically, we focus on\napplying GPT-4 to automate heuristic evaluation, which currently entails a\nhuman expert assessing a UI's compliance with a set of design guidelines. We\nimplemented a Figma plugin that takes in a UI design and a set of written\nheuristics, and renders automatically-generated feedback as constructive\nsuggestions. We assessed performance on 51 UIs using three sets of guidelines,\ncompared GPT-4-generated design suggestions with those from human experts, and\nconducted a study with 12 expert designers to understand fit with existing\npractice. We found that GPT-4-based feedback is useful for catching subtle\nerrors, improving text, and considering UI semantics, but feedback also\ndecreased in utility over iterations. Participants described several uses for\nthis plugin despite its imperfect suggestions.","updated":1710879676000,"published":1710879676000,"authors":["Peitong Duan","Jeremy Warner","Yang Li","Bjoern Hartmann"],"comments":"To appear at ACM CHI 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642782","journal_ref":null,"peer_reviewed":true},"167":{"arxiv_id":"2403.13173v1","url":"http:\/\/arxiv.org\/abs\/2403.13173v1","title":"GerontoVis: Data Visualization at the Confluence of Aging","summary":"Despite the explosive growth of the aging population worldwide, older adults\nhave been largely overlooked by visualization research. This paper is a\ncritical reflection on the underrepresentation of older adults in visualization\nresearch. We discuss why investigating visualization at the intersection of\naging matters, why older adults may have been omitted from sample populations\nin visualization research, how aging may affect visualization use, and how this\ndiffers from traditional accessibility research. To encourage further\ndiscussion and novel scholarship in this area, we introduce GerontoVis, a term\nwhich encapsulates research and practice of data visualization design that\nprimarily focuses on older adults. By introducing this new subfield of\nvisualization research, we hope to shine a spotlight on this growing user\npopulation and stimulate innovation toward the development of aging-aware\nvisualization tools. We offer a birds-eye view of the GerontoVis landscape,\nexplore some of its unique challenges, and identify promising areas for future\nresearch.","updated":1710885343000,"published":1710885343000,"authors":["Zack While","R. Jordan Crouser","Ali Sarvghad"],"comments":"14 pages, 2 figures, accepted to EuroVis '24","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"168":{"arxiv_id":"2403.13301v1","url":"http:\/\/arxiv.org\/abs\/2403.13301v1","title":"Reading Users' Minds from What They Say: An Investigation into LLM-based\n  Empathic Mental Inference","summary":"In human-centered design, developing a comprehensive and in-depth\nunderstanding of user experiences, i.e., empathic understanding, is paramount\nfor designing products that truly meet human needs. Nevertheless, accurately\ncomprehending the real underlying mental states of a large human population\nremains a significant challenge today. This difficulty mainly arises from the\ntrade-off between depth and scale of user experience research: gaining in-depth\ninsights from a small group of users does not easily scale to a larger\npopulation, and vice versa. This paper investigates the use of Large Language\nModels (LLMs) for performing mental inference tasks, specifically inferring\nusers' underlying goals and fundamental psychological needs (FPNs). Baseline\nand benchmark datasets were collected from human users and designers to develop\nan empathic accuracy metric for measuring the mental inference performance of\nLLMs. The empathic accuracy of inferring goals and FPNs of different LLMs with\nvaried zero-shot prompt engineering techniques are experimented against that of\nhuman designers. Experimental results suggest that LLMs can infer and\nunderstand the underlying goals and FPNs of users with performance comparable\nto that of human designers, suggesting a promising avenue for enhancing the\nscalability of empathic design approaches through the integration of advanced\nartificial intelligence technologies. This work has the potential to\nsignificantly augment the toolkit available to designers during human-centered\ndesign, enabling the development of both large-scale and in-depth understanding\nof users' experiences.","updated":1710910652000,"published":1710910652000,"authors":["Qihao Zhu","Leah Chong","Maria Yang","Jianxi Luo"],"comments":"Submitted to IDETC-CIE2024","categories":["cs.HC","cs.CL"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"169":{"arxiv_id":"2403.13944v1","url":"http:\/\/arxiv.org\/abs\/2403.13944v1","title":"Shortchanged: Uncovering and Analyzing Intimate Partner Financial Abuse\n  in Consumer Complaints","summary":"Digital financial services can introduce new digital-safety risks for users,\nparticularly survivors of intimate partner financial abuse (IPFA). To offer\nimproved support for such users, a comprehensive understanding of their support\nneeds and the barriers they face to redress by financial institutions is\nessential. Drawing from a dataset of 2.7 million customer complaints, we\nimplement a bespoke workflow that utilizes language-modeling techniques and\nexpert human review to identify complaints describing IPFA. Our mixed-method\nanalysis provides insight into the most common digital financial products\ninvolved in these attacks, and the barriers consumers report encountering when\ndoing so. Our contributions are twofold; we offer the first human-labeled\ndataset for this overlooked harm and provide practical implications for\ntechnical practice, research, and design for better supporting and protecting\nsurvivors of IPFA.","updated":1710963141000,"published":1710963141000,"authors":["Arkaprabha Bhattacharya","Kevin Lee","Vineeth Ravi","Jessica Staddon","Rosanna Bellini"],"comments":"20 pages, 9 figures, 8 tables, This paper will be published in CHI\n  '24: Proceedings of the 2024 CHI Conference on Human Factors in Computing\n  Systems","categories":["cs.CY","cs.CR","cs.HC"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"170":{"arxiv_id":"2403.14014v1","url":"http:\/\/arxiv.org\/abs\/2403.14014v1","title":"Crowdsourcing Task Traces for Service Robotics","summary":"Demonstration is an effective end-user development paradigm for teaching\nrobots how to perform new tasks. In this paper, we posit that demonstration is\nuseful not only as a teaching tool, but also as a way to understand and assist\nend-user developers in thinking about a task at hand. As a first step toward\ngaining this understanding, we constructed a lightweight web interface to\ncrowdsource step-by-step instructions of common household tasks, leveraging the\nimaginations and past experiences of potential end-user developers. As evidence\nof the utility of our interface, we deployed the interface on Amazon Mechanical\nTurk and collected 207 task traces that span 18 different task categories. We\ndescribe our vision for how these task traces can be operationalized as task\nmodels within end-user development tools and provide a roadmap for future work.","updated":1710973492000,"published":1710973492000,"authors":["David Porfirio","Allison Saupp\u00e9","Maya Cakmak","Aws Albarghouthi","Bilge Mutlu"],"comments":"Published in the companion proceedings of the 2023 ACM\/IEEE\n  International Conference on Human-Robot Interaction","categories":["cs.HC","cs.RO"],"primary_category":"cs.HC","doi":"10.1145\/3568294.3580112","journal_ref":null,"peer_reviewed":true},"171":{"arxiv_id":"2403.14025v1","url":"http:\/\/arxiv.org\/abs\/2403.14025v1","title":"HRI Curriculum for a Liberal Arts Education","summary":"In this paper, we discuss the opportunities and challenges of teaching a\nhuman-robot interaction course at an undergraduate liberal arts college. We\nprovide a sample syllabus adapted from a previous version of a course.","updated":1710975089000,"published":1710975089000,"authors":["Jason R. Wilson","Emily Jensen"],"comments":"Presented at the Designing an Intro to HRI Course Workshop at HRI\n  2024 (arXiv:2403.05588)","categories":["cs.CY","cs.HC","cs.RO"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"172":{"arxiv_id":"2403.14071v1","url":"http:\/\/arxiv.org\/abs\/2403.14071v1","title":"Empowering Personalized Learning through a Conversation-based Tutoring\n  System with Student Modeling","summary":"As the recent Large Language Models(LLM's) become increasingly competent in\nzero-shot and few-shot reasoning across various domains, educators are showing\na growing interest in leveraging these LLM's in conversation-based tutoring\nsystems. However, building a conversation-based personalized tutoring system\nposes considerable challenges in accurately assessing the student and\nstrategically incorporating the assessment into teaching within the\nconversation. In this paper, we discuss design considerations for a\npersonalized tutoring system that involves the following two key components:\n(1) a student modeling with diagnostic components, and (2) a conversation-based\ntutor utilizing LLM with prompt engineering that incorporates student\nassessment outcomes and various instructional strategies. Based on these design\nconsiderations, we created a proof-of-concept tutoring system focused on\npersonalization and tested it with 20 participants. The results substantiate\nthat our system's framework facilitates personalization, with particular\nemphasis on the elements constituting student modeling. A web demo of our\nsystem is available at http:\/\/rlearning-its.com.","updated":1710985217000,"published":1710985217000,"authors":["Minju Park","Sojung Kim","Seunghyun Lee","Soonwoo Kwon","Kyuseok Kim"],"comments":"Accepted to ACM CHI 2024 LBW","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3613905.3651122","journal_ref":null,"peer_reviewed":true},"173":{"arxiv_id":"2403.14557v1","url":"http:\/\/arxiv.org\/abs\/2403.14557v1","title":"Changing human's impression of empathy from agent by verbalizing agent's\n  position","summary":"As anthropomorphic agents (AI and robots) are increasingly used in society,\nempathy and trust between people and agents are becoming increasingly\nimportant. A better understanding of agents by people will help to improve the\nproblems caused by the future use of agents in society. In the past, there has\nbeen a focus on the importance of self-disclosure and the relationship between\nagents and humans in their interactions. In this study, we focused on the\nattributes of self-disclosure and the relationship between agents and people.\nAn experiment was conducted to investigate hypotheses on trust and empathy with\nagents through six attributes of self-disclosure (opinions and attitudes,\nhobbies, work, money, personality, and body) and through competitive and\ncooperative relationships before a robotic agent performs a joint task. The\nexperiment consisted of two between-participant factors: six levels of\nself-disclosure attributes and two levels of relationship with the agent. The\nresults showed that the two factors had no effect on trust in the agent, but\nthere was statistical significance for the attribute of self-disclosure\nregarding a person's empathy toward the agent. In addition, statistical\nsignificance was found regarding the agent's ability to empathize with a person\nas perceived by the person only in the case where the type of relationship,\ncompetitive or cooperative, was presented. The results of this study could lead\nto an effective method for building relationships with agents, which are\nincreasingly used in society.","updated":1711040206000,"published":1711040206000,"authors":["Takahiro Tsumura","Seiji Yamada"],"comments":"8 pages, 5 figures, 4 tables, submitted RO-MAN2024. arXiv admin note:\n  text overlap with arXiv:2306.09447","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"174":{"arxiv_id":"2403.14686v1","url":"http:\/\/arxiv.org\/abs\/2403.14686v1","title":"Evaluating Pedagogical Incentives in Undergraduate Computing: A Mixed\n  Methods Approach Using Learning Analytics","summary":"In the context of higher education's evolving dynamics post-COVID-19, this\npaper assesses the impact of new pedagogical incentives implemented in a\nfirst-year undergraduate computing module at University College London. We\nemploy a mixed methods approach, combining learning analytics with qualitative\ndata, to evaluate the effectiveness of these incentives on increasing student\nengagement.\n  A longitudinal overview of resource interactions is mapped through Bayesian\nnetwork analysis of Moodle activity logs from 204 students. This analysis\nidentifies early resource engagement as a predictive indicator of continued\nengagement while also suggesting that the new incentives disproportionately\nbenefit highly engaged students. Focus group discussions complement this\nanalysis, providing insights into student perceptions of the pedagogical\nchanges and the module design. These qualitative findings underscore the\nchallenge of sustaining engagement through the new incentives and highlight the\nimportance of communication in blended learning environments.\n  Our paper introduces an interpretable and actionable model for student\nengagement, which integrates objective, data-driven analysis with students'\nperspectives. This model provides educators with a tool to evaluate and improve\ninstructional strategies. By demonstrating the effectiveness of our mixed\nmethods approach in capturing the intricacies of student behaviour in digital\nlearning environments, we underscore the model's potential to improve online\npedagogical practices across diverse educational settings.","updated":1710347978000,"published":1710347978000,"authors":["Laura J. Johnston","Takoua Jendoubi"],"comments":"5 pages, 1 figure. Accepted by IEEE Global Engineering Education\n  Conference 2024","categories":["cs.CY"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"175":{"arxiv_id":"2403.14697v1","url":"http:\/\/arxiv.org\/abs\/2403.14697v1","title":"An AIC-based approach for articulating unpredictable problems in open\n  complex environments","summary":"This research paper presents an approach to enhancing the predictive\ncapability of architects in the design and assurance of systems, focusing on\nsystems operating in dynamic and unpredictable environments. By adopting a\nsystems approach, we aim to improve architects' predictive capabilities in\ndesigning dependable systems (for example, ML-based systems). An aerospace case\nstudy is used to illustrate the approach. Multiple factors (challenges)\ninfluencing aircraft detection are identified, demonstrating the effectiveness\nof our approach in a complex operational setting. Our approach primarily aimed\nto enhance the architect's predictive capability.","updated":1710534602000,"published":1710534602000,"authors":["Haider AL-Shareefy","Michael Butler","Thai Son Hoang"],"comments":"S. Bernardi, T. Zoppi (Editors), \"Fast Abstracts and Student Forum\n  Proceedings - EDCC 2024 - 19th European Dependable Computing Conference,\n  Leuven, Belgium, 8-11 April 2024\"","categories":["cs.CY","cs.AI","cs.SE"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"176":{"arxiv_id":"2403.14702v1","url":"http:\/\/arxiv.org\/abs\/2403.14702v1","title":"Large language model-powered chatbots for internationalizing student\n  support in higher education","summary":"This research explores the integration of chatbot technology powered by\nGPT-3.5 and GPT-4 Turbo into higher education to enhance internationalization\nand leverage digital transformation. It delves into the design, implementation,\nand application of Large Language Models (LLMs) for improving student\nengagement, information access, and support. Utilizing technologies like Python\n3, GPT API, LangChain, and Chroma Vector Store, the research emphasizes\ncreating a high-quality, timely, and relevant transcript dataset for chatbot\ntesting. Findings indicate the chatbot's efficacy in providing comprehensive\nresponses, its preference over traditional methods by users, and a low error\nrate. Highlighting the chatbot's real-time engagement, memory capabilities, and\ncritical data access, the study demonstrates its potential to elevate\naccessibility, efficiency, and satisfaction. Concluding, the research suggests\nthe chatbot significantly aids higher education internationalization, proposing\nfurther investigation into digital technology's role in educational enhancement\nand strategy development.","updated":1710633019000,"published":1710633019000,"authors":["Achraf Hsain","Hamza El Housni"],"comments":"Key Words: Chatbot, Higher Education, Large Language model, Student\n  Support, Information retrieval. Presented in the conference: The\n  Internationalization of Higher Education and Digital Transformation:\n  Addressing Current and Future Possibilities in Oujda, Morocco","categories":["cs.CY","cs.IR","I.2, H.3"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"177":{"arxiv_id":"2403.14708v1","url":"http:\/\/arxiv.org\/abs\/2403.14708v1","title":"Visualizing Progress in Broadening Participation in Computing: The Value\n  of Context","summary":"Concerns about representation in computing within the U.S. have driven\nnumerous activities to broaden participation. Assessment of the impact of these\nefforts and, indeed, a clear assessment of the actual \"problem\" being addressed\nare limited by the nature of the most common data analysis which looks at the\nrepresentation of each population as a percentage of the number of students\ngraduating with a degree in computing. This use of a single metric cannot\nadequately assess the impact of broadening participation efforts. First, this\napproach fails to account for changing demographics of the undergraduate\npopulation in terms of overall numbers and relative proportion of the Federally\ndesignated gender, race, and ethnicity groupings. A second issue is that the\nmajority of literature on broadening participation in computing (BPC) reports\ndata on gender or on race\/ethnicity, omitting data on students' intersectional\nidentities. This leads to an incorrect understanding of both the data and the\nchallenges we face as a field. In this paper we present several different\napproaches to tracking the impact of BPC efforts. We make three\nrecommendations: 1) cohort-based analysis should be used to accurately show\nstudent engagement in computing; 2) the field as a whole needs to adopt the\nnorm of always reporting intersectional data; 3) university demographic context\nmatters when looking at how well a CS department is doing to broaden\nparticipation in computing, including longitudinal analysis of university\ndemographic shifts that impact the local demographics of computing.","updated":1710724322000,"published":1710724322000,"authors":["Valerie Barr","Carla E. Brodley","Manuel A. P\u00e9rez-Qui\u00f1ones"],"comments":"Accepted for publication in Communications of the ACM, late summer or\n  Fall 2024","categories":["cs.CY","97P99 (Primary)","K.3"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"178":{"arxiv_id":"2403.14709v1","url":"http:\/\/arxiv.org\/abs\/2403.14709v1","title":"ClimateQ&A: Bridging the gap between climate scientists and the general\n  public","summary":"This research paper investigates public views on climate change and\nbiodiversity loss by analyzing questions asked to the ClimateQ&A platform.\nClimateQ&A is a conversational agent that uses LLMs to respond to queries based\non over 14,000 pages of scientific literature from the IPCC and IPBES reports.\nLaunched online in March 2023, the tool has gathered over 30,000 questions,\nmainly from a French audience. Its chatbot interface allows for the free\nformulation of questions related to nature*. While its main goal is to make\nnature science more accessible, it also allows for the collection and analysis\nof questions and their themes. Unlike traditional surveys involving closed\nquestions, this novel method offers a fresh perspective on individual\ninterrogations about nature. Running NLP clustering algorithms on a sample of\n3,425 questions, we find that a significant 25.8% inquire about how climate\nchange and biodiversity loss will affect them personally (e.g., where they live\nor vacation, their consumption habits) and the specific impacts of their\nactions on nature (e.g., transportation or food choices). This suggests that\ntraditional methods of surveying may not identify all existing knowledge gaps,\nand that relying solely on IPCC and IPBES reports may not address all\nindividual inquiries about climate and biodiversity, potentially affecting\npublic understanding and action on these issues. *we use 'nature' as an\numbrella term for 'climate change' and 'biodiversity loss'","updated":1710749762000,"published":1710749762000,"authors":["Natalia De La Calzada","Th\u00e9o Alves Da Costa","Annabelle Blangero","Nicolas Chesneau"],"comments":"Accepted as a workshop paper at \"Tackling Climate Change with Machine\n  Learning\", ICLR 2024","categories":["cs.CY","cs.LG"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"179":{"arxiv_id":"2403.14711v1","url":"http:\/\/arxiv.org\/abs\/2403.14711v1","title":"Human-in-the-Loop AI for Cheating Ring Detection","summary":"Online exams have become popular in recent years due to their accessibility.\nHowever, some concerns have been raised about the security of the online exams,\nparticularly in the context of professional cheating services aiding malicious\ntest takers in passing exams, forming so-called \"cheating rings\". In this\npaper, we introduce a human-in-the-loop AI cheating ring detection system\ndesigned to detect and deter these cheating rings. We outline the underlying\nlogic of this human-in-the-loop AI system, exploring its design principles\ntailored to achieve its objectives of detecting cheaters. Moreover, we\nillustrate the methodologies used to evaluate its performance and fairness,\naiming to mitigate the unintended risks associated with the AI system. The\ndesign and development of the system adhere to Responsible AI (RAI) standards,\nensuring that ethical considerations are integrated throughout the entire\ndevelopment process.","updated":1710768357000,"published":1710768357000,"authors":["Yong-Siang Shih","Manqian Liao","Ruidong Liu","Mirza Basim Baig"],"comments":"Accepted to the AI4Ed Workshop at AAAI 2024 as a short paper","categories":["cs.CY","cs.AI","cs.HC","cs.LG"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"180":{"arxiv_id":"2403.15214v1","url":"http:\/\/arxiv.org\/abs\/2403.15214v1","title":"InstaSynth: Opportunities and Challenges in Generating Synthetic\n  Instagram Data with ChatGPT for Sponsored Content Detection","summary":"Large Language Models (LLMs) raise concerns about lowering the cost of\ngenerating texts that could be used for unethical or illegal purposes,\nespecially on social media. This paper investigates the promise of such models\nto help enforce legal requirements related to the disclosure of sponsored\ncontent online. We investigate the use of LLMs for generating synthetic\nInstagram captions with two objectives: The first objective (fidelity) is to\nproduce realistic synthetic datasets. For this, we implement content-level and\nnetwork-level metrics to assess whether synthetic captions are realistic. The\nsecond objective (utility) is to create synthetic data that is useful for\nsponsored content detection. For this, we evaluate the effectiveness of the\ngenerated synthetic data for training classifiers to identify undisclosed\nadvertisements on Instagram. Our investigations show that the objectives of\nfidelity and utility may conflict and that prompt engineering is a useful but\ninsufficient strategy. Additionally, we find that while individual synthetic\nposts may appear realistic, collectively they lack diversity, topic\nconnectivity, and realistic user interaction patterns.","updated":1711115922000,"published":1711115922000,"authors":["Thales Bertaglia","Lily Heisig","Rishabh Kaushal","Adriana Iamnitchi"],"comments":"To appear at the 18th International AAAI Conference on Web and Social\n  Media (ICWSM 2024) -- please cite accordingly","categories":["cs.CY","cs.CL","cs.SI"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"181":{"arxiv_id":"2403.15325v1","url":"http:\/\/arxiv.org\/abs\/2403.15325v1","title":"A Technological Perspective on Misuse of Available AI","summary":"Potential malicious misuse of civilian artificial intelligence (AI) poses\nserious threats to security on a national and international level. Besides\ndefining autonomous systems from a technological viewpoint and explaining how\nAI development is characterized, we show how already existing and openly\navailable AI technology could be misused. To underline this, we developed three\nexemplary use cases of potentially misused AI that threaten political, digital\nand physical security. The use cases can be built from existing AI technologies\nand components from academia, the private sector and the developer-community.\nThis shows how freely available AI can be combined into autonomous weapon\nsystems. Based on the use cases, we deduce points of control and further\nmeasures to prevent the potential threat through misused AI. Further, we\npromote the consideration of malicious misuse of civilian AI systems in the\ndiscussion on autonomous weapon systems (AWS).","updated":1711125058000,"published":1711125058000,"authors":["Lukas P\u00f6hler","Valentin Schrader","Alexander Ladwein","Florian von Keller"],"comments":"Presented at the UN Meeting of the Group of Governmental Experts on\n  Lethal Autonomous Weapons Systems, 30 August 2018","categories":["cs.CY","cs.AI"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"182":{"arxiv_id":"2205.11624v5","url":"http:\/\/arxiv.org\/abs\/2205.11624v5","title":"Effective Integration of Weighted Cost-to-go and Conflict Heuristic\n  within Suboptimal CBS","summary":"Conflict-Based Search (CBS) is a popular multi-agent path finding (MAPF)\nsolver that employs a low-level single agent planner and a high-level\nconstraint tree to resolve conflicts. The vast majority of modern MAPF solvers\nfocus on improving CBS by reducing the size of this tree through various\nstrategies with few methods modifying the low level planner. Typically low\nlevel planners in existing CBS methods use an unweighted cost-to-go heuristic,\nwith suboptimal CBS methods also using a conflict heuristic to help the high\nlevel search. In this paper, we show that, contrary to prevailing CBS beliefs,\na weighted cost-to-go heuristic can be used effectively alongside the conflict\nheuristic in two possible variants. In particular, one of these variants can\nobtain large speedups, 2-100x, across several scenarios and suboptimal CBS\nmethods. Importantly, we discover that performance is related not to the\nweighted cost-to-go heuristic but rather to the relative conflict heuristic\nweight's ability to effectively balance low-level and high-level work.\nAdditionally, to the best of our knowledge, we show the first theoretical\nrelation of prioritized planning and bounded suboptimal CBS and demonstrate\nthat our methods are their natural generalization. Update March 2024: We found\nthat the relative speedup decreases to around 1.2-10x depending on how the\nconflict heuristic is computed (see appendix for more details).","updated":1711310618000,"published":1653338980000,"authors":["Rishi Veerapaneni","Tushar Kusnur","Maxim Likhachev"],"comments":"Published in AAAI 2023","categories":["cs.AI","cs.MA","cs.RO"],"primary_category":"cs.AI","doi":"10.1609\/aaai.v37i10.26381","journal_ref":null,"peer_reviewed":true},"183":{"arxiv_id":"2305.03039v2","url":"http:\/\/arxiv.org\/abs\/2305.03039v2","title":"SuperNOVA: Design Strategies and Opportunities for Interactive\n  Visualization in Computational Notebooks","summary":"Computational notebooks, such as Jupyter Notebook, have become data\nscientists' de facto programming environments. Many visualization researchers\nand practitioners have developed interactive visualization tools that support\nnotebooks, yet little is known about the appropriate design of these tools. To\naddress this critical research gap, we investigate the design strategies in\nthis space by analyzing 163 notebook visualization tools. Our analysis\nencompasses 64 systems from academic papers and 105 systems sourced from a pool\nof 55k notebooks containing interactive visualizations that we obtain via\nscraping 8.6 million notebooks on GitHub. Through this study, we identify key\ndesign implications and trade-offs, such as leveraging multimodal data in\nnotebooks as well as balancing the degree of visualization-notebook\nintegration. Furthermore, we provide empirical evidence that tools compatible\nwith more notebook platforms have a greater impact. Finally, we develop\nSuperNOVA, an open-source interactive browser to help researchers explore\nexisting notebook visualization tools. SuperNOVA is publicly accessible at:\nhttps:\/\/poloclub.github.io\/supernova\/.","updated":1711655515000,"published":1683223074000,"authors":["Zijie J. Wang","David Munechika","Seongmin Lee","Duen Horng Chau"],"comments":"Accepted at CHI 2024 (Late-Breaking Work). 17 pages, 11 figures, 1\n  table. SuperNOVA is available at: http:\/\/poloclub.github.io\/supernova\/. The\n  code is available at: https:\/\/github.com\/poloclub\/supernova","categories":["cs.HC","cs.LG"],"primary_category":"cs.HC","doi":"10.1145\/3613905.3650848","journal_ref":null,"peer_reviewed":true},"184":{"arxiv_id":"2307.05300v4","url":"http:\/\/arxiv.org\/abs\/2307.05300v4","title":"Unleashing the Emergent Cognitive Synergy in Large Language Models: A\n  Task-Solving Agent through Multi-Persona Self-Collaboration","summary":"Human intelligence thrives on cognitive synergy, where collaboration among\ndifferent minds yield superior outcomes compared to isolated individuals. In\nthis work, we propose Solo Performance Prompting (SPP), which transforms a\nsingle LLM into a cognitive synergist by engaging in multi-turn\nself-collaboration with multiple personas. A cognitive synergist is an\nintelligent agent that collaboratively combines multiple minds' strengths and\nknowledge to enhance problem-solving in complex tasks. By dynamically\nidentifying and simulating different personas based on task inputs, SPP\nunleashes the potential of cognitive synergy in LLMs. Our in-depth analysis\nshows that assigning multiple fine-grained personas in LLMs improves\nproblem-solving abilities compared to using a single or fixed number of\npersonas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,\nCodenames Collaborative, and Logic Grid Puzzle, encompassing both\nknowledge-intensive and reasoning-intensive types. Unlike previous works, such\nas Chain-of-Thought, that solely enhance the reasoning abilities in LLMs,\nexperimental results demonstrate that SPP effectively reduces factual\nhallucination, and maintains strong reasoning capabilities. Additionally,\ncomparative experiments show that cognitive synergy only emerges in GPT-4 and\ndoes not appear in less capable models, such as GPT-3.5-turbo and\nLlama2-13b-chat, which draws an interesting analogy to human development. Code,\ndata, and prompts can be found at:\nhttps:\/\/github.com\/MikeWangWZHL\/Solo-Performance-Prompting.git.","updated":1711463553000,"published":1689086719000,"authors":["Zhenhailong Wang","Shaoguang Mao","Wenshan Wu","Tao Ge","Furu Wei","Heng Ji"],"comments":"Accepted as a main conference paper at NAACL 2024","categories":["cs.AI","cs.CL"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"185":{"arxiv_id":"2308.11585v2","url":"http:\/\/arxiv.org\/abs\/2308.11585v2","title":"Causal Intersectionality and Dual Form of Gradient Descent for\n  Multimodal Analysis: a Case Study on Hateful Memes","summary":"Amidst the rapid expansion of Machine Learning (ML) and Large Language Models\n(LLMs), understanding the semantics within their mechanisms is vital. Causal\nanalyses define semantics, while gradient-based methods are essential to\neXplainable AI (XAI), interpreting the model's 'black box'. Integrating these,\nwe investigate how a model's mechanisms reveal its causal effect on\nevidence-based decision-making. Research indicates intersectionality - the\ncombined impact of an individual's demographics - can be framed as an Average\nTreatment Effect (ATE). This paper demonstrates that hateful meme detection can\nbe viewed as an ATE estimation using intersectionality principles, and\nsummarized gradient-based attention scores highlight distinct behaviors of\nthree Transformer models. We further reveal that LLM Llama-2 can discern the\nintersectional aspects of the detection through in-context learning and that\nthe learning process could be explained via meta-gradient, a secondary form of\ngradient. In conclusion, this work furthers the dialogue on Causality and XAI.\nOur code is available online (see External Resources section).","updated":1711202874000,"published":1692450855000,"authors":["Yosuke Miyanishi","Minh Le Nguyen"],"comments":"Accepted to LREC-COLING 2024","categories":["cs.AI","cs.CL"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"186":{"arxiv_id":"2308.12194v2","url":"http:\/\/arxiv.org\/abs\/2308.12194v2","title":"Inferring Human Intentions from Predicted Action Probabilities","summary":"Predicting the next action that a human is most likely to perform is key to\nhuman-AI collaboration and has consequently attracted increasing research\ninterests in recent years. An important factor for next action prediction are\nhuman intentions: If the AI agent knows the intention it can predict future\nactions and plan collaboration more effectively. Existing Bayesian methods for\nthis task struggle with complex visual input while deep neural network (DNN)\nbased methods do not provide uncertainty quantifications. In this work we\ncombine both approaches for the first time and show that the predicted next\naction probabilities contain information that can be used to infer the\nunderlying intention. We propose a two-step approach to human intention\nprediction: While a DNN predicts the probabilities of the next action,\nMCMC-based Bayesian inference is used to infer the underlying intention from\nthese predictions. This approach not only allows for independent design of the\nDNN architecture but also the subsequently fast, design-independent inference\nof human intentions. We evaluate our method using a series of experiments on\nthe Watch-And-Help (WAH) and a keyboard and mouse interaction dataset. Our\nresults show that our approach can accurately predict human intentions from\nobserved actions and the implicit information contained in next action\nprobabilities. Furthermore, we show that our approach can predict the correct\nintention even if only few actions have been observed.","updated":1711383541000,"published":1692804902000,"authors":["Lei Shi","Paul-Christian B\u00fcrkner","Andreas Bulling"],"comments":"Accepted by Workshop on Theory of Mind in Human-AI Interaction at CHI\n  2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"187":{"arxiv_id":"2309.07289v3","url":"http:\/\/arxiv.org\/abs\/2309.07289v3","title":"User Training with Error Augmentation for Electromyogram-based Gesture\n  Classification","summary":"We designed and tested a system for real-time control of a user interface by\nextracting surface electromyographic (sEMG) activity from eight electrodes in a\nwrist-band configuration. sEMG data were streamed into a machine-learning\nalgorithm that classified hand gestures in real-time. After an initial model\ncalibration, participants were presented with one of three types of feedback\nduring a human-learning stage: veridical feedback, in which predicted\nprobabilities from the gesture classification algorithm were displayed without\nalteration, modified feedback, in which we applied a hidden augmentation of\nerror to these probabilities, and no feedback. User performance was then\nevaluated in a series of minigames, in which subjects were required to use\neight gestures to manipulate their game avatar to complete a task. Experimental\nresults indicated that, relative to baseline, the modified feedback condition\nled to significantly improved accuracy and improved gesture class separation.\nThese findings suggest that real-time feedback in a gamified user interface\nwith manipulation of feedback may enable intuitive, rapid, and accurate task\nacquisition for sEMG-based gesture recognition applications.","updated":1711141875000,"published":1694636125000,"authors":["Yunus Bicer","Niklas Smedemark-Margulies","Basak Celik","Elifnur Sunger","Ryan Orendorff","Stephanie Naufel","Tales Imbiriba","Deniz Erdo\u011fmu\u015f","Eugene Tunik","Mathew Yarossi"],"comments":"10 pages, 10 figures. V2: Fix latex characters in author name. V3:\n  Add published DOI and Copyright notice","categories":["cs.HC","cs.LG","eess.SP"],"primary_category":"cs.HC","doi":"10.1109\/TNSRE.2024.3372512","journal_ref":"in IEEE Transactions on Neural Systems and Rehabilitation\n  Engineering, vol. 32, pp. 1187-1197, 2024","peer_reviewed":true},"188":{"arxiv_id":"2310.00117v4","url":"http:\/\/arxiv.org\/abs\/2310.00117v4","title":"ABScribe: Rapid Exploration & Organization of Multiple Writing\n  Variations in Human-AI Co-Writing Tasks using Large Language Models","summary":"Exploring alternative ideas by rewriting text is integral to the writing\nprocess. State-of-the-art Large Language Models (LLMs) can simplify writing\nvariation generation. However, current interfaces pose challenges for\nsimultaneous consideration of multiple variations: creating new variations\nwithout overwriting text can be difficult, and pasting them sequentially can\nclutter documents, increasing workload and disrupting writers' flow. To tackle\nthis, we present ABScribe, an interface that supports rapid, yet visually\nstructured, exploration and organization of writing variations in human-AI\nco-writing tasks. With ABScribe, users can swiftly modify variations using LLM\nprompts, which are auto-converted into reusable buttons. Variations are stored\nadjacently within text fields for rapid in-place comparisons using mouse-over\ninteractions on a popup toolbar. Our user study with 12 writers shows that\nABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances\nuser perceptions of the revision process (d = 2.41, p < 0.001) compared to a\npopular baseline workflow, and provides insights into how writers explore\nvariations using LLMs.","updated":1711546680000,"published":1696018275000,"authors":["Mohi Reza","Nathan Laundry","Ilya Musabirov","Peter Dushniku","Zhi Yuan \"Michael\" Yu","Kashish Mittal","Tovi Grossman","Michael Liut","Anastasia Kuzminykh","Joseph Jay Williams"],"comments":"CHI 2024","categories":["cs.HC","cs.AI","cs.LG"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3641899","journal_ref":null,"peer_reviewed":true},"189":{"arxiv_id":"2311.02099v4","url":"http:\/\/arxiv.org\/abs\/2311.02099v4","title":"A Safe Preference Learning Approach for Personalization with\n  Applications to Autonomous Vehicles","summary":"This work introduces a preference learning method that ensures adherence to\ngiven specifications, with an application to autonomous vehicles. Our approach\nincorporates the priority ordering of Signal Temporal Logic (STL) formulas\ndescribing traffic rules into a learning framework. By leveraging Parametric\nWeighted Signal Temporal Logic (PWSTL), we formulate the problem of\nsafety-guaranteed preference learning based on pairwise comparisons and propose\nan approach to solve this learning problem. Our approach finds a feasible\nvaluation for the weights of the given PWSTL formula such that, with these\nweights, preferred signals have weighted quantitative satisfaction measures\ngreater than their non-preferred counterparts. The feasible valuation of\nweights given by our approach leads to a weighted STL formula that can be used\nin correct-and-custom-by-construction controller synthesis. We demonstrate the\nperformance of our method with a pilot human subject study in two different\nsimulated driving scenarios involving a stop sign and a pedestrian crossing.\nOur approach yields competitive results compared to existing preference\nlearning methods in terms of capturing preferences and notably outperforms them\nwhen safety is considered.","updated":1711463152000,"published":1698702757000,"authors":["Ruya Karagulle","Nikos Arechiga","Andrew Best","Jonathan DeCastro","Necmiye Ozay"],"comments":"9 pages, 3 figures, 2 tables. This work has been published at IEEE\n  Robotics and Automation Letters. Copyright may be transferred without notice,\n  after which this version may no longer be accessible","categories":["cs.AI","cs.SY","eess.SY"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"190":{"arxiv_id":"2311.02760v2","url":"http:\/\/arxiv.org\/abs\/2311.02760v2","title":"Causal Question Answering with Reinforcement Learning","summary":"Causal questions inquire about causal relationships between different events\nor phenomena. They are important for a variety of use cases, including virtual\nassistants and search engines. However, many current approaches to causal\nquestion answering cannot provide explanations or evidence for their answers.\nHence, in this paper, we aim to answer causal questions with a causality graph,\na large-scale dataset of causal relations between noun phrases along with the\nrelations' provenance data. Inspired by recent, successful applications of\nreinforcement learning to knowledge graph tasks, such as link prediction and\nfact-checking, we explore the application of reinforcement learning on a\ncausality graph for causal question answering. We introduce an\nActor-Critic-based agent which learns to search through the graph to answer\ncausal questions. We bootstrap the agent with a supervised learning procedure\nto deal with large action spaces and sparse rewards. Our evaluation shows that\nthe agent successfully prunes the search space to answer binary causal\nquestions by visiting less than 30 nodes per question compared to over 3,000\nnodes by a naive breadth-first search. Our ablation study indicates that our\nsupervised learning strategy provides a strong foundation upon which our\nreinforcement learning agent improves. The paths returned by our agent explain\nthe mechanisms by which a cause produces an effect. Moreover, for each edge on\na path, our causality graph provides its original source allowing for easy\nverification of paths.","updated":1711357067000,"published":1699216398000,"authors":["Lukas Bl\u00fcbaum","Stefan Heindorf"],"comments":"Accepted at WWW 2024","categories":["cs.AI","cs.LG"],"primary_category":"cs.AI","doi":"10.1145\/3589334.3645610","journal_ref":null,"peer_reviewed":true},"191":{"arxiv_id":"2311.07954v2","url":"http:\/\/arxiv.org\/abs\/2311.07954v2","title":"A Closer Look at the Self-Verification Abilities of Large Language\n  Models in Logical Reasoning","summary":"Logical reasoning has been an ongoing pursuit in the field of AI. Despite\nsignificant advancements made by large language models (LLMs), they still\nstruggle with complex logical reasoning problems. To enhance reasoning\nperformance, one promising direction is scalable oversight, which requires LLMs\nto identify their own errors and then improve by themselves. Various\nself-verification methods have been proposed in pursuit of this goal.\nNevertheless, whether existing models understand their own errors well is still\nunder investigation. In this paper, we take a closer look at the\nself-verification abilities of LLMs in the context of logical reasoning,\nfocusing on their ability to identify logical fallacies accurately. We\nintroduce a dataset, FALLACIES, containing 232 types of reasoning fallacies\ncategorized in a hierarchical taxonomy. By conducting exhaustive experiments on\nFALLACIES, we obtain comprehensive and detailed analyses of a series of models\non their verification abilities. Our main findings suggest that existing LLMs\ncould struggle to identify fallacious reasoning steps accurately and may fall\nshort of guaranteeing the validity of self-verification methods. Drawing from\nthese observations, we offer suggestions for future research and practical\napplications of self-verification methods.","updated":1711202084000,"published":1699945990000,"authors":["Ruixin Hong","Hongming Zhang","Xinyu Pang","Dong Yu","Changshui Zhang"],"comments":"NAACL 2024 Main Conference","categories":["cs.AI","cs.CL"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"192":{"arxiv_id":"2312.03009v2","url":"http:\/\/arxiv.org\/abs\/2312.03009v2","title":"I-PHYRE: Interactive Physical Reasoning","summary":"Current evaluation protocols predominantly assess physical reasoning in\nstationary scenes, creating a gap in evaluating agents' abilities to interact\nwith dynamic events. While contemporary methods allow agents to modify initial\nscene configurations and observe consequences, they lack the capability to\ninteract with events in real time. To address this, we introduce I-PHYRE, a\nframework that challenges agents to simultaneously exhibit intuitive physical\nreasoning, multi-step planning, and in-situ intervention. Here, intuitive\nphysical reasoning refers to a quick, approximate understanding of physics to\naddress complex problems; multi-step denotes the need for extensive sequence\nplanning in I-PHYRE, considering each intervention can significantly alter\nsubsequent choices; and in-situ implies the necessity for timely object\nmanipulation within a scene, where minor timing deviations can result in task\nfailure. We formulate four game splits to scrutinize agents' learning and\ngeneralization of essential principles of interactive physical reasoning,\nfostering learning through interaction with representative scenarios. Our\nexploration involves three planning strategies and examines several supervised\nand reinforcement agents' zero-shot generalization proficiency on I-PHYRE. The\noutcomes highlight a notable gap between existing learning algorithms and human\nperformance, emphasizing the imperative for more research in enhancing agents\nwith interactive physical reasoning capabilities. The environment and baselines\nwill be made publicly available.","updated":1711343044000,"published":1701716479000,"authors":["Shiqian Li","Kewen Wu","Chi Zhang","Yixin Zhu"],"comments":"21 pages, ICLR 2024","categories":["cs.AI","cs.CV","cs.LG","cs.RO"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"193":{"arxiv_id":"2312.06203v2","url":"http:\/\/arxiv.org\/abs\/2312.06203v2","title":"Offloading and Quality Control for AI Generated Content Services in 6G\n  Mobile Edge Computing Networks","summary":"AI-Generated Content (AIGC), as a novel manner of providing Metaverse\nservices in the forthcoming Internet paradigm, can resolve the obstacles of\nimmersion requirements. Concurrently, edge computing, as an evolutionary\nparadigm of computing in communication systems, effectively augments real-time\ninteractive services. In pursuit of enhancing the accessibility of AIGC\nservices, the deployment of AIGC models (e.g., diffusion models) to edge\nservers and local devices has become a prevailing trend. Nevertheless, this\napproach faces constraints imposed by battery life and computational resources\nwhen tasks are offloaded to local devices, limiting the capacity to deliver\nhigh-quality content to users while adhering to stringent latency requirements.\nSo there will be a tradeoff between the utility of AIGC models and offloading\ndecisions in the edge computing paradigm. This paper proposes a joint\noptimization algorithm for offloading decisions, computation time, and\ndiffusion steps of the diffusion models in the reverse diffusion stage.\nMoreover, we take the average error into consideration as the metric for\nevaluating the quality of the generated results. Experimental results\nconclusively demonstrate that the proposed algorithm achieves superior joint\noptimization performance compared to the baselines.","updated":1711175917000,"published":1702283787000,"authors":["Yitong Wang","Chang Liu","Jun Zhao"],"comments":"This paper appears in the 2024 IEEE 99th Vehicular Technology\n  Conference (VTC)","categories":["cs.AI","cs.NI","cs.PF"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"194":{"arxiv_id":"2312.10370v2","url":"http:\/\/arxiv.org\/abs\/2312.10370v2","title":"Do Similar Entities have Similar Embeddings?","summary":"Knowledge graph embedding models (KGEMs) developed for link prediction learn\nvector representations for entities in a knowledge graph, known as embeddings.\nA common tacit assumption is the KGE entity similarity assumption, which states\nthat these KGEMs retain the graph's structure within their embedding space,\n\\textit{i.e.}, position similar entities within the graph close to one another.\nThis desirable property make KGEMs widely used in downstream tasks such as\nrecommender systems or drug repurposing. Yet, the relation of entity similarity\nand similarity in the embedding space has rarely been formally evaluated.\nTypically, KGEMs are assessed based on their sole link prediction capabilities,\nusing ranked-based metrics such as Hits@K or Mean Rank. This paper challenges\nthe prevailing assumption that entity similarity in the graph is inherently\nmirrored in the embedding space. Therefore, we conduct extensive experiments to\nmeasure the capability of KGEMs to cluster similar entities together, and\ninvestigate the nature of the underlying factors. Moreover, we study if\ndifferent KGEMs expose a different notion of similarity. Datasets, pre-trained\nembeddings and code are available at:\nhttps:\/\/github.com\/nicolas-hbt\/similar-embeddings\/.","updated":1711617141000,"published":1702714116000,"authors":["Nicolas Hubert","Heiko Paulheim","Armelle Brun","Davy Monticolo"],"comments":"Accepted at ESWC 2024","categories":["cs.AI","cs.IR","cs.LG"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"195":{"arxiv_id":"2401.06247v2","url":"http:\/\/arxiv.org\/abs\/2401.06247v2","title":"Trickery: Educational Dark Pattern Analogies for Use in Serious Games","summary":"Dark patterns are often used in interface design to manipulate users into\nperforming actions they would otherwise not take, such as consenting to\nexcessive data collection. We present a narrative serious game concept, along\nwith seven educational dark pattern analogies designed to create awareness of\nand bolster resistance against dark patterns through direct consequences of\nplayer actions. We performed a qualitative laboratory gameplay study\ninvestigating player behavior when confronted with educational dark pattern\nanalogies in a serious game and an online survey study evaluating the perceived\nhelpfulness of our educational dark pattern analogies. Our results provide\ninsights into influencing factors for adapting dark patterns into gameplay, as\nwell as player motivations and driving forces influencing player behavior, and\nshow educational dark patterns to be a promising solution to increase user\nunderstanding of dark pattern concepts.","updated":1711287780000,"published":1705004500000,"authors":["Kirill Kronhardt","Kevin Rolfes","Jens Gerken"],"comments":"[V2]: Submitted to CHI PLAY 2024 [V1]: Submitted to Proceedings on\n  Privacy Enhancing Technologies 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"196":{"arxiv_id":"2401.09210v2","url":"http:\/\/arxiv.org\/abs\/2401.09210v2","title":"Narratives of Collective Action in YouTube's Discourse on Veganism","summary":"Narratives can be powerful tools for inspiring action on pressing societal\nissues such as climate change. While social science theories offer frameworks\nfor understanding the narratives that arise within collective movements, these\nare rarely applied to the vast data available from social media platforms,\nwhich play a significant role in shaping public opinion and mobilizing\ncollective action. This gap in the empirical evaluation of online narratives\nlimits our understanding of their relationship with public response. In this\nstudy, we focus on plant-based diets as a form of pro-environmental action and\nemploy natural language processing to operationalize a theoretical framework of\nmoral narratives specific to the vegan movement. We apply this framework to\nnarratives found in YouTube videos promoting environmental initiatives such as\nVeganuary, Meatless March, and No Meat May. Our analysis reveals that several\nnarrative types, as defined by the theory, are empirically present in the data.\nTo identify narratives with the potential to elicit positive public engagement,\nwe used text processing to estimate the proportion of comments supporting\ncollective action across narrative types. Video narratives advocating social\nfight, whether through protest or through efforts to convert others to the\ncause, are associated with a stronger sense of collective action in the\nrespective comments. These narrative types also demonstrate increased semantic\ncoherence and alignment between the message and public response, markers\ntypically associated with successful collective action. Our work offers new\ninsights into the complex factors that influence the emergence of collective\naction, thereby informing the development of effective communication strategies\nwithin social movements.","updated":1711625999000,"published":1705499076000,"authors":["Arianna Pera","Luca Maria Aiello"],"comments":"15 pages, 7 figures, 7 tables. Accepted at ICWSM 2024","categories":["cs.CY","physics.soc-ph"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"197":{"arxiv_id":"2401.12423v3","url":"http:\/\/arxiv.org\/abs\/2401.12423v3","title":"Rank, Pack, or Approve: Voting Methods in Participatory Budgeting","summary":"Participatory budgeting is a popular method to engage residents in budgeting\ndecisions by local governments. The Stanford Participatory Budgeting platform\nis an online platform that has been used to engage residents in more than 150\nbudgeting processes. We present a data set with anonymized budget opinions from\nthese processes with K-approval, K-ranking or knapsack primary ballots. For a\nsubset of the voters, it includes paired votes with a different elicitation\nmethod in the same process. This presents a unique data set, as the voters,\nprojects and setting are all related to real-world decisions that the voters\nhave an actual interest in. With data from primary ballots we find that while\nballot complexity (number of projects to choose from, number of projects to\nselect and ballot length) is correlated with a higher median time spent by\nvoters, it is not correlated with a higher abandonment rate.\n  We use vote pairs with different voting methods to analyze the effect of\nvoting methods on the cost of selected projects, more comprehensively than was\npreviously possible. In most elections, voters selected significantly more\nexpensive projects using K-approval than using knapsack, although we also find\na small number of examples with a significant effect in the opposite direction.\nThis effect happens at the aggregate level as well as for individual voters,\nand is influenced both by the implicit constraints of the voting method and the\nexplicit constraints of the voting interface. Finally, we validate the use of\nK-ranking elicitation to offer a paper alternative for knapsack voting.","updated":1711390725000,"published":1705972784000,"authors":["Lodewijk Gelauff","Ashish Goel"],"comments":"Accepted for publication at ICWSM. Data set is available through:\n  https:\/\/doi.org\/10.25740\/db709zg9088","categories":["cs.CY"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"198":{"arxiv_id":"2402.01786v2","url":"http:\/\/arxiv.org\/abs\/2402.01786v2","title":"COA-GPT: Generative Pre-trained Transformers for Accelerated Course of\n  Action Development in Military Operations","summary":"The development of Courses of Action (COAs) in military operations is\ntraditionally a time-consuming and intricate process. Addressing this\nchallenge, this study introduces COA-GPT, a novel algorithm employing Large\nLanguage Models (LLMs) for rapid and efficient generation of valid COAs.\nCOA-GPT incorporates military doctrine and domain expertise to LLMs through\nin-context learning, allowing commanders to input mission information - in both\ntext and image formats - and receive strategically aligned COAs for review and\napproval. Uniquely, COA-GPT not only accelerates COA development, producing\ninitial COAs within seconds, but also facilitates real-time refinement based on\ncommander feedback. This work evaluates COA-GPT in a military-relevant scenario\nwithin a militarized version of the StarCraft II game, comparing its\nperformance against state-of-the-art reinforcement learning algorithms. Our\nresults demonstrate COA-GPT's superiority in generating strategically sound\nCOAs more swiftly, with added benefits of enhanced adaptability and alignment\nwith commander intentions. COA-GPT's capability to rapidly adapt and update\nCOAs during missions presents a transformative potential for military planning,\nparticularly in addressing planning discrepancies and capitalizing on emergent\nwindows of opportunities.","updated":1711639362000,"published":1706824269000,"authors":["Vinicius G. Goecks","Nicholas Waytowich"],"comments":"Accepted at the NATO Science and Technology Organization Symposium\n  (ICMCIS) organized by the Information Systems Technology (IST) Panel,\n  IST-205-RSY - the ICMCIS, held in Koblenz, Germany, 23-24 April 2024","categories":["cs.AI","cs.CL","cs.HC","cs.LG","I.2.6; I.2.7; J.7"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"199":{"arxiv_id":"2402.07946v2","url":"http:\/\/arxiv.org\/abs\/2402.07946v2","title":"Re-Envisioning Command and Control","summary":"Future warfare will require Command and Control (C2) decision-making to occur\nin more complex, fast-paced, ill-structured, and demanding conditions. C2 will\nbe further complicated by operational challenges such as Denied, Degraded,\nIntermittent, and Limited (DDIL) communications and the need to account for\nmany data streams, potentially across multiple domains of operation. Yet,\ncurrent C2 practices -- which stem from the industrial era rather than the\nemerging intelligence era -- are linear and time-consuming. Critically, these\napproaches may fail to maintain overmatch against adversaries on the future\nbattlefield. To address these challenges, we propose a vision for future C2\nbased on robust partnerships between humans and artificial intelligence (AI)\nsystems. This future vision is encapsulated in three operational impacts:\nstreamlining the C2 operations process, maintaining unity of effort, and\ndeveloping adaptive collective knowledge systems. This paper illustrates the\nenvisaged future C2 capabilities, discusses the assumptions that shaped them,\nand describes how the proposed developments could transform C2 in future\nwarfare.","updated":1711639050000,"published":1707495029000,"authors":["Kaleb McDowell","Ellen Novoseller","Anna Madison","Vinicius G. Goecks","Christopher Kelshaw"],"comments":"Accepted at the NATO Science and Technology Organization Symposium\n  (ICMCIS) organized by the Information Systems Technology (IST) Panel,\n  IST-205-RSY - the ICMCIS, held in Koblenz, Germany, 23-24 April 2024","categories":["cs.HC","cs.AI","cs.CL","cs.LG","I.2.6; I.2.7; J.7"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"200":{"arxiv_id":"2402.09494v2","url":"http:\/\/arxiv.org\/abs\/2402.09494v2","title":"Can AI and humans genuinely communicate?","summary":"Can AI and humans genuinely communicate? In this article, after giving some\nbackground and motivating my proposal (sections 1 to 3), I explore a way to\nanswer this question that I call the \"mental-behavioral methodology\" (sections\n4 and 5). This methodology follows the following three steps: First, spell out\nwhat mental capacities are sufficient for human communication (as opposed to\ncommunication more generally). Second, spell out the experimental paradigms\nrequired to test whether a behavior exhibits these capacities. Third, apply or\nadapt these paradigms to test whether an AI displays the relevant behaviors. If\nthe first two steps are successfully completed, and if the AI passes the tests\nwith human-like results, this constitutes evidence that this AI and humans can\ngenuinely communicate. This mental-behavioral methodology has the advantage\nthat we don't need to understand the workings of black-box algorithms, such as\nstandard deep neural networks. This is comparable to the fact that we don't\nneed to understand how human brains work to know that humans can genuinely\ncommunicate. This methodology also has its disadvantages and I will discuss\nsome of them (section 6).","updated":1711384364000,"published":1707915640000,"authors":["Constant Bonard"],"comments":"March 2024 preprint","categories":["cs.HC","cs.AI"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"201":{"arxiv_id":"2402.14159v2","url":"http:\/\/arxiv.org\/abs\/2402.14159v2","title":"Mapping the Landscape of Independent Food Delivery Platforms in the\n  United States","summary":"Beyond the well-known giants like Uber Eats and DoorDash, there are hundreds\nof independent food delivery platforms in the United States. However, little is\nknown about the sociotechnical landscape of these ``indie'' platforms. In this\npaper, we analyzed these platforms to understand why they were created, how\nthey operate, and what technologies they use. We collected data on 495 indie\nplatforms and detailed survey responses from 29 platforms. We found that\npersonalized, timely service is a central value of indie platforms, as is a\nsense of responsibility to the local community they serve. Indie platforms are\nmotivated to provide fair rates for restaurants and couriers. These alternative\nbusiness practices differentiate them from mainstream platforms. Though indie\nplatforms have plans to expand, a lack of customizability in off-the-shelf\nsoftware prevents independent platforms from personalizing services for their\nlocal communities. We show that these platforms are a widespread and\nlongstanding fixture of the food delivery market. We illustrate the diversity\nof motivations and values to explain why a one-size-fits-all support is\ninsufficient, and we discuss the siloing of technology that inhibits platforms'\ngrowth. Through these insights, we aim to promote future HCI research into the\npotential development of public-interest technologies for local food delivery.","updated":1711382681000,"published":1708555486000,"authors":["Yuhan Liu","Amna Liaqat","Owen Xingjian Zhang","Mariana Consuelo Fern\u00e1ndez Espinosa","Ankhitha Manjunatha","Alexander Yang","Orestis Papakyriakopoulos","Andr\u00e9s Monroy-Hern\u00e1ndez"],"comments":"To appear in CSCW 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3637369","journal_ref":null,"peer_reviewed":true},"202":{"arxiv_id":"2403.06693v2","url":"http:\/\/arxiv.org\/abs\/2403.06693v2","title":"Chart4Blind: An Intelligent Interface for Chart Accessibility Conversion","summary":"In a world driven by data visualization, ensuring the inclusive accessibility\nof charts for Blind and Visually Impaired (BVI) individuals remains a\nsignificant challenge. Charts are usually presented as raster graphics without\ntextual and visual metadata needed for an equivalent exploration experience for\nBVI people. Additionally, converting these charts into accessible formats\nrequires considerable effort from sighted individuals. Digitizing charts with\nmetadata extraction is just one aspect of the issue; transforming it into\naccessible modalities, such as tactile graphics, presents another difficulty.\nTo address these disparities, we propose Chart4Blind, an intelligent user\ninterface that converts bitmap image representations of line charts into\nuniversally accessible formats. Chart4Blind achieves this transformation by\ngenerating Scalable Vector Graphics (SVG), Comma-Separated Values (CSV), and\nalternative text exports, all comply with established accessibility standards.\nThrough interviews and a formal user study, we demonstrate that even\ninexperienced sighted users can make charts accessible in an average of 4\nminutes using Chart4Blind, achieving a System Usability Scale rating of 90%. In\ncomparison to existing approaches, Chart4Blind provides a comprehensive\nsolution, generating end-to-end accessible SVGs suitable for assistive\ntechnologies such as embossed prints (papers and laser cut), 2D tactile\ndisplays, and screen readers. For additional information, including open-source\ncodes and demos, please visit our project page\nhttps:\/\/moured.github.io\/chart4blind\/.","updated":1711380831000,"published":1710162466000,"authors":["Omar Moured","Morris Baumgarten-Egemole","Alina Roitberg","Karin Muller","Thorsten Schwarz","Rainer Stiefelhagen"],"comments":"Accepted to IUI 2024. 19 pages, 7 figures, 2 table. For a demo video,\n  see this https:\/\/moured.github.io\/chart4blind\/ . The source code is available\n  at https:\/\/github.com\/moured\/chart4blind_code\/","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3640543.3645175","journal_ref":null,"peer_reviewed":true},"203":{"arxiv_id":"2403.09856v2","url":"http:\/\/arxiv.org\/abs\/2403.09856v2","title":"A Tale of Two Communities: Exploring Academic References on Stack\n  Overflow","summary":"Stack Overflow is widely recognized by software practitioners as the go-to\nresource for addressing technical issues and sharing practical solutions. While\nnot typically seen as a scholarly forum, users on Stack Overflow commonly refer\nto academic sources in their discussions. Yet, little is known about these\nreferenced academic works and how they intersect the needs and interests of the\nStack Overflow community. To bridge this gap, we conducted an exploratory\nlarge-scale study on the landscape of academic references in Stack Overflow.\nOur findings reveal that Stack Overflow communities with different domains of\ninterest engage with academic literature at varying frequencies and speeds. The\ncontradicting patterns suggest that some disciplines may have diverged in their\ninterests and development trajectories from the corresponding practitioner\ncommunity. Finally, we discuss the potential of Stack Overflow in gauging the\nreal-world relevance of academic research.","updated":1711646388000,"published":1710448435000,"authors":["Run Huang","Souti Chattopadhyay"],"comments":"Accepted for publication in The Web Conference (WWW) 2024, Short\n  Paper Track","categories":["cs.CY","cs.SE"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"204":{"arxiv_id":"2403.09986v2","url":"http:\/\/arxiv.org\/abs\/2403.09986v2","title":"Designing Sousveillance Tools for Gig Workers","summary":"As independently-contracted employees, gig workers disproportionately suffer\nthe consequences of workplace surveillance, which include increased pressures\nto work, breaches of privacy, and decreased digital autonomy. Despite the\nnegative impacts of workplace surveillance, gig workers lack the tools,\nstrategies, and workplace social support to protect themselves against these\nharms. Meanwhile, some critical theorists have proposed sousveillance as a\npotential means of countering such abuses of power, whereby those under\nsurveillance monitor those in positions of authority (e.g., gig workers collect\ndata about requesters\/platforms). To understand the benefits of sousveillance\nsystems in the gig economy, we conducted semi-structured interviews and led\nco-design activities with gig workers. We use \"care ethics\" as a guiding\nconcept to understand our interview and co-design data, while also focusing on\nempathic sousveillance technology design recommendations. Through our study, we\nidentify gig workers' attitudes towards and past experiences with\nsousveillance. We also uncover the type of sousveillance technologies imagined\nby workers, provide design recommendations, and finish by discussing how to\ncreate empowering, empathic spaces on gig platforms.","updated":1711199280000,"published":1710472106000,"authors":["Maya De Los Santos","Kimberly Do","Michael Muller","Saiph Savage"],"comments":"Published as a conference paper at the ACM Conference on Human\n  Factors in Computing Systems, CHI 2024, 3 figures, 30 pages","categories":["cs.CY","cs.HC","cs.SI"],"primary_category":"cs.CY","doi":"10.1145\/3613904.3642614","journal_ref":null,"peer_reviewed":true},"205":{"arxiv_id":"2403.12151v2","url":"http:\/\/arxiv.org\/abs\/2403.12151v2","title":"Fusing Domain-Specific Content from Large Language Models into Knowledge\n  Graphs for Enhanced Zero Shot Object State Classification","summary":"Domain-specific knowledge can significantly contribute to addressing a wide\nvariety of vision tasks. However, the generation of such knowledge entails\nconsiderable human labor and time costs. This study investigates the potential\nof Large Language Models (LLMs) in generating and providing domain-specific\ninformation through semantic embeddings. To achieve this, an LLM is integrated\ninto a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors\nin the context of the Vision-based Zero-shot Object State Classification task.\nWe thoroughly examine the behavior of the LLM through an extensive ablation\nstudy. Our findings reveal that the integration of LLM-based embeddings, in\ncombination with general-purpose pre-trained embeddings, leads to substantial\nperformance improvements. Drawing insights from this ablation study, we conduct\na comparative analysis against competing models, thereby highlighting the\nstate-of-the-art performance achieved by the proposed approach.","updated":1711392606000,"published":1710785324000,"authors":["Filippos Gouidis","Katerina Papantoniou","Konstantinos Papoutsakis Theodore Patkos","Antonis Argyros","Dimitris Plexousakis"],"comments":"Accepted at the AAAI-MAKE 24","categories":["cs.AI","cs.CL","cs.CV","cs.LG"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"206":{"arxiv_id":"2403.13517v2","url":"http:\/\/arxiv.org\/abs\/2403.13517v2","title":"Putting Our Minds Together: Iterative Exploration for Collaborative Mind\n  Mapping","summary":"We delineate the development of a mind-mapping system designed concurrently\nfor both VR and desktop platforms. Employing an iterative methodology with\ngroups of users, we systematically examined and improved various facets of our\nsystem, including interactions, communication mechanisms and gamification\nelements, to streamline the mind-mapping process while augmenting situational\nawareness and promoting active engagement among collaborators. We also report\nour observational findings on these facets from this iterative design process.","updated":1711181721000,"published":1710934485000,"authors":["Ying Yang","Tim Dwyer","Zachari Swiecki","Benjamin Lee","Michael Wybrow","Maxime Cordeil","Teresa Wulandari","Bruce H. Thomas","Mark Billinghurst"],"comments":"Accepted at AHs 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3652920.3653043","journal_ref":null,"peer_reviewed":true},"207":{"arxiv_id":"2403.14117v2","url":"http:\/\/arxiv.org\/abs\/2403.14117v2","title":"A Design Space for Intelligent and Interactive Writing Assistants","summary":"In our era of rapid technological advancement, the research landscape for\nwriting assistants has become increasingly fragmented across various research\ncommunities. We seek to address this challenge by proposing a design space as a\nstructured way to examine and explore the multidimensional space of intelligent\nand interactive writing assistants. Through a large community collaboration, we\nexplore five aspects of writing assistants: task, user, technology,\ninteraction, and ecosystem. Within each aspect, we define dimensions (i.e.,\nfundamental components of an aspect) and codes (i.e., potential options for\neach dimension) by systematically reviewing 115 papers. Our design space aims\nto offer researchers and designers a practical tool to navigate, comprehend,\nand compare the various possibilities of writing assistants, and aid in the\nenvisioning and design of new writing assistants.","updated":1711457594000,"published":1710993796000,"authors":["Mina Lee","Katy Ilonka Gero","John Joon Young Chung","Simon Buckingham Shum","Vipul Raheja","Hua Shen","Subhashini Venugopalan","Thiemo Wambsganss","David Zhou","Emad A. Alghamdi","Tal August","Avinash Bhat","Madiha Zahrah Choksi","Senjuti Dutta","Jin L. C. Guo","Md Naimul Hoque","Yewon Kim","Simon Knight","Seyed Parsa Neshaei","Agnia Sergeyuk","Antonette Shibani","Disha Shrivastava","Lila Shroff","Jessi Stark","Sarah Sterman","Sitong Wang","Antoine Bosselut","Daniel Buschek","Joseph Chee Chang","Sherol Chen","Max Kreminski","Joonsuk Park","Roy Pea","Eugenia H. Rho","Shannon Zejiang Shen","Pao Siangliulue"],"comments":"Published as a conference paper at CHI 2024","categories":["cs.HC","cs.CL"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642697","journal_ref":null,"peer_reviewed":true},"208":{"arxiv_id":"2403.15321v2","url":"http:\/\/arxiv.org\/abs\/2403.15321v2","title":"Visual Highlighting for Situated Brushing and Linking","summary":"Brushing and linking is widely used for visual analytics in desktop\nenvironments. However, using this approach to link many data items between\nsituated (e.g., a virtual screen with data) and embedded views (e.g.,\nhighlighted objects in the physical environment) is largely unexplored. To this\nend, we study the effectiveness of visual highlighting techniques in helping\nusers identify and link physical referents to brushed data marks in a situated\nscatterplot. In an exploratory virtual reality user study (N=20), we evaluated\nfour highlighting techniques under different physical layouts and tasks. We\ndiscuss the effectiveness of these techniques, as well as implications for the\ndesign of brushing and linking operations in situated analytics.","updated":1711631111000,"published":1711124271000,"authors":["Nina Doerr","Benjamin Lee","Katarina Baricova","Dieter Schmalstieg","Michael Sedlmair"],"comments":"To appear in EuroVis 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"209":{"arxiv_id":"2403.15574v1","url":"http:\/\/arxiv.org\/abs\/2403.15574v1","title":"SensoryT5: Infusing Sensorimotor Norms into T5 for Enhanced Fine-grained\n  Emotion Classification","summary":"In traditional research approaches, sensory perception and emotion\nclassification have traditionally been considered separate domains. Yet, the\nsignificant influence of sensory experiences on emotional responses is\nundeniable. The natural language processing (NLP) community has often missed\nthe opportunity to merge sensory knowledge with emotion classification. To\naddress this gap, we propose SensoryT5, a neuro-cognitive approach that\nintegrates sensory information into the T5 (Text-to-Text Transfer Transformer)\nmodel, designed specifically for fine-grained emotion classification. This\nmethodology incorporates sensory cues into the T5's attention mechanism,\nenabling a harmonious balance between contextual understanding and sensory\nawareness. The resulting model amplifies the richness of emotional\nrepresentations. In rigorous tests across various detailed emotion\nclassification datasets, SensoryT5 showcases improved performance, surpassing\nboth the foundational T5 model and current state-of-the-art works. Notably,\nSensoryT5's success signifies a pivotal change in the NLP domain, highlighting\nthe potential influence of neuro-cognitive data in refining machine learning\nmodels' emotional sensitivity.","updated":1711134205000,"published":1711134205000,"authors":["Yuhan Xia","Qingqing Zhao","Yunfei Long","Ge Xu","Jia Wang"],"comments":"Accepted by CogALex 2024 conference","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"210":{"arxiv_id":"2403.15604v1","url":"http:\/\/arxiv.org\/abs\/2403.15604v1","title":"Investigating Use Cases of AI-Powered Scene Description Applications for\n  Blind and Low Vision People","summary":"\"Scene description\" applications that describe visual content in a photo are\nuseful daily tools for blind and low vision (BLV) people. Researchers have\nstudied their use, but they have only explored those that leverage remote\nsighted assistants; little is known about applications that use AI to generate\ntheir descriptions. Thus, to investigate their use cases, we conducted a\ntwo-week diary study where 16 BLV participants used an AI-powered scene\ndescription application we designed. Through their diary entries and follow-up\ninterviews, users shared their information goals and assessments of the visual\ndescriptions they received. We analyzed the entries and found frequent use\ncases, such as identifying visual features of known objects, and surprising\nones, such as avoiding contact with dangerous objects. We also found users\nscored the descriptions relatively low on average, 2.76 out of 5 (SD=1.49) for\nsatisfaction and 2.43 out of 4 (SD=1.16) for trust, showing that descriptions\nstill need significant improvements to deliver satisfying and trustworthy\nexperiences. We discuss future opportunities for AI as it becomes a more\npowerful accessibility tool for BLV users.","updated":1711138615000,"published":1711138615000,"authors":["Ricardo Gonzalez","Jazmin Collins","Shiri Azenkot","Cynthia Bennett"],"comments":"21 pages, 18 figures, 5 tables, to appear CHI2024","categories":["cs.HC","cs.AI"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"211":{"arxiv_id":"2403.15760v1","url":"http:\/\/arxiv.org\/abs\/2403.15760v1","title":"An Upload-Efficient Scheme for Transferring Knowledge From a Server-Side\n  Pre-trained Generator to Clients in Heterogeneous Federated Learning","summary":"Heterogeneous Federated Learning (HtFL) enables collaborative learning on\nmultiple clients with different model architectures while preserving privacy.\nDespite recent research progress, knowledge sharing in HtFL is still difficult\ndue to data and model heterogeneity. To tackle this issue, we leverage the\nknowledge stored in pre-trained generators and propose a new upload-efficient\nknowledge transfer scheme called Federated Knowledge-Transfer Loop (FedKTL).\nOur FedKTL can produce client-task-related prototypical image-vector pairs via\nthe generator's inference on the server. With these pairs, each client can\ntransfer pre-existing knowledge from the generator to its local model through\nan additional supervised local task. We conduct extensive experiments on four\ndatasets under two types of data heterogeneity with 14 kinds of models\nincluding CNNs and ViTs. Results show that our upload-efficient FedKTL\nsurpasses seven state-of-the-art methods by up to 7.31% in accuracy. Moreover,\nour knowledge transfer scheme is applicable in scenarios with only one edge\nclient. Code: https:\/\/github.com\/TsingZ0\/FedKTL","updated":1711182249000,"published":1711182249000,"authors":["Jianqing Zhang","Yang Liu","Yang Hua","Jian Cao"],"comments":"Accepted by CVPR2024","categories":["cs.AI","cs.DC"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"212":{"arxiv_id":"2403.15875v1","url":"http:\/\/arxiv.org\/abs\/2403.15875v1","title":"LAMPER: LanguAge Model and Prompt EngineeRing for zero-shot time series\n  classification","summary":"This study constructs the LanguAge Model with Prompt EngineeRing (LAMPER)\nframework, designed to systematically evaluate the adaptability of pre-trained\nlanguage models (PLMs) in accommodating diverse prompts and their integration\nin zero-shot time series (TS) classification. We deploy LAMPER in experimental\nassessments using 128 univariate TS datasets sourced from the UCR archive. Our\nfindings indicate that the feature representation capacity of LAMPER is\ninfluenced by the maximum input token threshold imposed by PLMs.","updated":1711209157000,"published":1711209157000,"authors":["Zhicheng Du","Zhaotian Xie","Yan Tong","Peiwu Qin"],"comments":"Accepted as tiny paper in ICLR 2024","categories":["cs.AI","cs.CL"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"213":{"arxiv_id":"2403.15916v1","url":"http:\/\/arxiv.org\/abs\/2403.15916v1","title":"Multi-agent transformer-accelerated RL for satisfaction of STL\n  specifications","summary":"One of the main challenges in multi-agent reinforcement learning is\nscalability as the number of agents increases. This issue is further\nexacerbated if the problem considered is temporally dependent. State-of-the-art\nsolutions today mainly follow centralized training with decentralized execution\nparadigm in order to handle the scalability concerns. In this paper, we propose\ntime-dependent multi-agent transformers which can solve the temporally\ndependent multi-agent problem efficiently with a centralized approach via the\nuse of transformers that proficiently handle the large input. We highlight the\nefficacy of this method on two problems and use tools from statistics to verify\nthe probability that the trajectories generated under the policy satisfy the\ntask. The experiments show that our approach has superior performance against\nthe literature baseline algorithms in both cases.","updated":1711221181000,"published":1711221181000,"authors":["Albin Larsson Forsberg","Alexandros Nikou","Aneta Vulgarakis Feljan","Jana Tumova"],"comments":"Submitted to L4DC 2024 conference","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"214":{"arxiv_id":"2403.16018v1","url":"http:\/\/arxiv.org\/abs\/2403.16018v1","title":"Understanding the Impact of Referent Design on Scale Perception in\n  Immersive Data Visualization","summary":"Referents are often used to enhance scale perception in immersive\nvisualizations. Common referent designs include the considerations of referent\nlayout (side-by-side vs. in-situ) and referent size (small vs. medium vs.\nlarge). This paper introduces a controlled user study to assess how different\nreferent designs affect the efficiency and accuracy of scale perception across\ndifferent data scales, on the performance of the size-matching task in the\nvirtual environment. Our results reveal that in-situ layouts significantly\nenhance accuracy and confidence across various data scales, particularly with\nlarge referents. Linear regression analyses further confirm that in-situ\nlayouts exhibit greater resilience to changes in data scale. For tasks\nrequiring efficiency, medium-sized referents emerge as the preferred choice.\nBased on these findings, we offer design guidelines for selecting referent\nlayouts and sizes in immersive visualizations.","updated":1711258709000,"published":1711258709000,"authors":["Yihan Hou","Hao Cui","Rongrong Chen","Wei Zeng"],"comments":"7 pages, 6 figures, Accepted to Extended Abstracts of the CHI\n  Conference on Human Factors in Computing Systems (CHI EA '24)","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3613905.3650783","journal_ref":null,"peer_reviewed":true},"215":{"arxiv_id":"2403.16066v1","url":"http:\/\/arxiv.org\/abs\/2403.16066v1","title":"A Temporal Graph Network Framework for Dynamic Recommendation","summary":"Recommender systems, crucial for user engagement on platforms like e-commerce\nand streaming services, often lag behind users' evolving preferences due to\nstatic data reliance. After Temporal Graph Networks (TGNs) were proposed,\nvarious studies have shown that TGN can significantly improve situations where\nthe features of nodes and edges dynamically change over time. However, despite\nits promising capabilities, it has not been directly applied in recommender\nsystems to date. Our study bridges this gap by directly implementing Temporal\nGraph Networks (TGN) in recommender systems, a first in this field. Using\nreal-world datasets and a range of graph and history embedding methods, we show\nTGN's adaptability, confirming its effectiveness in dynamic recommendation\nscenarios.","updated":1711269193000,"published":1711269193000,"authors":["Yejin Kim","Youngbin Lee","Vincent Yuan","Annika Lee","Yongjae Lee"],"comments":"Presented at the AAAI 2024 Workshop on Recommendation Ecosystems:\n  Modeling, Optimization and Incentive Design","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"216":{"arxiv_id":"2403.16100v1","url":"http:\/\/arxiv.org\/abs\/2403.16100v1","title":"Specifying Agent Ethics (Blue Sky Ideas)","summary":"We consider the question of what properties a Machine Ethics system should\nhave. This question is complicated by the existence of ethical dilemmas with no\nagreed upon solution. We provide an example to motivate why we do not believe\nfalling back on the elicitation of values from stakeholders is sufficient to\nguarantee correctness of such systems. We go on to define two broad categories\nof ethical property that have arisen in our own work and present a challenge to\nthe community to approach this question in a more systematic way.","updated":1711279963000,"published":1711279963000,"authors":["Louise A. Dennis","Michael Fisher"],"comments":"To appear in Coordination, Organizations, Institutions, Norms and\n  Ethics for Governance of Multi-Agent Systems 2024","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"217":{"arxiv_id":"2403.16107v1","url":"http:\/\/arxiv.org\/abs\/2403.16107v1","title":"Designing Upper-Body Gesture Interaction with and for People with Spinal\n  Muscular Atrophy in VR","summary":"Recent research proposed gaze-assisted gestures to enhance interaction within\nvirtual reality (VR), providing opportunities for people with motor impairments\nto experience VR. Compared to people with other motor impairments, those with\nSpinal Muscular Atrophy (SMA) exhibit enhanced distal limb mobility, providing\nthem with more design space. However, it remains unknown what gaze-assisted\nupper-body gestures people with SMA would want and be able to perform. We\nconducted an elicitation study in which 12 VR-experienced people with SMA\ndesigned upper-body gestures for 26 VR commands, and collected 312 user-defined\ngestures. Participants predominantly favored creating gestures with their\nhands. The type of tasks and participants' abilities influence their choice of\nbody parts for gesture design. Participants tended to enhance their body\ninvolvement and preferred gestures that required minimal physical effort, and\nwere aesthetically pleasing. Our research will contribute to creating better\ngesture-based input methods for people with motor impairments to interact with\nVR.","updated":1711281049000,"published":1711281049000,"authors":["Jingze Tian","Yingna Wang","Keye Yu","Liyi Xu","Junan Xie","Franklin Mingzhe Li","Yafeng Niu","Mingming Fan"],"comments":"Proceedings of the CHI Conference on Human Factors in Computing\n  Systems (CHI '24), May 11--16, 2024, Honolulu, HI, USA","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"218":{"arxiv_id":"2403.16190v1","url":"http:\/\/arxiv.org\/abs\/2403.16190v1","title":"Logic-based Explanations for Linear Support Vector Classifiers with\n  Reject Option","summary":"Support Vector Classifier (SVC) is a well-known Machine Learning (ML) model\nfor linear classification problems. It can be used in conjunction with a reject\noption strategy to reject instances that are hard to correctly classify and\ndelegate them to a specialist. This further increases the confidence of the\nmodel. Given this, obtaining an explanation of the cause of rejection is\nimportant to not blindly trust the obtained results. While most of the related\nwork has developed means to give such explanations for machine learning models,\nto the best of our knowledge none have done so for when reject option is\npresent. We propose a logic-based approach with formal guarantees on the\ncorrectness and minimality of explanations for linear SVCs with reject option.\nWe evaluate our approach by comparing it to Anchors, which is a heuristic\nalgorithm for generating explanations. Obtained results show that our proposed\nmethod gives shorter explanations with reduced time cost.","updated":1711293284000,"published":1711293284000,"authors":["Francisco Mateus Rocha Filho","Thiago Alves Rocha","Reginaldo Pereira Fernandes Ribeiro","Ajalmar R\u00eago da Rocha Neto"],"comments":"16 pages, submitted to BRACIS 2023 (Brazilian Conference on\n  Intelligent Systems), accepted version published in Intelligent Systems,\n  LNCS, vol 14195","categories":["cs.AI","cs.LG","cs.LO","I.2.4; I.2.6"],"primary_category":"cs.AI","doi":"10.1007\/978-3-031-45368-7_10","journal_ref":null,"peer_reviewed":true},"219":{"arxiv_id":"2403.16222v2","url":"http:\/\/arxiv.org\/abs\/2403.16222v2","title":"Cyber-Security Knowledge Graph Generation by Hierarchical Nonnegative\n  Matrix Factorization","summary":"Much of human knowledge in cybersecurity is encapsulated within the\never-growing volume of scientific papers. As this textual data continues to\nexpand, the importance of document organization methods becomes increasingly\ncrucial for extracting actionable insights hidden within large text datasets.\nKnowledge Graphs (KGs) serve as a means to store factual information in a\nstructured manner, providing explicit, interpretable knowledge that includes\ndomain-specific information from the cybersecurity scientific literature. One\nof the challenges in constructing a KG from scientific literature is the\nextraction of ontology from unstructured text. In this paper, we address this\ntopic and introduce a method for building a multi-modal KG by extracting\nstructured ontology from scientific papers. We demonstrate this concept in the\ncybersecurity domain. One modality of the KG represents observable information\nfrom the papers, such as the categories in which they were published or the\nauthors. The second modality uncovers latent (hidden) patterns of text\nextracted through hierarchical and semantic non-negative matrix factorization\n(NMF), such as named entities, topics or clusters, and keywords. We illustrate\nthis concept by consolidating more than two million scientific papers uploaded\nto arXiv into the cyber-domain, using hierarchical and semantic NMF, and by\nbuilding a cyber-domain-specific KG.","updated":1711466907000,"published":1711297805000,"authors":["Ryan Barron","Maksim E. Eren","Manish Bhattarai","Selma Wanna","Nicholas Solovyev","Kim Rasmussen","Boian S. Alexandrov","Charles Nicholas","Cynthia Matuszek"],"comments":"Accepted at IEEE ISDFS","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"220":{"arxiv_id":"2403.16289v1","url":"http:\/\/arxiv.org\/abs\/2403.16289v1","title":"Engineering Safety Requirements for Autonomous Driving with Large\n  Language Models","summary":"Changes and updates in the requirement artifacts, which can be frequent in\nthe automotive domain, are a challenge for SafetyOps. Large Language Models\n(LLMs), with their impressive natural language understanding and generating\ncapabilities, can play a key role in automatically refining and decomposing\nrequirements after each update. In this study, we propose a prototype of a\npipeline of prompts and LLMs that receives an item definition and outputs\nsolutions in the form of safety requirements. This pipeline also performs a\nreview of the requirement dataset and identifies redundant or contradictory\nrequirements. We first identified the necessary characteristics for performing\nHARA and then defined tests to assess an LLM's capability in meeting these\ncriteria. We used design science with multiple iterations and let experts from\ndifferent companies evaluate each cycle quantitatively and qualitatively.\nFinally, the prototype was implemented at a case company and the responsible\nteam evaluated its efficiency.","updated":1711312851000,"published":1711312851000,"authors":["Ali Nouri","Beatriz Cabrero-Daniel","Fredrik T\u00f6rner","H\u0227kan Sivencrona","Christian Berger"],"comments":"Accepted in 32nd IEEE International Requirements Engineering 2024\n  conference, Iceland","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"221":{"arxiv_id":"2403.16311v1","url":"http:\/\/arxiv.org\/abs\/2403.16311v1","title":"\"It Is Easy Using My Apps:\" Understanding Technology Use and Needs of\n  Adults with Down Syndrome","summary":"Assistive technologies for adults with Down syndrome (DS) need designs\ntailored to their specific technology requirements. While prior research has\nexplored technology design for individuals with intellectual disabilities,\nlittle is understood about the needs and expectations of adults with DS.\nAssistive technologies should leverage the abilities and interests of the\npopulation, while incorporating age- and context-considerate content. In this\nwork, we interviewed six adults with DS, seven parents of adults with DS, and\nthree experts in speech-language pathology, special education, and occupational\ntherapy to determine how technology could support adults with DS. In our\nthematic analysis, four main themes emerged, including (1) community vs. home\nsocial involvement; (2) misalignment of skill expectations between adults with\nDS and parents; (3) family limitations in technology support; and (4)\nconsiderations for technology development. Our findings extend prior literature\nby including the voices of adults with DS in how and when they use technology.","updated":1711319692000,"published":1711319692000,"authors":["Hailey L. Johnson","Audra Sterling","Bilge Mutlu"],"comments":"17 pages, 4 figures, to be published in ACM CHI 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642950","journal_ref":null,"peer_reviewed":true},"222":{"arxiv_id":"2403.16508v1","url":"http:\/\/arxiv.org\/abs\/2403.16508v1","title":"Return to Tradition: Learning Reliable Heuristics with Classical Machine\n  Learning","summary":"Current approaches for learning for planning have yet to achieve competitive\nperformance against classical planners in several domains, and have poor\noverall performance. In this work, we construct novel graph representations of\nlifted planning tasks and use the WL algorithm to generate features from them.\nThese features are used with classical machine learning methods which have up\nto 2 orders of magnitude fewer parameters and train up to 3 orders of magnitude\nfaster than the state-of-the-art deep learning for planning models. Our novel\napproach, WL-GOOSE, reliably learns heuristics from scratch and outperforms the\n$h^{\\text{FF}}$ heuristic in a fair competition setting. It also outperforms or\nties with LAMA on 4 out of 10 domains on coverage and 7 out of 10 domains on\nplan quality. WL-GOOSE is the first learning for planning model which achieves\nthese feats. Furthermore, we study the connections between our novel WL feature\ngeneration method, previous theoretically flavoured learning architectures, and\nDescription Logic Features for planning.","updated":1711352872000,"published":1711352872000,"authors":["Dillon Z. Chen","Felipe Trevizan","Sylvie Thi\u00e9baux"],"comments":"Extended version of ICAPS 2024 paper","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"223":{"arxiv_id":"2403.16524v1","url":"http:\/\/arxiv.org\/abs\/2403.16524v1","title":"Harnessing the power of LLMs for normative reasoning in MASs","summary":"Software agents, both human and computational, do not exist in isolation and\noften need to collaborate or coordinate with others to achieve their goals. In\nhuman society, social mechanisms such as norms ensure efficient functioning,\nand these techniques have been adopted by researchers in multi-agent systems\n(MAS) to create socially aware agents. However, traditional techniques have\nlimitations, such as operating in limited environments often using brittle\nsymbolic reasoning. The advent of Large Language Models (LLMs) offers a\npromising solution, providing a rich and expressive vocabulary for norms and\nenabling norm-capable agents that can perform a range of tasks such as norm\ndiscovery, normative reasoning and decision-making. This paper examines the\npotential of LLM-based agents to acquire normative capabilities, drawing on\nrecent Natural Language Processing (NLP) and LLM research. We present our\nvision for creating normative LLM agents. In particular, we discuss how the\nrecently proposed \"LLM agent\" approaches can be extended to implement such\nnormative LLM agents. We also highlight challenges in this emerging field. This\npaper thus aims to foster collaboration between MAS, NLP and LLM researchers in\norder to advance the field of normative agents.","updated":1711354141000,"published":1711354141000,"authors":["Bastin Tony Roy Savarimuthu","Surangika Ranathunga","Stephen Cranefield"],"comments":"12 pages, 1 figure, accepted to COINE 2024 workshop at AAMAS 2024\n  (https:\/\/coin-workshop.github.io\/coine-2024-auckland\/accepted_papers.html)","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"224":{"arxiv_id":"2403.16667v1","url":"http:\/\/arxiv.org\/abs\/2403.16667v1","title":"Deep Reinforcement Learning and Mean-Variance Strategies for Responsible\n  Portfolio Optimization","summary":"Portfolio optimization involves determining the optimal allocation of\nportfolio assets in order to maximize a given investment objective.\nTraditionally, some form of mean-variance optimization is used with the aim of\nmaximizing returns while minimizing risk, however, more recently, deep\nreinforcement learning formulations have been explored. Increasingly, investors\nhave demonstrated an interest in incorporating ESG objectives when making\ninvestment decisions, and modifications to the classical mean-variance\noptimization framework have been developed. In this work, we study the use of\ndeep reinforcement learning for responsible portfolio optimization, by\nincorporating ESG states and objectives, and provide comparisons against\nmodified mean-variance approaches. Our results show that deep reinforcement\nlearning policies can provide competitive performance against mean-variance\napproaches for responsible portfolio allocation across additive and\nmultiplicative utility functions of financial and ESG responsibility\nobjectives.","updated":1711368243000,"published":1711368243000,"authors":["Fernando Acero","Parisa Zehtabi","Nicolas Marchesotti","Michael Cashmore","Daniele Magazzeni","Manuela Veloso"],"comments":"Presented at the AAAI 2024 Workshop on AI in Finance for Social\n  Impact","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"225":{"arxiv_id":"2403.16750v1","url":"http:\/\/arxiv.org\/abs\/2403.16750v1","title":"All Artificial, Less Intelligence: GenAI through the Lens of Formal\n  Verification","summary":"Modern hardware designs have grown increasingly efficient and complex.\nHowever, they are often susceptible to Common Weakness Enumerations (CWEs).\nThis paper is focused on the formal verification of CWEs in a dataset of\nhardware designs written in SystemVerilog from Regenerative Artificial\nIntelligence (AI) powered by Large Language Models (LLMs). We applied formal\nverification to categorize each hardware design as vulnerable or CWE-free. This\ndataset was generated by 4 different LLMs and features a unique set of designs\nfor each of the 10 CWEs we target in our paper. We have associated the\nidentified vulnerabilities with CWE numbers for a dataset of 60,000 generated\nSystemVerilog Register Transfer Level (RTL) code. It was also found that most\nLLMs are not aware of any hardware CWEs; hence they are usually not considered\nwhen generating the hardware code. Our study reveals that approximately 60% of\nthe hardware designs generated by LLMs are prone to CWEs, posing potential\nsafety and security risks. The dataset could be ideal for training LLMs and\nMachine Learning (ML) algorithms to abstain from generating CWE-prone hardware\ndesigns.","updated":1711373004000,"published":1711373004000,"authors":["Deepak Narayan Gadde","Aman Kumar","Thomas Nalapat","Evgenii Rezunov","Fabio Cappellini"],"comments":"Published in DVCon U.S. 2024","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"226":{"arxiv_id":"2403.16808v2","url":"http:\/\/arxiv.org\/abs\/2403.16808v2","title":"Navigating the EU AI Act: A Methodological Approach to Compliance for\n  Safety-critical Products","summary":"In December 2023, the European Parliament provisionally agreed on the EU AI\nAct. This unprecedented regulatory framework for AI systems lays out guidelines\nto ensure the safety, legality, and trustworthiness of AI products. This paper\npresents a methodology for interpreting the EU AI Act requirements for\nhigh-risk AI systems by leveraging product quality models. We first propose an\nextended product quality model for AI systems, incorporating attributes\nrelevant to the Act not covered by current quality models. We map the Act\nrequirements to relevant quality attributes with the goal of refining them into\nmeasurable characteristics. We then propose a contract-based approach to derive\ntechnical requirements at the stakeholder level. This facilitates the\ndevelopment and assessment of AI systems that not only adhere to established\nquality standards, but also comply with the regulatory requirements outlined in\nthe Act for high-risk (including safety-critical) AI systems. We demonstrate\nthe applicability of this methodology on an exemplary automotive supply chain\nuse case, where several stakeholders interact to achieve EU AI Act compliance.","updated":1711443557000,"published":1711377138000,"authors":["J. Kelly","S. Zafar","L. Heidemann","J. Zacchi","D. Espinoza","N. Mata"],"comments":"To be published in: 2024 IEEE Conference on Artificial Intelligence\n  (CAI 2024)","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"227":{"arxiv_id":"2403.16824v1","url":"http:\/\/arxiv.org\/abs\/2403.16824v1","title":"On Policy Reuse: An Expressive Language for Representing and Executing\n  General Policies that Call Other Policies","summary":"Recently, a simple but powerful language for expressing and learning general\npolicies and problem decompositions (sketches) has been introduced in terms of\nrules defined over a set of Boolean and numerical features. In this work, we\nconsider three extensions of this language aimed at making policies and\nsketches more flexible and reusable: internal memory states, as in finite state\ncontrollers; indexical features, whose values are a function of the state and a\nnumber of internal registers that can be loaded with objects; and modules that\nwrap up policies and sketches and allow them to call each other by passing\nparameters. In addition, unlike general policies that select state transitions\nrather than ground actions, the new language allows for the selection of such\nactions. The expressive power of the resulting language for policies and\nsketches is illustrated through a number of examples.","updated":1711378134000,"published":1711378134000,"authors":["Blai Bonet","Dominik Drexler","Hector Geffner"],"comments":"ICAPS 2024","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"228":{"arxiv_id":"2403.16858v1","url":"http:\/\/arxiv.org\/abs\/2403.16858v1","title":"XAIport: A Service Framework for the Early Adoption of XAI in AI Model\n  Development","summary":"In this study, we propose the early adoption of Explainable AI (XAI) with a\nfocus on three properties: Quality of explanation, the explanation summaries\nshould be consistent across multiple XAI methods; Architectural Compatibility,\nfor effective integration in XAI, the architecture styles of both the XAI\nmethods and the models to be explained must be compatible with the framework;\nConfigurable operations, XAI explanations are operable, akin to machine\nlearning operations. Thus, an explanation for AI models should be reproducible\nand tractable to be trustworthy. We present XAIport, a framework of XAI\nmicroservices encapsulated into Open APIs to deliver early explanations as\nobservation for learning model quality assurance. XAIport enables configurable\nXAI operations along with machine learning development. We quantify the\noperational costs of incorporating XAI with three cloud computer vision\nservices on Microsoft Azure Cognitive Services, Google Cloud Vertex AI, and\nAmazon Rekognition. Our findings show comparable operational costs between XAI\nand traditional machine learning, with XAIport significantly improving both\ncloud AI model performance and explanation stability.","updated":1711380126000,"published":1711380126000,"authors":["Zerui Wang","Yan Liu","Abishek Arumugam Thiruselvi","Abdelwahab Hamou-Lhadj"],"comments":"Accepted at the ICSE'24 conference, NIER track","categories":["cs.AI"],"primary_category":"cs.AI","doi":"10.1145\/3639476.3639759","journal_ref":null,"peer_reviewed":true},"229":{"arxiv_id":"2403.17215v1","url":"http:\/\/arxiv.org\/abs\/2403.17215v1","title":"An Undergraduate Consortium for Addressing the Leaky Pipeline to\n  Computing Research","summary":"Despite an increasing number of successful interventions designed to broaden\nparticipation in computing research, there is still significant attrition among\nhistorically marginalized groups in the computing research pipeline. This\nexperience report describes a first-of-its-kind Undergraduate Consortium (UC)\nthat addresses this challenge by empowering students with a culmination of\ntheir undergraduate research in a conference setting. The UC, conducted at the\nAAAI Conference on Artificial Intelligence (AAAI), aims to broaden\nparticipation in the AI research community by recruiting students, particularly\nthose from historically marginalized groups, supporting them with mentorship,\nadvising, and networking as an accelerator toward graduate school, AI research,\nand their scientific identity. This paper presents our program design, inspired\nby a rich set of evidence-based practices, and a preliminary evaluation of the\nfirst years that points to the UC achieving many of its desired outcomes. We\nconclude by discussing insights to improve our program and expand to other\ncomputing communities.","updated":1711403023000,"published":1711403023000,"authors":["James Boerkoel","Mehmet Ergezer"],"comments":"Presented at SIGCSE TS 2023","categories":["cs.CY"],"primary_category":"cs.CY","doi":"10.1145\/3545945.3569841","journal_ref":"In Proceedings of the 54th ACM Technical Symposium on Computer\n  Science Education V. 1 (SIGCSE 2023)","peer_reviewed":true},"230":{"arxiv_id":"2403.17312v1","url":"http:\/\/arxiv.org\/abs\/2403.17312v1","title":"ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV\n  Caching","summary":"The Transformer architecture has significantly advanced natural language\nprocessing (NLP) and has been foundational in developing large language models\n(LLMs) such as LLaMA and OPT, which have come to dominate a broad range of NLP\ntasks. Despite their superior accuracy, LLMs present unique challenges in\npractical inference, concerning the compute and memory-intensive nature. Thanks\nto the autoregressive characteristic of LLM inference, KV caching for the\nattention layers in Transformers can effectively accelerate LLM inference by\nsubstituting quadratic-complexity computation with linear-complexity memory\naccesses. Yet, this approach requires increasing memory as demand grows for\nprocessing longer sequences. The overhead leads to reduced throughput due to\nI\/O bottlenecks and even out-of-memory errors, particularly on\nresource-constrained systems like a single commodity GPU. In this paper, we\npropose ALISA, a novel algorithm-system co-design solution to address the\nchallenges imposed by KV caching. On the algorithm level, ALISA prioritizes\ntokens that are most important in generating a new token via a Sparse Window\nAttention (SWA) algorithm. SWA introduces high sparsity in attention layers and\nreduces the memory footprint of KV caching at negligible accuracy loss. On the\nsystem level, ALISA employs three-phase token-level dynamical scheduling and\noptimizes the trade-off between caching and recomputation, thus maximizing the\noverall performance in resource-constrained systems. In a single GPU-CPU\nsystem, we demonstrate that under varying workloads, ALISA improves the\nthroughput of baseline systems such as FlexGen and vLLM by up to 3X and 1.9X,\nrespectively.","updated":1711417594000,"published":1711417594000,"authors":["Youpeng Zhao","Di Wu","Jun Wang"],"comments":"ISCA 2024","categories":["cs.AI","cs.LG","cs.PF"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"231":{"arxiv_id":"2403.17358v1","url":"http:\/\/arxiv.org\/abs\/2403.17358v1","title":"Addressing Myopic Constrained POMDP Planning with Recursive Dual Ascent","summary":"Lagrangian-guided Monte Carlo tree search with global dual ascent has been\napplied to solve large constrained partially observable Markov decision\nprocesses (CPOMDPs) online. In this work, we demonstrate that these global dual\nparameters can lead to myopic action selection during exploration, ultimately\nleading to suboptimal decision making. To address this, we introduce\nhistory-dependent dual variables that guide local action selection and are\noptimized with recursive dual ascent. We empirically compare the performance of\nour approach on a motivating toy example and two large CPOMDPs, demonstrating\nimproved exploration, and ultimately, safer outcomes.","updated":1711424793000,"published":1711424793000,"authors":["Paula Stocco","Suhas Chundi","Arec Jamgochian","Mykel J. Kochenderfer"],"comments":"Accepted to the 2024 International Conference on Automated Planning\n  and Scheduling (ICAPS)","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"232":{"arxiv_id":"2403.17419v1","url":"http:\/\/arxiv.org\/abs\/2403.17419v1","title":"AI Safety: Necessary, but insufficient and possibly problematic","summary":"This article critically examines the recent hype around AI safety. We first\nstart with noting the nature of the AI safety hype as being dominated by\ngovernments and corporations, and contrast it with other avenues within AI\nresearch on advancing social good. We consider what 'AI safety' actually means,\nand outline the dominant concepts that the digital footprint of AI safety\naligns with. We posit that AI safety has a nuanced and uneasy relationship with\ntransparency and other allied notions associated with societal good, indicating\nthat it is an insufficient notion if the goal is that of societal good in a\nbroad sense. We note that the AI safety debate has already influenced some\nregulatory efforts in AI, perhaps in not so desirable directions. We also share\nour concerns on how AI safety may normalize AI that advances structural harm\nthrough providing exploitative and harmful AI with a veneer of safety.","updated":1711433922000,"published":1711433922000,"authors":["Deepak P"],"comments":"AI & Soc (2024)","categories":["cs.AI","cs.CY"],"primary_category":"cs.AI","doi":"10.1007\/s00146-024-01899-y","journal_ref":null,"peer_reviewed":true},"233":{"arxiv_id":"2403.17532v1","url":"http:\/\/arxiv.org\/abs\/2403.17532v1","title":"KC-GenRe: A Knowledge-constrained Generative Re-ranking Method Based on\n  Large Language Models for Knowledge Graph Completion","summary":"The goal of knowledge graph completion (KGC) is to predict missing facts\namong entities. Previous methods for KGC re-ranking are mostly built on\nnon-generative language models to obtain the probability of each candidate.\nRecently, generative large language models (LLMs) have shown outstanding\nperformance on several tasks such as information extraction and dialog systems.\nLeveraging them for KGC re-ranking is beneficial for leveraging the extensive\npre-trained knowledge and powerful generative capabilities. However, it may\nencounter new problems when accomplishing the task, namely mismatch,\nmisordering and omission. To this end, we introduce KC-GenRe, a\nknowledge-constrained generative re-ranking method based on LLMs for KGC. To\novercome the mismatch issue, we formulate the KGC re-ranking task as a\ncandidate identifier sorting generation problem implemented by generative LLMs.\nTo tackle the misordering issue, we develop a knowledge-guided interactive\ntraining method that enhances the identification and ranking of candidates. To\naddress the omission issue, we design a knowledge-augmented constrained\ninference method that enables contextual prompting and controlled generation,\nso as to obtain valid rankings. Experimental results show that KG-GenRe\nachieves state-of-the-art performance on four datasets, with gains of up to\n6.7% and 7.7% in the MRR and Hits@1 metric compared to previous methods, and\n9.0% and 11.1% compared to that without re-ranking. Extensive analysis\ndemonstrates the effectiveness of components in KG-GenRe.","updated":1711445819000,"published":1711445819000,"authors":["Yilin Wang","Minghao Hu","Zhen Huang","Dongsheng Li","Dong Yang","Xicheng Lu"],"comments":"This paper has been accepted for publication in the proceedings of\n  LREC-COLING 2024","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"234":{"arxiv_id":"2403.17632v1","url":"http:\/\/arxiv.org\/abs\/2403.17632v1","title":"Data-driven Energy Consumption Modelling for Electric Micromobility\n  using an Open Dataset","summary":"The escalating challenges of traffic congestion and environmental degradation\nunderscore the critical importance of embracing E-Mobility solutions in urban\nspaces. In particular, micro E-Mobility tools such as E-scooters and E-bikes,\nplay a pivotal role in this transition, offering sustainable alternatives for\nurban commuters. However, the energy consumption patterns for these tools are a\ncritical aspect that impacts their effectiveness in real-world scenarios and is\nessential for trip planning and boosting user confidence in using these. To\nthis effect, recent studies have utilised physical models customised for\nspecific mobility tools and conditions, but these models struggle with\ngeneralization and effectiveness in real-world scenarios due to a notable\nabsence of open datasets for thorough model evaluation and verification. To\nfill this gap, our work presents an open dataset, collected in Dublin, Ireland,\nspecifically designed for energy modelling research related to E-Scooters and\nE-Bikes. Furthermore, we provide a comprehensive analysis of energy consumption\nmodelling based on the dataset using a set of representative machine learning\nalgorithms and compare their performance against the contemporary mathematical\nmodels as a baseline. Our results demonstrate a notable advantage for\ndata-driven models in comparison to the corresponding mathematical models for\nestimating energy consumption. Specifically, data-driven models outperform\nphysical models in accuracy by up to 83.83% for E-Bikes and 82.16% for\nE-Scooters based on an in-depth analysis of the dataset under certain\nassumptions.","updated":1711454885000,"published":1711454885000,"authors":["Yue Ding","Sen Yan","Maqsood Hussain Shah","Hongyuan Fang","Ji Li","Mingming Liu"],"comments":"7 pages, 5 figures, 4 tables. This manuscript has been accepted by\n  the IEEE ITEC 2024","categories":["cs.AI","cs.CY","cs.LG"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"235":{"arxiv_id":"2403.17693v1","url":"http:\/\/arxiv.org\/abs\/2403.17693v1","title":"ExpressEdit: Video Editing with Natural Language and Sketching","summary":"Informational videos serve as a crucial source for explaining conceptual and\nprocedural knowledge to novices and experts alike. When producing informational\nvideos, editors edit videos by overlaying text\/images or trimming footage to\nenhance the video quality and make it more engaging. However, video editing can\nbe difficult and time-consuming, especially for novice video editors who often\nstruggle with expressing and implementing their editing ideas. To address this\nchallenge, we first explored how multimodality$-$natural language (NL) and\nsketching, which are natural modalities humans use for expression$-$can be\nutilized to support video editors in expressing video editing ideas. We\ngathered 176 multimodal expressions of editing commands from 10 video editors,\nwhich revealed the patterns of use of NL and sketching in describing edit\nintents. Based on the findings, we present ExpressEdit, a system that enables\nediting videos via NL text and sketching on the video frame. Powered by LLM and\nvision models, the system interprets (1) temporal, (2) spatial, and (3)\noperational references in an NL command and spatial references from sketching.\nThe system implements the interpreted edits, which then the user can iterate\non. An observational study (N=10) showed that ExpressEdit enhanced the ability\nof novice video editors to express and implement their edit ideas. The system\nallowed participants to perform edits more efficiently and generate more ideas\nby generating edits based on user's multimodal edit commands and supporting\niterations on the editing commands. This work offers insights into the design\nof future multimodal interfaces and AI-based pipelines for video editing.","updated":1711460061000,"published":1711460061000,"authors":["Bekzat Tilekbay","Saelyne Yang","Michal Lewkowicz","Alex Suryapranata","Juho Kim"],"comments":"22 pages, 5 figures, to be published in ACM IUI 2024","categories":["cs.HC","cs.AI"],"primary_category":"cs.HC","doi":"10.1145\/3640543.3645164","journal_ref":null,"peer_reviewed":true},"236":{"arxiv_id":"2403.17784v1","url":"http:\/\/arxiv.org\/abs\/2403.17784v1","title":"SciCapenter: Supporting Caption Composition for Scientific Figures with\n  Machine-Generated Captions and Ratings","summary":"Crafting effective captions for figures is important. Readers heavily depend\non these captions to grasp the figure's message. However, despite a\nwell-developed set of AI technologies for figures and captions, these have\nrarely been tested for usefulness in aiding caption writing. This paper\nintroduces SciCapenter, an interactive system that puts together cutting-edge\nAI technologies for scientific figure captions to aid caption composition.\nSciCapenter generates a variety of captions for each figure in a scholarly\narticle, providing scores and a comprehensive checklist to assess caption\nquality across multiple critical aspects, such as helpfulness, OCR mention, key\ntakeaways, and visual properties reference. Users can directly edit captions in\nSciCapenter, resubmit for revised evaluations, and iteratively refine them. A\nuser study with Ph.D. students indicates that SciCapenter significantly lowers\nthe cognitive load of caption writing. Participants' feedback further offers\nvaluable design insights for future systems aiming to enhance caption writing.","updated":1711466174000,"published":1711466174000,"authors":["Ting-Yao Hsu","Chieh-Yang Huang","Shih-Hong Huang","Ryan Rossi","Sungchul Kim","Tong Yu","C. Lee Giles","Ting-Hao K. Huang"],"comments":"CHI EA '24: Extended Abstracts of the 2024 CHI Conference on Human\n  Factors in Computing Systems","categories":["cs.HC","cs.AI"],"primary_category":"cs.HC","doi":"10.1145\/3613905.3650738","journal_ref":null,"peer_reviewed":true},"237":{"arxiv_id":"2403.17807v1","url":"http:\/\/arxiv.org\/abs\/2403.17807v1","title":"Towards Inclusive Video Commenting: Introducing Signmaku for the Deaf\n  and Hard-of-Hearing","summary":"Previous research underscored the potential of danmaku--a text-based\ncommenting feature on videos--in engaging hearing audiences. Yet, for many Deaf\nand hard-of-hearing (DHH) individuals, American Sign Language (ASL) takes\nprecedence over English. To improve inclusivity, we introduce \"Signmaku,\" a new\ncommenting mechanism that uses ASL, serving as a sign language counterpart to\ndanmaku. Through a need-finding study (N=12) and a within-subject experiment\n(N=20), we evaluated three design styles: real human faces, cartoon-like\nfigures, and robotic representations. The results showed that cartoon-like\nsignmaku not only entertained but also encouraged participants to create and\nshare ASL comments, with fewer privacy concerns compared to the other designs.\nConversely, the robotic representations faced challenges in accurately\ndepicting hand movements and facial expressions, resulting in higher cognitive\ndemands on users. Signmaku featuring real human faces elicited the lowest\ncognitive load and was the most comprehensible among all three types. Our\nfindings offered novel design implications for leveraging generative AI to\ncreate signmaku comments, enriching co-learning experiences for DHH\nindividuals.","updated":1711467907000,"published":1711467907000,"authors":["Si Chen","Haocong Cheng","Jason Situ","Desir\u00e9e Kirst","Suzy Su","Saumya Malhotra","Lawrence Angrave","Qi Wang","Yun Huang"],"comments":"14 pages, CHI 2024","categories":["cs.HC","F.2.2; I.2.7"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"238":{"arxiv_id":"2403.17826v1","url":"http:\/\/arxiv.org\/abs\/2403.17826v1","title":"On the Computational Complexity of Stackelberg Planning and\n  Meta-Operator Verification: Technical Report","summary":"Stackelberg planning is a recently introduced single-turn two-player\nadversarial planning model, where two players are acting in a joint classical\nplanning task, the objective of the first player being hampering the second\nplayer from achieving its goal. This places the Stackelberg planning problem\nsomewhere between classical planning and general combinatorial two-player\ngames. But, where exactly? All investigations of Stackelberg planning so far\nfocused on practical aspects. We close this gap by conducting the first\ntheoretical complexity analysis of Stackelberg planning. We show that in\ngeneral Stackelberg planning is actually no harder than classical planning.\nUnder a polynomial plan-length restriction, however, Stackelberg planning is a\nlevel higher up in the polynomial complexity hierarchy, suggesting that\ncompilations into classical planning come with a worst-case exponential\nplan-length increase. In attempts to identify tractable fragments, we further\nstudy its complexity under various planning task restrictions, showing that\nStackelberg planning remains intractable where classical planning is not. We\nfinally inspect the complexity of meta-operator verification, a problem that\nhas been recently connected to Stackelberg planning.","updated":1711469193000,"published":1711469193000,"authors":["Gregor Behnke","Marcel Steinmetz"],"comments":"Presented at ICAPS24","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"239":{"arxiv_id":"2403.17873v1","url":"http:\/\/arxiv.org\/abs\/2403.17873v1","title":"Addressing Social Misattributions of Large Language Models: An\n  HCXAI-based Approach","summary":"Human-centered explainable AI (HCXAI) advocates for the integration of social\naspects into AI explanations. Central to the HCXAI discourse is the Social\nTransparency (ST) framework, which aims to make the socio-organizational\ncontext of AI systems accessible to their users. In this work, we suggest\nextending the ST framework to address the risks of social misattributions in\nLarge Language Models (LLMs), particularly in sensitive areas like mental\nhealth. In fact LLMs, which are remarkably capable of simulating roles and\npersonas, may lead to mismatches between designers' intentions and users'\nperceptions of social attributes, risking to promote emotional manipulation and\ndangerous behaviors, cases of epistemic injustice, and unwarranted trust. To\naddress these issues, we propose enhancing the ST framework with a fifth\n'W-question' to clarify the specific social attributions assigned to LLMs by\nits designers and users. This addition aims to bridge the gap between LLM\ncapabilities and user perceptions, promoting the ethically responsible\ndevelopment and use of LLM-based technology.","updated":1711472562000,"published":1711472562000,"authors":["Andrea Ferrario","Alberto Termine","Alessandro Facchini"],"comments":"Extended version of the manuscript accepted for the ACM CHI Workshop\n  on Human-Centered Explainable AI 2024 (HCXAI24)","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"240":{"arxiv_id":"2403.17911v1","url":"http:\/\/arxiv.org\/abs\/2403.17911v1","title":"Domain-Specific Evaluation Strategies for AI in Journalism","summary":"News organizations today rely on AI tools to increase efficiency and\nproductivity across various tasks in news production and distribution. These\ntools are oriented towards stakeholders such as reporters, editors, and\nreaders. However, practitioners also express reservations around adopting AI\ntechnologies into the newsroom, due to the technical and ethical challenges\ninvolved in evaluating AI technology and its return on investments. This is to\nsome extent a result of the lack of domain-specific strategies to evaluate AI\nmodels and applications. In this paper, we consider different aspects of AI\nevaluation (model outputs, interaction, and ethics) that can benefit from\ndomain-specific tailoring, and suggest examples of how journalistic\nconsiderations can lead to specialized metrics or strategies. In doing so, we\nlay out a potential framework to guide AI evaluation in journalism, such as\nseen in other disciplines (e.g. law, healthcare). We also consider directions\nfor future work, as well as how our approach might generalize to other domains.","updated":1711475245000,"published":1711475245000,"authors":["Sachita Nishal","Charlotte Li","Nicholas Diakopoulos"],"comments":"Accepted to the Workshop on Evaluating AI at the ACM CHI conference\n  on Human Factors in Computing Systems","categories":["cs.CY","I.2.1; H.5; K.4"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"241":{"arxiv_id":"2403.17914v1","url":"http:\/\/arxiv.org\/abs\/2403.17914v1","title":"Hierarchical Multi-label Classification for Fine-level Event Extraction\n  from Aviation Accident Reports","summary":"A large volume of accident reports is recorded in the aviation domain, which\ngreatly values improving aviation safety. To better use those reports, we need\nto understand the most important events or impact factors according to the\naccident reports. However, the increasing number of accident reports requires\nlarge efforts from domain experts to label those reports. In order to make the\nlabeling process more efficient, many researchers have started developing\nalgorithms to identify the underlying events from accident reports\nautomatically. This article argues that we can identify the events more\naccurately by leveraging the event taxonomy. More specifically, we consider the\nproblem a hierarchical classification task where we first identify the\ncoarse-level information and then predict the fine-level information. We\nachieve this hierarchical classification process by incorporating a novel\nhierarchical attention module into BERT. To further utilize the information\nfrom event taxonomy, we regularize the proposed model according to the\nrelationship and distribution among labels. The effectiveness of our framework\nis evaluated with the data collected by National Transportation Safety Board\n(NTSB). It has been shown that fine-level prediction accuracy is highly\nimproved, and the regularization term can be beneficial to the rare event\nidentification problem.","updated":1711475466000,"published":1711475466000,"authors":["Xinyu Zhao","Hao Yan","Yongming Liu"],"comments":"Accepted in INFORMS Journal of Data Science","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"242":{"arxiv_id":"2403.18120v1","url":"http:\/\/arxiv.org\/abs\/2403.18120v1","title":"Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with\n  Autoformalization","summary":"Large language models (LLM), such as Google's Minerva and OpenAI's GPT\nfamilies, are becoming increasingly capable of solving mathematical\nquantitative reasoning problems. However, they still make unjustified logical\nand computational errors in their reasoning steps and answers. In this paper,\nwe leverage the fact that if the training corpus of LLMs contained sufficiently\nmany examples of formal mathematics (e.g. in Isabelle, a formal theorem proving\nenvironment), they can be prompted to translate i.e. autoformalize informal\nmathematical statements into formal Isabelle code -- which can be verified\nautomatically for internal consistency. This provides a mechanism to\nautomatically reject solutions whose formalized versions are inconsistent\nwithin themselves or with the formalized problem statement. We evaluate our\nmethod on GSM8K, MATH and MultiArith datasets and demonstrate that our approach\nprovides a consistently better heuristic than vanilla majority voting -- the\npreviously best method to identify correct answers, by more than 12% on GSM8K.\nIn our experiments it improves results consistently across all datasets and LLM\nmodel sizes. The code can be found at https:\/\/github.com\/jinpz\/dtv.","updated":1711490473000,"published":1711490473000,"authors":["Jin Peng Zhou","Charles Staats","Wenda Li","Christian Szegedy","Kilian Q. Weinberger","Yuhuai Wu"],"comments":"ICLR 2024","categories":["cs.AI","cs.CL","cs.LG"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"243":{"arxiv_id":"2403.18145v1","url":"http:\/\/arxiv.org\/abs\/2403.18145v1","title":"A Real-Time Rescheduling Algorithm for Multi-robot Plan Execution","summary":"One area of research in multi-agent path finding is to determine how\nreplanning can be efficiently achieved in the case of agents being delayed\nduring execution. One option is to reschedule the passing order of agents,\ni.e., the sequence in which agents visit the same location. In response, we\npropose Switchable-Edge Search (SES), an A*-style algorithm designed to find\noptimal passing orders. We prove the optimality of SES and evaluate its\nefficiency via simulations. The best variant of SES takes less than 1 second\nfor small- and medium-sized problems and runs up to 4 times faster than\nbaselines for large-sized problems.","updated":1711494641000,"published":1711494641000,"authors":["Ying Feng","Adittyo Paul","Zhe Chen","Jiaoyang Li"],"comments":"ICAPS 2024","categories":["cs.AI","cs.MA","cs.RO"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"244":{"arxiv_id":"2403.18160v1","url":"http:\/\/arxiv.org\/abs\/2403.18160v1","title":"Eternagram: Probing Player Attitudes in Alternate Climate Scenarios\n  Through a ChatGPT-Driven Text Adventure","summary":"Conventional methods of assessing attitudes towards climate change are\nlimited in capturing authentic opinions, primarily stemming from a lack of\ncontext-specific assessment strategies and an overreliance on simplistic\nsurveys. Game-based Assessments (GBA) have demonstrated the ability to overcome\nthese issues by immersing participants in engaging gameplay within carefully\ncrafted, scenario-based environments. Concurrently, advancements in AI and\nNatural Language Processing (NLP) show promise in enhancing the gamified\ntesting environment, achieving this by generating context-aware, human-like\ndialogues that contribute to a more natural and effective assessment. Our study\nintroduces a new technique for probing climate change attitudes by actualizing\na GPT-driven chatbot system in harmony with a game design depicting a\nfuturistic climate scenario. The correlation analysis reveals an assimilation\neffect, where players' post-game climate awareness tends to align with their\nin-game perceptions. Key predictors of pro-climate attitudes are identified as\ntraits like 'Openness' and 'Agreeableness', and a preference for democratic\nvalues.","updated":1711497289000,"published":1711497289000,"authors":["Suifang Zhou","Latisha Besariani Hendra","Qinshi Zhang","Jussi Holopainen","RAY LC"],"comments":"22 pages, 6 figures, Accepted by CHI Conference on Human Factors in\n  Computing Systems 2024","categories":["cs.HC","H.5.2"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"245":{"arxiv_id":"2403.18173v1","url":"http:\/\/arxiv.org\/abs\/2403.18173v1","title":"LLMs in HCI Data Work: Bridging the Gap Between Information Retrieval\n  and Responsible Research Practices","summary":"Efficient and accurate information extraction from scientific papers is\nsignificant in the rapidly developing human-computer interaction research in\nthe literature review process. Our paper introduces and analyses a new\ninformation retrieval system using state-of-the-art Large Language Models\n(LLMs) in combination with structured text analysis techniques to extract\nexperimental data from HCI literature, emphasizing key elements. Then We\nanalyze the challenges and risks of using LLMs in the world of research. We\nperformed a comprehensive analysis on our conducted dataset, which contained\nthe specified information of 300 CHI 2020-2022 papers, to evaluate the\nperformance of the two large language models, GPT-3.5 (text-davinci-003) and\nLlama-2-70b, paired with structured text analysis techniques. The GPT-3.5 model\ngains an accuracy of 58\\% and a mean absolute error of 7.00. In contrast, the\nLlama2 model indicates an accuracy of 56\\% with a mean absolute error of 7.63.\nThe ability to answer questions was also included in the system in order to\nwork with streamlined data. By evaluating the risks and opportunities presented\nby LLMs, our work contributes to the ongoing dialogue on establishing\nmethodological validity and ethical guidelines for LLM use in HCI data work.","updated":1711501269000,"published":1711501269000,"authors":["Neda Taghizadeh Serajeh","Iman Mohammadi","Vittorio Fuccella","Mattia De Rosa"],"comments":"5 pages, CHI2024 Workshop on LLMs as Research Tools: Applications and\n  Evaluations in HCI Data Work","categories":["cs.HC","cs.IR"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"246":{"arxiv_id":"2403.18203v1","url":"http:\/\/arxiv.org\/abs\/2403.18203v1","title":"EndToEndML: An Open-Source End-to-End Pipeline for Machine Learning\n  Applications","summary":"Artificial intelligence (AI) techniques are widely applied in the life\nsciences. However, applying innovative AI techniques to understand and\ndeconvolute biological complexity is hindered by the learning curve for life\nscience scientists to understand and use computing languages. An open-source,\nuser-friendly interface for AI models, that does not require programming skills\nto analyze complex biological data will be extremely valuable to the\nbioinformatics community. With easy access to different sequencing technologies\nand increased interest in different 'omics' studies, the number of biological\ndatasets being generated has increased and analyzing these high-throughput\ndatasets is computationally demanding. The majority of AI libraries today\nrequire advanced programming skills as well as machine learning, data\npreprocessing, and visualization skills. In this research, we propose a\nweb-based end-to-end pipeline that is capable of preprocessing, training,\nevaluating, and visualizing machine learning (ML) models without manual\nintervention or coding expertise. By integrating traditional machine learning\nand deep neural network models with visualizations, our library assists in\nrecognizing, classifying, clustering, and predicting a wide range of\nmulti-modal, multi-sensor datasets, including images, languages, and\none-dimensional numerical data, for drug discovery, pathogen classification,\nand medical diagnostics.","updated":1711506278000,"published":1711506278000,"authors":["Nisha Pillai","Athish Ram Das","Moses Ayoola","Ganga Gireesan","Bindu Nanduri","Mahalingam Ramkumar"],"comments":"2024 7th International Conference on Information and Computer\n  Technologies (ICICT)","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"247":{"arxiv_id":"2403.18278v1","url":"http:\/\/arxiv.org\/abs\/2403.18278v1","title":"Identification and Uses of Deep Learning Backbones via Pattern Mining","summary":"Deep learning is extensively used in many areas of data mining as a black-box\nmethod with impressive results. However, understanding the core mechanism of\nhow deep learning makes predictions is a relatively understudied problem. Here\nwe explore the notion of identifying a backbone of deep learning for a given\ngroup of instances. A group here can be instances of the same class or even\nmisclassified instances of the same class. We view each instance for a given\ngroup as activating a subset of neurons and attempt to find a subgraph of\nneurons associated with a given concept\/group. We formulate this problem as a\nset cover style problem and show it is intractable and presents a highly\nconstrained integer linear programming (ILP) formulation. As an alternative, we\nexplore a coverage-based heuristic approach related to pattern mining, and show\nit converges to a Pareto equilibrium point of the ILP formulation.\nExperimentally we explore these backbones to identify mistakes and improve\nperformance, explanation, and visualization. We demonstrate application-based\nresults using several challenging data sets, including Bird Audio Detection\n(BAD) Challenge and Labeled Faces in the Wild (LFW), as well as the classic\nMNIST data.","updated":1711520019000,"published":1711520019000,"authors":["Michael Livanos","Ian Davidson"],"comments":"9 pages, 6 figures, published SIAM SDM24","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"248":{"arxiv_id":"2403.18331v1","url":"http:\/\/arxiv.org\/abs\/2403.18331v1","title":"Neighbor-Environment Observer: An Intelligent Agent for Immersive\n  Working Companionship","summary":"Human-computer symbiosis is a crucial direction for the development of\nartificial intelligence. As intelligent systems become increasingly prevalent\nin our work and personal lives, it is important to develop strategies to\nsupport users across physical and virtual environments. While technological\nadvances in personal digital devices, such as personal computers and virtual\nreality devices, can provide immersive experiences, they can also disrupt\nusers' awareness of their surroundings and enhance the frustration caused by\ndisturbances. In this paper, we propose a joint observation strategy for\nartificial agents to support users across virtual and physical environments. We\nintroduce a prototype system, neighbor-environment observer (NEO), that\nutilizes non-invasive sensors to assist users in dealing with disruptions to\ntheir immersive experience. System experiments evaluate NEO from different\nperspectives and demonstrate the effectiveness of the joint observation\nstrategy. A user study is conducted to evaluate its usability. The results show\nthat NEO could lessen users' workload with the learned user preference. We\nsuggest that the proposed strategy can be applied to various smart home\nscenarios.","updated":1711527105000,"published":1711527105000,"authors":["Zhe Sun","Qixuan Liang","Meng Wang","Zhenliang Zhang"],"comments":"UIST 2023","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"249":{"arxiv_id":"2403.18338v1","url":"http:\/\/arxiv.org\/abs\/2403.18338v1","title":"mALBERT: Is a Compact Multilingual BERT Model Still Worth It?","summary":"Within the current trend of Pretained Language Models (PLM), emerge more and\nmore criticisms about the ethical andecological impact of such models. In this\narticle, considering these critical remarks, we propose to focus on\nsmallermodels, such as compact models like ALBERT, which are more ecologically\nvirtuous than these PLM. However,PLMs enable huge breakthroughs in Natural\nLanguage Processing tasks, such as Spoken and Natural LanguageUnderstanding,\nclassification, Question--Answering tasks. PLMs also have the advantage of\nbeing multilingual, and,as far as we know, a multilingual version of compact\nALBERT models does not exist. Considering these facts, wepropose the free\nrelease of the first version of a multilingual compact ALBERT model,\npre-trained using Wikipediadata, which complies with the ethical aspect of such\na language model. We also evaluate the model against classicalmultilingual PLMs\nin classical NLP tasks. Finally, this paper proposes a rare study on the\nsubword tokenizationimpact on language performances.","updated":1711527928000,"published":1711527928000,"authors":["Christophe Servan","Sahar Ghannay","Sophie Rosset"],"comments":"The 2024 Joint International Conference on Computational Linguistics,\n  Language Resources and Evaluation, May 2024, Torino, Italy","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true},"250":{"arxiv_id":"2403.18433v1","url":"http:\/\/arxiv.org\/abs\/2403.18433v1","title":"iFace: Hand-Over-Face Gesture Recognition Leveraging Impedance Sensing","summary":"Hand-over-face gestures can provide important implicit interactions during\nconversations, such as frustration or excitement. However, in situations where\ninterlocutors are not visible, such as phone calls or textual communication,\nthe potential meaning contained in the hand-over-face gestures is lost. In this\nwork, we present iFace, an unobtrusive, wearable impedance-sensing solution for\nrecognizing different hand-over-face gestures. In contrast to most existing\nworks, iFace does not require the placement of sensors on the user's face or\nhands. Instead, we proposed a novel sensing configuration, the shoulders, which\nremains invisible to both the user and outside observers. The system can\nmonitor the shoulder-to-shoulder impedance variation caused by gestures through\nelectrodes attached to each shoulder. We evaluated iFace in a user study with\neight participants, collecting six kinds of hand-over-face gestures with\ndifferent meanings. Using a convolutional neural network and a user-dependent\nclassification, iFace reaches 82.58 \\% macro F1 score. We discuss potential\napplication scenarios of iFace as an implicit interaction interface.","updated":1711536008000,"published":1711536008000,"authors":["Mengxi Liu","Hymalai Bello","Bo Zhou","Paul Lukowicz","Jakob Karolus"],"comments":"Accepted by Augmented Humans 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"251":{"arxiv_id":"2403.18957v1","url":"http:\/\/arxiv.org\/abs\/2403.18957v1","title":"Moderating Illicit Online Image Promotion for Unsafe User-Generated\n  Content Games Using Large Vision-Language Models","summary":"Online user-generated content games (UGCGs) are increasingly popular among\nchildren and adolescents for social interaction and more creative online\nentertainment. However, they pose a heightened risk of exposure to explicit\ncontent, raising growing concerns for the online safety of children and\nadolescents. Despite these concerns, few studies have addressed the issue of\nillicit image-based promotions of unsafe UGCGs on social media, which can\ninadvertently attract young users. This challenge arises from the difficulty of\nobtaining comprehensive training data for UGCG images and the unique nature of\nthese images, which differ from traditional unsafe content. In this work, we\ntake the first step towards studying the threat of illicit promotions of unsafe\nUGCGs. We collect a real-world dataset comprising 2,924 images that display\ndiverse sexually explicit and violent content used to promote UGCGs by their\ngame creators. Our in-depth studies reveal a new understanding of this problem\nand the urgent need for automatically flagging illicit UGCG promotions. We\nadditionally create a cutting-edge system, UGCG-Guard, designed to aid social\nmedia platforms in effectively identifying images used for illicit UGCG\npromotions. This system leverages recently introduced large vision-language\nmodels (VLMs) and employs a novel conditional prompting strategy for zero-shot\ndomain adaptation, along with chain-of-thought (CoT) reasoning for contextual\nidentification. UGCG-Guard achieves outstanding results, with an accuracy rate\nof 94% in detecting these images used for the illicit promotion of such games\nin real-world scenarios.","updated":1711566133000,"published":1711566133000,"authors":["Keyan Guo","Ayush Utkarsh","Wenbo Ding","Isabelle Ondracek","Ziming Zhao","Guo Freeman","Nishant Vishwamitra","Hongxin Hu"],"comments":"To Appear in the 33rd USENIX Security Symposium, August 14-16, 2024","categories":["cs.CY","cs.CL","cs.LG","cs.SI"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true},"252":{"arxiv_id":"2403.19436v1","url":"http:\/\/arxiv.org\/abs\/2403.19436v1","title":"\"At the end of the day, I am accountable\": Gig Workers' Self-Tracking\n  for Multi-Dimensional Accountability Management","summary":"Tracking is inherent in and central to the gig economy. Platforms track gig\nworkers' performance through metrics such as acceptance rate and punctuality,\nwhile gig workers themselves engage in self-tracking. Although prior research\nhas extensively examined how gig platforms track workers through metrics --\nwith some studies briefly acknowledging the phenomenon of self-tracking among\nworkers -- there is a dearth of studies that explore how and why gig workers\ntrack themselves. To address this, we conducted 25 semi-structured interviews,\nrevealing how gig workers self-tracking to manage accountabilities to\nthemselves and external entities across three identities: the holistic self,\nthe entrepreneurial self, and the platformized self. We connect our findings to\nneoliberalism, through which we contextualize gig workers' self-accountability\nand the invisible labor of self-tracking. We further discuss how self-tracking\nmitigates information and power asymmetries in gig work and offer design\nimplications to support gig workers' multi-dimensional self-tracking.","updated":1711634670000,"published":1711634670000,"authors":["Rie Helene Hernandez","Qiurong Song","Yubo Kou","Xinning Gui"],"comments":"Accepted to CHI 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true},"253":{"arxiv_id":"2403.19704v1","url":"http:\/\/arxiv.org\/abs\/2403.19704v1","title":"Monitoring Wandering Behavior of Persons Suffering from Dementia Using\n  BLE Based Localization System","summary":"With the aging of our populations, dementia will become a problem which would\ndirectly or indirectly affect a large number of people. One of the most\ndangerous dementia symptoms is wandering. It consists in aimless walking and\nspatial disorientation, which might lead to various unpleasant situations like\nfalling down accidents at home to leaving the living place and going missing.\nTherefore, in order to ensure elderly people's safety it is crucial to detect\nand alarm the caregivers in case of such incidents. It can be done by tracking\nthe sufferers movements and detecting signs of repetitiveness. The paper\npresents the results of the study, in which the wandering behavior of people\nsuffering from dementia was monitored using a Bluetooth Low Energy based\npositioning system. The paper includes the description of the system used for\npatients localization and the results of the tests performed in a long term\ncare facility.","updated":1711141304000,"published":1711141304000,"authors":["Marcin Kolakowski","Bartosz Blachucki"],"comments":"Originally presented at 2019 27th Telecommunication Forum (TELFOR),\n  Belgrade, Serbia","categories":["cs.HC","eess.SP"],"primary_category":"cs.HC","doi":"10.1109\/TELFOR48224.2019.8971136","journal_ref":null,"peer_reviewed":true}}