{"0":{"arxiv_id":"2307.05300v4","reader_id":"rid0","len_gpt_jargon":3,"len_human_jargon":2,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":3,"fuzzy_false_negatives":2,"len_jargon_diff":1,"url":"http:\/\/arxiv.org\/abs\/2307.05300v4","title":"Unleashing the Emergent Cognitive Synergy in Large Language Models: A\n  Task-Solving Agent through Multi-Persona Self-Collaboration","summary":"Human intelligence thrives on cognitive synergy, where collaboration among different minds yield superior outcomes compared to isolated individuals. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist is an intelligent agent that collaboratively combines multiple minds' strengths and knowledge to enhance problem-solving in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis shows that assigning multiple fine-grained personas in LLMs improves problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive and reasoning-intensive types. Unlike previous works, such as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, experimental results demonstrate that SPP effectively reduces factual hallucination, and maintains strong reasoning capabilities. Additionally, comparative experiments show that cognitive synergy only emerges in GPT-4 and does not appear in less capable models, such as GPT-3.5-turbo and Llama2-13b-chat, which draws an interesting analogy to human development. Code, data, and prompts can be found at: https:\/\/github.com\/MikeWangWZHL\/Solo-Performance-Prompting.git.","updated":1711463553000,"published":1689086719000,"authors":["Zhenhailong Wang","Shaoguang Mao","Wenshan Wu","Tao Ge","Furu Wei","Heng Ji"],"comments":"Accepted as a main conference paper at NAACL 2024","categories":["cs.AI","cs.CL"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Artificial Intelligence","gpt4_jargon_list":"cognitive synergist, fine-grained personas, factual hallucination","human_jargon_list":"multi-turn,persona"},"1":{"arxiv_id":"2402.09565v2","reader_id":"rid1","len_gpt_jargon":10,"len_human_jargon":2,"precision":0.1,"recall":0.5,"f1_score":0.1666666667,"f2_score":0.2777777778,"fuzzy_true_positives":1,"fuzzy_false_positives":9,"fuzzy_false_negatives":1,"len_jargon_diff":8,"url":"http:\/\/arxiv.org\/abs\/2402.09565v2","title":"Graph-Skeleton: ~1% Nodes are Sufficient to Represent Billion-Scale\n  Graph","summary":"Due to the ubiquity of graph data on the web, web graph mining has become a hot research spot. Nonetheless, the prevalence of large-scale web graphs in real applications poses significant challenges to storage, computational capacity and graph model design. Despite numerous studies to enhance the scalability of graph models, a noticeable gap remains between academic research and practical web graph mining applications. One major cause is that in most industrial scenarios, only a small part of nodes in a web graph are actually required to be analyzed, where we term these nodes as target nodes, while others as background nodes. In this paper, we argue that properly fetching and condensing the background nodes from massive web graph data might be a more economical shortcut to tackle the obstacles fundamentally. To this end, we make the first attempt to study the problem of massive background nodes compression for target nodes classification. Through extensive experiments, we reveal two critical roles played by the background nodes in target node classification: enhancing structural connectivity between target nodes, and feature correlation with target nodes. Followingthis, we propose a novel Graph-Skeleton1 model, which properly fetches the background nodes, and further condenses the semantic and topological information of background nodes within similar target-background local structures. Extensive experiments on various web graph datasets demonstrate the effectiveness and efficiency of the proposed method. In particular, for MAG240M dataset with 0.24 billion nodes, our generated skeleton graph achieves highly comparable performance while only containing 1.8% nodes of the original graph.","updated":1709763753000,"published":1707942791000,"authors":["Linfeng Cao","Haoran Deng","Yang Yang","Chunping Wang","Lei Chen"],"comments":"21 pages, 11 figures, In Proceedings of the ACM Web Conference 2024\n  (WWW'24)","categories":["cs.AI"],"primary_category":"cs.AI","doi":"10.1145\/3589334.3645452","journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Artificial Intelligence","gpt4_jargon_list":"web graph mining, graph model design, target nodes, background nodes, massive background nodes compression, structural connectivity, feature correlation, Graph-Skeleton1 model, semantic and topological information, skeleton graph","human_jargon_list":"background nodes,MAG240M dataset,"},"2":{"arxiv_id":"2312.14106v2","reader_id":"rid0","len_gpt_jargon":10,"len_human_jargon":2,"precision":0.1,"recall":0.5,"f1_score":0.1666666667,"f2_score":0.2777777778,"fuzzy_true_positives":1,"fuzzy_false_positives":9,"fuzzy_false_negatives":1,"len_jargon_diff":8,"url":"http:\/\/arxiv.org\/abs\/2312.14106v2","title":"Learning Human-like Representations to Enable Learning Human Values","summary":"How can we build AI systems that are aligned with human values to avoid causing harm or violating societal standards for acceptable behavior? We argue that representational alignment between humans and AI agents facilitates value alignment. Making AI systems learn human-like representations of the world has many known benefits, including improving generalization, robustness to domain shifts, and few-shot learning performance. We propose that this kind of representational alignment between machine learning (ML) models and humans can also support value alignment, allowing ML systems to conform to human values and societal norms. We focus on ethics as one aspect of value alignment and train ML agents using a variety of methods in a multi-armed bandit setting, where rewards reflect the moral acceptability of the chosen action. We use a synthetic experiment to demonstrate that agents' representational alignment with the environment bounds their learning performance. We then repeat this procedure in a realistic setting, using textual action descriptions and similarity judgments collected from humans and a variety of language models, to show that the results generalize and are model-agnostic when grounded in an ethically relevant context.","updated":1710293875000,"published":1703183493000,"authors":["Andrea Wynn","Ilia Sucholutsky","Thomas L. Griffiths"],"comments":"Paper accepted in Human-Centric Representation Learning workshop at\n  AAAI 2024 (https:\/\/hcrl-workshop.github.io\/2024\/)","categories":["cs.AI","cs.LG"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Artificial Intelligence","gpt4_jargon_list":"representational alignment, value alignment, domain shifts, few-shot learning, multi-armed bandit setting, synthetic experiment, textual action descriptions, similarity judgments, model-agnostic, ethically relevant context","human_jargon_list":"domain shifts,moral acceptability,"},"3":{"arxiv_id":"2312.14106v2","reader_id":"rid1","len_gpt_jargon":9,"len_human_jargon":2,"precision":0.1111111111,"recall":0.5,"f1_score":0.1818181818,"f2_score":0.2941176471,"fuzzy_true_positives":1,"fuzzy_false_positives":8,"fuzzy_false_negatives":1,"len_jargon_diff":7,"url":"http:\/\/arxiv.org\/abs\/2312.14106v2","title":"Learning Human-like Representations to Enable Learning Human Values","summary":"How can we build AI systems that are aligned with human values to avoid causing harm or violating societal standards for acceptable behavior? We argue that representational alignment between humans and AI agents facilitates value alignment. Making AI systems learn human-like representations of the world has many known benefits, including improving generalization, robustness to domain shifts, and few-shot learning performance. We propose that this kind of representational alignment between machine learning (ML) models and humans can also support value alignment, allowing ML systems to conform to human values and societal norms. We focus on ethics as one aspect of value alignment and train ML agents using a variety of methods in a multi-armed bandit setting, where rewards reflect the moral acceptability of the chosen action. We use a synthetic experiment to demonstrate that agents' representational alignment with the environment bounds their learning performance. We then repeat this procedure in a realistic setting, using textual action descriptions and similarity judgments collected from humans and a variety of language models, to show that the results generalize and are model-agnostic when grounded in an ethically relevant context.","updated":1710293875000,"published":1703183493000,"authors":["Andrea Wynn","Ilia Sucholutsky","Thomas L. Griffiths"],"comments":"Paper accepted in Human-Centric Representation Learning workshop at\n  AAAI 2024 (https:\/\/hcrl-workshop.github.io\/2024\/)","categories":["cs.AI","cs.LG"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Artificial Intelligence","gpt4_jargon_list":"representational alignment, value alignment, domain shifts, few-shot learning, multi-armed bandit, synthetic experiment, textual action descriptions, similarity judgments, model-agnostic","human_jargon_list":"ML agents,multi-armed bandit setting,"},"4":{"arxiv_id":"2403.16289v1","reader_id":"rid0","len_gpt_jargon":3,"len_human_jargon":3,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":3,"fuzzy_false_negatives":3,"len_jargon_diff":0,"url":"http:\/\/arxiv.org\/abs\/2403.16289v1","title":"Engineering Safety Requirements for Autonomous Driving with Large\n  Language Models","summary":"Changes and updates in the requirement artifacts, which can be frequent in the automotive domain, are a challenge for SafetyOps. Large Language Models (LLMs), with their impressive natural language understanding and generating capabilities, can play a key role in automatically refining and decomposing requirements after each update. In this study, we propose a prototype of a pipeline of prompts and LLMs that receives an item definition and outputs solutions in the form of safety requirements. This pipeline also performs a review of the requirement dataset and identifies redundant or contradictory requirements. We first identified the necessary characteristics for performing HARA and then defined tests to assess an LLM's capability in meeting these criteria. We used design science with multiple iterations and let experts from different companies evaluate each cycle quantitatively and qualitatively. Finally, the prototype was implemented at a case company and the responsible team evaluated its efficiency.","updated":1711312851000,"published":1711312851000,"authors":["Ali Nouri","Beatriz Cabrero-Daniel","Fredrik T\u00f6rner","H\u0227kan Sivencrona","Christian Berger"],"comments":"Accepted in 32nd IEEE International Requirements Engineering 2024\n  conference, Iceland","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Artificial Intelligence","gpt4_jargon_list":"SafetyOps, HARA, design science","human_jargon_list":"requirement artifacts,decomposing requirements,requirement dataset"},"5":{"arxiv_id":"2402.01786v2","reader_id":"rid0","len_gpt_jargon":8,"len_human_jargon":1,"precision":0.125,"recall":1.0,"f1_score":0.2222222222,"f2_score":0.4166666667,"fuzzy_true_positives":1,"fuzzy_false_positives":7,"fuzzy_false_negatives":0,"len_jargon_diff":7,"url":"http:\/\/arxiv.org\/abs\/2402.01786v2","title":"COA-GPT: Generative Pre-trained Transformers for Accelerated Course of\n  Action Development in Military Operations","summary":"The development of Courses of Action (COAs) in military operations is traditionally a time-consuming and intricate process. Addressing this challenge, this study introduces COA-GPT, a novel algorithm employing Large Language Models (LLMs) for rapid and efficient generation of valid COAs. COA-GPT incorporates military doctrine and domain expertise to LLMs through in-context learning, allowing commanders to input mission information - in both text and image formats - and receive strategically aligned COAs for review and approval. Uniquely, COA-GPT not only accelerates COA development, producing initial COAs within seconds, but also facilitates real-time refinement based on commander feedback. This work evaluates COA-GPT in a military-relevant scenario within a militarized version of the StarCraft II game, comparing its performance against state-of-the-art reinforcement learning algorithms. Our results demonstrate COA-GPT's superiority in generating strategically sound COAs more swiftly, with added benefits of enhanced adaptability and alignment with commander intentions. COA-GPT's capability to rapidly adapt and update COAs during missions presents a transformative potential for military planning, particularly in addressing planning discrepancies and capitalizing on emergent windows of opportunities.","updated":1711639362000,"published":1706824269000,"authors":["Vinicius G. Goecks","Nicholas Waytowich"],"comments":"Accepted at the NATO Science and Technology Organization Symposium\n  (ICMCIS) organized by the Information Systems Technology (IST) Panel,\n  IST-205-RSY - the ICMCIS, held in Koblenz, Germany, 23-24 April 2024","categories":["cs.AI","cs.CL","cs.HC","cs.LG","I.2.6; I.2.7; J.7"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Artificial Intelligence","gpt4_jargon_list":"Courses of Action (COAs),in-context learning,military doctrine,domain expertise,state-of-the-art reinforcement learning algorithms,militarized version of the StarCraft II game,planning discrepancies,emergent windows of opportunities","human_jargon_list":"Courses of Action (COAs),"},"6":{"arxiv_id":"2402.01786v2","reader_id":"rid1","len_gpt_jargon":9,"len_human_jargon":3,"precision":0.1111111111,"recall":0.3333333333,"f1_score":0.1666666667,"f2_score":0.2380952381,"fuzzy_true_positives":1,"fuzzy_false_positives":8,"fuzzy_false_negatives":2,"len_jargon_diff":6,"url":"http:\/\/arxiv.org\/abs\/2402.01786v2","title":"COA-GPT: Generative Pre-trained Transformers for Accelerated Course of\n  Action Development in Military Operations","summary":"The development of Courses of Action (COAs) in military operations is traditionally a time-consuming and intricate process. Addressing this challenge, this study introduces COA-GPT, a novel algorithm employing Large Language Models (LLMs) for rapid and efficient generation of valid COAs. COA-GPT incorporates military doctrine and domain expertise to LLMs through in-context learning, allowing commanders to input mission information - in both text and image formats - and receive strategically aligned COAs for review and approval. Uniquely, COA-GPT not only accelerates COA development, producing initial COAs within seconds, but also facilitates real-time refinement based on commander feedback. This work evaluates COA-GPT in a military-relevant scenario within a militarized version of the StarCraft II game, comparing its performance against state-of-the-art reinforcement learning algorithms. Our results demonstrate COA-GPT's superiority in generating strategically sound COAs more swiftly, with added benefits of enhanced adaptability and alignment with commander intentions. COA-GPT's capability to rapidly adapt and update COAs during missions presents a transformative potential for military planning, particularly in addressing planning discrepancies and capitalizing on emergent windows of opportunities.","updated":1711639362000,"published":1706824269000,"authors":["Vinicius G. Goecks","Nicholas Waytowich"],"comments":"Accepted at the NATO Science and Technology Organization Symposium\n  (ICMCIS) organized by the Information Systems Technology (IST) Panel,\n  IST-205-RSY - the ICMCIS, held in Koblenz, Germany, 23-24 April 2024","categories":["cs.AI","cs.CL","cs.HC","cs.LG","I.2.6; I.2.7; J.7"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Artificial Intelligence","gpt4_jargon_list":"Courses of Action (COAs),in-context learning,military doctrine,domain expertise,reinforcement learning algorithms,military-relevant scenario,militarized version of the StarCraft II game,planning discrepancies,emergent windows of opportunities","human_jargon_list":"Courses of Action,COA-GPT,commander feedback"},"7":{"arxiv_id":"2403.05112v1","reader_id":"rid0","len_gpt_jargon":7,"len_human_jargon":2,"precision":0.1428571429,"recall":0.5,"f1_score":0.2222222222,"f2_score":0.3333333333,"fuzzy_true_positives":1,"fuzzy_false_positives":6,"fuzzy_false_negatives":1,"len_jargon_diff":5,"url":"http:\/\/arxiv.org\/abs\/2403.05112v1","title":"RLPeri: Accelerating Visual Perimetry Test with Reinforcement Learning\n  and Convolutional Feature Extraction","summary":"Visual perimetry is an important eye examination that helps detect vision problems caused by ocular or neurological conditions. During the test, a patient's gaze is fixed at a specific location while light stimuli of varying intensities are presented in central and peripheral vision. Based on the patient's responses to the stimuli, the visual field mapping and sensitivity are determined. However, maintaining high levels of concentration throughout the test can be challenging for patients, leading to increased examination times and decreased accuracy.  In this work, we present RLPeri, a reinforcement learning-based approach to optimize visual perimetry testing. By determining the optimal sequence of locations and initial stimulus values, we aim to reduce the examination time without compromising accuracy. Additionally, we incorporate reward shaping techniques to further improve the testing performance. To monitor the patient's responses over time during testing, we represent the test's state as a pair of 3D matrices. We apply two different convolutional kernels to extract spatial features across locations as well as features across different stimulus values for each location. Through experiments, we demonstrate that our approach results in a 10-20% reduction in examination time while maintaining the accuracy as compared to state-of-the-art methods. With the presented approach, we aim to make visual perimetry testing more efficient and patient-friendly, while still providing accurate results.","updated":1709882383000,"published":1709882383000,"authors":["Tanvi Verma","Linh Le Dinh","Nicholas Tan","Xinxing Xu","Chingyu Cheng","Yong Liu"],"comments":"Published at AAAI-24","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":"The 38th Annual AAAI Conference on Artificial Intelligence, 2024","peer_reviewed":true,"primary_category_readable":"Artificial Intelligence","gpt4_jargon_list":"visual perimetry, light stimuli, visual field mapping, reinforcement learning-based approach, reward shaping techniques, convolutional kernels, spatial features","human_jargon_list":"reward shaping\ntechniques,initial stimulus values,"},"8":{"arxiv_id":"2401.09210v2","reader_id":"rid1","len_gpt_jargon":12,"len_human_jargon":1,"precision":0.0833333333,"recall":1.0,"f1_score":0.1538461538,"f2_score":0.3125,"fuzzy_true_positives":1,"fuzzy_false_positives":11,"fuzzy_false_negatives":0,"len_jargon_diff":11,"url":"http:\/\/arxiv.org\/abs\/2401.09210v2","title":"Narratives of Collective Action in YouTube's Discourse on Veganism","summary":"Narratives can be powerful tools for inspiring action on pressing societal issues such as climate change. While social science theories offer frameworks for understanding the narratives that arise within collective movements, these are rarely applied to the vast data available from social media platforms, which play a significant role in shaping public opinion and mobilizing collective action. This gap in the empirical evaluation of online narratives limits our understanding of their relationship with public response. In this study, we focus on plant-based diets as a form of pro-environmental action and employ natural language processing to operationalize a theoretical framework of moral narratives specific to the vegan movement. We apply this framework to narratives found in YouTube videos promoting environmental initiatives such as Veganuary, Meatless March, and No Meat May. Our analysis reveals that several narrative types, as defined by the theory, are empirically present in the data. To identify narratives with the potential to elicit positive public engagement, we used text processing to estimate the proportion of comments supporting collective action across narrative types. Video narratives advocating social fight, whether through protest or through efforts to convert others to the cause, are associated with a stronger sense of collective action in the respective comments. These narrative types also demonstrate increased semantic coherence and alignment between the message and public response, markers typically associated with successful collective action. Our work offers new insights into the complex factors that influence the emergence of collective action, thereby informing the development of effective communication strategies within social movements.","updated":1711625999000,"published":1705499076000,"authors":["Arianna Pera","Luca Maria Aiello"],"comments":"15 pages, 7 figures, 7 tables. Accepted at ICWSM 2024","categories":["cs.CY","physics.soc-ph"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Computers and Society","gpt4_jargon_list":"social science theories, frameworks, empirical evaluation, pro-environmental action, operationalize, theoretical framework, moral narratives, text processing, narrative types, semantic coherence, collective action, communication strategies","human_jargon_list":"semantic\ncoherence"},"9":{"arxiv_id":"2403.04760v1","reader_id":"rid1","len_gpt_jargon":9,"len_human_jargon":2,"precision":0.1111111111,"recall":0.5,"f1_score":0.1818181818,"f2_score":0.2941176471,"fuzzy_true_positives":1,"fuzzy_false_positives":8,"fuzzy_false_negatives":1,"len_jargon_diff":7,"url":"http:\/\/arxiv.org\/abs\/2403.04760v1","title":"iScore: Visual Analytics for Interpreting How Language Models\n  Automatically Score Summaries","summary":"The recent explosion in popularity of large language models (LLMs) has inspired learning engineers to incorporate them into adaptive educational tools that automatically score summary writing. Understanding and evaluating LLMs is vital before deploying them in critical learning environments, yet their unprecedented size and expanding number of parameters inhibits transparency and impedes trust when they underperform. Through a collaborative user-centered design process with several learning engineers building and deploying summary scoring LLMs, we characterized fundamental design challenges and goals around interpreting their models, including aggregating large text inputs, tracking score provenance, and scaling LLM interpretability methods. To address their concerns, we developed iScore, an interactive visual analytics tool for learning engineers to upload, score, and compare multiple summaries simultaneously. Tightly integrated views allow users to iteratively revise the language in summaries, track changes in the resulting LLM scores, and visualize model weights at multiple levels of abstraction. To validate our approach, we deployed iScore with three learning engineers over the course of a month. We present a case study where interacting with iScore led a learning engineer to improve their LLM's score accuracy by three percentage points. Finally, we conducted qualitative interviews with the learning engineers that revealed how iScore enabled them to understand, evaluate, and build trust in their LLMs during deployment.","updated":1709837799000,"published":1709837799000,"authors":["Adam Coscia","Langdon Holmes","Wesley Morris","Joon Suh Choi","Scott Crossley","Alex Endert"],"comments":"Accepted to IUI 2024. 16 pages, 5 figures, 1 table. For a demo video,\n  see https:\/\/youtu.be\/EYJX-_fQPf0 . For a live demo, visit\n  https:\/\/adamcoscia.com\/papers\/iscore\/demo\/ . The source code is available at\n  https:\/\/github.com\/AdamCoscia\/iScore","categories":["cs.HC","cs.AI","cs.CY","cs.LG"],"primary_category":"cs.HC","doi":"10.1145\/3640543.3645142","journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"large language models,unprecedented size,expanding number of parameters,aggregating large text inputs,score provenance,scaling LLM interpretability methods,interactive visual analytics tool,multiple levels of abstraction,qualitative interviews","human_jargon_list":"score provenance,iScore,"},"10":{"arxiv_id":"2403.06267v1","reader_id":"rid0","len_gpt_jargon":7,"len_human_jargon":1,"precision":0.1428571429,"recall":1.0,"f1_score":0.25,"f2_score":0.4545454545,"fuzzy_true_positives":1,"fuzzy_false_positives":6,"fuzzy_false_negatives":0,"len_jargon_diff":6,"url":"http:\/\/arxiv.org\/abs\/2403.06267v1","title":"FARPLS: A Feature-Augmented Robot Trajectory Preference Labeling System\n  to Assist Human Labelers' Preference Elicitation","summary":"Preference-based learning aims to align robot task objectives with human values. One of the most common methods to infer human preferences is by pairwise comparisons of robot task trajectories. Traditional comparison-based preference labeling systems seldom support labelers to digest and identify critical differences between complex trajectories recorded in videos. Our formative study (N = 12) suggests that individuals may overlook non-salient task features and establish biased preference criteria during their preference elicitation process because of partial observations. In addition, they may experience mental fatigue when given many pairs to compare, causing their label quality to deteriorate. To mitigate these issues, we propose FARPLS, a Feature-Augmented Robot trajectory Preference Labeling System. FARPLS highlights potential outliers in a wide variety of task features that matter to humans and extracts the corresponding video keyframes for easy review and comparison. It also dynamically adjusts the labeling order according to users' familiarities, difficulties of the trajectory pair, and level of disagreements. At the same time, the system monitors labelers' consistency and provides feedback on labeling progress to keep labelers engaged. A between-subjects study (N = 42, 105 pairs of robot pick-and-place trajectories per person) shows that FARPLS can help users establish preference criteria more easily and notice more relevant details in the presented trajectories than the conventional interface. FARPLS also improves labeling consistency and engagement, mitigating challenges in preference elicitation without raising cognitive loads significantly","updated":1710090440000,"published":1710090440000,"authors":["Hanfang Lyu","Yuanchen Bai","Xin Liang","Ujaan Das","Chuhan Shi","Leiliang Gong","Yingchi Li","Mingfei Sun","Ming Ge","Xiaojuan Ma"],"comments":"Accepted to ACM Conference on Intelligent User Interfaces (IUI) 2024,\n  March 18-21, 2024, Greenville, SC, USA","categories":["cs.HC","cs.AI"],"primary_category":"cs.HC","doi":"10.1145\/3640543.3645145","journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"pairwise comparisons of robot task trajectories, Feature-Augmented Robot trajectory Preference Labeling System, video keyframes, labeling order, trajectory pair, between-subjects study, robot pick-and-place trajectories","human_jargon_list":"robot task trajectories"},"11":{"arxiv_id":"2403.06267v1","reader_id":"rid1","len_gpt_jargon":19,"len_human_jargon":5,"precision":0.1052631579,"recall":0.4,"f1_score":0.1666666667,"f2_score":0.2564102564,"fuzzy_true_positives":2,"fuzzy_false_positives":17,"fuzzy_false_negatives":3,"len_jargon_diff":14,"url":"http:\/\/arxiv.org\/abs\/2403.06267v1","title":"FARPLS: A Feature-Augmented Robot Trajectory Preference Labeling System\n  to Assist Human Labelers' Preference Elicitation","summary":"Preference-based learning aims to align robot task objectives with human values. One of the most common methods to infer human preferences is by pairwise comparisons of robot task trajectories. Traditional comparison-based preference labeling systems seldom support labelers to digest and identify critical differences between complex trajectories recorded in videos. Our formative study (N = 12) suggests that individuals may overlook non-salient task features and establish biased preference criteria during their preference elicitation process because of partial observations. In addition, they may experience mental fatigue when given many pairs to compare, causing their label quality to deteriorate. To mitigate these issues, we propose FARPLS, a Feature-Augmented Robot trajectory Preference Labeling System. FARPLS highlights potential outliers in a wide variety of task features that matter to humans and extracts the corresponding video keyframes for easy review and comparison. It also dynamically adjusts the labeling order according to users' familiarities, difficulties of the trajectory pair, and level of disagreements. At the same time, the system monitors labelers' consistency and provides feedback on labeling progress to keep labelers engaged. A between-subjects study (N = 42, 105 pairs of robot pick-and-place trajectories per person) shows that FARPLS can help users establish preference criteria more easily and notice more relevant details in the presented trajectories than the conventional interface. FARPLS also improves labeling consistency and engagement, mitigating challenges in preference elicitation without raising cognitive loads significantly","updated":1710090440000,"published":1710090440000,"authors":["Hanfang Lyu","Yuanchen Bai","Xin Liang","Ujaan Das","Chuhan Shi","Leiliang Gong","Yingchi Li","Mingfei Sun","Ming Ge","Xiaojuan Ma"],"comments":"Accepted to ACM Conference on Intelligent User Interfaces (IUI) 2024,\n  March 18-21, 2024, Greenville, SC, USA","categories":["cs.HC","cs.AI"],"primary_category":"cs.HC","doi":"10.1145\/3640543.3645145","journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"pairwise comparisons, robot task trajectories, preference labeling systems, labelers, non-salient task features, preference elicitation process, mental fatigue, Feature-Augmented Robot trajectory Preference Labeling System, outliers, task features, video keyframes, labeling order, disagreements, labelers' consistency, between-subjects study, robot pick-and-place trajectories, labeling consistency, preference elicitation, cognitive loads","human_jargon_list":"Preference-based learning,non-salient,FARPLS,trajectory pair,cognitive loads"},"12":{"arxiv_id":"2403.06431v1","reader_id":"rid1","len_gpt_jargon":5,"len_human_jargon":1,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":5,"fuzzy_false_negatives":1,"len_jargon_diff":4,"url":"http:\/\/arxiv.org\/abs\/2403.06431v1","title":"From Fitting Participation to Forging Relationships: The Art of\n  Participatory ML","summary":"Participatory machine learning (ML) encourages the inclusion of end users and people affected by ML systems in design and development processes. We interviewed 18 participation brokers -- individuals who facilitate such inclusion and transform the products of participants' labour into inputs for an ML artefact or system -- across a range of organisational settings and project locations. Our findings demonstrate the inherent challenges of integrating messy contextual information generated through participation with the structured data formats required by ML workflows and the uneven power dynamics in project contexts. We advocate for evolution in the role of brokers to more equitably balance value generated in Participatory ML projects for design and development teams with value created for participants. To move beyond `fitting' participation to existing processes and empower participants to envision alternative futures through ML, brokers must become educators and advocates for end users, while attending to frustration and dissent from indirect stakeholders.","updated":1710132274000,"published":1710132274000,"authors":["Ned Cooper","Alex Zafiroglu"],"comments":"To appear in Proceedings of the 2024 CHI Conference on Human Factors\n  in Computing Systems (CHI '24)","categories":["cs.HC","cs.CY"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"participation brokers, messy contextual information, structured data formats, uneven power dynamics, indirect stakeholders","human_jargon_list":"Participatory machine learning,"},"13":{"arxiv_id":"2403.06651v1","reader_id":"rid1","len_gpt_jargon":17,"len_human_jargon":2,"precision":0.1176470588,"recall":1.0,"f1_score":0.2105263158,"f2_score":0.4,"fuzzy_true_positives":2,"fuzzy_false_positives":15,"fuzzy_false_negatives":0,"len_jargon_diff":15,"url":"http:\/\/arxiv.org\/abs\/2403.06651v1","title":"SoniWeight Shoes: Investigating Effects and Personalization of a\n  Wearable Sound Device for Altering Body Perception and Behavior","summary":"Changes in body perception influence behavior and emotion and can be induced through multisensory feedback. Auditory feedback to one's actions can trigger such alterations; however, it is unclear which individual factors modulate these effects. We employ and evaluate SoniWeight Shoes, a wearable device based on literature for altering one's weight perception through manipulated footstep sounds. In a healthy population sample across a spectrum of individuals (n=84) with varying degrees of eating disorder symptomatology, physical activity levels, body concerns, and mental imagery capacities, we explore the effects of three sound conditions (low-frequency, high-frequency and control) on extensive body perception measures (demographic, behavioral, physiological, psychological, and subjective). Analyses revealed an impact of individual differences in each of these dimensions. Besides replicating previous findings, we reveal and highlight the role of individual differences in body perception, offering avenues for personalized sonification strategies. Datasets, technical refinements, and novel body map quantification tools are provided.","updated":1710159374000,"published":1710159374000,"authors":["A. D'Adamo","M. Roel-Lesur","L. Turmo-Vidal","M. M. Dehshibi","D. De La Prida","J. R. Diaz-Duran","L. A. Azpicueta-Ruiz","A. V\u00e4ljam\u00e4e","A. Tajadura-Jim\u00e9nez"],"comments":"Conditionally Accepted in CHI '24 Conference","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642651","journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"multisensory feedback, auditory feedback, SoniWeight Shoes, manipulated footstep sounds, eating disorder symptomatology, physical activity levels, body concerns, mental imagery capacities, sound conditions, extensive body perception measures, demographic, behavioral, physiological, psychological, subjective, personalized sonification strategies, body map quantification tools","human_jargon_list":"multisensory feedback,SoniWeight Shoes"},"14":{"arxiv_id":"2403.19436v1","reader_id":"rid1","len_gpt_jargon":11,"len_human_jargon":2,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":11,"fuzzy_false_negatives":2,"len_jargon_diff":9,"url":"http:\/\/arxiv.org\/abs\/2403.19436v1","title":"\"At the end of the day, I am accountable\": Gig Workers' Self-Tracking\n  for Multi-Dimensional Accountability Management","summary":"Tracking is inherent in and central to the gig economy. Platforms track gig workers' performance through metrics such as acceptance rate and punctuality, while gig workers themselves engage in self-tracking. Although prior research has extensively examined how gig platforms track workers through metrics -- with some studies briefly acknowledging the phenomenon of self-tracking among workers -- there is a dearth of studies that explore how and why gig workers track themselves. To address this, we conducted 25 semi-structured interviews, revealing how gig workers self-tracking to manage accountabilities to themselves and external entities across three identities: the holistic self, the entrepreneurial self, and the platformized self. We connect our findings to neoliberalism, through which we contextualize gig workers' self-accountability and the invisible labor of self-tracking. We further discuss how self-tracking mitigates information and power asymmetries in gig work and offer design implications to support gig workers' multi-dimensional self-tracking.","updated":1711634670000,"published":1711634670000,"authors":["Rie Helene Hernandez","Qiurong Song","Yubo Kou","Xinning Gui"],"comments":"Accepted to CHI 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"acceptance rate, punctuality, semi-structured interviews, accountabilities, entrepreneurial self, platformized self, neoliberalism, invisible labor, information and power asymmetries, design implications, multi-dimensional self-tracking","human_jargon_list":"gig economy,dearthneoliberalism,"},"15":{"arxiv_id":"2401.10838v2","reader_id":"rid0","len_gpt_jargon":7,"len_human_jargon":1,"precision":0.1428571429,"recall":1.0,"f1_score":0.25,"f2_score":0.4545454545,"fuzzy_true_positives":1,"fuzzy_false_positives":6,"fuzzy_false_negatives":0,"len_jargon_diff":6,"url":"http:\/\/arxiv.org\/abs\/2401.10838v2","title":"Rambler: Supporting Writing With Speech via LLM-Assisted Gist\n  Manipulation","summary":"Dictation enables efficient text input on mobile devices. However, writing with speech can produce disfluent, wordy, and incoherent text and thus requires heavy post-processing. This paper presents Rambler, an LLM-powered graphical user interface that supports gist-level manipulation of dictated text with two main sets of functions: gist extraction and macro revision. Gist extraction generates keywords and summaries as anchors to support the review and interaction with spoken text. LLM-assisted macro revisions allow users to respeak, split, merge and transform dictated text without specifying precise editing locations. Together they pave the way for interactive dictation and revision that help close gaps between spontaneous spoken words and well-structured writing. In a comparative study with 12 participants performing verbal composition tasks, Rambler outperformed the baseline of a speech-to-text editor + ChatGPT, as it better facilitates iterative revisions with enhanced user control over the content while supporting surprisingly diverse user strategies.","updated":1709865965000,"published":1705685996000,"authors":["Susan Lin","Jeremy Warner","J. D. Zamfirescu-Pereira","Matthew G. Lee","Sauhard Jain","Michael Xuelin Huang","Piyawat Lertvittayakumjorn","Shanqing Cai","Shumin Zhai","Bj\u00f6rn Hartmann","Can Liu"],"comments":"To appear at ACM CHI 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642217","journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"LLM-powered, gist-level manipulation, gist extraction, macro revision, LLM-assisted macro revisions, respeak, verbal composition tasks","human_jargon_list":"gist-level manipulation"},"16":{"arxiv_id":"2403.01055v1","reader_id":"rid1","len_gpt_jargon":5,"len_human_jargon":1,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":5,"fuzzy_false_negatives":1,"len_jargon_diff":4,"url":"http:\/\/arxiv.org\/abs\/2403.01055v1","title":"Towards Full Authorship with AI: Supporting Revision with AI-Generated\n  Views","summary":"Large language models (LLMs) are shaping a new user interface (UI) paradigm in writing tools by enabling users to generate text through prompts. This paradigm shifts some creative control from the user to the system, thereby diminishing the user's authorship and autonomy in the writing process. To restore autonomy, we introduce Textfocals, a UI prototype designed to investigate a human-centered approach that emphasizes the user's role in writing. Textfocals supports the writing process by providing LLM-generated summaries, questions, and advice (i.e., LLM views) in a sidebar of a text editor, encouraging reflection and self-driven revision in writing without direct text generation. Textfocals' UI affordances, including contextually adaptive views and scaffolding for prompt selection and customization, offer a novel way to interact with LLMs where users maintain full authorship of their writing. A formative user study with Textfocals showed promising evidence that this approach might help users develop underdeveloped ideas, cater to the rhetorical audience, and clarify their writing. However, the study also showed interaction design challenges related to document navigation and scoping, prompt engineering, and context management. Our work highlights the breadth of the design space of writing support interfaces powered by generative AI that maintain authorship integrity.","updated":1709341895000,"published":1709341895000,"authors":["Jiho Kim","Ray C. Flanagan","Noelle E. Haviland","ZeAi Sun","Souad N. Yakubu","Edom A. Maru","Kenneth C. Arnold"],"comments":"15 pages, 2 figures; Accepted to 5th Workshop on Human-AI Co-Creation\n  with Generative Models (HAI-GEN) at ACM IUI 2024","categories":["cs.HC","cs.AI","cs.CY","H.5.2; I.7.1; I.2.7"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"UI affordances, contextually adaptive views, scaffolding, prompt engineering, context management","human_jargon_list":"Textfocals"},"17":{"arxiv_id":"2305.11927v2","reader_id":"rid1","len_gpt_jargon":8,"len_human_jargon":1,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":8,"fuzzy_false_negatives":1,"len_jargon_diff":7,"url":"http:\/\/arxiv.org\/abs\/2305.11927v2","title":"Evaluating how interactive visualizations can assist in finding samples\n  where and how computer vision models make mistakes","summary":"Creating Computer Vision (CV) models remains a complex practice, despite their ubiquity. Access to data, the requirement for ML expertise, and model opacity are just a few points of complexity that limit the ability of end-users to build, inspect, and improve these models. Interactive ML perspectives have helped address some of these issues by considering a teacher in the loop where planning, teaching, and evaluating tasks take place. We present and evaluate two interactive visualizations in the context of Sprite, a system for creating CV classification and detection models for images originating from videos. We study how these visualizations help Sprite's users identify (evaluate) and select (plan) images where a model is struggling and can lead to improved performance, compared to a baseline condition where users used a query language. We found that users who had used the visualizations found more images across a wider set of potential types of model errors.","updated":1710526996000,"published":1684507380000,"authors":["Hayeong Song","Gonzalo Ramos","Peter Bodik"],"comments":"Hayeong Song, Gonzalo Ramos, and Peter Bodik. \"Evaluating how\n  interactive visualizations can assist in finding samples where and how\n  computer vision models make mistakes\" 2024 IEEE Pacific Visualization\n  Symposium (PacificVis). Ieee, 2024","categories":["cs.HC","cs.CV","cs.LG"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"Computer Vision (CV) models, model opacity, Interactive ML perspectives, teacher in the loop, interactive visualizations, CV classification and detection models, baseline condition, query language","human_jargon_list":"Sprite"},"18":{"arxiv_id":"2403.00632v1","reader_id":"rid0","len_gpt_jargon":4,"len_human_jargon":1,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":4,"fuzzy_false_negatives":1,"len_jargon_diff":3,"url":"http:\/\/arxiv.org\/abs\/2403.00632v1","title":"Metamorpheus: Interactive, Affective, and Creative Dream Narration\n  Through Metaphorical Visual Storytelling","summary":"Human emotions are essentially molded by lived experiences, from which we construct personalised meaning. The engagement in such meaning-making process has been practiced as an intervention in various psychotherapies to promote wellness. Nevertheless, to support recollecting and recounting lived experiences in everyday life remains under explored in HCI. It also remains unknown how technologies such as generative AI models can facilitate the meaning making process, and ultimately support affective mindfulness. In this paper we present Metamorpheus, an affective interface that engages users in a creative visual storytelling of emotional experiences during dreams. Metamorpheus arranges the storyline based on a dream's emotional arc, and provokes self-reflection through the creation of metaphorical images and text depictions. The system provides metaphor suggestions, and generates visual metaphors and text depictions using generative AI models, while users can apply generations to recolour and re-arrange the interface to be visually affective. Our experience-centred evaluation manifests that, by interacting with Metamorpheus, users can recall their dreams in vivid detail, through which they relive and reflect upon their experiences in a meaningful way.","updated":1709309372000,"published":1709309372000,"authors":["Qian Wan","Xin Feng","Yining Bei","Zhiqi Gao","Zhicong Lu"],"comments":"Accepted by CHI 2024","categories":["cs.HC","cs.AI","cs.CL","cs.CY"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"affective mindfulness, emotional arc, visual metaphors, experience-centred evaluation","human_jargon_list":"affective interface"},"19":{"arxiv_id":"2403.00632v1","reader_id":"rid1","len_gpt_jargon":4,"len_human_jargon":1,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":4,"fuzzy_false_negatives":1,"len_jargon_diff":3,"url":"http:\/\/arxiv.org\/abs\/2403.00632v1","title":"Metamorpheus: Interactive, Affective, and Creative Dream Narration\n  Through Metaphorical Visual Storytelling","summary":"Human emotions are essentially molded by lived experiences, from which we construct personalised meaning. The engagement in such meaning-making process has been practiced as an intervention in various psychotherapies to promote wellness. Nevertheless, to support recollecting and recounting lived experiences in everyday life remains under explored in HCI. It also remains unknown how technologies such as generative AI models can facilitate the meaning making process, and ultimately support affective mindfulness. In this paper we present Metamorpheus, an affective interface that engages users in a creative visual storytelling of emotional experiences during dreams. Metamorpheus arranges the storyline based on a dream's emotional arc, and provokes self-reflection through the creation of metaphorical images and text depictions. The system provides metaphor suggestions, and generates visual metaphors and text depictions using generative AI models, while users can apply generations to recolour and re-arrange the interface to be visually affective. Our experience-centred evaluation manifests that, by interacting with Metamorpheus, users can recall their dreams in vivid detail, through which they relive and reflect upon their experiences in a meaningful way.","updated":1709309372000,"published":1709309372000,"authors":["Qian Wan","Xin Feng","Yining Bei","Zhiqi Gao","Zhicong Lu"],"comments":"Accepted by CHI 2024","categories":["cs.HC","cs.AI","cs.CL","cs.CY"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"affective interface, emotional arc, metaphorical images, experience-centred evaluation","human_jargon_list":"Metamorpheus"}}