{"0":{"arxiv_id":"2307.05300v4","reader_id":"rid0","len_gpt_jargon":3,"len_human_jargon":2,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":3,"fuzzy_false_negatives":2,"len_jargon_diff":1,"url":"http:\/\/arxiv.org\/abs\/2307.05300v4","title":"Unleashing the Emergent Cognitive Synergy in Large Language Models: A\n  Task-Solving Agent through Multi-Persona Self-Collaboration","summary":"Human intelligence thrives on cognitive synergy, where collaboration among different minds yield superior outcomes compared to isolated individuals. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist is an intelligent agent that collaboratively combines multiple minds' strengths and knowledge to enhance problem-solving in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis shows that assigning multiple fine-grained personas in LLMs improves problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive and reasoning-intensive types. Unlike previous works, such as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, experimental results demonstrate that SPP effectively reduces factual hallucination, and maintains strong reasoning capabilities. Additionally, comparative experiments show that cognitive synergy only emerges in GPT-4 and does not appear in less capable models, such as GPT-3.5-turbo and Llama2-13b-chat, which draws an interesting analogy to human development. Code, data, and prompts can be found at: https:\/\/github.com\/MikeWangWZHL\/Solo-Performance-Prompting.git.","updated":1711463553000,"published":1689086719000,"authors":["Zhenhailong Wang","Shaoguang Mao","Wenshan Wu","Tao Ge","Furu Wei","Heng Ji"],"comments":"Accepted as a main conference paper at NAACL 2024","categories":["cs.AI","cs.CL"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Artificial Intelligence","gpt4_jargon_list":"cognitive synergist, fine-grained personas, factual hallucination","human_jargon_list":"multi-turn,persona"},"1":{"arxiv_id":"2307.05300v4","reader_id":"rid1","len_gpt_jargon":6,"len_human_jargon":3,"precision":0.1666666667,"recall":0.3333333333,"f1_score":0.2222222222,"f2_score":0.2777777778,"fuzzy_true_positives":1,"fuzzy_false_positives":5,"fuzzy_false_negatives":2,"len_jargon_diff":3,"url":"http:\/\/arxiv.org\/abs\/2307.05300v4","title":"Unleashing the Emergent Cognitive Synergy in Large Language Models: A\n  Task-Solving Agent through Multi-Persona Self-Collaboration","summary":"Human intelligence thrives on cognitive synergy, where collaboration among different minds yield superior outcomes compared to isolated individuals. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist is an intelligent agent that collaboratively combines multiple minds' strengths and knowledge to enhance problem-solving in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis shows that assigning multiple fine-grained personas in LLMs improves problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive and reasoning-intensive types. Unlike previous works, such as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, experimental results demonstrate that SPP effectively reduces factual hallucination, and maintains strong reasoning capabilities. Additionally, comparative experiments show that cognitive synergy only emerges in GPT-4 and does not appear in less capable models, such as GPT-3.5-turbo and Llama2-13b-chat, which draws an interesting analogy to human development. Code, data, and prompts can be found at: https:\/\/github.com\/MikeWangWZHL\/Solo-Performance-Prompting.git.","updated":1711463553000,"published":1689086719000,"authors":["Zhenhailong Wang","Shaoguang Mao","Wenshan Wu","Tao Ge","Furu Wei","Heng Ji"],"comments":"Accepted as a main conference paper at NAACL 2024","categories":["cs.AI","cs.CL"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Artificial Intelligence","gpt4_jargon_list":"cognitive synergy, cognitive synergist, multi-turn self-collaboration, dynamically identifying and simulating, fine-grained personas, factual hallucination","human_jargon_list":"Solo Performance Prompting,multi-turn,Chain-of-Thought,"},"2":{"arxiv_id":"2402.09565v2","reader_id":"rid1","len_gpt_jargon":10,"len_human_jargon":2,"precision":0.1,"recall":0.5,"f1_score":0.1666666667,"f2_score":0.2777777778,"fuzzy_true_positives":1,"fuzzy_false_positives":9,"fuzzy_false_negatives":1,"len_jargon_diff":8,"url":"http:\/\/arxiv.org\/abs\/2402.09565v2","title":"Graph-Skeleton: ~1% Nodes are Sufficient to Represent Billion-Scale\n  Graph","summary":"Due to the ubiquity of graph data on the web, web graph mining has become a hot research spot. Nonetheless, the prevalence of large-scale web graphs in real applications poses significant challenges to storage, computational capacity and graph model design. Despite numerous studies to enhance the scalability of graph models, a noticeable gap remains between academic research and practical web graph mining applications. One major cause is that in most industrial scenarios, only a small part of nodes in a web graph are actually required to be analyzed, where we term these nodes as target nodes, while others as background nodes. In this paper, we argue that properly fetching and condensing the background nodes from massive web graph data might be a more economical shortcut to tackle the obstacles fundamentally. To this end, we make the first attempt to study the problem of massive background nodes compression for target nodes classification. Through extensive experiments, we reveal two critical roles played by the background nodes in target node classification: enhancing structural connectivity between target nodes, and feature correlation with target nodes. Followingthis, we propose a novel Graph-Skeleton1 model, which properly fetches the background nodes, and further condenses the semantic and topological information of background nodes within similar target-background local structures. Extensive experiments on various web graph datasets demonstrate the effectiveness and efficiency of the proposed method. In particular, for MAG240M dataset with 0.24 billion nodes, our generated skeleton graph achieves highly comparable performance while only containing 1.8% nodes of the original graph.","updated":1709763753000,"published":1707942791000,"authors":["Linfeng Cao","Haoran Deng","Yang Yang","Chunping Wang","Lei Chen"],"comments":"21 pages, 11 figures, In Proceedings of the ACM Web Conference 2024\n  (WWW'24)","categories":["cs.AI"],"primary_category":"cs.AI","doi":"10.1145\/3589334.3645452","journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Artificial Intelligence","gpt4_jargon_list":"web graph mining, graph model design, target nodes, background nodes, massive background nodes compression, structural connectivity, feature correlation, Graph-Skeleton1 model, semantic and topological information, skeleton graph","human_jargon_list":"background nodes,MAG240M dataset,"},"3":{"arxiv_id":"2309.03685v2","reader_id":"rid0","len_gpt_jargon":6,"len_human_jargon":1,"precision":0.1666666667,"recall":1.0,"f1_score":0.2857142857,"f2_score":0.5,"fuzzy_true_positives":1,"fuzzy_false_positives":5,"fuzzy_false_negatives":0,"len_jargon_diff":5,"url":"http:\/\/arxiv.org\/abs\/2309.03685v2","title":"PyGraft: Configurable Generation of Synthetic Schemas and Knowledge\n  Graphs at Your Fingertips","summary":"Knowledge graphs (KGs) have emerged as a prominent data representation and management paradigm. Being usually underpinned by a schema (e.g., an ontology), KGs capture not only factual information but also contextual knowledge. In some tasks, a few KGs established themselves as standard benchmarks. However, recent works outline that relying on a limited collection of datasets is not sufficient to assess the generalization capability of an approach. In some data-sensitive fields such as education or medicine, access to public datasets is even more limited. To remedy the aforementioned issues, we release PyGraft, a Python-based tool that generates highly customized, domain-agnostic schemas and KGs. The synthesized schemas encompass various RDFS and OWL constructs, while the synthesized KGs emulate the characteristics and scale of real-world KGs. Logical consistency of the generated resources is ultimately ensured by running a description logic (DL) reasoner. By providing a way of generating both a schema and KG in a single pipeline, PyGraft's aim is to empower the generation of a more diverse array of KGs for benchmarking novel approaches in areas such as graph-based machine learning (ML), or more generally KG processing. In graph-based ML in particular, this should foster a more holistic evaluation of model performance and generalization capability, thereby going beyond the limited collection of available benchmarks. PyGraft is available at: https:\/\/github.com\/nicolas-hbt\/pygraft.","updated":1709675803000,"published":1694091609000,"authors":["Nicolas Hubert","Pierre Monnin","Mathieu d'Aquin","Davy Monticolo","Armelle Brun"],"comments":"Accepted in ESWC 2024","categories":["cs.AI","cs.SE"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Artificial Intelligence","gpt4_jargon_list":"schema, ontology, RDFS, OWL constructs, description logic (DL) reasoner, graph-based machine learning","human_jargon_list":"description logic (DL) reasoner"},"4":{"arxiv_id":"2312.14106v2","reader_id":"rid0","len_gpt_jargon":10,"len_human_jargon":2,"precision":0.1,"recall":0.5,"f1_score":0.1666666667,"f2_score":0.2777777778,"fuzzy_true_positives":1,"fuzzy_false_positives":9,"fuzzy_false_negatives":1,"len_jargon_diff":8,"url":"http:\/\/arxiv.org\/abs\/2312.14106v2","title":"Learning Human-like Representations to Enable Learning Human Values","summary":"How can we build AI systems that are aligned with human values to avoid causing harm or violating societal standards for acceptable behavior? We argue that representational alignment between humans and AI agents facilitates value alignment. Making AI systems learn human-like representations of the world has many known benefits, including improving generalization, robustness to domain shifts, and few-shot learning performance. We propose that this kind of representational alignment between machine learning (ML) models and humans can also support value alignment, allowing ML systems to conform to human values and societal norms. We focus on ethics as one aspect of value alignment and train ML agents using a variety of methods in a multi-armed bandit setting, where rewards reflect the moral acceptability of the chosen action. We use a synthetic experiment to demonstrate that agents' representational alignment with the environment bounds their learning performance. We then repeat this procedure in a realistic setting, using textual action descriptions and similarity judgments collected from humans and a variety of language models, to show that the results generalize and are model-agnostic when grounded in an ethically relevant context.","updated":1710293875000,"published":1703183493000,"authors":["Andrea Wynn","Ilia Sucholutsky","Thomas L. Griffiths"],"comments":"Paper accepted in Human-Centric Representation Learning workshop at\n  AAAI 2024 (https:\/\/hcrl-workshop.github.io\/2024\/)","categories":["cs.AI","cs.LG"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Artificial Intelligence","gpt4_jargon_list":"representational alignment, value alignment, domain shifts, few-shot learning, multi-armed bandit setting, synthetic experiment, textual action descriptions, similarity judgments, model-agnostic, ethically relevant context","human_jargon_list":"domain shifts,moral acceptability,"},"5":{"arxiv_id":"2312.14106v2","reader_id":"rid1","len_gpt_jargon":9,"len_human_jargon":2,"precision":0.1111111111,"recall":0.5,"f1_score":0.1818181818,"f2_score":0.2941176471,"fuzzy_true_positives":1,"fuzzy_false_positives":8,"fuzzy_false_negatives":1,"len_jargon_diff":7,"url":"http:\/\/arxiv.org\/abs\/2312.14106v2","title":"Learning Human-like Representations to Enable Learning Human Values","summary":"How can we build AI systems that are aligned with human values to avoid causing harm or violating societal standards for acceptable behavior? We argue that representational alignment between humans and AI agents facilitates value alignment. Making AI systems learn human-like representations of the world has many known benefits, including improving generalization, robustness to domain shifts, and few-shot learning performance. We propose that this kind of representational alignment between machine learning (ML) models and humans can also support value alignment, allowing ML systems to conform to human values and societal norms. We focus on ethics as one aspect of value alignment and train ML agents using a variety of methods in a multi-armed bandit setting, where rewards reflect the moral acceptability of the chosen action. We use a synthetic experiment to demonstrate that agents' representational alignment with the environment bounds their learning performance. We then repeat this procedure in a realistic setting, using textual action descriptions and similarity judgments collected from humans and a variety of language models, to show that the results generalize and are model-agnostic when grounded in an ethically relevant context.","updated":1710293875000,"published":1703183493000,"authors":["Andrea Wynn","Ilia Sucholutsky","Thomas L. Griffiths"],"comments":"Paper accepted in Human-Centric Representation Learning workshop at\n  AAAI 2024 (https:\/\/hcrl-workshop.github.io\/2024\/)","categories":["cs.AI","cs.LG"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Artificial Intelligence","gpt4_jargon_list":"representational alignment, value alignment, domain shifts, few-shot learning, multi-armed bandit, synthetic experiment, textual action descriptions, similarity judgments, model-agnostic","human_jargon_list":"ML agents,multi-armed bandit setting,"},"6":{"arxiv_id":"2403.16289v1","reader_id":"rid0","len_gpt_jargon":3,"len_human_jargon":3,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":3,"fuzzy_false_negatives":3,"len_jargon_diff":0,"url":"http:\/\/arxiv.org\/abs\/2403.16289v1","title":"Engineering Safety Requirements for Autonomous Driving with Large\n  Language Models","summary":"Changes and updates in the requirement artifacts, which can be frequent in the automotive domain, are a challenge for SafetyOps. Large Language Models (LLMs), with their impressive natural language understanding and generating capabilities, can play a key role in automatically refining and decomposing requirements after each update. In this study, we propose a prototype of a pipeline of prompts and LLMs that receives an item definition and outputs solutions in the form of safety requirements. This pipeline also performs a review of the requirement dataset and identifies redundant or contradictory requirements. We first identified the necessary characteristics for performing HARA and then defined tests to assess an LLM's capability in meeting these criteria. We used design science with multiple iterations and let experts from different companies evaluate each cycle quantitatively and qualitatively. Finally, the prototype was implemented at a case company and the responsible team evaluated its efficiency.","updated":1711312851000,"published":1711312851000,"authors":["Ali Nouri","Beatriz Cabrero-Daniel","Fredrik T\u00f6rner","H\u0227kan Sivencrona","Christian Berger"],"comments":"Accepted in 32nd IEEE International Requirements Engineering 2024\n  conference, Iceland","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Artificial Intelligence","gpt4_jargon_list":"SafetyOps, HARA, design science","human_jargon_list":"requirement artifacts,decomposing requirements,requirement dataset"},"7":{"arxiv_id":"2403.16289v1","reader_id":"rid1","len_gpt_jargon":11,"len_human_jargon":2,"precision":0.1818181818,"recall":1.0,"f1_score":0.3076923077,"f2_score":0.5263157895,"fuzzy_true_positives":2,"fuzzy_false_positives":9,"fuzzy_false_negatives":0,"len_jargon_diff":9,"url":"http:\/\/arxiv.org\/abs\/2403.16289v1","title":"Engineering Safety Requirements for Autonomous Driving with Large\n  Language Models","summary":"Changes and updates in the requirement artifacts, which can be frequent in the automotive domain, are a challenge for SafetyOps. Large Language Models (LLMs), with their impressive natural language understanding and generating capabilities, can play a key role in automatically refining and decomposing requirements after each update. In this study, we propose a prototype of a pipeline of prompts and LLMs that receives an item definition and outputs solutions in the form of safety requirements. This pipeline also performs a review of the requirement dataset and identifies redundant or contradictory requirements. We first identified the necessary characteristics for performing HARA and then defined tests to assess an LLM's capability in meeting these criteria. We used design science with multiple iterations and let experts from different companies evaluate each cycle quantitatively and qualitatively. Finally, the prototype was implemented at a case company and the responsible team evaluated its efficiency.","updated":1711312851000,"published":1711312851000,"authors":["Ali Nouri","Beatriz Cabrero-Daniel","Fredrik T\u00f6rner","H\u0227kan Sivencrona","Christian Berger"],"comments":"Accepted in 32nd IEEE International Requirements Engineering 2024\n  conference, Iceland","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Artificial Intelligence","gpt4_jargon_list":"requirement artifacts, SafetyOps, Large Language Models (LLMs), refining and decomposing requirements, prototype of a pipeline of prompts and LLMs, item definition, safety requirements, requirement dataset, HARA, design science, case company","human_jargon_list":"SafetyOps,HARA,"},"8":{"arxiv_id":"2403.17419v1","reader_id":"rid0","len_gpt_jargon":3,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":3,"fuzzy_false_negatives":0,"len_jargon_diff":3,"url":"http:\/\/arxiv.org\/abs\/2403.17419v1","title":"AI Safety: Necessary, but insufficient and possibly problematic","summary":"This article critically examines the recent hype around AI safety. We first start with noting the nature of the AI safety hype as being dominated by governments and corporations, and contrast it with other avenues within AI research on advancing social good. We consider what 'AI safety' actually means, and outline the dominant concepts that the digital footprint of AI safety aligns with. We posit that AI safety has a nuanced and uneasy relationship with transparency and other allied notions associated with societal good, indicating that it is an insufficient notion if the goal is that of societal good in a broad sense. We note that the AI safety debate has already influenced some regulatory efforts in AI, perhaps in not so desirable directions. We also share our concerns on how AI safety may normalize AI that advances structural harm through providing exploitative and harmful AI with a veneer of safety.","updated":1711433922000,"published":1711433922000,"authors":["Deepak P"],"comments":"AI & Soc (2024)","categories":["cs.AI","cs.CY"],"primary_category":"cs.AI","doi":"10.1007\/s00146-024-01899-y","journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Artificial Intelligence","gpt4_jargon_list":"AI safety, digital footprint of AI safety, structural harm","human_jargon_list":""},"9":{"arxiv_id":"2402.01786v2","reader_id":"rid0","len_gpt_jargon":8,"len_human_jargon":1,"precision":0.125,"recall":1.0,"f1_score":0.2222222222,"f2_score":0.4166666667,"fuzzy_true_positives":1,"fuzzy_false_positives":7,"fuzzy_false_negatives":0,"len_jargon_diff":7,"url":"http:\/\/arxiv.org\/abs\/2402.01786v2","title":"COA-GPT: Generative Pre-trained Transformers for Accelerated Course of\n  Action Development in Military Operations","summary":"The development of Courses of Action (COAs) in military operations is traditionally a time-consuming and intricate process. Addressing this challenge, this study introduces COA-GPT, a novel algorithm employing Large Language Models (LLMs) for rapid and efficient generation of valid COAs. COA-GPT incorporates military doctrine and domain expertise to LLMs through in-context learning, allowing commanders to input mission information - in both text and image formats - and receive strategically aligned COAs for review and approval. Uniquely, COA-GPT not only accelerates COA development, producing initial COAs within seconds, but also facilitates real-time refinement based on commander feedback. This work evaluates COA-GPT in a military-relevant scenario within a militarized version of the StarCraft II game, comparing its performance against state-of-the-art reinforcement learning algorithms. Our results demonstrate COA-GPT's superiority in generating strategically sound COAs more swiftly, with added benefits of enhanced adaptability and alignment with commander intentions. COA-GPT's capability to rapidly adapt and update COAs during missions presents a transformative potential for military planning, particularly in addressing planning discrepancies and capitalizing on emergent windows of opportunities.","updated":1711639362000,"published":1706824269000,"authors":["Vinicius G. Goecks","Nicholas Waytowich"],"comments":"Accepted at the NATO Science and Technology Organization Symposium\n  (ICMCIS) organized by the Information Systems Technology (IST) Panel,\n  IST-205-RSY - the ICMCIS, held in Koblenz, Germany, 23-24 April 2024","categories":["cs.AI","cs.CL","cs.HC","cs.LG","I.2.6; I.2.7; J.7"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Artificial Intelligence","gpt4_jargon_list":"Courses of Action (COAs),in-context learning,military doctrine,domain expertise,state-of-the-art reinforcement learning algorithms,militarized version of the StarCraft II game,planning discrepancies,emergent windows of opportunities","human_jargon_list":"Courses of Action (COAs),"},"10":{"arxiv_id":"2402.01786v2","reader_id":"rid1","len_gpt_jargon":9,"len_human_jargon":3,"precision":0.1111111111,"recall":0.3333333333,"f1_score":0.1666666667,"f2_score":0.2380952381,"fuzzy_true_positives":1,"fuzzy_false_positives":8,"fuzzy_false_negatives":2,"len_jargon_diff":6,"url":"http:\/\/arxiv.org\/abs\/2402.01786v2","title":"COA-GPT: Generative Pre-trained Transformers for Accelerated Course of\n  Action Development in Military Operations","summary":"The development of Courses of Action (COAs) in military operations is traditionally a time-consuming and intricate process. Addressing this challenge, this study introduces COA-GPT, a novel algorithm employing Large Language Models (LLMs) for rapid and efficient generation of valid COAs. COA-GPT incorporates military doctrine and domain expertise to LLMs through in-context learning, allowing commanders to input mission information - in both text and image formats - and receive strategically aligned COAs for review and approval. Uniquely, COA-GPT not only accelerates COA development, producing initial COAs within seconds, but also facilitates real-time refinement based on commander feedback. This work evaluates COA-GPT in a military-relevant scenario within a militarized version of the StarCraft II game, comparing its performance against state-of-the-art reinforcement learning algorithms. Our results demonstrate COA-GPT's superiority in generating strategically sound COAs more swiftly, with added benefits of enhanced adaptability and alignment with commander intentions. COA-GPT's capability to rapidly adapt and update COAs during missions presents a transformative potential for military planning, particularly in addressing planning discrepancies and capitalizing on emergent windows of opportunities.","updated":1711639362000,"published":1706824269000,"authors":["Vinicius G. Goecks","Nicholas Waytowich"],"comments":"Accepted at the NATO Science and Technology Organization Symposium\n  (ICMCIS) organized by the Information Systems Technology (IST) Panel,\n  IST-205-RSY - the ICMCIS, held in Koblenz, Germany, 23-24 April 2024","categories":["cs.AI","cs.CL","cs.HC","cs.LG","I.2.6; I.2.7; J.7"],"primary_category":"cs.AI","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Artificial Intelligence","gpt4_jargon_list":"Courses of Action (COAs),in-context learning,military doctrine,domain expertise,reinforcement learning algorithms,military-relevant scenario,militarized version of the StarCraft II game,planning discrepancies,emergent windows of opportunities","human_jargon_list":"Courses of Action,COA-GPT,commander feedback"},"11":{"arxiv_id":"2403.05112v1","reader_id":"rid0","len_gpt_jargon":7,"len_human_jargon":2,"precision":0.1428571429,"recall":0.5,"f1_score":0.2222222222,"f2_score":0.3333333333,"fuzzy_true_positives":1,"fuzzy_false_positives":6,"fuzzy_false_negatives":1,"len_jargon_diff":5,"url":"http:\/\/arxiv.org\/abs\/2403.05112v1","title":"RLPeri: Accelerating Visual Perimetry Test with Reinforcement Learning\n  and Convolutional Feature Extraction","summary":"Visual perimetry is an important eye examination that helps detect vision problems caused by ocular or neurological conditions. During the test, a patient's gaze is fixed at a specific location while light stimuli of varying intensities are presented in central and peripheral vision. Based on the patient's responses to the stimuli, the visual field mapping and sensitivity are determined. However, maintaining high levels of concentration throughout the test can be challenging for patients, leading to increased examination times and decreased accuracy.  In this work, we present RLPeri, a reinforcement learning-based approach to optimize visual perimetry testing. By determining the optimal sequence of locations and initial stimulus values, we aim to reduce the examination time without compromising accuracy. Additionally, we incorporate reward shaping techniques to further improve the testing performance. To monitor the patient's responses over time during testing, we represent the test's state as a pair of 3D matrices. We apply two different convolutional kernels to extract spatial features across locations as well as features across different stimulus values for each location. Through experiments, we demonstrate that our approach results in a 10-20% reduction in examination time while maintaining the accuracy as compared to state-of-the-art methods. With the presented approach, we aim to make visual perimetry testing more efficient and patient-friendly, while still providing accurate results.","updated":1709882383000,"published":1709882383000,"authors":["Tanvi Verma","Linh Le Dinh","Nicholas Tan","Xinxing Xu","Chingyu Cheng","Yong Liu"],"comments":"Published at AAAI-24","categories":["cs.AI"],"primary_category":"cs.AI","doi":null,"journal_ref":"The 38th Annual AAAI Conference on Artificial Intelligence, 2024","peer_reviewed":true,"primary_category_readable":"Artificial Intelligence","gpt4_jargon_list":"visual perimetry, light stimuli, visual field mapping, reinforcement learning-based approach, reward shaping techniques, convolutional kernels, spatial features","human_jargon_list":"reward shaping\ntechniques,initial stimulus values,"},"12":{"arxiv_id":"2403.17911v1","reader_id":"rid0","len_gpt_jargon":4,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":4,"fuzzy_false_negatives":0,"len_jargon_diff":4,"url":"http:\/\/arxiv.org\/abs\/2403.17911v1","title":"Domain-Specific Evaluation Strategies for AI in Journalism","summary":"News organizations today rely on AI tools to increase efficiency and productivity across various tasks in news production and distribution. These tools are oriented towards stakeholders such as reporters, editors, and readers. However, practitioners also express reservations around adopting AI technologies into the newsroom, due to the technical and ethical challenges involved in evaluating AI technology and its return on investments. This is to some extent a result of the lack of domain-specific strategies to evaluate AI models and applications. In this paper, we consider different aspects of AI evaluation (model outputs, interaction, and ethics) that can benefit from domain-specific tailoring, and suggest examples of how journalistic considerations can lead to specialized metrics or strategies. In doing so, we lay out a potential framework to guide AI evaluation in journalism, such as seen in other disciplines (e.g. law, healthcare). We also consider directions for future work, as well as how our approach might generalize to other domains.","updated":1711475245000,"published":1711475245000,"authors":["Sachita Nishal","Charlotte Li","Nicholas Diakopoulos"],"comments":"Accepted to the Workshop on Evaluating AI at the ACM CHI conference\n  on Human Factors in Computing Systems","categories":["cs.CY","I.2.1; H.5; K.4"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Computers and Society","gpt4_jargon_list":"domain-specific strategies, model outputs, interaction, return on investments","human_jargon_list":""},"13":{"arxiv_id":"2403.17911v1","reader_id":"rid1","len_gpt_jargon":5,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":5,"fuzzy_false_negatives":0,"len_jargon_diff":5,"url":"http:\/\/arxiv.org\/abs\/2403.17911v1","title":"Domain-Specific Evaluation Strategies for AI in Journalism","summary":"News organizations today rely on AI tools to increase efficiency and productivity across various tasks in news production and distribution. These tools are oriented towards stakeholders such as reporters, editors, and readers. However, practitioners also express reservations around adopting AI technologies into the newsroom, due to the technical and ethical challenges involved in evaluating AI technology and its return on investments. This is to some extent a result of the lack of domain-specific strategies to evaluate AI models and applications. In this paper, we consider different aspects of AI evaluation (model outputs, interaction, and ethics) that can benefit from domain-specific tailoring, and suggest examples of how journalistic considerations can lead to specialized metrics or strategies. In doing so, we lay out a potential framework to guide AI evaluation in journalism, such as seen in other disciplines (e.g. law, healthcare). We also consider directions for future work, as well as how our approach might generalize to other domains.","updated":1711475245000,"published":1711475245000,"authors":["Sachita Nishal","Charlotte Li","Nicholas Diakopoulos"],"comments":"Accepted to the Workshop on Evaluating AI at the ACM CHI conference\n  on Human Factors in Computing Systems","categories":["cs.CY","I.2.1; H.5; K.4"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Computers and Society","gpt4_jargon_list":"domain-specific strategies, model outputs, interaction, return on investments, specialized metrics","human_jargon_list":""},"14":{"arxiv_id":"2403.11952v1","reader_id":"rid0","len_gpt_jargon":6,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":6,"fuzzy_false_negatives":0,"len_jargon_diff":6,"url":"http:\/\/arxiv.org\/abs\/2403.11952v1","title":"Exploring Estonia's Open Government Data Development as a Journey\n  towards Excellence: Unveiling the Progress of Local Governments in Open Data\n  Provision","summary":"Estonia has a global reputation of a digital state or e-country. However, despite the success in digital governance, the country has faced challenges in the realm of Open Government Data (OGD) area, with significant advancements in its OGD ecosystem, as reflected in various open data rankings from 2020 and onwards, in the recent years being recognized among trend-setters. This paper aims to explore the evolution and positioning of Estonia's OGD development, encompassing national and local levels, through an integrated analysis of various indices, primary data from the Estonian OGD portal, and a thorough literature review. The research shows that Estonia has made progress in the national level open data ecosystem, primarily due to improvements in the OGD portal usability and legislation amendments. However, the local level is not as developed, with local governments lagging behind in OGD provision. The literature review highlights the lack of previous research focusing on Estonian and European local open data, emphasizing the need for future studies to explore the barriers and enablers of municipal OGD. This study contributes to a nuanced understanding of Estonia's dynamic journey in the OGD landscape, shedding light on both achievements and areas warranting further attention for establishing a sustainable open data ecosystem.","updated":1710780605000,"published":1710780605000,"authors":["Katrin Rajam\u00e4e-Soosaar","Anastasija Nikiforova"],"comments":"This paper has been accepted for publication in Proceedings of the\n  25th Annual International Conference on Digital Government Research and this\n  is a pre-print version of the manuscript. It is posted here for your personal\n  use. Not for redistribution","categories":["cs.CY","cs.CE","cs.DB","cs.SE","cs.SI"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Computers and Society","gpt4_jargon_list":"Open Government Data (OGD), OGD ecosystem, OGD portal, open data ecosystem, municipal OGD, open data ecosystem","human_jargon_list":""},"15":{"arxiv_id":"2403.11952v1","reader_id":"rid1","len_gpt_jargon":6,"len_human_jargon":1,"precision":0.1666666667,"recall":1.0,"f1_score":0.2857142857,"f2_score":0.5,"fuzzy_true_positives":1,"fuzzy_false_positives":5,"fuzzy_false_negatives":0,"len_jargon_diff":5,"url":"http:\/\/arxiv.org\/abs\/2403.11952v1","title":"Exploring Estonia's Open Government Data Development as a Journey\n  towards Excellence: Unveiling the Progress of Local Governments in Open Data\n  Provision","summary":"Estonia has a global reputation of a digital state or e-country. However, despite the success in digital governance, the country has faced challenges in the realm of Open Government Data (OGD) area, with significant advancements in its OGD ecosystem, as reflected in various open data rankings from 2020 and onwards, in the recent years being recognized among trend-setters. This paper aims to explore the evolution and positioning of Estonia's OGD development, encompassing national and local levels, through an integrated analysis of various indices, primary data from the Estonian OGD portal, and a thorough literature review. The research shows that Estonia has made progress in the national level open data ecosystem, primarily due to improvements in the OGD portal usability and legislation amendments. However, the local level is not as developed, with local governments lagging behind in OGD provision. The literature review highlights the lack of previous research focusing on Estonian and European local open data, emphasizing the need for future studies to explore the barriers and enablers of municipal OGD. This study contributes to a nuanced understanding of Estonia's dynamic journey in the OGD landscape, shedding light on both achievements and areas warranting further attention for establishing a sustainable open data ecosystem.","updated":1710780605000,"published":1710780605000,"authors":["Katrin Rajam\u00e4e-Soosaar","Anastasija Nikiforova"],"comments":"This paper has been accepted for publication in Proceedings of the\n  25th Annual International Conference on Digital Government Research and this\n  is a pre-print version of the manuscript. It is posted here for your personal\n  use. Not for redistribution","categories":["cs.CY","cs.CE","cs.DB","cs.SE","cs.SI"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Computers and Society","gpt4_jargon_list":"Open Government Data (OGD), OGD ecosystem, OGD portal, open data ecosystem, legislation amendments, municipal OGD","human_jargon_list":"Open Government Data (OGD),"},"16":{"arxiv_id":"2403.08451v1","reader_id":"rid0","len_gpt_jargon":3,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":3,"fuzzy_false_negatives":0,"len_jargon_diff":3,"url":"http:\/\/arxiv.org\/abs\/2403.08451v1","title":"An Integrated Usability Framework for Evaluating Open Government Data\n  Portals: Comparative Analysis of EU and GCC Countries","summary":"This study explores the critical role of open government data (OGD) portals in fostering transparency and collaboration between diverse stakeholders. Recognizing the challenges of usability, communication with diverse populations, and strategic value creation, this paper develops an integrated framework for evaluating OGD portal effectiveness that accommodates user diversity (regardless of their data literacy and language), evaluates collaboration and participation, and the ability of users to explore and understand the data provided through them. The framework is validated by applying it to 33 national portals across European Union and Gulf Cooperation Council (GCC) countries, as a result of which we rank OGD portals, identify some good practices that lower-performing portals can learn from, and common shortcomings. Notably, the study unveils the competitive and innovative nature of GCC OGD portals, pinpointing specific improvement areas such as multilingual support and data understandability. The findings underscore the growing trend of exposing data quality metrics and advocate for enhanced two-way communication channels between users and portal representatives. Overall, the study contributes to accelerating the development of user-friendly, collaborative, and sustainable OGD portals while addressing gaps identified in previous research.","updated":1710331602000,"published":1710331602000,"authors":["Fillip Molodtsov","Anastasija Nikiforova"],"comments":"This paper has been accepted for publication in Proceedings of the\n  25th Annual International Conference on Digital Government Research and this\n  is a preprint version of the manuscript","categories":["cs.CY","cs.SE"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Computers and Society","gpt4_jargon_list":"open government data (OGD) portals, data literacy, data quality metrics","human_jargon_list":""},"17":{"arxiv_id":"2403.07082v1","reader_id":"rid0","len_gpt_jargon":3,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":3,"fuzzy_false_negatives":0,"len_jargon_diff":3,"url":"http:\/\/arxiv.org\/abs\/2403.07082v1","title":"Exploring the Impact of ChatGPT on Student Interactions in\n  Computer-Supported Collaborative Learning","summary":"The growing popularity of generative AI, particularly ChatGPT, has sparked both enthusiasm and caution among practitioners and researchers in education. To effectively harness the full potential of ChatGPT in educational contexts, it is crucial to analyze its impact and suitability for different educational purposes. This paper takes an initial step in exploring the applicability of ChatGPT in a computer-supported collaborative learning (CSCL) environment. Using statistical analysis, we validate the shifts in student interactions during an asynchronous group brainstorming session by introducing ChatGPT as an instantaneous question-answering agent.","updated":1710181098000,"published":1710181098000,"authors":["Han Kyul Kim","Shriniwas Nayak","Aleyeh Roknaldin","Xiaoci Zhang","Marlon Twyman","Stephen Lu"],"comments":"AAAI2024 Workshop on AI for Education (AI4ED)","categories":["cs.CY"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Computers and Society","gpt4_jargon_list":"computer-supported collaborative learning (CSCL), asynchronous group brainstorming session, instantaneous question-answering agent","human_jargon_list":""},"18":{"arxiv_id":"2401.09210v2","reader_id":"rid1","len_gpt_jargon":12,"len_human_jargon":1,"precision":0.0833333333,"recall":1.0,"f1_score":0.1538461538,"f2_score":0.3125,"fuzzy_true_positives":1,"fuzzy_false_positives":11,"fuzzy_false_negatives":0,"len_jargon_diff":11,"url":"http:\/\/arxiv.org\/abs\/2401.09210v2","title":"Narratives of Collective Action in YouTube's Discourse on Veganism","summary":"Narratives can be powerful tools for inspiring action on pressing societal issues such as climate change. While social science theories offer frameworks for understanding the narratives that arise within collective movements, these are rarely applied to the vast data available from social media platforms, which play a significant role in shaping public opinion and mobilizing collective action. This gap in the empirical evaluation of online narratives limits our understanding of their relationship with public response. In this study, we focus on plant-based diets as a form of pro-environmental action and employ natural language processing to operationalize a theoretical framework of moral narratives specific to the vegan movement. We apply this framework to narratives found in YouTube videos promoting environmental initiatives such as Veganuary, Meatless March, and No Meat May. Our analysis reveals that several narrative types, as defined by the theory, are empirically present in the data. To identify narratives with the potential to elicit positive public engagement, we used text processing to estimate the proportion of comments supporting collective action across narrative types. Video narratives advocating social fight, whether through protest or through efforts to convert others to the cause, are associated with a stronger sense of collective action in the respective comments. These narrative types also demonstrate increased semantic coherence and alignment between the message and public response, markers typically associated with successful collective action. Our work offers new insights into the complex factors that influence the emergence of collective action, thereby informing the development of effective communication strategies within social movements.","updated":1711625999000,"published":1705499076000,"authors":["Arianna Pera","Luca Maria Aiello"],"comments":"15 pages, 7 figures, 7 tables. Accepted at ICWSM 2024","categories":["cs.CY","physics.soc-ph"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Computers and Society","gpt4_jargon_list":"social science theories, frameworks, empirical evaluation, pro-environmental action, operationalize, theoretical framework, moral narratives, text processing, narrative types, semantic coherence, collective action, communication strategies","human_jargon_list":"semantic\ncoherence"},"19":{"arxiv_id":"2308.10148v3","reader_id":"rid0","len_gpt_jargon":1,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":1,"fuzzy_false_negatives":0,"len_jargon_diff":1,"url":"http:\/\/arxiv.org\/abs\/2308.10148v3","title":"Privacy Perceptions and Behaviors of Google Personal Account Holders in\n  Saudi Arabia","summary":"While privacy perceptions and behaviors have been investigated in Western societies, little is known about these issues in non-Western societies. To bridge this gap, we interviewed 30 Google personal account holders in Saudi Arabia about their privacy perceptions and behaviors regarding the activity data that Google saves about them. Our study focuses on Google's Activity Controls, which enable users to control whether, and how, Google saves their Web \\& App Activity, Location History, and YouTube History. Our results show that although most participants have some level of awareness about Google's data practices and the Activity Controls, many have only vague awareness, and the majority have not used the available controls. When participants viewed their saved activity data, many were surprised by what had been saved. While many participants find Google's use of their data to improve the services provided to them acceptable, the majority find the use of their data for ad purposes unacceptable. We observe that our Saudi participants exhibit similar trends and patterns in privacy awareness, attitudes, preferences, concerns, and behaviors to what has been found in studies in the US. Our results emphasize the need for: 1) improved techniques to inform users about privacy settings during account sign-up, to remind users about their settings, and to raise awareness about privacy settings; 2) improved privacy setting interfaces to reduce the costs that deter many users from changing the settings; and 3) further research to explore privacy concerns in non-Western cultures.","updated":1710788822000,"published":1692501918000,"authors":["Eman Alashwali","Lorrie Faith Cranor"],"comments":"To appear in Proceedings of Human Computer Interaction International\n  (HCII) 2024","categories":["cs.CY","cs.CR","cs.HC"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Computers and Society","gpt4_jargon_list":"No jargon found.","human_jargon_list":""},"20":{"arxiv_id":"2308.10148v3","reader_id":"rid1","len_gpt_jargon":4,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":4,"fuzzy_false_negatives":0,"len_jargon_diff":4,"url":"http:\/\/arxiv.org\/abs\/2308.10148v3","title":"Privacy Perceptions and Behaviors of Google Personal Account Holders in\n  Saudi Arabia","summary":"While privacy perceptions and behaviors have been investigated in Western societies, little is known about these issues in non-Western societies. To bridge this gap, we interviewed 30 Google personal account holders in Saudi Arabia about their privacy perceptions and behaviors regarding the activity data that Google saves about them. Our study focuses on Google's Activity Controls, which enable users to control whether, and how, Google saves their Web \\& App Activity, Location History, and YouTube History. Our results show that although most participants have some level of awareness about Google's data practices and the Activity Controls, many have only vague awareness, and the majority have not used the available controls. When participants viewed their saved activity data, many were surprised by what had been saved. While many participants find Google's use of their data to improve the services provided to them acceptable, the majority find the use of their data for ad purposes unacceptable. We observe that our Saudi participants exhibit similar trends and patterns in privacy awareness, attitudes, preferences, concerns, and behaviors to what has been found in studies in the US. Our results emphasize the need for: 1) improved techniques to inform users about privacy settings during account sign-up, to remind users about their settings, and to raise awareness about privacy settings; 2) improved privacy setting interfaces to reduce the costs that deter many users from changing the settings; and 3) further research to explore privacy concerns in non-Western cultures.","updated":1710788822000,"published":1692501918000,"authors":["Eman Alashwali","Lorrie Faith Cranor"],"comments":"To appear in Proceedings of Human Computer Interaction International\n  (HCII) 2024","categories":["cs.CY","cs.CR","cs.HC"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Computers and Society","gpt4_jargon_list":"Activity Controls,Web & App Activity,Location History,YouTube History","human_jargon_list":""},"21":{"arxiv_id":"2403.02908v1","reader_id":"rid0","len_gpt_jargon":2,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":2,"fuzzy_false_negatives":0,"len_jargon_diff":2,"url":"http:\/\/arxiv.org\/abs\/2403.02908v1","title":"Preserving Tangible and Intangible Cultural Heritage: the Cases of\n  Volterra and Atari","summary":"At first glance, the ruins of the Roman Theatre in the Italian town of Volterra have little in common with cassette tapes containing Atari games. One is certainly considered an important historical landmark, while the consensus on the importance of the other is partial at best. Still, both are remnants of times vastly different from the present and are at risk of oblivion. Unearthed architectural structures are exposed to the elements just as the deteriorating signals stored on magnetic tapes. However, the rate of deterioration is much faster with the magnetic media, as their life expectancy is counted in decades, whereas the Roman Theater, which is already in ruin, measures its lifespan in centuries. Hence, both would benefit from some form of digital preservation and reconstruction. In this panel, we discuss how to sustainably preserve tangible and intangible cultural artifacts for future generations.","updated":1709641088000,"published":1709641088000,"authors":["Maciej Grzeszczuk","Kinga Skorupska","Pawe\u0142 Grabarczyk","W\u0142adys\u0142aw Fuchs","Paul F. Aubin","Mark E. Dietrick","Barbara Karpowicz","Rafa\u0142 Mas\u0142yk","Pavlo Zinevych","Wiktor Stawski","Stanis\u0142aw Knapi\u0144ski","Wies\u0142aw Kope\u0107"],"comments":"8 pages, including 1 page of bibliography, 9 figures. Panel summary\n  to be published in proceedings from 11th Machine Intelligence and Digital\n  Interaction MIDI Conference","categories":["cs.CY","cs.DL","cs.HC"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Computers and Society","gpt4_jargon_list":"intangible cultural artifacts, digital preservation","human_jargon_list":""},"22":{"arxiv_id":"2403.02908v1","reader_id":"rid1","len_gpt_jargon":1,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":1,"fuzzy_false_negatives":0,"len_jargon_diff":1,"url":"http:\/\/arxiv.org\/abs\/2403.02908v1","title":"Preserving Tangible and Intangible Cultural Heritage: the Cases of\n  Volterra and Atari","summary":"At first glance, the ruins of the Roman Theatre in the Italian town of Volterra have little in common with cassette tapes containing Atari games. One is certainly considered an important historical landmark, while the consensus on the importance of the other is partial at best. Still, both are remnants of times vastly different from the present and are at risk of oblivion. Unearthed architectural structures are exposed to the elements just as the deteriorating signals stored on magnetic tapes. However, the rate of deterioration is much faster with the magnetic media, as their life expectancy is counted in decades, whereas the Roman Theater, which is already in ruin, measures its lifespan in centuries. Hence, both would benefit from some form of digital preservation and reconstruction. In this panel, we discuss how to sustainably preserve tangible and intangible cultural artifacts for future generations.","updated":1709641088000,"published":1709641088000,"authors":["Maciej Grzeszczuk","Kinga Skorupska","Pawe\u0142 Grabarczyk","W\u0142adys\u0142aw Fuchs","Paul F. Aubin","Mark E. Dietrick","Barbara Karpowicz","Rafa\u0142 Mas\u0142yk","Pavlo Zinevych","Wiktor Stawski","Stanis\u0142aw Knapi\u0144ski","Wies\u0142aw Kope\u0107"],"comments":"8 pages, including 1 page of bibliography, 9 figures. Panel summary\n  to be published in proceedings from 11th Machine Intelligence and Digital\n  Interaction MIDI Conference","categories":["cs.CY","cs.DL","cs.HC"],"primary_category":"cs.CY","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Computers and Society","gpt4_jargon_list":"No jargon terms identified.","human_jargon_list":""},"23":{"arxiv_id":"2310.06155v3","reader_id":"rid1","len_gpt_jargon":6,"len_human_jargon":2,"precision":0.1666666667,"recall":0.5,"f1_score":0.25,"f2_score":0.3571428571,"fuzzy_true_positives":1,"fuzzy_false_positives":5,"fuzzy_false_negatives":1,"len_jargon_diff":4,"url":"http:\/\/arxiv.org\/abs\/2310.06155v3","title":"CoQuest: Exploring Research Question Co-Creation with an LLM-based Agent","summary":"Developing novel research questions (RQs) often requires extensive literature reviews, especially in interdisciplinary fields. To support RQ development through human-AI co-creation, we leveraged Large Language Models (LLMs) to build an LLM-based agent system named CoQuest. We conducted an experiment with 20 HCI researchers to examine the impact of two interaction designs: breadth-first and depth-first RQ generation. The findings revealed that participants perceived the breadth-first approach as more creative and trustworthy upon task completion. Conversely, during the task, participants considered the depth-first generated RQs as more creative. Additionally, we discovered that AI processing delays allowed users to reflect on multiple RQs simultaneously, leading to a higher quantity of generated RQs and an enhanced sense of control. Our work makes both theoretical and practical contributions by proposing and evaluating a mental model for human-AI co-creation of RQs. We also address potential ethical issues, such as biases and over-reliance on AI, advocating for using the system to improve human research creativity rather than automating scientific inquiry.","updated":1710967383000,"published":1696885527000,"authors":["Yiren Liu","Si Chen","Haocong Cheng","Mengxia Yu","Xiao Ran","Andrew Mo","Yiliu Tang","Yun Huang"],"comments":"Accepted to SIGCHI 2024","categories":["cs.HC","cs.CE"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"human-AI co-creation, Large Language Models (LLMs), LLM-based agent system, breadth-first and depth-first RQ generation, mental model for human-AI co-creation, biases and over-reliance on AI","human_jargon_list":"research questions,breadth-first and depth-first RQ generation,"},"24":{"arxiv_id":"2403.04760v1","reader_id":"rid1","len_gpt_jargon":9,"len_human_jargon":2,"precision":0.1111111111,"recall":0.5,"f1_score":0.1818181818,"f2_score":0.2941176471,"fuzzy_true_positives":1,"fuzzy_false_positives":8,"fuzzy_false_negatives":1,"len_jargon_diff":7,"url":"http:\/\/arxiv.org\/abs\/2403.04760v1","title":"iScore: Visual Analytics for Interpreting How Language Models\n  Automatically Score Summaries","summary":"The recent explosion in popularity of large language models (LLMs) has inspired learning engineers to incorporate them into adaptive educational tools that automatically score summary writing. Understanding and evaluating LLMs is vital before deploying them in critical learning environments, yet their unprecedented size and expanding number of parameters inhibits transparency and impedes trust when they underperform. Through a collaborative user-centered design process with several learning engineers building and deploying summary scoring LLMs, we characterized fundamental design challenges and goals around interpreting their models, including aggregating large text inputs, tracking score provenance, and scaling LLM interpretability methods. To address their concerns, we developed iScore, an interactive visual analytics tool for learning engineers to upload, score, and compare multiple summaries simultaneously. Tightly integrated views allow users to iteratively revise the language in summaries, track changes in the resulting LLM scores, and visualize model weights at multiple levels of abstraction. To validate our approach, we deployed iScore with three learning engineers over the course of a month. We present a case study where interacting with iScore led a learning engineer to improve their LLM's score accuracy by three percentage points. Finally, we conducted qualitative interviews with the learning engineers that revealed how iScore enabled them to understand, evaluate, and build trust in their LLMs during deployment.","updated":1709837799000,"published":1709837799000,"authors":["Adam Coscia","Langdon Holmes","Wesley Morris","Joon Suh Choi","Scott Crossley","Alex Endert"],"comments":"Accepted to IUI 2024. 16 pages, 5 figures, 1 table. For a demo video,\n  see https:\/\/youtu.be\/EYJX-_fQPf0 . For a live demo, visit\n  https:\/\/adamcoscia.com\/papers\/iscore\/demo\/ . The source code is available at\n  https:\/\/github.com\/AdamCoscia\/iScore","categories":["cs.HC","cs.AI","cs.CY","cs.LG"],"primary_category":"cs.HC","doi":"10.1145\/3640543.3645142","journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"large language models,unprecedented size,expanding number of parameters,aggregating large text inputs,score provenance,scaling LLM interpretability methods,interactive visual analytics tool,multiple levels of abstraction,qualitative interviews","human_jargon_list":"score provenance,iScore,"},"25":{"arxiv_id":"2403.06267v1","reader_id":"rid0","len_gpt_jargon":7,"len_human_jargon":1,"precision":0.1428571429,"recall":1.0,"f1_score":0.25,"f2_score":0.4545454545,"fuzzy_true_positives":1,"fuzzy_false_positives":6,"fuzzy_false_negatives":0,"len_jargon_diff":6,"url":"http:\/\/arxiv.org\/abs\/2403.06267v1","title":"FARPLS: A Feature-Augmented Robot Trajectory Preference Labeling System\n  to Assist Human Labelers' Preference Elicitation","summary":"Preference-based learning aims to align robot task objectives with human values. One of the most common methods to infer human preferences is by pairwise comparisons of robot task trajectories. Traditional comparison-based preference labeling systems seldom support labelers to digest and identify critical differences between complex trajectories recorded in videos. Our formative study (N = 12) suggests that individuals may overlook non-salient task features and establish biased preference criteria during their preference elicitation process because of partial observations. In addition, they may experience mental fatigue when given many pairs to compare, causing their label quality to deteriorate. To mitigate these issues, we propose FARPLS, a Feature-Augmented Robot trajectory Preference Labeling System. FARPLS highlights potential outliers in a wide variety of task features that matter to humans and extracts the corresponding video keyframes for easy review and comparison. It also dynamically adjusts the labeling order according to users' familiarities, difficulties of the trajectory pair, and level of disagreements. At the same time, the system monitors labelers' consistency and provides feedback on labeling progress to keep labelers engaged. A between-subjects study (N = 42, 105 pairs of robot pick-and-place trajectories per person) shows that FARPLS can help users establish preference criteria more easily and notice more relevant details in the presented trajectories than the conventional interface. FARPLS also improves labeling consistency and engagement, mitigating challenges in preference elicitation without raising cognitive loads significantly","updated":1710090440000,"published":1710090440000,"authors":["Hanfang Lyu","Yuanchen Bai","Xin Liang","Ujaan Das","Chuhan Shi","Leiliang Gong","Yingchi Li","Mingfei Sun","Ming Ge","Xiaojuan Ma"],"comments":"Accepted to ACM Conference on Intelligent User Interfaces (IUI) 2024,\n  March 18-21, 2024, Greenville, SC, USA","categories":["cs.HC","cs.AI"],"primary_category":"cs.HC","doi":"10.1145\/3640543.3645145","journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"pairwise comparisons of robot task trajectories, Feature-Augmented Robot trajectory Preference Labeling System, video keyframes, labeling order, trajectory pair, between-subjects study, robot pick-and-place trajectories","human_jargon_list":"robot task trajectories"},"26":{"arxiv_id":"2403.06267v1","reader_id":"rid1","len_gpt_jargon":19,"len_human_jargon":5,"precision":0.1052631579,"recall":0.4,"f1_score":0.1666666667,"f2_score":0.2564102564,"fuzzy_true_positives":2,"fuzzy_false_positives":17,"fuzzy_false_negatives":3,"len_jargon_diff":14,"url":"http:\/\/arxiv.org\/abs\/2403.06267v1","title":"FARPLS: A Feature-Augmented Robot Trajectory Preference Labeling System\n  to Assist Human Labelers' Preference Elicitation","summary":"Preference-based learning aims to align robot task objectives with human values. One of the most common methods to infer human preferences is by pairwise comparisons of robot task trajectories. Traditional comparison-based preference labeling systems seldom support labelers to digest and identify critical differences between complex trajectories recorded in videos. Our formative study (N = 12) suggests that individuals may overlook non-salient task features and establish biased preference criteria during their preference elicitation process because of partial observations. In addition, they may experience mental fatigue when given many pairs to compare, causing their label quality to deteriorate. To mitigate these issues, we propose FARPLS, a Feature-Augmented Robot trajectory Preference Labeling System. FARPLS highlights potential outliers in a wide variety of task features that matter to humans and extracts the corresponding video keyframes for easy review and comparison. It also dynamically adjusts the labeling order according to users' familiarities, difficulties of the trajectory pair, and level of disagreements. At the same time, the system monitors labelers' consistency and provides feedback on labeling progress to keep labelers engaged. A between-subjects study (N = 42, 105 pairs of robot pick-and-place trajectories per person) shows that FARPLS can help users establish preference criteria more easily and notice more relevant details in the presented trajectories than the conventional interface. FARPLS also improves labeling consistency and engagement, mitigating challenges in preference elicitation without raising cognitive loads significantly","updated":1710090440000,"published":1710090440000,"authors":["Hanfang Lyu","Yuanchen Bai","Xin Liang","Ujaan Das","Chuhan Shi","Leiliang Gong","Yingchi Li","Mingfei Sun","Ming Ge","Xiaojuan Ma"],"comments":"Accepted to ACM Conference on Intelligent User Interfaces (IUI) 2024,\n  March 18-21, 2024, Greenville, SC, USA","categories":["cs.HC","cs.AI"],"primary_category":"cs.HC","doi":"10.1145\/3640543.3645145","journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"pairwise comparisons, robot task trajectories, preference labeling systems, labelers, non-salient task features, preference elicitation process, mental fatigue, Feature-Augmented Robot trajectory Preference Labeling System, outliers, task features, video keyframes, labeling order, disagreements, labelers' consistency, between-subjects study, robot pick-and-place trajectories, labeling consistency, preference elicitation, cognitive loads","human_jargon_list":"Preference-based learning,non-salient,FARPLS,trajectory pair,cognitive loads"},"27":{"arxiv_id":"2403.06039v1","reader_id":"rid0","len_gpt_jargon":1,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":1,"fuzzy_false_negatives":0,"len_jargon_diff":1,"url":"http:\/\/arxiv.org\/abs\/2403.06039v1","title":"A Preliminary Exploration of YouTubers' Use of Generative-AI in Content\n  Creation","summary":"Content creators increasingly utilize generative artificial intelligence (Gen-AI) on platforms such as YouTube, TikTok, Instagram, and various blogging sites to produce imaginative images, AI-generated videos, and articles using Large Language Models (LLMs). Despite its growing popularity, there remains an underexplored area concerning the specific domains where AI-generated content is being applied, and the methodologies content creators employ with Gen-AI tools during the creation process. This study initially explores this emerging area through a qualitative analysis of 68 YouTube videos demonstrating Gen-AI usage. Our research focuses on identifying the content domains, the variety of tools used, the activities performed, and the nature of the final products generated by Gen-AI in the context of user-generated content.","updated":1710026576000,"published":1710026576000,"authors":["Yao Lyu","He Zhang","Shuo Niu","Jie Cai"],"comments":"Accepted at CHI LBW 2024","categories":["cs.HC","cs.AI"],"primary_category":"cs.HC","doi":"10.1145\/3613905.3651057","journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"qualitative analysis","human_jargon_list":""},"28":{"arxiv_id":"2403.06039v1","reader_id":"rid1","len_gpt_jargon":4,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":4,"fuzzy_false_negatives":0,"len_jargon_diff":4,"url":"http:\/\/arxiv.org\/abs\/2403.06039v1","title":"A Preliminary Exploration of YouTubers' Use of Generative-AI in Content\n  Creation","summary":"Content creators increasingly utilize generative artificial intelligence (Gen-AI) on platforms such as YouTube, TikTok, Instagram, and various blogging sites to produce imaginative images, AI-generated videos, and articles using Large Language Models (LLMs). Despite its growing popularity, there remains an underexplored area concerning the specific domains where AI-generated content is being applied, and the methodologies content creators employ with Gen-AI tools during the creation process. This study initially explores this emerging area through a qualitative analysis of 68 YouTube videos demonstrating Gen-AI usage. Our research focuses on identifying the content domains, the variety of tools used, the activities performed, and the nature of the final products generated by Gen-AI in the context of user-generated content.","updated":1710026576000,"published":1710026576000,"authors":["Yao Lyu","He Zhang","Shuo Niu","Jie Cai"],"comments":"Accepted at CHI LBW 2024","categories":["cs.HC","cs.AI"],"primary_category":"cs.HC","doi":"10.1145\/3613905.3651057","journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"generative artificial intelligence, qualitative analysis, content domains, user-generated content","human_jargon_list":""},"29":{"arxiv_id":"2402.09494v2","reader_id":"rid0","len_gpt_jargon":3,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":3,"fuzzy_false_negatives":0,"len_jargon_diff":3,"url":"http:\/\/arxiv.org\/abs\/2402.09494v2","title":"Can AI and humans genuinely communicate?","summary":"Can AI and humans genuinely communicate? In this article, after giving some background and motivating my proposal (sections 1 to 3), I explore a way to answer this question that I call the \"mental-behavioral methodology\" (sections 4 and 5). This methodology follows the following three steps: First, spell out what mental capacities are sufficient for human communication (as opposed to communication more generally). Second, spell out the experimental paradigms required to test whether a behavior exhibits these capacities. Third, apply or adapt these paradigms to test whether an AI displays the relevant behaviors. If the first two steps are successfully completed, and if the AI passes the tests with human-like results, this constitutes evidence that this AI and humans can genuinely communicate. This mental-behavioral methodology has the advantage that we don't need to understand the workings of black-box algorithms, such as standard deep neural networks. This is comparable to the fact that we don't need to understand how human brains work to know that humans can genuinely communicate. This methodology also has its disadvantages and I will discuss some of them (section 6).","updated":1711384364000,"published":1707915640000,"authors":["Constant Bonard"],"comments":"March 2024 preprint","categories":["cs.HC","cs.AI"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"mental-behavioral methodology,black-box algorithms,standard deep neural networks","human_jargon_list":""},"30":{"arxiv_id":"2403.06431v1","reader_id":"rid0","len_gpt_jargon":2,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":2,"fuzzy_false_negatives":0,"len_jargon_diff":2,"url":"http:\/\/arxiv.org\/abs\/2403.06431v1","title":"From Fitting Participation to Forging Relationships: The Art of\n  Participatory ML","summary":"Participatory machine learning (ML) encourages the inclusion of end users and people affected by ML systems in design and development processes. We interviewed 18 participation brokers -- individuals who facilitate such inclusion and transform the products of participants' labour into inputs for an ML artefact or system -- across a range of organisational settings and project locations. Our findings demonstrate the inherent challenges of integrating messy contextual information generated through participation with the structured data formats required by ML workflows and the uneven power dynamics in project contexts. We advocate for evolution in the role of brokers to more equitably balance value generated in Participatory ML projects for design and development teams with value created for participants. To move beyond `fitting' participation to existing processes and empower participants to envision alternative futures through ML, brokers must become educators and advocates for end users, while attending to frustration and dissent from indirect stakeholders.","updated":1710132274000,"published":1710132274000,"authors":["Ned Cooper","Alex Zafiroglu"],"comments":"To appear in Proceedings of the 2024 CHI Conference on Human Factors\n  in Computing Systems (CHI '24)","categories":["cs.HC","cs.CY"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"participation brokers, ML artefact","human_jargon_list":""},"31":{"arxiv_id":"2403.06431v1","reader_id":"rid1","len_gpt_jargon":5,"len_human_jargon":1,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":5,"fuzzy_false_negatives":1,"len_jargon_diff":4,"url":"http:\/\/arxiv.org\/abs\/2403.06431v1","title":"From Fitting Participation to Forging Relationships: The Art of\n  Participatory ML","summary":"Participatory machine learning (ML) encourages the inclusion of end users and people affected by ML systems in design and development processes. We interviewed 18 participation brokers -- individuals who facilitate such inclusion and transform the products of participants' labour into inputs for an ML artefact or system -- across a range of organisational settings and project locations. Our findings demonstrate the inherent challenges of integrating messy contextual information generated through participation with the structured data formats required by ML workflows and the uneven power dynamics in project contexts. We advocate for evolution in the role of brokers to more equitably balance value generated in Participatory ML projects for design and development teams with value created for participants. To move beyond `fitting' participation to existing processes and empower participants to envision alternative futures through ML, brokers must become educators and advocates for end users, while attending to frustration and dissent from indirect stakeholders.","updated":1710132274000,"published":1710132274000,"authors":["Ned Cooper","Alex Zafiroglu"],"comments":"To appear in Proceedings of the 2024 CHI Conference on Human Factors\n  in Computing Systems (CHI '24)","categories":["cs.HC","cs.CY"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"participation brokers, messy contextual information, structured data formats, uneven power dynamics, indirect stakeholders","human_jargon_list":"Participatory machine learning,"},"32":{"arxiv_id":"2403.06034v1","reader_id":"rid1","len_gpt_jargon":6,"len_human_jargon":1,"precision":0.1666666667,"recall":1.0,"f1_score":0.2857142857,"f2_score":0.5,"fuzzy_true_positives":1,"fuzzy_false_positives":5,"fuzzy_false_negatives":0,"len_jargon_diff":5,"url":"http:\/\/arxiv.org\/abs\/2403.06034v1","title":"Content Moderation Justice and Fairness on Social Media: Comparisons\n  Across Different Contexts and Platforms","summary":"Social media users may perceive moderation decisions by the platform differently, which can lead to frustration and dropout. This study investigates users' perceived justice and fairness of online moderation decisions when they are exposed to various illegal versus legal scenarios, retributive versus restorative moderation strategies, and user-moderated versus commercially moderated platforms. We conduct an online experiment on 200 American social media users of Reddit and Twitter. Results show that retributive moderation delivers higher justice and fairness for commercially moderated than for user-moderated platforms in illegal violations; restorative moderation delivers higher fairness for legal violations than illegal ones. We discuss the opportunities for platform policymaking to improve moderation system design.","updated":1710024606000,"published":1710024606000,"authors":["Jie Cai","Aashka Patel","Azadeh Naderi","Donghee Yvette Wohn"],"comments":"Accepted by CHI LBW 2024","categories":["cs.HC","cs.CY"],"primary_category":"cs.HC","doi":"10.1145\/3613905.3650882","journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"perceived justice, restorative moderation, retributive moderation, commercially moderated platforms, user-moderated platforms, moderation system design","human_jargon_list":"retributive versus restorative moderation strategies"},"33":{"arxiv_id":"2403.06651v1","reader_id":"rid0","len_gpt_jargon":6,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":6,"fuzzy_false_negatives":0,"len_jargon_diff":6,"url":"http:\/\/arxiv.org\/abs\/2403.06651v1","title":"SoniWeight Shoes: Investigating Effects and Personalization of a\n  Wearable Sound Device for Altering Body Perception and Behavior","summary":"Changes in body perception influence behavior and emotion and can be induced through multisensory feedback. Auditory feedback to one's actions can trigger such alterations; however, it is unclear which individual factors modulate these effects. We employ and evaluate SoniWeight Shoes, a wearable device based on literature for altering one's weight perception through manipulated footstep sounds. In a healthy population sample across a spectrum of individuals (n=84) with varying degrees of eating disorder symptomatology, physical activity levels, body concerns, and mental imagery capacities, we explore the effects of three sound conditions (low-frequency, high-frequency and control) on extensive body perception measures (demographic, behavioral, physiological, psychological, and subjective). Analyses revealed an impact of individual differences in each of these dimensions. Besides replicating previous findings, we reveal and highlight the role of individual differences in body perception, offering avenues for personalized sonification strategies. Datasets, technical refinements, and novel body map quantification tools are provided.","updated":1710159374000,"published":1710159374000,"authors":["A. D'Adamo","M. Roel-Lesur","L. Turmo-Vidal","M. M. Dehshibi","D. De La Prida","J. R. Diaz-Duran","L. A. Azpicueta-Ruiz","A. V\u00e4ljam\u00e4e","A. Tajadura-Jim\u00e9nez"],"comments":"Conditionally Accepted in CHI '24 Conference","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642651","journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"SoniWeight Shoes, multisensory feedback, manipulated footstep sounds, extensive body perception measures, personalized sonification strategies, body map quantification tools","human_jargon_list":""},"34":{"arxiv_id":"2403.06651v1","reader_id":"rid1","len_gpt_jargon":17,"len_human_jargon":2,"precision":0.1176470588,"recall":1.0,"f1_score":0.2105263158,"f2_score":0.4,"fuzzy_true_positives":2,"fuzzy_false_positives":15,"fuzzy_false_negatives":0,"len_jargon_diff":15,"url":"http:\/\/arxiv.org\/abs\/2403.06651v1","title":"SoniWeight Shoes: Investigating Effects and Personalization of a\n  Wearable Sound Device for Altering Body Perception and Behavior","summary":"Changes in body perception influence behavior and emotion and can be induced through multisensory feedback. Auditory feedback to one's actions can trigger such alterations; however, it is unclear which individual factors modulate these effects. We employ and evaluate SoniWeight Shoes, a wearable device based on literature for altering one's weight perception through manipulated footstep sounds. In a healthy population sample across a spectrum of individuals (n=84) with varying degrees of eating disorder symptomatology, physical activity levels, body concerns, and mental imagery capacities, we explore the effects of three sound conditions (low-frequency, high-frequency and control) on extensive body perception measures (demographic, behavioral, physiological, psychological, and subjective). Analyses revealed an impact of individual differences in each of these dimensions. Besides replicating previous findings, we reveal and highlight the role of individual differences in body perception, offering avenues for personalized sonification strategies. Datasets, technical refinements, and novel body map quantification tools are provided.","updated":1710159374000,"published":1710159374000,"authors":["A. D'Adamo","M. Roel-Lesur","L. Turmo-Vidal","M. M. Dehshibi","D. De La Prida","J. R. Diaz-Duran","L. A. Azpicueta-Ruiz","A. V\u00e4ljam\u00e4e","A. Tajadura-Jim\u00e9nez"],"comments":"Conditionally Accepted in CHI '24 Conference","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642651","journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"multisensory feedback, auditory feedback, SoniWeight Shoes, manipulated footstep sounds, eating disorder symptomatology, physical activity levels, body concerns, mental imagery capacities, sound conditions, extensive body perception measures, demographic, behavioral, physiological, psychological, subjective, personalized sonification strategies, body map quantification tools","human_jargon_list":"multisensory feedback,SoniWeight Shoes"},"35":{"arxiv_id":"2403.08041v1","reader_id":"rid0","len_gpt_jargon":4,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":4,"fuzzy_false_negatives":0,"len_jargon_diff":4,"url":"http:\/\/arxiv.org\/abs\/2403.08041v1","title":"What would Plato say? Concepts and notions from Greek philosophy applied\n  to gamification mechanics for a meaningful and ethical gamification","summary":"Gamification, the integration of game mechanics in non-game settings, has become increasingly prevalent in various digital platforms; however, its ethical and societal impacts are often overlooked. This paper delves into how Platonic and Aristotelian philosophies can provide a critical framework for understanding and evaluating the ethical dimensions of gamification. Plato's allegory of the cave and theory of forms are used to analyse the perception of reality in gamified environments, questioning their authenticity and the value of virtual achievements, while Aristotle's virtue ethics, with its emphasis on moderation, virtue, and eudaimonia (true and full happiness), can help assess how gamification influences user behaviour and ethical decision-making. The paper critically examines various gamification elements, such as the hero's journey, altruistic actions, badge levels, and user autonomy, through these philosophical lenses, and addresses the ethical responsibilities of gamification designers, advocating for a balanced approach that prioritizes user well-being and ethical development over commercial interests. By bridging ancient philosophical insights with modern digital culture, this research contributes to a deeper understanding of the ethical implications of gamification, emphasizing the need for responsible and virtuous design in digital applications.","updated":1710271513000,"published":1710271513000,"authors":["Kostas Karpouzis"],"comments":"Accepted for presentation at GamiFIN 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"Plato's allegory of the cave, theory of forms, virtue ethics, eudaimonia","human_jargon_list":""},"36":{"arxiv_id":"2403.12344v1","reader_id":"rid0","len_gpt_jargon":5,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":5,"fuzzy_false_negatives":0,"len_jargon_diff":5,"url":"http:\/\/arxiv.org\/abs\/2403.12344v1","title":"Human Factors in Space Exploration: Opportunities for International and\n  Interdisciplinary Collaboration","summary":"As humanity pushes the boundaries of space exploration, human factors research becomes more important. Human factors encompass a broad spectrum of psychological, physiological, and ergonomic factors that affect human performance, well-being, and safety in the unique and challenging space environment. This panel explores the multifaceted field of human factors in space exploration and highlights the opportunities that lie in fostering international and interdisciplinary cooperation. This exploration delves into the current state of research on human factors in space missions, addressing the physiological and psychological challenges astronauts face during long space flights. It emphasizes the importance of interdisciplinary collaboration, combining knowledge from fields such as psychology, medicine, engineering, and design to address the complex interaction of factors affecting human performance and adaptation to the space environment","updated":1710811635000,"published":1710811635000,"authors":["Wies\u0142aw Kope\u0107","Grzegorz Pochwatko","Monika Kornacka","Wiktor Stawski","Maciej Grzeszczuk","Kinga Skorupska","Barbara Karpowicz","Rafa\u0142 Mas\u0142yk","Pavlo Zinevych","Stanis\u0142aw Knapi\u0144ski","Steven Barnes","Cezary Biele"],"comments":"13 pages including bibliography, 4 figures. To be published by\n  Springer as MIDI 2023 Conference proceedings","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"psychological, physiological, ergonomic factors, human performance, well-being","human_jargon_list":""},"37":{"arxiv_id":"2403.19436v1","reader_id":"rid0","len_gpt_jargon":5,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":5,"fuzzy_false_negatives":0,"len_jargon_diff":5,"url":"http:\/\/arxiv.org\/abs\/2403.19436v1","title":"\"At the end of the day, I am accountable\": Gig Workers' Self-Tracking\n  for Multi-Dimensional Accountability Management","summary":"Tracking is inherent in and central to the gig economy. Platforms track gig workers' performance through metrics such as acceptance rate and punctuality, while gig workers themselves engage in self-tracking. Although prior research has extensively examined how gig platforms track workers through metrics -- with some studies briefly acknowledging the phenomenon of self-tracking among workers -- there is a dearth of studies that explore how and why gig workers track themselves. To address this, we conducted 25 semi-structured interviews, revealing how gig workers self-tracking to manage accountabilities to themselves and external entities across three identities: the holistic self, the entrepreneurial self, and the platformized self. We connect our findings to neoliberalism, through which we contextualize gig workers' self-accountability and the invisible labor of self-tracking. We further discuss how self-tracking mitigates information and power asymmetries in gig work and offer design implications to support gig workers' multi-dimensional self-tracking.","updated":1711634670000,"published":1711634670000,"authors":["Rie Helene Hernandez","Qiurong Song","Yubo Kou","Xinning Gui"],"comments":"Accepted to CHI 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"neoliberalism, entrepreneurial self, platformized self, semi-structured interviews, design implications","human_jargon_list":""},"38":{"arxiv_id":"2403.19436v1","reader_id":"rid1","len_gpt_jargon":11,"len_human_jargon":2,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":11,"fuzzy_false_negatives":2,"len_jargon_diff":9,"url":"http:\/\/arxiv.org\/abs\/2403.19436v1","title":"\"At the end of the day, I am accountable\": Gig Workers' Self-Tracking\n  for Multi-Dimensional Accountability Management","summary":"Tracking is inherent in and central to the gig economy. Platforms track gig workers' performance through metrics such as acceptance rate and punctuality, while gig workers themselves engage in self-tracking. Although prior research has extensively examined how gig platforms track workers through metrics -- with some studies briefly acknowledging the phenomenon of self-tracking among workers -- there is a dearth of studies that explore how and why gig workers track themselves. To address this, we conducted 25 semi-structured interviews, revealing how gig workers self-tracking to manage accountabilities to themselves and external entities across three identities: the holistic self, the entrepreneurial self, and the platformized self. We connect our findings to neoliberalism, through which we contextualize gig workers' self-accountability and the invisible labor of self-tracking. We further discuss how self-tracking mitigates information and power asymmetries in gig work and offer design implications to support gig workers' multi-dimensional self-tracking.","updated":1711634670000,"published":1711634670000,"authors":["Rie Helene Hernandez","Qiurong Song","Yubo Kou","Xinning Gui"],"comments":"Accepted to CHI 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"acceptance rate, punctuality, semi-structured interviews, accountabilities, entrepreneurial self, platformized self, neoliberalism, invisible labor, information and power asymmetries, design implications, multi-dimensional self-tracking","human_jargon_list":"gig economy,dearthneoliberalism,"},"39":{"arxiv_id":"2401.10838v2","reader_id":"rid0","len_gpt_jargon":7,"len_human_jargon":1,"precision":0.1428571429,"recall":1.0,"f1_score":0.25,"f2_score":0.4545454545,"fuzzy_true_positives":1,"fuzzy_false_positives":6,"fuzzy_false_negatives":0,"len_jargon_diff":6,"url":"http:\/\/arxiv.org\/abs\/2401.10838v2","title":"Rambler: Supporting Writing With Speech via LLM-Assisted Gist\n  Manipulation","summary":"Dictation enables efficient text input on mobile devices. However, writing with speech can produce disfluent, wordy, and incoherent text and thus requires heavy post-processing. This paper presents Rambler, an LLM-powered graphical user interface that supports gist-level manipulation of dictated text with two main sets of functions: gist extraction and macro revision. Gist extraction generates keywords and summaries as anchors to support the review and interaction with spoken text. LLM-assisted macro revisions allow users to respeak, split, merge and transform dictated text without specifying precise editing locations. Together they pave the way for interactive dictation and revision that help close gaps between spontaneous spoken words and well-structured writing. In a comparative study with 12 participants performing verbal composition tasks, Rambler outperformed the baseline of a speech-to-text editor + ChatGPT, as it better facilitates iterative revisions with enhanced user control over the content while supporting surprisingly diverse user strategies.","updated":1709865965000,"published":1705685996000,"authors":["Susan Lin","Jeremy Warner","J. D. Zamfirescu-Pereira","Matthew G. Lee","Sauhard Jain","Michael Xuelin Huang","Piyawat Lertvittayakumjorn","Shanqing Cai","Shumin Zhai","Bj\u00f6rn Hartmann","Can Liu"],"comments":"To appear at ACM CHI 2024","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642217","journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"LLM-powered, gist-level manipulation, gist extraction, macro revision, LLM-assisted macro revisions, respeak, verbal composition tasks","human_jargon_list":"gist-level manipulation"},"40":{"arxiv_id":"2403.16018v1","reader_id":"rid0","len_gpt_jargon":7,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":7,"fuzzy_false_negatives":0,"len_jargon_diff":7,"url":"http:\/\/arxiv.org\/abs\/2403.16018v1","title":"Understanding the Impact of Referent Design on Scale Perception in\n  Immersive Data Visualization","summary":"Referents are often used to enhance scale perception in immersive visualizations. Common referent designs include the considerations of referent layout (side-by-side vs. in-situ) and referent size (small vs. medium vs. large). This paper introduces a controlled user study to assess how different referent designs affect the efficiency and accuracy of scale perception across different data scales, on the performance of the size-matching task in the virtual environment. Our results reveal that in-situ layouts significantly enhance accuracy and confidence across various data scales, particularly with large referents. Linear regression analyses further confirm that in-situ layouts exhibit greater resilience to changes in data scale. For tasks requiring efficiency, medium-sized referents emerge as the preferred choice. Based on these findings, we offer design guidelines for selecting referent layouts and sizes in immersive visualizations.","updated":1711258709000,"published":1711258709000,"authors":["Yihan Hou","Hao Cui","Rongrong Chen","Wei Zeng"],"comments":"7 pages, 6 figures, Accepted to Extended Abstracts of the CHI\n  Conference on Human Factors in Computing Systems (CHI EA '24)","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3613905.3650783","journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"referent layout, in-situ, size-matching task, in-situ layouts, Linear regression analyses, data scale, immersive visualizations","human_jargon_list":""},"41":{"arxiv_id":"2403.16018v1","reader_id":"rid1","len_gpt_jargon":11,"len_human_jargon":3,"precision":0.1818181818,"recall":0.6666666667,"f1_score":0.2857142857,"f2_score":0.4347826087,"fuzzy_true_positives":2,"fuzzy_false_positives":9,"fuzzy_false_negatives":1,"len_jargon_diff":8,"url":"http:\/\/arxiv.org\/abs\/2403.16018v1","title":"Understanding the Impact of Referent Design on Scale Perception in\n  Immersive Data Visualization","summary":"Referents are often used to enhance scale perception in immersive visualizations. Common referent designs include the considerations of referent layout (side-by-side vs. in-situ) and referent size (small vs. medium vs. large). This paper introduces a controlled user study to assess how different referent designs affect the efficiency and accuracy of scale perception across different data scales, on the performance of the size-matching task in the virtual environment. Our results reveal that in-situ layouts significantly enhance accuracy and confidence across various data scales, particularly with large referents. Linear regression analyses further confirm that in-situ layouts exhibit greater resilience to changes in data scale. For tasks requiring efficiency, medium-sized referents emerge as the preferred choice. Based on these findings, we offer design guidelines for selecting referent layouts and sizes in immersive visualizations.","updated":1711258709000,"published":1711258709000,"authors":["Yihan Hou","Hao Cui","Rongrong Chen","Wei Zeng"],"comments":"7 pages, 6 figures, Accepted to Extended Abstracts of the CHI\n  Conference on Human Factors in Computing Systems (CHI EA '24)","categories":["cs.HC"],"primary_category":"cs.HC","doi":"10.1145\/3613905.3650783","journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"Referents, immersive visualizations, referent layout, referent size, controlled user study, scale perception, data scales, size-matching task, virtual environment, in-situ layouts, Linear regression analyses","human_jargon_list":"Referents,side-by-side,in-situ,"},"42":{"arxiv_id":"2403.08057v1","reader_id":"rid0","len_gpt_jargon":7,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":7,"fuzzy_false_negatives":0,"len_jargon_diff":7,"url":"http:\/\/arxiv.org\/abs\/2403.08057v1","title":"MineXR: Mining Personalized Extended Reality Interfaces","summary":"Extended Reality (XR) interfaces offer engaging user experiences, but their effective design requires a nuanced understanding of user behavior and preferences. This knowledge is challenging to obtain without the widespread adoption of XR devices. We introduce MineXR, a design mining workflow and data analysis platform for collecting and analyzing personalized XR user interaction and experience data. MineXR enables elicitation of personalized interfaces from participants of a data collection: for any particular context, participants create interface elements using application screenshots from their own smartphone, place them in the environment, and simultaneously preview the resulting XR layout on a headset. Using MineXR, we contribute a dataset of personalized XR interfaces collected from 31 participants, consisting of 695 XR widgets created from 178 unique applications. We provide insights for XR widget functionalities, categories, clusters, UI element types, and placement. Our open-source tools and data support researchers and designers in developing future XR interfaces.","updated":1710273914000,"published":1710273914000,"authors":["Hyunsung Cho","Yukang Yan","Kashyap Todi","Mark Parent","Missie Smith","Tanya R. Jonker","Hrvoje Benko","David Lindlbauer"],"comments":"17 pages, 18 figures, Proceedings of the 2024 CHI Conference on Human\n  Factors in Computing Systems","categories":["cs.HC","H.5.2"],"primary_category":"cs.HC","doi":"10.1145\/3613904.3642394","journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"MineXR, design mining workflow, personalized XR user interaction, XR widget functionalities, XR widget categories, XR widget clusters, UI element types","human_jargon_list":""},"43":{"arxiv_id":"2403.12730v1","reader_id":"rid0","len_gpt_jargon":9,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":9,"fuzzy_false_negatives":0,"len_jargon_diff":9,"url":"http:\/\/arxiv.org\/abs\/2403.12730v1","title":"What Does Evaluation of Explainable Artificial Intelligence Actually\n  Tell Us? A Case for Compositional and Contextual Validation of XAI Building\n  Blocks","summary":"Despite significant progress, evaluation of explainable artificial intelligence remains elusive and challenging. In this paper we propose a fine-grained validation framework that is not overly reliant on any one facet of these sociotechnical systems, and that recognises their inherent modular structure: technical building blocks, user-facing explanatory artefacts and social communication protocols. While we concur that user studies are invaluable in assessing the quality and effectiveness of explanation presentation and delivery strategies from the explainees' perspective in a particular deployment context, the underlying explanation generation mechanisms require a separate, predominantly algorithmic validation strategy that accounts for the technical and human-centred desiderata of their (numerical) outputs. Such a comprehensive sociotechnical utility-based evaluation framework could allow to systematically reason about the properties and downstream influence of different building blocks from which explainable artificial intelligence systems are composed -- accounting for a diverse range of their engineering and social aspects -- in view of the anticipated use case.","updated":1710855934000,"published":1710855934000,"authors":["Kacper Sokol","Julia E. Vogt"],"comments":"Published in Extended Abstracts of the 2024 CHI Conference on Human\n  Factors in Computing Systems (CHI EA '24)","categories":["cs.HC","cs.AI"],"primary_category":"cs.HC","doi":"10.1145\/3613905.3651047","journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"fine-grained validation framework, sociotechnical systems, technical building blocks, user-facing explanatory artefacts, social communication protocols, explanation generation mechanisms, predominantly algorithmic validation strategy, human-centred desiderata, sociotechnical utility-based evaluation framework","human_jargon_list":""},"44":{"arxiv_id":"2403.12730v1","reader_id":"rid1","len_gpt_jargon":6,"len_human_jargon":1,"precision":0.1666666667,"recall":1.0,"f1_score":0.2857142857,"f2_score":0.5,"fuzzy_true_positives":1,"fuzzy_false_positives":5,"fuzzy_false_negatives":0,"len_jargon_diff":5,"url":"http:\/\/arxiv.org\/abs\/2403.12730v1","title":"What Does Evaluation of Explainable Artificial Intelligence Actually\n  Tell Us? A Case for Compositional and Contextual Validation of XAI Building\n  Blocks","summary":"Despite significant progress, evaluation of explainable artificial intelligence remains elusive and challenging. In this paper we propose a fine-grained validation framework that is not overly reliant on any one facet of these sociotechnical systems, and that recognises their inherent modular structure: technical building blocks, user-facing explanatory artefacts and social communication protocols. While we concur that user studies are invaluable in assessing the quality and effectiveness of explanation presentation and delivery strategies from the explainees' perspective in a particular deployment context, the underlying explanation generation mechanisms require a separate, predominantly algorithmic validation strategy that accounts for the technical and human-centred desiderata of their (numerical) outputs. Such a comprehensive sociotechnical utility-based evaluation framework could allow to systematically reason about the properties and downstream influence of different building blocks from which explainable artificial intelligence systems are composed -- accounting for a diverse range of their engineering and social aspects -- in view of the anticipated use case.","updated":1710855934000,"published":1710855934000,"authors":["Kacper Sokol","Julia E. Vogt"],"comments":"Published in Extended Abstracts of the 2024 CHI Conference on Human\n  Factors in Computing Systems (CHI EA '24)","categories":["cs.HC","cs.AI"],"primary_category":"cs.HC","doi":"10.1145\/3613905.3651047","journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"sociotechnical systems, explanatory artefacts, social communication protocols, explanation generation mechanisms, human-centred desiderata, sociotechnical utility-based evaluation framework","human_jargon_list":"sociotechnical systems,"},"45":{"arxiv_id":"2403.18173v1","reader_id":"rid0","len_gpt_jargon":4,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":4,"fuzzy_false_negatives":0,"len_jargon_diff":4,"url":"http:\/\/arxiv.org\/abs\/2403.18173v1","title":"LLMs in HCI Data Work: Bridging the Gap Between Information Retrieval\n  and Responsible Research Practices","summary":"Efficient and accurate information extraction from scientific papers is significant in the rapidly developing human-computer interaction research in the literature review process. Our paper introduces and analyses a new information retrieval system using state-of-the-art Large Language Models (LLMs) in combination with structured text analysis techniques to extract experimental data from HCI literature, emphasizing key elements. Then We analyze the challenges and risks of using LLMs in the world of research. We performed a comprehensive analysis on our conducted dataset, which contained the specified information of 300 CHI 2020-2022 papers, to evaluate the performance of the two large language models, GPT-3.5 (text-davinci-003) and Llama-2-70b, paired with structured text analysis techniques. The GPT-3.5 model gains an accuracy of 58\\% and a mean absolute error of 7.00. In contrast, the Llama2 model indicates an accuracy of 56\\% with a mean absolute error of 7.63. The ability to answer questions was also included in the system in order to work with streamlined data. By evaluating the risks and opportunities presented by LLMs, our work contributes to the ongoing dialogue on establishing methodological validity and ethical guidelines for LLM use in HCI data work.","updated":1711501269000,"published":1711501269000,"authors":["Neda Taghizadeh Serajeh","Iman Mohammadi","Vittorio Fuccella","Mattia De Rosa"],"comments":"5 pages, CHI2024 Workshop on LLMs as Research Tools: Applications and\n  Evaluations in HCI Data Work","categories":["cs.HC","cs.IR"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"Large Language Models (LLMs), GPT-3.5 (text-davinci-003), Llama-2-70b, mean absolute error","human_jargon_list":""},"46":{"arxiv_id":"2403.18173v1","reader_id":"rid1","len_gpt_jargon":4,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":4,"fuzzy_false_negatives":0,"len_jargon_diff":4,"url":"http:\/\/arxiv.org\/abs\/2403.18173v1","title":"LLMs in HCI Data Work: Bridging the Gap Between Information Retrieval\n  and Responsible Research Practices","summary":"Efficient and accurate information extraction from scientific papers is significant in the rapidly developing human-computer interaction research in the literature review process. Our paper introduces and analyses a new information retrieval system using state-of-the-art Large Language Models (LLMs) in combination with structured text analysis techniques to extract experimental data from HCI literature, emphasizing key elements. Then We analyze the challenges and risks of using LLMs in the world of research. We performed a comprehensive analysis on our conducted dataset, which contained the specified information of 300 CHI 2020-2022 papers, to evaluate the performance of the two large language models, GPT-3.5 (text-davinci-003) and Llama-2-70b, paired with structured text analysis techniques. The GPT-3.5 model gains an accuracy of 58\\% and a mean absolute error of 7.00. In contrast, the Llama2 model indicates an accuracy of 56\\% with a mean absolute error of 7.63. The ability to answer questions was also included in the system in order to work with streamlined data. By evaluating the risks and opportunities presented by LLMs, our work contributes to the ongoing dialogue on establishing methodological validity and ethical guidelines for LLM use in HCI data work.","updated":1711501269000,"published":1711501269000,"authors":["Neda Taghizadeh Serajeh","Iman Mohammadi","Vittorio Fuccella","Mattia De Rosa"],"comments":"5 pages, CHI2024 Workshop on LLMs as Research Tools: Applications and\n  Evaluations in HCI Data Work","categories":["cs.HC","cs.IR"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"Large Language Models,structured text analysis techniques,methodological validity,ethical guidelines","human_jargon_list":""},"47":{"arxiv_id":"2403.01697v1","reader_id":"rid0","len_gpt_jargon":6,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":6,"fuzzy_false_negatives":0,"len_jargon_diff":6,"url":"http:\/\/arxiv.org\/abs\/2403.01697v1","title":"Dismantling Gender Blindness in Online Discussion of a Crime\/Gender\n  Dichotomy","summary":"Contemporary feminists utilize social media for activism, while backlashes come along. The gender-related discourses are often diminished when addressing public events regarding sexism and gender inequality on social media platforms. The dichotomized debate around the Tangshan beating incident in China epitomized how criminal interpretations of gender-related violence became a backlash against feminist expressions. By analyzing posts on Weibo using mixed methods, we describe the emerging discursive patterns around crime and gender, uncovering the inherent gender-blind sexism that refutes feminist discourses on the social platform. We also highlight the critical restrictions facing grassroots feminist activism in Chinese cyberspace and propose implications for the design and research related to digital feminist activism.","updated":1709522274000,"published":1709522274000,"authors":["Yigang Qin","Weilun Duan","Qunfang Wu","Zhicong Lu"],"comments":"31 pages, 3 figures, Accepted for publication in Proceedings of the\n  ACM on Human-Computer Interaction (CSCW 2024)","categories":["cs.HC"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"Tangshan beating incident, mixed methods, gender-blind sexism, grassroots feminist activism, Chinese cyberspace, digital feminist activism","human_jargon_list":""},"48":{"arxiv_id":"2403.01055v1","reader_id":"rid0","len_gpt_jargon":8,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":8,"fuzzy_false_negatives":0,"len_jargon_diff":8,"url":"http:\/\/arxiv.org\/abs\/2403.01055v1","title":"Towards Full Authorship with AI: Supporting Revision with AI-Generated\n  Views","summary":"Large language models (LLMs) are shaping a new user interface (UI) paradigm in writing tools by enabling users to generate text through prompts. This paradigm shifts some creative control from the user to the system, thereby diminishing the user's authorship and autonomy in the writing process. To restore autonomy, we introduce Textfocals, a UI prototype designed to investigate a human-centered approach that emphasizes the user's role in writing. Textfocals supports the writing process by providing LLM-generated summaries, questions, and advice (i.e., LLM views) in a sidebar of a text editor, encouraging reflection and self-driven revision in writing without direct text generation. Textfocals' UI affordances, including contextually adaptive views and scaffolding for prompt selection and customization, offer a novel way to interact with LLMs where users maintain full authorship of their writing. A formative user study with Textfocals showed promising evidence that this approach might help users develop underdeveloped ideas, cater to the rhetorical audience, and clarify their writing. However, the study also showed interaction design challenges related to document navigation and scoping, prompt engineering, and context management. Our work highlights the breadth of the design space of writing support interfaces powered by generative AI that maintain authorship integrity.","updated":1709341895000,"published":1709341895000,"authors":["Jiho Kim","Ray C. Flanagan","Noelle E. Haviland","ZeAi Sun","Souad N. Yakubu","Edom A. Maru","Kenneth C. Arnold"],"comments":"15 pages, 2 figures; Accepted to 5th Workshop on Human-AI Co-Creation\n  with Generative Models (HAI-GEN) at ACM IUI 2024","categories":["cs.HC","cs.AI","cs.CY","H.5.2; I.7.1; I.2.7"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"LLM views, UI affordances, contextually adaptive views, scaffolding for prompt selection and customization, formative user study, interaction design challenges, prompt engineering, context management","human_jargon_list":""},"49":{"arxiv_id":"2403.01055v1","reader_id":"rid1","len_gpt_jargon":5,"len_human_jargon":1,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":5,"fuzzy_false_negatives":1,"len_jargon_diff":4,"url":"http:\/\/arxiv.org\/abs\/2403.01055v1","title":"Towards Full Authorship with AI: Supporting Revision with AI-Generated\n  Views","summary":"Large language models (LLMs) are shaping a new user interface (UI) paradigm in writing tools by enabling users to generate text through prompts. This paradigm shifts some creative control from the user to the system, thereby diminishing the user's authorship and autonomy in the writing process. To restore autonomy, we introduce Textfocals, a UI prototype designed to investigate a human-centered approach that emphasizes the user's role in writing. Textfocals supports the writing process by providing LLM-generated summaries, questions, and advice (i.e., LLM views) in a sidebar of a text editor, encouraging reflection and self-driven revision in writing without direct text generation. Textfocals' UI affordances, including contextually adaptive views and scaffolding for prompt selection and customization, offer a novel way to interact with LLMs where users maintain full authorship of their writing. A formative user study with Textfocals showed promising evidence that this approach might help users develop underdeveloped ideas, cater to the rhetorical audience, and clarify their writing. However, the study also showed interaction design challenges related to document navigation and scoping, prompt engineering, and context management. Our work highlights the breadth of the design space of writing support interfaces powered by generative AI that maintain authorship integrity.","updated":1709341895000,"published":1709341895000,"authors":["Jiho Kim","Ray C. Flanagan","Noelle E. Haviland","ZeAi Sun","Souad N. Yakubu","Edom A. Maru","Kenneth C. Arnold"],"comments":"15 pages, 2 figures; Accepted to 5th Workshop on Human-AI Co-Creation\n  with Generative Models (HAI-GEN) at ACM IUI 2024","categories":["cs.HC","cs.AI","cs.CY","H.5.2; I.7.1; I.2.7"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"UI affordances, contextually adaptive views, scaffolding, prompt engineering, context management","human_jargon_list":"Textfocals"},"50":{"arxiv_id":"2305.11927v2","reader_id":"rid0","len_gpt_jargon":4,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":4,"fuzzy_false_negatives":0,"len_jargon_diff":4,"url":"http:\/\/arxiv.org\/abs\/2305.11927v2","title":"Evaluating how interactive visualizations can assist in finding samples\n  where and how computer vision models make mistakes","summary":"Creating Computer Vision (CV) models remains a complex practice, despite their ubiquity. Access to data, the requirement for ML expertise, and model opacity are just a few points of complexity that limit the ability of end-users to build, inspect, and improve these models. Interactive ML perspectives have helped address some of these issues by considering a teacher in the loop where planning, teaching, and evaluating tasks take place. We present and evaluate two interactive visualizations in the context of Sprite, a system for creating CV classification and detection models for images originating from videos. We study how these visualizations help Sprite's users identify (evaluate) and select (plan) images where a model is struggling and can lead to improved performance, compared to a baseline condition where users used a query language. We found that users who had used the visualizations found more images across a wider set of potential types of model errors.","updated":1710526996000,"published":1684507380000,"authors":["Hayeong Song","Gonzalo Ramos","Peter Bodik"],"comments":"Hayeong Song, Gonzalo Ramos, and Peter Bodik. \"Evaluating how\n  interactive visualizations can assist in finding samples where and how\n  computer vision models make mistakes\" 2024 IEEE Pacific Visualization\n  Symposium (PacificVis). Ieee, 2024","categories":["cs.HC","cs.CV","cs.LG"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"model opacity, teacher in the loop, CV classification and detection models, baseline condition","human_jargon_list":""},"51":{"arxiv_id":"2305.11927v2","reader_id":"rid1","len_gpt_jargon":8,"len_human_jargon":1,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":8,"fuzzy_false_negatives":1,"len_jargon_diff":7,"url":"http:\/\/arxiv.org\/abs\/2305.11927v2","title":"Evaluating how interactive visualizations can assist in finding samples\n  where and how computer vision models make mistakes","summary":"Creating Computer Vision (CV) models remains a complex practice, despite their ubiquity. Access to data, the requirement for ML expertise, and model opacity are just a few points of complexity that limit the ability of end-users to build, inspect, and improve these models. Interactive ML perspectives have helped address some of these issues by considering a teacher in the loop where planning, teaching, and evaluating tasks take place. We present and evaluate two interactive visualizations in the context of Sprite, a system for creating CV classification and detection models for images originating from videos. We study how these visualizations help Sprite's users identify (evaluate) and select (plan) images where a model is struggling and can lead to improved performance, compared to a baseline condition where users used a query language. We found that users who had used the visualizations found more images across a wider set of potential types of model errors.","updated":1710526996000,"published":1684507380000,"authors":["Hayeong Song","Gonzalo Ramos","Peter Bodik"],"comments":"Hayeong Song, Gonzalo Ramos, and Peter Bodik. \"Evaluating how\n  interactive visualizations can assist in finding samples where and how\n  computer vision models make mistakes\" 2024 IEEE Pacific Visualization\n  Symposium (PacificVis). Ieee, 2024","categories":["cs.HC","cs.CV","cs.LG"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"Computer Vision (CV) models, model opacity, Interactive ML perspectives, teacher in the loop, interactive visualizations, CV classification and detection models, baseline condition, query language","human_jargon_list":"Sprite"},"52":{"arxiv_id":"2403.00632v1","reader_id":"rid0","len_gpt_jargon":4,"len_human_jargon":1,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":4,"fuzzy_false_negatives":1,"len_jargon_diff":3,"url":"http:\/\/arxiv.org\/abs\/2403.00632v1","title":"Metamorpheus: Interactive, Affective, and Creative Dream Narration\n  Through Metaphorical Visual Storytelling","summary":"Human emotions are essentially molded by lived experiences, from which we construct personalised meaning. The engagement in such meaning-making process has been practiced as an intervention in various psychotherapies to promote wellness. Nevertheless, to support recollecting and recounting lived experiences in everyday life remains under explored in HCI. It also remains unknown how technologies such as generative AI models can facilitate the meaning making process, and ultimately support affective mindfulness. In this paper we present Metamorpheus, an affective interface that engages users in a creative visual storytelling of emotional experiences during dreams. Metamorpheus arranges the storyline based on a dream's emotional arc, and provokes self-reflection through the creation of metaphorical images and text depictions. The system provides metaphor suggestions, and generates visual metaphors and text depictions using generative AI models, while users can apply generations to recolour and re-arrange the interface to be visually affective. Our experience-centred evaluation manifests that, by interacting with Metamorpheus, users can recall their dreams in vivid detail, through which they relive and reflect upon their experiences in a meaningful way.","updated":1709309372000,"published":1709309372000,"authors":["Qian Wan","Xin Feng","Yining Bei","Zhiqi Gao","Zhicong Lu"],"comments":"Accepted by CHI 2024","categories":["cs.HC","cs.AI","cs.CL","cs.CY"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"affective mindfulness, emotional arc, visual metaphors, experience-centred evaluation","human_jargon_list":"affective interface"},"53":{"arxiv_id":"2403.00632v1","reader_id":"rid1","len_gpt_jargon":4,"len_human_jargon":1,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":4,"fuzzy_false_negatives":1,"len_jargon_diff":3,"url":"http:\/\/arxiv.org\/abs\/2403.00632v1","title":"Metamorpheus: Interactive, Affective, and Creative Dream Narration\n  Through Metaphorical Visual Storytelling","summary":"Human emotions are essentially molded by lived experiences, from which we construct personalised meaning. The engagement in such meaning-making process has been practiced as an intervention in various psychotherapies to promote wellness. Nevertheless, to support recollecting and recounting lived experiences in everyday life remains under explored in HCI. It also remains unknown how technologies such as generative AI models can facilitate the meaning making process, and ultimately support affective mindfulness. In this paper we present Metamorpheus, an affective interface that engages users in a creative visual storytelling of emotional experiences during dreams. Metamorpheus arranges the storyline based on a dream's emotional arc, and provokes self-reflection through the creation of metaphorical images and text depictions. The system provides metaphor suggestions, and generates visual metaphors and text depictions using generative AI models, while users can apply generations to recolour and re-arrange the interface to be visually affective. Our experience-centred evaluation manifests that, by interacting with Metamorpheus, users can recall their dreams in vivid detail, through which they relive and reflect upon their experiences in a meaningful way.","updated":1709309372000,"published":1709309372000,"authors":["Qian Wan","Xin Feng","Yining Bei","Zhiqi Gao","Zhicong Lu"],"comments":"Accepted by CHI 2024","categories":["cs.HC","cs.AI","cs.CL","cs.CY"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"affective interface, emotional arc, metaphorical images, experience-centred evaluation","human_jargon_list":"Metamorpheus"},"54":{"arxiv_id":"2309.15723v2","reader_id":"rid0","len_gpt_jargon":7,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":7,"fuzzy_false_negatives":0,"len_jargon_diff":7,"url":"http:\/\/arxiv.org\/abs\/2309.15723v2","title":"Where Are We So Far? Understanding Data Storytelling Tools from the\n  Perspective of Human-AI Collaboration","summary":"Data storytelling is powerful for communicating data insights, but it requires diverse skills and considerable effort from human creators. Recent research has widely explored the potential for artificial intelligence (AI) to support and augment humans in data storytelling. However, there lacks a systematic review to understand data storytelling tools from the perspective of human-AI collaboration, which hinders researchers from reflecting on the existing collaborative tool designs that promote humans' and AI's advantages and mitigate their shortcomings. This paper investigated existing tools with a framework from two perspectives: the stages in the storytelling workflow where a tool serves, including analysis, planning, implementation, and communication, and the roles of humans and AI in each stage, such as creators, assistants, optimizers, and reviewers. Through our analysis, we recognize the common collaboration patterns in existing tools, summarize lessons learned from these patterns, and further illustrate research opportunities for human-AI collaboration in data storytelling.","updated":1710766817000,"published":1695828650000,"authors":["Haotian Li","Yun Wang","Huamin Qu"],"comments":"Accepted by CHI 2024","categories":["cs.HC","cs.AI"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"data storytelling tools, storytelling workflow, creators, assistants, optimizers, reviewers, collaboration patterns","human_jargon_list":""},"55":{"arxiv_id":"2309.15723v2","reader_id":"rid1","len_gpt_jargon":4,"len_human_jargon":0,"precision":0.0,"recall":0.0,"f1_score":0.0,"f2_score":0.0,"fuzzy_true_positives":0,"fuzzy_false_positives":4,"fuzzy_false_negatives":0,"len_jargon_diff":4,"url":"http:\/\/arxiv.org\/abs\/2309.15723v2","title":"Where Are We So Far? Understanding Data Storytelling Tools from the\n  Perspective of Human-AI Collaboration","summary":"Data storytelling is powerful for communicating data insights, but it requires diverse skills and considerable effort from human creators. Recent research has widely explored the potential for artificial intelligence (AI) to support and augment humans in data storytelling. However, there lacks a systematic review to understand data storytelling tools from the perspective of human-AI collaboration, which hinders researchers from reflecting on the existing collaborative tool designs that promote humans' and AI's advantages and mitigate their shortcomings. This paper investigated existing tools with a framework from two perspectives: the stages in the storytelling workflow where a tool serves, including analysis, planning, implementation, and communication, and the roles of humans and AI in each stage, such as creators, assistants, optimizers, and reviewers. Through our analysis, we recognize the common collaboration patterns in existing tools, summarize lessons learned from these patterns, and further illustrate research opportunities for human-AI collaboration in data storytelling.","updated":1710766817000,"published":1695828650000,"authors":["Haotian Li","Yun Wang","Huamin Qu"],"comments":"Accepted by CHI 2024","categories":["cs.HC","cs.AI"],"primary_category":"cs.HC","doi":null,"journal_ref":null,"peer_reviewed":true,"primary_category_readable":"Human-Computer Interaction","gpt4_jargon_list":"data storytelling tools, human-AI collaboration, storytelling workflow, collaboration patterns","human_jargon_list":""}}