{
    "0": "In the rapidly evolving field of artificial intelligence, the ability to harness and integrate knowledge across various domains stands as a paramount challenge and opportunity. This study introduces a novel approach to cross-domain knowledge discovery through the deployment of multi-AI agents, each specialized in distinct knowledge domains. These AI agents, designed to function as domain-specific experts, collaborate in a unified framework to synthesize and provide comprehensive insights that transcend the limitations of single-domain expertise. By facilitating seamless interaction among these agents, our platform aims to leverage the unique strengths and perspectives of each, thereby enhancing the process of knowledge discovery and decision-making. We present a comparative analysis of the different multi-agent workflow scenarios evaluating their performance in terms of efficiency, accuracy, and the breadth of knowledge integration. Through a series of experiments involving complex, interdisciplinary queries, our findings demonstrate the superior capability of domain specific multi-AI agent system in identifying and bridging knowledge gaps. This research not only underscores the significance of collaborative AI in driving innovation but also sets the stage for future advancements in AI-driven, cross-disciplinary research and application. Our methods were evaluated on a small pilot data and it showed a trend we expected, if we increase the amount of data we custom train the agents, the trend is expected to be more smooth.Keywords\u2002: Multi-AI agents \u00a0\u22c5\u22c5\\cdot\u22c5\nCross-domain knowledge \u00a0\u22c5\u22c5\\cdot\u22c5\nKnowledge discovery \u00a0\u22c5\u22c5\\cdot\u22c5\nCollaborative artificial intelligence \u00a0\u22c5\u22c5\\cdot\u22c5\nDomain-specific expertise \u00a0\u22c5\u22c5\\cdot\u22c5\nComparative analysisThe realm of artificial intelligence (AI) has undergone remarkable transformations since its inception, evolving from rudimentary computational algorithms to sophisticated systems capable of mimicking human-like cognitive functions. This rapid advancement has propelled AI into the forefront of technological innovation, making it integral in addressing complex challenges across diverse fields.[1][2][3] However, as the scope of AI applications broadens, the necessity for cross-domain knowledge discovery emerges as a critical endeavor [4, 5]. Traditional AI systems, while adept within their specific areas of expertise, often falter when tasked with synthesizing information across disparate domains. This limitation not only hampers the potential for innovation but also restricts the depth of analysis and understanding that can be achieved. The burgeoning field of AI now stands at a juncture where the integration of knowledge from various disciplines is not just advantageous but essential for pushing the boundaries of what AI can accomplish.The integration of cross-domain knowledge poses significant challenges in the current landscape of artificial intelligence [6]. Traditional AI models are typically designed with a narrow focus, excelling in tasks within their domain but lacking the capacity to interpret and utilize information beyond their programmed expertise. This siloed approach to knowledge processing leads to a compartmentalization of insights, preventing the holistic understanding necessary for tackling complex, multifaceted problems. Moreover, the task of merging diverse knowledge bases involves intricate considerations of context, relevance, and applicability, areas where conventional AI systems often fall short. As a result, there exists a palpable gap in the ability of existing AI models to seamlessly interact and integrate insights across different domains, limiting their effectiveness in scenarios where interdisciplinary analysis is paramount. The challenge, therefore, lies not only in enhancing the individual capabilities of AI agents but also in fostering a collaborative ecosystem where these agents can collectively leverage their domain-specific expertise for enriched knowledge discovery and decision-making.We have used MetaGPT for most of our workflows in this research paper. MetaGPT is introduced, an innovative framework that incorporates efficient human workflows as a meta programming approach into LLM-based multi-agent collaboration and leverages the assembly line paradigm to assign diverse roles to various agents, thereby establishing a framework that can effectively and cohesively deconstruct complex multi- agent collaborative problems [7]. Heree, We can specify the orders of generating answers by the agents. The information passed onto the next agent is the complete context of the results of previous agents which is managed by MetaGPT internally.The primary aim of this research is to explore the capabilities of existing multi-AI agent platforms for enhancing cross-domain knowledge discovery. This exploration seeks to understand how the orchestration of AI agents, each with domain-specific expertise, can collaboratively tackle complex problems that individual AI capabilities cannot address alone. The research focuses on assessing the efficiency, accuracy, and breadth of knowledge integration facilitated by various multi-AI agent configurations. The goal is to highlight the potential of leveraging collective intelligence among AI agents, marking a significant shift towards more integrative and holistic approaches in cross-disciplinary research and applications.This research is poised to make significant contributions to the field of artificial intelligence, especially in the realm of collaborative AI technologies. By analyzing the integration capabilities of existing multi-AI agent platforms, it addresses a vital gap in the current AI paradigm: the effective synthesis of domain-specific knowledge from multiple AI agents. The study\u2019s contributions are multifaceted. It offers an in-depth evaluation of the potential and limitations of multi-agent AI systems in cross-domain knowledge discovery. It also enriches the theoretical foundations of AI integration, providing insights into the mechanics of effective AI collaboration. Moreover, by showcasing the practical applications and potential benefits of multi-agent AI systems, the research lays the foundation for future advancements in AI-driven, interdisciplinary research and problem-solving. Through these efforts, the study not only advances the understanding of collaborative AI but also illustrates the importance of leveraging AI technologies for comprehensive and inclusive approaches to addressing complex global challenges.The architecture of our multi-AI agent platform is designed to facilitate seamless integration and collaboration between specialized ReAct AI agents [8]. As depicted in the first figure, each agent within the architecture has a specific role:observe: The agents begin by observing the environment or the data, gathering information that is relevant to their domain expertise.think: Each agent processes the observed information, using its domain-specific knowledge to analyze and interpret the data.act: Based on the analysis, the agent decides on a course of action and executes it, contributing to the problem-solving process.In creating an AI agent, we begin by defining its knowledge domain and designing algorithms tailored to that domain. Each agent is trained on a dataset consisting of approximately 1000 research papers relevant to its expertise, integrating this specialized knowledge with the broader OpenAI knowledge base.The second figure showcases the five distinct types of AI agents integrated into the multi-agent system:Boron Nitride AgentElectrochemical AgentBandgap (Physics) AgentNanomaterial AgentAI AgentEach agent has been developed with deep domain knowledge in its respective field, acquired from extensive research literature.Our multi-agent system employs a sophisticated collaboration mechanism, where AI agents communicate and share knowledge via a shared platform. The communication protocol ensures that the knowledge transfer between agents is contextually relevant and precise. The agents collaborate on problem-solving tasks by passing insights and processed information through a well-defined sequence that optimizes the collective intelligence of the system. We have used MetaGPT multi-agent orchestrator that internally passes the context of the entire previous results of agents previously in the MAS system on to the next agent in the sequence.The multi-agent system utilizes four distinct flows for knowledge integration:MetaGPT+OpenAI+RAG: A RAG System to create a single agent that augments OpenAI\u2019s knowledge with custom data from research papers, orchestrated by the MetaGPT framework.Here individual agent is generated by incorporating custom knowledge from domain specific sources into a generative model. The process begins with a user query, which is sent to a retriever module. This retriever is likely powered by Elasticsearch, and its role is to find relevant documents from a customized knowledge source. These articles are pre-processed through a step called \"Chunking documents\", which might be to divide the text into manageable pieces. The retriever creates dense vectors using an embedding model, which represents the text chunks numerically in a way that captures semantic meaning.The retrieved documents are then evaluated: they can be categorized as \u2019Correct\u2019, \u2019Ambiguous\u2019, or \u2019Incorrect\u2019 based on their relevance to the query. For \u2019Ambiguous\u2019 or \u2019Incorrect\u2019 retrievals, a web search is initiated to augment the data. Finally, the \u2019User Query\u2019 along with \u2019Context\u2019 and additional \u2019Prompt\u2019 information are fed into a OpenAI GPT model. This model generates the output based on both the initial user query and the augmented information retrieved from various sources, thereby integrating custom knowledge into the generative process. The response is then formulated and presented to the user. The process flows is shown in Figure 3.Sequential Flow + OpenAI Assistant: A sequential integration of OpenAI Assistant API - based agents, each enriched with domain-specific knowledge from research papers, without the use of an additional orchestration framework, passing the message of immediate past agent to the next agent in the sequence. Here, indiviudal agent is created from OpenAI assistant whose capabilities are expanded through retrieval, incorporating external knowledge beyond its inherent model. This can include proprietary product data or documents supplied by users.When a document is uploaded and transmitted to the Assistant, OpenAI will automatically chunk the document it into smaller segments, create an index, store the embeddings, and utilise vector search to fetch pertinent information for responding to user inquiries.MetaGPT + OpenAI Assistant: An integration similar to the second flow, but using the MetaGPT orchestration framework to facilitate the interaction between the created agents.MetaGPT + OpenAI: A baseline flow that employs the unmodified OpenAI agent in sequence, serving as a control for assessing the performance enhancement brought by custom knowledge integration.To evaluate the efficacy of our multi-agent system against existing platforms, we established a set of metrics and benchmarks. These include:Efficiency: Measuring the time taken to reach a solution or provide an insight.Accuracy: Evaluating the correctness and relevance of the information provided by the agents.Breadth of Knowledge Integration: Assessing the diversity of information and the depth of the synthesized knowledge from multiple domains.These metrics provide a quantitative basis for comparing our proposed system with existing AI agents and platforms, highlighting the improvements in cross-domain knowledge discovery and application.The experimental setup was meticulously designed to test the effectiveness of the multi-AI agent system in delivering accurate and relevant information across different domains. The setup involved the following components:Questions: We formulated a set of questions within the specific expertise of Boron Nitride, Electrochemical, Bandgap, Nanomaterial, and General AI\u2014each question crafted to assess the depth and breadth of knowledge that the respective agent could access and interpret.Expected Answers: For each question, we established a set of expected answers that would be considered accurate and comprehensive, serving as a benchmark for the agents\u2019 performance.Test Design Rationale: The reason for this particular design was to emulate realistic scenarios where domain-specific expert knowledge is crucial. The questions were chosen for their impact and relevance in their respective fields to evaluate whether the AI agents could replicate the level of insight expected from human experts.The performance of the multi-agent system was assessed through two main criteria:Working Performance: We measured the speed of the system in tokens per second, calculating the time taken from receiving the question to generating the final answer. This metric gauged the efficiency of the system under different workflows.ROUGE: ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. This is a really popular metric that you\u2019ll definitely find in the literature around text summarization. The metric is based on calculating the syntactic overlap between candidate and reference summaries (or any other text pieces). Rouge-1 calculates the overlap of unigram(individual word) between the candidate and reference text pieces. The mathematical formulation of Rouge-1 precision, recall is shown below -Cosine Similarity:\nCosine similarity is a metric used to measure how similar two vectors are irrespective of their size. Mathematically, it calculates the cosine of the angle between two vectors projected in a multi-dimensional space. This similarity is particularly useful in high-dimensional positive spaces like those used in text analysis and information retrieval.Where:\ud835\udc00\ud835\udc00\\mathbf{A}bold_A and \ud835\udc01\ud835\udc01\\mathbf{B}bold_B are the vector representations of two documents (or sentences in this case).\ud835\udc00\u22c5\ud835\udc01\u22c5\ud835\udc00\ud835\udc01\\mathbf{A}\\cdot\\mathbf{B}bold_A \u22c5 bold_B is the dot product of vectors \ud835\udc00\ud835\udc00\\mathbf{A}bold_A and \ud835\udc01\ud835\udc01\\mathbf{B}bold_B.\u2016\ud835\udc00\u2016norm\ud835\udc00\\|\\mathbf{A}\\|\u2225 bold_A \u2225 and \u2016\ud835\udc01\u2016norm\ud835\udc01\\|\\mathbf{B}\\|\u2225 bold_B \u2225 are the norms (or magnitudes) of the vectors, calculated as the square root of the sum of the squared components of each vector.\nThe comparative analysis involved contrasting the performance metrics across the different multi-agent workflows. This included:Speed of Answer Generation: Comparing the tokens per second across workflows to determine which configuration provided the fastest responses, the average speed of flow 1 was 8.53 tokens per second, flow 2 was 7.63 tokens per second, flow 3 was 8.50 tokens per second whereas flow 4 was 64.23 tokens per second.\nROUGE-1: Comparing the precision across workflows to determine which configuration provided the fastest responses, the average precision of flow 1 was 0.49, flow 2 was 0.05, flow 3 was 0.05 whereas flow 4 was 0.06.\nCosine Similarity:\nComparing the cosine similarity, the average cosine similarity value of flow 1 was 0.26 , flow 2 was 0.22, flow 3 was 0.22 whereas flow 4 was 0.25.We discovered that the different multi-agent workflows exhibited varying strengths. Some workflows were faster but less accurate, while others took longer but provided more comprehensive answers. The expert evaluations were instrumental in understanding the practical utility of each system configuration. They highlighted that the most impactful questions often required not just speed but depth of analysis, a quality that some workflows managed better than others. The results from these experiments provide valuable insights into how to balance efficiency and thoroughness in the design of multi-agent AI systems.Our study marks a transformative advancement in artificial intelligence, with a particular emphasis on the fusion of cross-domain knowledge discovery and integration. Through the deployment of a sophisticated network of multi-AI agents, each expert in distinct knowledge areas, we uncover the vast potential of collaborative AI systems equipped with the capability to harness custom tools and open data. This innovative strategy not only dismantles the traditional barriers that have impeded AI applications but also heralds a new era of intelligent systems designed for evolution and adaptation to serve a wide array of research disciplines.The remarkable ability of our AI agents to efficiently and accurately synthesize knowledge from diverse domains, as demonstrated by our experiments, showcases the significant advantages of utilizing domain-specific expertise through a unified, collaborative framework. Such enhanced performance is largely due to the dynamic interplay and cooperation among the AI agents. This allows for a deeper and more nuanced comprehension of complex queries, enriched further by the agents\u2019 ability to access and utilize custom tools and open data sources, thus broadening the scope and depth of their analytical capabilities.A pivotal aspect of our findings is the AI agents\u2019 unique capability not just to collaborate but also to innovate and develop new AI models specifically tailored to address intricate research challenges. This introduces an unprecedented level of adaptability and evolutionary potential within AI systems. As these agents craft and refine novel models, their problem-solving methods are continually enhanced, leading to a rapid acceleration in knowledge discovery and practical application.Despite these encouraging outcomes, we recognize several hurdles, such as the necessity for advanced coordination mechanisms to manage the interactions among AI agents and to ensure the system\u2019s scalability as the diversity and number of domains and agents grow. Future endeavors should concentrate on fine-tuning these collaborative dynamics and devising more efficient strategies for the agile creation and implementation of new AI models within the ecosystem, leveraging the full spectrum of custom tools and open data available to them.The research conducted provided substantial insights into the effectiveness of multi-AI agent systems in enhancing cross-domain knowledge discovery. The key outcomes indicated that the integration of domain-specific knowledge significantly improves the quality of the AI\u2019s output. Flow 1 emerged as the most effective, with experts rating it highest for answer quality. This flow capitalized on the MetaGPT framework\u2019s ability to maintain conversational context across all agents, proving essential for delivering high-quality and contextually relevant responses. Flow 3 followed closely, benefiting from the domain-specific knowledge incorporated into the OpenAI Assistant, despite lacking the full conversational context provided in Flow 3. Flow 2, which employed a custom RAG system atop the OpenAI model, demonstrated that while the RAG approach adds value, the OpenAI Assistant\u2019s capabilities surpassed it when it comes to answer quality. Flow 4, relying solely on the generalized OpenAI GPT model without additional domain knowledge, lagged in performance, underscoring the importance of domain-specific information in delivering quality AI responses.The findings from this study open several avenues for future research. Future work could explore the refinement of the MetaGPT orchestration framework to enhance the efficiency and quality of multi-agent collaborations further. Additionally, expanding the domain-specific knowledge databases and integrating real-time learning capabilities could make the system more robust and applicable to a wider range of cross-disciplinary queries. There is also potential in exploring the use of different machine learning models and architectures that could complement the capabilities of GPT-powered agents.This study has underscored the significant role that collaborative AI can play in advancing knowledge discovery across various disciplines. The integration of domain-specific expertise within AI systems is crucial for tackling complex, multi-faceted problems that are beyond the scope of any single domain. By harnessing the collective intelligence of specialized AI agents, we can open up new possibilities for innovation and understanding, breaking down the barriers that have traditionally segmented knowledge. The importance of collaborative AI will only grow as we continue to push the boundaries of what AI can achieve, and this research lays a critical foundation for those future endeavors.",
    "1": "[1,2]\\fnmArthur \\surLedaguenel1]\\orgnameIRT SystemX, \\orgaddress\\cityPalaiseau, \\countryFrance2]\\orgdivMICS, \\orgnameCentraleSup\u00e9lec, \\orgaddress\\stateSaclay, \\countryFranceNeurosymbolic artificial intelligence is a growing field of research aiming to combine neural network learning capabilities with the reasoning abilities of symbolic systems. Informed multi-label classification is a sub-field of neurosymbolic AI which studies how to leverage prior knowledge to improve neural classification systems. A well known family of neurosymbolic techniques for informed classification use probabilistic reasoning to integrate this knowledge during learning, inference or both. Therefore, the asymptotic complexity of probabilistic reasoning is of cardinal importance to assess the scalability of such techniques. However, this topic is rarely tackled in the neurosymbolic literature, which can lead to a poor understanding of the limits of probabilistic neurosymbolic techniques. In this paper, we introduce a formalism for informed supervised classification tasks and techniques. We then build upon this formalism to define three abstract neurosymbolic techniques based on probabilistic reasoning. Finally, we show computational complexity results on several representation languages for prior knowledge commonly found in the neurosymbolic literature.Neurosymbolic Artificial Intelligence (AI) is a growing field of research aiming to combine neural network learning capabilities with the reasoning abilities of symbolic systems. This hybridization can take many shapes depending on how the neural and symbolic components interact, like shown in [1, 2].An important sub-field of neurosymbolic AI is Informed Machine Learning [3], which studies how to leverage background knowledge to improve neural systems. There again, proposed techniques in the literature can be of very different nature depending on the type of task (e.g.\u00a0regression, classification, detection, generation, etc.), the language used to represent the background knowledge (e.g.\u00a0mathematical equations, knowledge graphs, logics, etc.), the stage at which knowledge is embedded (e.g.\u00a0data processing, neural architecture design, learning procedure, inference procedure, etc.) and benefits expected from the hybridization (e.g.\u00a0explainability, performance, frugality, etc.).In this paper, we tackle supervised classification tasks informed by prior knowledge represented as a logical theory. We consider neurosymbolic techniques that integrate prior knowledge during learning, inference or both and that mainly aim at improving the performance of a neural classification system. More specifically, we study a family of neurosymbolic techniques that leverage probabilistic reasoning to integrate prior knowledge, a trend that has gained significant traction in the recent literature. In this context, the asymptotic complexity of probabilistic reasoning is of cardinal importance to assess the scalability of such probabilistic neurosymbolic techniques to large classification tasks (e.g.\u00a0ImageNet dataset [4] contains 1,000 classes, and up to 1,860 when adding parent classes in the WordNet hierarchy [5], Census Cars dataset [6] contains 2,675 of cars and iNaturalist dataset [7] contains 5,089 classes of natural species). However, because neurosymbolic techniques are typically evaluated on toy datasets where complexity issues are not yet relevant and because performance metrics are the main focus, this topic is rarely tackled in the neurosymbolic literature, which can lead to a poor understanding of the applicability and limits of a given technique. For instance, [8] highlights scalability issues of existing neurosymbolic techniques on the multi-digit MNIST-addition task and propose an approximate solution, but [9] later shows that the task can be tackled in time linear in the number of digits. This is also due to a persistent lack of communication between the neurosymbolic community and the knowledge representation community. We hope this work will help to fill this gap.The contributions and outline of the paper are as follows: we start with preliminary definitions of logics, probabilistic reasoning, and neural classification in Section 2. Then, we introduce in Section 3 a formalism to define informed supervised classification tasks and techniques. We build upon this formalism to re-frame three abstract neurosymbolic techniques that can be applied in any logic. To the best of our knowledge, we are the first to provide such a general overview of probabilistic neurosymbolic techniques for informed supervised classification. Finally, we examine in Section 4 the asymptotic computational complexity of several classes of probabilistic reasoning problems that are often encountered in the neurosymbolic literature. We show that probabilistic techniques can not scale on certain popular tasks found in the literature, whereas others thought intractable can actually be computed efficiently. We conclude with possible future research questions in Section 5.All logics have two sides. The syntax gives the grammar for describing the world on which the logic operates: it characterizes a set of states this world can be in and a admissible statements that can be made about the world. The semantic determines the relation between these statements and states: it specifies in which states a statement can be considered true or false.In this paper, we focus on logics in which the states correspond to assignments from a discrete set of variables. Let\u2019s assume a discrete set of variables \ud835\udc18\ud835\udc18\\mathbf{Y}bold_Y, a state is an element of \ud835\udd39\ud835\udc18superscript\ud835\udd39\ud835\udc18\\mathbb{B}^{\\mathbf{Y}}blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT, where \ud835\udd39:={0,1}assign\ud835\udd3901\\mathbb{B}:=\\{0,1\\}blackboard_B := { 0 , 1 } is the set of boolean values. It can be viewed either as a function that maps variables in \ud835\udc18\ud835\udc18\\mathbf{Y}bold_Y to boolean values or as a subset of the variables. A boolean function is a an element of \ud835\udd39\ud835\udd39\ud835\udc18superscript\ud835\udd39superscript\ud835\udd39\ud835\udc18\\mathbb{B}^{\\mathbb{B}^{\\mathbf{Y}}}blackboard_B start_POSTSUPERSCRIPT blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT. It can be viewed either as a function that maps states in \ud835\udd39\ud835\udc18superscript\ud835\udd39\ud835\udc18\\mathbb{B}^{\\mathbf{Y}}blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT to boolean values or as a subset of \ud835\udd39\ud835\udc18superscript\ud835\udd39\ud835\udc18\\mathbb{B}^{\\mathbf{Y}}blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT. A boolean function can be used to represent the set of possible states given our knowledge about the world. When the set of variables \ud835\udc18\ud835\udc18\\mathbf{Y}bold_Y is finite, we will often assume without loss of generality that \ud835\udc18:={Yj}1\u2264j\u2264kassign\ud835\udc18subscriptsubscript\ud835\udc4c\ud835\udc571\ud835\udc57\ud835\udc58\\mathbf{Y}:=\\{Y_{j}\\}_{1\\leq j\\leq k}bold_Y := { italic_Y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT 1 \u2264 italic_j \u2264 italic_k end_POSTSUBSCRIPT with k=|\ud835\udc18|\ud835\udc58\ud835\udc18k=|\\mathbf{Y}|italic_k = | bold_Y |. An abstract logic can be seen as a language for expressing boolean functions on finite sets of variables in a concise way.\nAn (abstract) logic is a couple \u2112:=(\ud835\udcaf,\ud835\udcc8)assign\u2112\ud835\udcaf\ud835\udcc8\\mathcal{L}:=(\\mathcal{T},\\mathscr{s})caligraphic_L := ( caligraphic_T , script_s ) such that for any discrete set of variables \ud835\udc18\ud835\udc18\\mathbf{Y}bold_Y:\ud835\udcaf\u2062(\ud835\udc18)\ud835\udcaf\ud835\udc18\\mathcal{T}(\\mathbf{Y})caligraphic_T ( bold_Y ) is the set of admissible theories on \ud835\udc18\ud835\udc18\\mathbf{Y}bold_Y\ud835\udcc8\u2062(\ud835\udc18)\ud835\udcc8\ud835\udc18\\mathscr{s}(\\mathbf{Y})script_s ( bold_Y ) determines which states on \ud835\udc18\ud835\udc18\\mathbf{Y}bold_Y satisfy a theory T\ud835\udc47Titalic_T:When the set of variables is clear from context, we simply note T\u2208\ud835\udcaf\ud835\udc47\ud835\udcafT\\in\\mathcal{T}italic_T \u2208 caligraphic_T and \ud835\udcc8\u2062(T)\ud835\udcc8\ud835\udc47\\mathscr{s}(T)script_s ( italic_T ) in place of T\u2208\ud835\udcaf\u2062(\ud835\udc18)\ud835\udc47\ud835\udcaf\ud835\udc18T\\in\\mathcal{T}(\\mathbf{Y})italic_T \u2208 caligraphic_T ( bold_Y ) and \ud835\udcc8\u2062(\ud835\udc18)\u2062(T)\ud835\udcc8\ud835\udc18\ud835\udc47\\mathscr{s}(\\mathbf{Y})(T)script_s ( bold_Y ) ( italic_T ).A logic is universal iff for any discrete set of variables \ud835\udc18\ud835\udc18\\mathbf{Y}bold_Y any boolean function \ud835\udcbb\u2208\ud835\udd39\ud835\udd39\ud835\udc18\ud835\udcbbsuperscript\ud835\udd39superscript\ud835\udd39\ud835\udc18\\mathscr{f}\\in\\mathbb{B}^{\\mathbb{B}^{\\mathbf{Y}}}script_f \u2208 blackboard_B start_POSTSUPERSCRIPT blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT can be represented by a theory, i.e. \u2200\ud835\udcbb\u2208\ud835\udd39\ud835\udd39\ud835\udc18\u2062\u2203T\u2208\ud835\udcaf\u2062(\ud835\udc18),\ud835\udcc8\u2062(T)=\ud835\udcbbformulae-sequencefor-all\ud835\udcbbsuperscript\ud835\udd39superscript\ud835\udd39\ud835\udc18\ud835\udc47\ud835\udcaf\ud835\udc18\ud835\udcc8\ud835\udc47\ud835\udcbb\\forall\\mathscr{f}\\in\\mathbb{B}^{\\mathbb{B}^{\\mathbf{Y}}}\\exists T\\in\\mathcal{%\nT}(\\mathbf{Y}),\\mathscr{s}(T)=\\mathscr{f}\u2200 script_f \u2208 blackboard_B start_POSTSUPERSCRIPT blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT \u2203 italic_T \u2208 caligraphic_T ( bold_Y ) , script_s ( italic_T ) = script_f.We say that a logic \u21122:=(\ud835\udcaf2,\ud835\udcc82)assignsubscript\u21122subscript\ud835\udcaf2subscript\ud835\udcc82\\mathcal{L}_{2}:=(\\mathcal{T}_{2},\\mathscr{s}_{2})caligraphic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT := ( caligraphic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , script_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) is a fragment of a logic \u21121:=(\ud835\udcaf1,\ud835\udcc81)assignsubscript\u21121subscript\ud835\udcaf1subscript\ud835\udcc81\\mathcal{L}_{1}:=(\\mathcal{T}_{1},\\mathscr{s}_{1})caligraphic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT := ( caligraphic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , script_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ), noted \u21122\u2282\u21121subscript\u21122subscript\u21121\\mathcal{L}_{2}\\subset\\mathcal{L}_{1}caligraphic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2282 caligraphic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT iff for any discrete set of variables \ud835\udc18\ud835\udc18\\mathbf{Y}bold_Y: \ud835\udcaf2\u2062(\ud835\udc18)\u2282\ud835\udcaf1\u2062(\ud835\udc18)subscript\ud835\udcaf2\ud835\udc18subscript\ud835\udcaf1\ud835\udc18\\mathcal{T}_{2}(\\mathbf{Y})\\subset\\mathcal{T}_{1}(\\mathbf{Y})caligraphic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( bold_Y ) \u2282 caligraphic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_Y ) and \ud835\udcc82\u2062(\ud835\udc18)subscript\ud835\udcc82\ud835\udc18\\mathscr{s}_{2}(\\mathbf{Y})script_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( bold_Y ) is the restriction of \ud835\udcc81\u2062(\ud835\udc18)subscript\ud835\udcc81\ud835\udc18\\mathscr{s}_{1}(\\mathbf{Y})script_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_Y ) to \ud835\udcaf2\u2062(\ud835\udc18)subscript\ud835\udcaf2\ud835\udc18\\mathcal{T}_{2}(\\mathbf{Y})caligraphic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( bold_Y ).A state \ud835\udc32\u2208\ud835\udd39\ud835\udc18\ud835\udc32superscript\ud835\udd39\ud835\udc18\\mathbf{y}\\in\\mathbb{B}^{\\mathbf{Y}}bold_y \u2208 blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT satisfies a theory T\u2208\ud835\udcaf\ud835\udc47\ud835\udcafT\\in\\mathcal{T}italic_T \u2208 caligraphic_T iff \ud835\udc32\u2208\ud835\udcc8\u2062(T)\ud835\udc32\ud835\udcc8\ud835\udc47\\mathbf{y}\\in\\mathscr{s}(T)bold_y \u2208 script_s ( italic_T ). We also say that T\ud835\udc47Titalic_T accepts \ud835\udc32\ud835\udc32\\mathbf{y}bold_y. A theory T\ud835\udc47Titalic_T is satisfiable if it is satisfied by a state, i.e. if \ud835\udcc8\u2062(T)\u2260\u2205\ud835\udcc8\ud835\udc47\\mathscr{s}(T)\\neq\\emptysetscript_s ( italic_T ) \u2260 \u2205. Two theories T1subscript\ud835\udc471T_{1}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and T2subscript\ud835\udc472T_{2}italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT are equivalent iff \ud835\udcc8\u2062(T1)=\ud835\udcc8\u2062(T2)\ud835\udcc8subscript\ud835\udc471\ud835\udcc8subscript\ud835\udc472\\mathscr{s}(T_{1})=\\mathscr{s}(T_{2})script_s ( italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) = script_s ( italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ).\nWe give below some examples of standard abstract logics often found in the neurosymbolic literature.Propositional logic \u2112P\u2062L:=(\ud835\udcafP\u2062L,\ud835\udcc8P\u2062L)assignsubscript\u2112\ud835\udc43\ud835\udc3fsubscript\ud835\udcaf\ud835\udc43\ud835\udc3fsubscript\ud835\udcc8\ud835\udc43\ud835\udc3f\\mathcal{L}_{PL}:=(\\mathcal{T}_{PL},\\mathscr{s}_{PL})caligraphic_L start_POSTSUBSCRIPT italic_P italic_L end_POSTSUBSCRIPT := ( caligraphic_T start_POSTSUBSCRIPT italic_P italic_L end_POSTSUBSCRIPT , script_s start_POSTSUBSCRIPT italic_P italic_L end_POSTSUBSCRIPT ) is the most common, typically used as an introduction to logical reasoning in many textbooks.a propositional formula \u03d5\u2208\u2131P\u2062L\u2062(\ud835\udc18)italic-\u03d5subscript\u2131\ud835\udc43\ud835\udc3f\ud835\udc18\\phi\\in\\mathcal{F}_{PL}(\\mathbf{Y})italic_\u03d5 \u2208 caligraphic_F start_POSTSUBSCRIPT italic_P italic_L end_POSTSUBSCRIPT ( bold_Y ) is formed inductively from variables and other formulas by using unary (\u00ac\\neg\u00ac) or binary (\u2228,\u2227\\lor,\\land\u2228 , \u2227) connectives:a theory in T\u2208\ud835\udcafP\u2062L\u2062(\ud835\udc18)\ud835\udc47subscript\ud835\udcaf\ud835\udc43\ud835\udc3f\ud835\udc18T\\in\\mathcal{T}_{PL}(\\mathbf{Y})italic_T \u2208 caligraphic_T start_POSTSUBSCRIPT italic_P italic_L end_POSTSUBSCRIPT ( bold_Y ) is a set of propositional formulas, i.e. T\u2208\ud835\udd39\u2131P\u2062L\u2062(\ud835\udc18)\ud835\udc47superscript\ud835\udd39subscript\u2131\ud835\udc43\ud835\udc3f\ud835\udc18T\\in\\mathbb{B}^{\\mathcal{F}_{PL}(\\mathbf{Y})}italic_T \u2208 blackboard_B start_POSTSUPERSCRIPT caligraphic_F start_POSTSUBSCRIPT italic_P italic_L end_POSTSUBSCRIPT ( bold_Y ) end_POSTSUPERSCRIPT or T\u2282\u2131P\u2062L\u2062(\ud835\udc18)\ud835\udc47subscript\u2131\ud835\udc43\ud835\udc3f\ud835\udc18T\\subset\\mathcal{F}_{PL}(\\mathbf{Y})italic_T \u2282 caligraphic_F start_POSTSUBSCRIPT italic_P italic_L end_POSTSUBSCRIPT ( bold_Y )a state \ud835\udc32\u2208\ud835\udd39\ud835\udc18\ud835\udc32superscript\ud835\udd39\ud835\udc18\\mathbf{y}\\in\\mathbb{B}^{\\mathbf{Y}}bold_y \u2208 blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT inductively defines a valuation \u03bd\ud835\udc32\u2208\ud835\udd39\u2131P\u2062L\u2062(\ud835\udc18)subscript\ud835\udf08\ud835\udc32superscript\ud835\udd39subscript\u2131\ud835\udc43\ud835\udc3f\ud835\udc18\\nu_{\\mathbf{y}}\\in\\mathbb{B}^{\\mathcal{F}_{PL}(\\mathbf{Y})}italic_\u03bd start_POSTSUBSCRIPT bold_y end_POSTSUBSCRIPT \u2208 blackboard_B start_POSTSUPERSCRIPT caligraphic_F start_POSTSUBSCRIPT italic_P italic_L end_POSTSUBSCRIPT ( bold_Y ) end_POSTSUPERSCRIPT such that:\ud835\udcc8P\u2062Lsubscript\ud835\udcc8\ud835\udc43\ud835\udc3f\\mathscr{s}_{PL}script_s start_POSTSUBSCRIPT italic_P italic_L end_POSTSUBSCRIPT is such that:Answer Set Programming (ASP) \u2112A\u2062S\u2062P:=(\ud835\udcafA\u2062S\u2062P,\ud835\udcc8A\u2062S\u2062P)assignsubscript\u2112\ud835\udc34\ud835\udc46\ud835\udc43subscript\ud835\udcaf\ud835\udc34\ud835\udc46\ud835\udc43subscript\ud835\udcc8\ud835\udc34\ud835\udc46\ud835\udc43\\mathcal{L}_{ASP}:=(\\mathcal{T}_{ASP},\\mathscr{s}_{ASP})caligraphic_L start_POSTSUBSCRIPT italic_A italic_S italic_P end_POSTSUBSCRIPT := ( caligraphic_T start_POSTSUBSCRIPT italic_A italic_S italic_P end_POSTSUBSCRIPT , script_s start_POSTSUBSCRIPT italic_A italic_S italic_P end_POSTSUBSCRIPT ) is one of the simplest examples of non-monotonic logics, which enable concise representations of complex knowledge at the cost of monotonicity.formulas in \u211b\u2062(\ud835\udc18)\u211b\ud835\udc18\\mathcal{R}(\\mathbf{Y})caligraphic_R ( bold_Y ) are called rules and have the shape:with a0\u2208\ud835\udc18\u222a\u22a5,a1,\u2026,am\u2208\ud835\udc18a_{0}\\in\\mathbf{Y}\\cup{\\bot},a_{1},...,a_{m}\\in\\mathbf{Y}italic_a start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u2208 bold_Y \u222a \u22a5 , italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_a start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT \u2208 bold_Y.\na0subscript\ud835\udc4e0a_{0}italic_a start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is called the head of the rule and a1,\u2026,al,not\u00a0\u2062al+1,\u2026,not\u00a0\u2062amsubscript\ud835\udc4e1\u2026subscript\ud835\udc4e\ud835\udc59not\u00a0subscript\ud835\udc4e\ud835\udc591\u2026not\u00a0subscript\ud835\udc4e\ud835\udc5aa_{1},...,a_{l},\\text{not }a_{l+1},...,\\text{not }a_{m}italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_a start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT , not italic_a start_POSTSUBSCRIPT italic_l + 1 end_POSTSUBSCRIPT , \u2026 , not italic_a start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT is called the body.\nWhen the body is empty the rule can be noted a0subscript\ud835\udc4e0a_{0}italic_a start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT.a theory \u03a0\u2208\ud835\udcafA\u2062S\u2062P\u03a0subscript\ud835\udcaf\ud835\udc34\ud835\udc46\ud835\udc43\\Pi\\in\\mathcal{T}_{ASP}roman_\u03a0 \u2208 caligraphic_T start_POSTSUBSCRIPT italic_A italic_S italic_P end_POSTSUBSCRIPT is a set of rules (i.e. \u03a0\u2282\u211b\u2062(\ud835\udc18)\u03a0\u211b\ud835\udc18\\Pi\\subset\\mathcal{R}(\\mathbf{Y})roman_\u03a0 \u2282 caligraphic_R ( bold_Y )) and is called a program.the reduct of a program \u03a0\u2282\u211b\u2062(\ud835\udc18)\u03a0\u211b\ud835\udc18\\Pi\\subset\\mathcal{R}(\\mathbf{Y})roman_\u03a0 \u2282 caligraphic_R ( bold_Y ) relative to a state \ud835\udc32\u2208\ud835\udd39\ud835\udc18\ud835\udc32superscript\ud835\udd39\ud835\udc18\\mathbf{y}\\in\\mathbb{B}^{\\mathbf{Y}}bold_y \u2208 blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT is the program \u03a0\ud835\udc32superscript\u03a0\ud835\udc32\\Pi^{\\mathbf{y}}roman_\u03a0 start_POSTSUPERSCRIPT bold_y end_POSTSUPERSCRIPT such that:To get \u03a0\ud835\udc32superscript\u03a0\ud835\udc32\\Pi^{\\mathbf{y}}roman_\u03a0 start_POSTSUPERSCRIPT bold_y end_POSTSUPERSCRIPT, we first eliminate all rules in \u03a0\u03a0\\Piroman_\u03a0 such that \ud835\udc32\ud835\udc32\\mathbf{y}bold_y does not satisfy the negative part of the body, then for remaining rules, we delete the negative part of the body and add them to \u03a0\ud835\udc32superscript\u03a0\ud835\udc32\\Pi^{\\mathbf{y}}roman_\u03a0 start_POSTSUPERSCRIPT bold_y end_POSTSUPERSCRIPT.we say that a state \ud835\udc32\u2208\ud835\udd39\ud835\udc18\ud835\udc32superscript\ud835\udd39\ud835\udc18\\mathbf{y}\\in\\mathbb{B}^{\\mathbf{Y}}bold_y \u2208 blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT satisfies a rule r:a0\u2190a1,\u2026,al,not\u00a0\u2062al+1,\u2026,not\u00a0\u2062am.:\ud835\udc5f\u2190subscript\ud835\udc4e0subscript\ud835\udc4e1\u2026subscript\ud835\udc4e\ud835\udc59not\u00a0subscript\ud835\udc4e\ud835\udc591\u2026not\u00a0subscript\ud835\udc4e\ud835\udc5ar:a_{0}\\leftarrow a_{1},...,a_{l},\\text{not }a_{l+1},...,\\text{not }a_{m}.italic_r : italic_a start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u2190 italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_a start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT , not italic_a start_POSTSUBSCRIPT italic_l + 1 end_POSTSUBSCRIPT , \u2026 , not italic_a start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT . iff \u03bd\ud835\udc32\u2062(\u03d5r)=1subscript\ud835\udf08\ud835\udc32subscriptitalic-\u03d5\ud835\udc5f1\\nu_{\\mathbf{y}}(\\phi_{r})=1italic_\u03bd start_POSTSUBSCRIPT bold_y end_POSTSUBSCRIPT ( italic_\u03d5 start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ) = 1 where:\ud835\udcc8A\u2062S\u2062Psubscript\ud835\udcc8\ud835\udc34\ud835\udc46\ud835\udc43\\mathscr{s}_{ASP}script_s start_POSTSUBSCRIPT italic_A italic_S italic_P end_POSTSUBSCRIPT follows the answer set semantics: a state \ud835\udc32\u2208\ud835\udd39\ud835\udc18\ud835\udc32superscript\ud835\udd39\ud835\udc18\\mathbf{y}\\in\\mathbb{B}^{\\mathbf{Y}}bold_y \u2208 blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT in an answer set for a program \u03a0\u03a0\\Piroman_\u03a0, noted \ud835\udc32\u2208\ud835\udcc8A\u2062S\u2062P\u2062(\u03a0)\ud835\udc32subscript\ud835\udcc8\ud835\udc34\ud835\udc46\ud835\udc43\u03a0\\mathbf{y}\\in\\mathscr{s}_{ASP}(\\Pi)bold_y \u2208 script_s start_POSTSUBSCRIPT italic_A italic_S italic_P end_POSTSUBSCRIPT ( roman_\u03a0 ), iff it is the smaller state (in terms of inclusion) to satisfy all rules of \u03a0\ud835\udc32superscript\u03a0\ud835\udc32\\Pi^{\\mathbf{y}}roman_\u03a0 start_POSTSUPERSCRIPT bold_y end_POSTSUPERSCRIPT.Linear Programming is traditionally associated to constrained optimization problems, but it can be used to define a logic \u2112L\u2062P:=(\ud835\udcafL\u2062P,\ud835\udcc8L\u2062P)assignsubscript\u2112\ud835\udc3f\ud835\udc43subscript\ud835\udcaf\ud835\udc3f\ud835\udc43subscript\ud835\udcc8\ud835\udc3f\ud835\udc43\\mathcal{L}_{LP}:=(\\mathcal{T}_{LP},\\mathscr{s}_{LP})caligraphic_L start_POSTSUBSCRIPT italic_L italic_P end_POSTSUBSCRIPT := ( caligraphic_T start_POSTSUBSCRIPT italic_L italic_P end_POSTSUBSCRIPT , script_s start_POSTSUBSCRIPT italic_L italic_P end_POSTSUBSCRIPT ) naturally suited to express many real-world problems.formulas in \u2112\u2062\ud835\udc9e\u2062(\ud835\udc18)\u2112\ud835\udc9e\ud835\udc18\\mathcal{LC}(\\mathbf{Y})caligraphic_L caligraphic_C ( bold_Y ) are called linear constraints and have the shape:with Yi1,\u2026,Yim\u2208\ud835\udc18subscript\ud835\udc4csubscript\ud835\udc561\u2026subscript\ud835\udc4csubscript\ud835\udc56\ud835\udc5a\ud835\udc18Y_{i_{1}},...,Y_{i_{m}}\\in\\mathbf{Y}italic_Y start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , \u2026 , italic_Y start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT end_POSTSUBSCRIPT \u2208 bold_Y and b1,\u2026,bm,c\u2208\u2124subscript\ud835\udc4f1\u2026subscript\ud835\udc4f\ud835\udc5a\ud835\udc50\u2124b_{1},...,b_{m},c\\in\\mathbb{Z}italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_b start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT , italic_c \u2208 blackboard_Z. A linear constraint is sometimes noted \u27e8\ud835\udc19,\ud835\udc1b\u27e9=c\ud835\udc19\ud835\udc1b\ud835\udc50\\langle\\mathbf{Z},\\mathbf{b}\\rangle=c\u27e8 bold_Z , bold_b \u27e9 = italic_c where \ud835\udc19:=(Yi1,\u2026,Yim)assign\ud835\udc19subscript\ud835\udc4csubscript\ud835\udc561\u2026subscript\ud835\udc4csubscript\ud835\udc56\ud835\udc5a\\mathbf{Z}:=(Y_{i_{1}},...,Y_{i_{m}})bold_Z := ( italic_Y start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , \u2026 , italic_Y start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) and \ud835\udc1b:=(b1,\u2026,bm)assign\ud835\udc1bsubscript\ud835\udc4f1\u2026subscript\ud835\udc4f\ud835\udc5a\\mathbf{b}:=(b_{1},...,b_{m})bold_b := ( italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_b start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ).a theory \u03a0\u2208\ud835\udcafL\u2062P\u03a0subscript\ud835\udcaf\ud835\udc3f\ud835\udc43\\Pi\\in\\mathcal{T}_{LP}roman_\u03a0 \u2208 caligraphic_T start_POSTSUBSCRIPT italic_L italic_P end_POSTSUBSCRIPT is a set of linear constraints (i.e. \u03a0\u2282\u2112\u2062\ud835\udc9e\u2062(\ud835\udc18)\u03a0\u2112\ud835\udc9e\ud835\udc18\\Pi\\subset\\mathcal{LC}(\\mathbf{Y})roman_\u03a0 \u2282 caligraphic_L caligraphic_C ( bold_Y )) and is called a linear program.a state \ud835\udc32\u2208\ud835\udd39\ud835\udc18\ud835\udc32superscript\ud835\udd39\ud835\udc18\\mathbf{y}\\in\\mathbb{B}^{\\mathbf{Y}}bold_y \u2208 blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT satisfies a linear constraint iff b1.yi1+\u2026+bm.yim\u2264cformulae-sequencesubscript\ud835\udc4f1subscript\ud835\udc66subscript\ud835\udc561\u2026subscript\ud835\udc4f\ud835\udc5asubscript\ud835\udc66subscript\ud835\udc56\ud835\udc5a\ud835\udc50b_{1}.y_{i_{1}}+...+b_{m}.y_{i_{m}}\\leq citalic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . italic_y start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT + \u2026 + italic_b start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT . italic_y start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT end_POSTSUBSCRIPT \u2264 italic_c in the usual arithmetical sense.a state \ud835\udc32\u2208\ud835\udd39\ud835\udc18\ud835\udc32superscript\ud835\udd39\ud835\udc18\\mathbf{y}\\in\\mathbb{B}^{\\mathbf{Y}}bold_y \u2208 blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT satisfies a linear program \u03a0\u2282\u2112\u2062\ud835\udc9e\u2062(\ud835\udc18)\u03a0\u2112\ud835\udc9e\ud835\udc18\\Pi\\subset\\mathcal{LC}(\\mathbf{Y})roman_\u03a0 \u2282 caligraphic_L caligraphic_C ( bold_Y ) (i.e. \ud835\udc32\u2208\ud835\udcc8L\u2062P\u2062(\u03a0)\ud835\udc32subscript\ud835\udcc8\ud835\udc3f\ud835\udc43\u03a0\\mathbf{y}\\in\\mathscr{s}_{LP}(\\Pi)bold_y \u2208 script_s start_POSTSUBSCRIPT italic_L italic_P end_POSTSUBSCRIPT ( roman_\u03a0 )) iff it satisfies all linear constraints in \u03a0\u03a0\\Piroman_\u03a0.Graph-based logics allow to reason about elements of a graph. Contrary to other logics described above, graph-based logics are not universal. They are often used to represent fragments of universal logics in a concise and more intuitive way.a theory for a discrete set of variables \ud835\udc18\ud835\udc18\\mathbf{Y}bold_Y is a couple (G,\u03c2)\ud835\udc3a\ud835\udf0d(G,\\varsigma)( italic_G , italic_\u03c2 ) where G=(V,E)\ud835\udc3a\ud835\udc49\ud835\udc38G=(V,E)italic_G = ( italic_V , italic_E ) is a graph and \u03c2:W\u21a6\ud835\udc18:\ud835\udf0dmaps-to\ud835\udc4a\ud835\udc18\\varsigma:W\\mapsto\\mathbf{Y}italic_\u03c2 : italic_W \u21a6 bold_Y, with W\u2282V\u222aE\ud835\udc4a\ud835\udc49\ud835\udc38W\\subset V\\cup Eitalic_W \u2282 italic_V \u222a italic_E, is injective.we say that a theory is edge-based (resp. vertice-based) if W=E\ud835\udc4a\ud835\udc38W=Eitalic_W = italic_E (resp. W=V\ud835\udc4a\ud835\udc49W=Vitalic_W = italic_V), and we note \ud835\udcaf\u2192\u2062(\ud835\udc18)subscript\ud835\udcaf\u2192\ud835\udc18\\mathcal{T}_{\\to}(\\mathbf{Y})caligraphic_T start_POSTSUBSCRIPT \u2192 end_POSTSUBSCRIPT ( bold_Y ) (resp. \ud835\udcaf\u2219\u2062(\ud835\udc18)subscript\ud835\udcaf\u2219\ud835\udc18\\mathcal{T}_{\\bullet}(\\mathbf{Y})caligraphic_T start_POSTSUBSCRIPT \u2219 end_POSTSUBSCRIPT ( bold_Y )) the set of edge-based (resp. vertice-based) theories on \ud835\udc18\ud835\udc18\\mathbf{Y}bold_Y.the semantics of the logic determines for a given graph G\ud835\udc3aGitalic_G if the selected elements in a state \ud835\udc32\u2208\ud835\udd39\ud835\udc18\ud835\udc32superscript\ud835\udd39\ud835\udc18\\mathbf{y}\\in\\mathbb{B}^{\\mathbf{Y}}bold_y \u2208 blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT form a valid substructure of G\ud835\udc3aGitalic_G.We give examples of graph-based logics in Sections 4.2.1, 4.2.3 and 4.2.4.\nOne challenge of neurosymbolic AI is to bridge the gap between the discrete nature of logic and the continuous nature of neural networks. Probabilistic reasoning can provide the interface between these two realms by allowing us to reason about uncertain facts. In this section, we introduce two probabilistic reasoning problems: Probabilistic Query Estimation (PQE), i.e. computing the probability of a theory to be satisfied, and Most Probable Explanation (MPE), i.e. finding the most probable state that satisfies a given theory.A probability distribution on a set of boolean variables \ud835\udc18\ud835\udc18\\mathbf{Y}bold_Y is an application \ud835\udcab:\ud835\udd39\ud835\udc18\u21a6\u211d+:\ud835\udcabmaps-tosuperscript\ud835\udd39\ud835\udc18superscript\u211d\\mathcal{P}:\\mathbb{B}^{\\mathbf{Y}}\\mapsto\\mathbb{R}^{+}caligraphic_P : blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT \u21a6 blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT that maps each state \ud835\udc32\ud835\udc32\\mathbf{y}bold_y to a probability \ud835\udcab\u2062(\ud835\udc32)\ud835\udcab\ud835\udc32\\mathcal{P}(\\mathbf{y})caligraphic_P ( bold_y ) such that \u2211\ud835\udc32\u2208\ud835\udd39\ud835\udc18\ud835\udcab\u2062(\ud835\udc32)=1subscript\ud835\udc32superscript\ud835\udd39\ud835\udc18\ud835\udcab\ud835\udc321\\sum_{\\mathbf{y}\\in\\mathbb{B}^{\\mathbf{Y}}}\\mathcal{P}(\\mathbf{y})=1\u2211 start_POSTSUBSCRIPT bold_y \u2208 blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_P ( bold_y ) = 1. To define internal operations between distributions, like multiplication, we extend this definition to un-normalized distributions \u2130:\ud835\udd39\ud835\udc18\u21a6\u211d+:\u2130maps-tosuperscript\ud835\udd39\ud835\udc18superscript\u211d\\mathcal{E}:\\mathbb{B}^{\\mathbf{Y}}\\mapsto\\mathbb{R}^{+}caligraphic_E : blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT \u21a6 blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT. The null distribution is the application that maps all states to 00. The partition function \ud835\uddb9:\u2130\u21a6\u2211\ud835\udc32\u2208\ud835\udd39\ud835\udc18\u2130\u2062(\ud835\udc32):\ud835\uddb9maps-to\u2130subscript\ud835\udc32superscript\ud835\udd39\ud835\udc18\u2130\ud835\udc32\\mathsf{Z}:\\mathcal{E}\\mapsto\\sum_{\\mathbf{y}\\in\\mathbb{B}^{\\mathbf{Y}}}%\n\\mathcal{E}(\\mathbf{y})sansserif_Z : caligraphic_E \u21a6 \u2211 start_POSTSUBSCRIPT bold_y \u2208 blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_E ( bold_y ) maps each distribution to its sum, and we note \u2130\u00af:=\u2130\ud835\uddb9\u2062(\u2130)assign\u00af\u2130\u2130\ud835\uddb9\u2130\\overline{\\mathcal{E}}:=\\frac{\\mathcal{E}}{\\mathsf{Z}(\\mathcal{E})}over\u00af start_ARG caligraphic_E end_ARG := divide start_ARG caligraphic_E end_ARG start_ARG sansserif_Z ( caligraphic_E ) end_ARG the normalized distribution (when \u2130\u2130\\mathcal{E}caligraphic_E is non-null). A boolean function is a distribution that maps all states to \ud835\udd39\ud835\udd39\\mathbb{B}blackboard_B. The mode of a distribution \u2130\u2130\\mathcal{E}caligraphic_E is its most probable state, ie argmax\ud835\udc32\u2208\ud835\udd39\ud835\udc18\u2062\u2130\u2062(\ud835\udc32)\ud835\udc32superscript\ud835\udd39\ud835\udc18argmax\u2130\ud835\udc32\\underset{\\mathbf{y}\\in\\mathbb{B}^{\\mathbf{Y}}}{\\operatorname*{argmax}}%\n\\mathcal{E}(\\mathbf{y})start_UNDERACCENT bold_y \u2208 blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT end_UNDERACCENT start_ARG roman_argmax end_ARG caligraphic_E ( bold_y ).Typically, when belief about random variables is expressed through a probability distribution and new information is collected in the form of evidence (i.e. a partial assignment of the variables), we are interested in two things: computing the probability of such evidence and updating our beliefs using Bayes\u2019 rules by conditioning the distribution on the evidence. Probabilistic reasoning allows us to perform the same operations with logical knowledge in place of evidence. Let\u2019s assume a probability distribution \ud835\udcab\ud835\udcab\\mathcal{P}caligraphic_P on variables \ud835\udc18:={Yj}1\u2264j\u2264kassign\ud835\udc18subscriptsubscript\ud835\udc4c\ud835\udc571\ud835\udc57\ud835\udc58\\mathbf{Y}:=\\{Y_{j}\\}_{1\\leq j\\leq k}bold_Y := { italic_Y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT 1 \u2264 italic_j \u2264 italic_k end_POSTSUBSCRIPT, an abstract logic \u2112:=(\ud835\udcaf,\ud835\udcc8)assign\u2112\ud835\udcaf\ud835\udcc8\\mathcal{L}:=(\\mathcal{T},\\mathscr{s})caligraphic_L := ( caligraphic_T , script_s ) and a satisfiable theory T\u2208\ud835\udcaf\u2062(\ud835\udc18)\ud835\udc47\ud835\udcaf\ud835\udc18T\\in\\mathcal{T}(\\mathbf{Y})italic_T \u2208 caligraphic_T ( bold_Y ). Notice that \ud835\udcab\ud835\udcab\\mathcal{P}caligraphic_P defines a probability distribution on the set of states of \ud835\udc18\ud835\udc18\\mathbf{Y}bold_Y and that \ud835\udcc8\u2062(T)\u2208\ud835\udd39\ud835\udd39\ud835\udc18\ud835\udcc8\ud835\udc47superscript\ud835\udd39superscript\ud835\udd39\ud835\udc18\\mathscr{s}(T)\\in\\mathbb{B}^{\\mathbb{B}^{\\mathbf{Y}}}script_s ( italic_T ) \u2208 blackboard_B start_POSTSUPERSCRIPT blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT is a boolean function on \ud835\udc18\ud835\udc18\\mathbf{Y}bold_Y.The probability of T\ud835\udc47Titalic_T under \ud835\udcab\ud835\udcab\\mathcal{P}caligraphic_P is:\nThe distribution \ud835\udcab\ud835\udcab\\mathcal{P}caligraphic_P conditioned on T\ud835\udc47Titalic_T, noted \ud835\udcab(\u22c5|T)\\mathcal{P}(\\cdot|T)caligraphic_P ( \u22c5 | italic_T ), is:By convention, if \ud835\udcab\u2062(T)=0\ud835\udcab\ud835\udc470\\mathcal{P}(T)=0caligraphic_P ( italic_T ) = 0, then \ud835\udcab(\u22c5|T)\\mathcal{P}(\\cdot|T)caligraphic_P ( \u22c5 | italic_T ) is the null-distribution. The semantic \ud835\udcc8\ud835\udcc8\\mathscr{s}script_s of the logic is assumed to be deduced implicitly from the context and therefore does not explicitly appear in the notations \ud835\udcab\u2062(T)\ud835\udcab\ud835\udc47\\mathcal{P}(T)caligraphic_P ( italic_T ) and \ud835\udcab(\u22c5|T)\\mathcal{P}(\\cdot|T)caligraphic_P ( \u22c5 | italic_T ).A standard distribution in deep learning is the exponential distribution, which is parameterized by a vector of scores \ud835\udc1a\u2208\u211dk\ud835\udc1asuperscript\u211d\ud835\udc58\\mathbf{a}\\in\\mathbb{R}^{k}bold_a \u2208 blackboard_R start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT, one for each variable in \ud835\udc18\ud835\udc18\\mathbf{Y}bold_Y. We note \u211dksuperscript\u211d\ud835\udc58\\mathbb{R}^{k}blackboard_R start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT as a simpler notation for \u211d\ud835\udc18superscript\u211d\ud835\udc18\\mathbb{R}^{\\mathbf{Y}}blackboard_R start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT assuming \ud835\udc18:={Yj}1\u2264j\u2264kassign\ud835\udc18subscriptsubscript\ud835\udc4c\ud835\udc571\ud835\udc57\ud835\udc58\\mathbf{Y}:=\\{Y_{j}\\}_{1\\leq j\\leq k}bold_Y := { italic_Y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT 1 \u2264 italic_j \u2264 italic_k end_POSTSUBSCRIPT.Given a vector \ud835\udc1a\u2208\u211dk\ud835\udc1asuperscript\u211d\ud835\udc58\\mathbf{a}\\in\\mathbb{R}^{k}bold_a \u2208 blackboard_R start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT, the exponential distribution is:We will also note \ud835\udcab(\u22c5|\ud835\udc1a)=\u2130(\u22c5|\ud835\udc1a)\u00af\\mathcal{P}(\\cdot|\\mathbf{a})=\\overline{\\mathcal{E}(\\cdot|\\mathbf{a})}caligraphic_P ( \u22c5 | bold_a ) = over\u00af start_ARG caligraphic_E ( \u22c5 | bold_a ) end_ARG the corresponding normalized probability distribution.The exponential probability distribution corresponds to the joint distribution of independent Bernoulli variables \u212c\u2062(pi)1\u2264i\u2264k\u212csubscriptsubscript\ud835\udc5d\ud835\udc561\ud835\udc56\ud835\udc58\\mathcal{B}(p_{i})_{1\\leq i\\leq k}caligraphic_B ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT 1 \u2264 italic_i \u2264 italic_k end_POSTSUBSCRIPT with pi=\ud835\uddcc\u2062(ai)subscript\ud835\udc5d\ud835\udc56\ud835\uddccsubscript\ud835\udc4e\ud835\udc56p_{i}=\\mathsf{s}(a_{i})italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = sansserif_s ( italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ), where \ud835\uddcc\u2062(ai)=eai1+eai\ud835\uddccsubscript\ud835\udc4e\ud835\udc56superscript\ud835\udc52subscript\ud835\udc4e\ud835\udc561superscript\ud835\udc52subscript\ud835\udc4e\ud835\udc56\\mathsf{s}(a_{i})=\\frac{e^{a_{i}}}{1+e^{a_{i}}}sansserif_s ( italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = divide start_ARG italic_e start_POSTSUPERSCRIPT italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT end_ARG start_ARG 1 + italic_e start_POSTSUPERSCRIPT italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT end_ARG is the sigmoid function. Since \ud835\udcab(\u22c5|\ud835\udc1a)\\mathcal{P}(\\cdot|\\mathbf{a})caligraphic_P ( \u22c5 | bold_a ) is strictly positive (for all \ud835\udc1a\ud835\udc1a\\mathbf{a}bold_a), if T\ud835\udc47Titalic_T is satisfiable then its probability under \ud835\udcab(\u22c5|\ud835\udc1a)\\mathcal{P}(\\cdot|\\mathbf{a})caligraphic_P ( \u22c5 | bold_a ) is also strictly positive. We note:\nComputing \ud835\udcab\u2062(T|\ud835\udc1a)\ud835\udcabconditional\ud835\udc47\ud835\udc1a\\mathcal{P}(T|\\mathbf{a})caligraphic_P ( italic_T | bold_a ) is a counting problem called Probabilistic Query Estimation (PQE). Computing the mode of \ud835\udcab(\u22c5|\ud835\udc1a,T)\\mathcal{P}(\\cdot|\\mathbf{a},T)caligraphic_P ( \u22c5 | bold_a , italic_T ) is an optimization problem called Most Probable Explanation (MPE). Notice that computing \ud835\udcab\u2062(\ud835\udc32|\ud835\udc1a,T)\ud835\udcabconditional\ud835\udc32\ud835\udc1a\ud835\udc47\\mathcal{P}(\\mathbf{y}|\\mathbf{a},T)caligraphic_P ( bold_y | bold_a , italic_T ) for a satisfying state \ud835\udc32\u2208\ud835\udcc8\u2062(T)\ud835\udc32\ud835\udcc8\ud835\udc47\\mathbf{y}\\in\\mathscr{s}(T)bold_y \u2208 script_s ( italic_T ) is equivalent to solving PQE because \ud835\udcab\u2062(\ud835\udc32|\ud835\udc1a)\ud835\udcabconditional\ud835\udc32\ud835\udc1a\\mathcal{P}(\\mathbf{y}|\\mathbf{a})caligraphic_P ( bold_y | bold_a ) can be computed in polynomial time and:Solving these probabilistic reasoning problems is at the core of many neurosymbolic techniques, as shown in Section 3.3.In machine learning, the objective is to usually learn a functional relationship f:\ud835\udcb3\u21a6\ud835\udcb4:\ud835\udc53maps-to\ud835\udcb3\ud835\udcb4f:\\mathcal{X}\\mapsto\\mathcal{Y}italic_f : caligraphic_X \u21a6 caligraphic_Y between an input domain \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X and an output domain \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y from data samples. Supervised multi-label classification is a subset of machine learning where input samples are labeled with subsets of a finite set of classes \ud835\udc18\ud835\udc18\\mathbf{Y}bold_Y. Therefore, labels can be understood as states on the set of variables \ud835\udc18\ud835\udc18\\mathbf{Y}bold_Y. In this case, the output space of the task, i.e. the set of all labels, is \ud835\udcb4=\ud835\udd39\ud835\udc18\ud835\udcb4superscript\ud835\udd39\ud835\udc18\\mathcal{Y}=\\mathbb{B}^{\\mathbf{Y}}caligraphic_Y = blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT. In informed supervised (multi-label) classification, prior knowledge (sometimes called background knowledge) specifies which states in the output domain are semantically valid, i.e. to which states can input samples be mapped. The set of valid states constitute a boolean function \ud835\udcbb\u2208\ud835\udd39\ud835\udd39\ud835\udc18\ud835\udcbbsuperscript\ud835\udd39superscript\ud835\udd39\ud835\udc18\\mathscr{f}\\in\\mathbb{B}^{\\mathbb{B}^{\\mathbf{Y}}}script_f \u2208 blackboard_B start_POSTSUPERSCRIPT blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT on the set of variables \ud835\udc18\ud835\udc18\\mathbf{Y}bold_Y. As shown in Section 2.1, a natural way to represent such knowledge is to use a logical theory, i.e. to provide an abstract logic (\ud835\udcaf,\ud835\udcc8)\ud835\udcaf\ud835\udcc8(\\mathcal{T},\\mathscr{s})( caligraphic_T , script_s ) and a satisfiable theory T\u2208\ud835\udcaf\u2062(\ud835\udc18)\ud835\udc47\ud835\udcaf\ud835\udc18T\\in\\mathcal{T}(\\mathbf{Y})italic_T \u2208 caligraphic_T ( bold_Y ) such that \ud835\udcc8\u2062(T)=\ud835\udcbb\ud835\udcc8\ud835\udc47\ud835\udcbb\\mathscr{s}(T)=\\mathscr{f}script_s ( italic_T ) = script_f. For instance, hierarchical and exclusion constraints are used in [10], propositional formulas in conjunctive normal form are used in [11], boolean circuits in [12], ASP programs in [13] and linear programs in [14].Following [15], we describe neural classification systems with three modules:a parametric differentiable (i.e. neural) module \ud835\uddac:\ud835\udcb3\u00d7\u0398\u21a6\u211dk:\ud835\uddacmaps-to\ud835\udcb3\u0398superscript\u211d\ud835\udc58\\mathsf{M}:\\mathcal{X}\\times\\Theta\\mapsto\\mathbb{R}^{k}sansserif_M : caligraphic_X \u00d7 roman_\u0398 \u21a6 blackboard_R start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT, called the model, which takes in an input sample and parameters and produces a score for each output variable.a non-parametric differentiable module \ud835\uddab:\u211dk\u00d7\ud835\udd39\ud835\udc18\u21a6\u211d+:\ud835\uddabmaps-tosuperscript\u211d\ud835\udc58superscript\ud835\udd39\ud835\udc18superscript\u211d\\mathsf{L}:\\mathbb{R}^{k}\\times\\mathbb{B}^{\\mathbf{Y}}\\mapsto\\mathbb{R}^{+}sansserif_L : blackboard_R start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT \u00d7 blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT \u21a6 blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT, called the loss module, which takes the output of the model and a label and produces a positive scalar.\na non-parametric module \ud835\udda8:\u211dk\u21a6\ud835\udd39\ud835\udc18:\ud835\udda8maps-tosuperscript\u211d\ud835\udc58superscript\ud835\udd39\ud835\udc18\\mathsf{I}:\\mathbb{R}^{k}\\mapsto\\mathbb{B}^{\\mathbf{Y}}sansserif_I : blackboard_R start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT \u21a6 blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT, called the inference module, which transforms the vector of scores produced by the network into a predicted state.The first two modules are standard in any machine learning system. The third module however, which bridges the gap between the continuous nature of the neural network (needed for backpropagation) and the discrete nature of the output space, is rarely explicitly specified.Some neural classification systems come with a natural probabilistic interpretation. The scores \ud835\uddac\u2062(x,\u03b8)\ud835\uddac\ud835\udc65\ud835\udf03\\mathsf{M}(x,\\theta)sansserif_M ( italic_x , italic_\u03b8 ) produced by the model implicitly represent the parameters of a conditional probability distribution \ud835\udc0f(\u22c5|\ud835\uddac(x,\u03b8))\\mathbf{P}(\\cdot|\\mathsf{M}(x,\\theta))bold_P ( \u22c5 | sansserif_M ( italic_x , italic_\u03b8 ) ) on the space of outputs \ud835\udd39\ud835\udc18superscript\ud835\udd39\ud835\udc18\\mathbb{B}^{\\mathbf{Y}}blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT. The loss module computes the negative log-likelihood of a label under that distribution. And the inference module computes the most probable output given the learned distribution.In this paper, we assume that the architecture of the neural model (e.g.\u00a0fully connected, convolutional, transformer-based, etc.) is mainly dependent on the modality of the input space (e.g.\u00a0images, texts, videos, time series, etc.). Therefore we only consider neurosymbolic techniques that integrate prior knowledge in the two other modules of our neural classification system, as illustrated on Figure 1. We call such techniques model agnostic neurosymbolic techniques and give a formal definition.An abstract (model agnostic) neurosymbolic technique is \ud835\udd17:=(\ud835\udd0f,\u2111)assign\ud835\udd17\ud835\udd0f\u2111\\mathfrak{T}:=(\\mathfrak{L},\\mathfrak{I})fraktur_T := ( fraktur_L , fraktur_I ) such that for any abstract logic \u2112:=(\ud835\udcaf,\ud835\udcc8)assign\u2112\ud835\udcaf\ud835\udcc8\\mathcal{L}:=(\\mathcal{T},\\mathscr{s})caligraphic_L := ( caligraphic_T , script_s ), finite set of variables \ud835\udc18\ud835\udc18\\mathbf{Y}bold_Y and theory T\u2208\ud835\udcaf\u2062(\ud835\udc18)\ud835\udc47\ud835\udcaf\ud835\udc18T\\in\\mathcal{T}(\\mathbf{Y})italic_T \u2208 caligraphic_T ( bold_Y ):We define below three abstract neurosymbolic techniques (semantic conditioning, semantic regularization and semantic conditioning at inference) and relate each technique to the existing neurosymbolic literature.Following the probabilistic interpretation introduced in 3.2, a natural way to integrate prior knowledge T\ud835\udc47Titalic_T into a neural classification system is to condition the distribution \ud835\udc0f(\u22c5|\ud835\uddac(x,\u03b8))\\mathbf{P}(\\cdot|\\mathsf{M}(x,\\theta))bold_P ( \u22c5 | sansserif_M ( italic_x , italic_\u03b8 ) ) on T\ud835\udc47Titalic_T. This conditioning affects the loss and inference modules, both underpinned by the conditional distribution. It was first introduced in [10] for Hierarchical-Exclusion (HEX) graphs constraints. Semantic probabilistic layers [12] can be used to implement semantic conditioning on circuit theories. Moreover they go beyond exponential distributions and allow for a more expressive family of distributions using probabilistic circuits. NeurASP [13] defines semantic conditioning on a predicate extension of ASP programs. Likewise, DeepProbLog [16] provides an interface between Problog [17] programs and neural networks. However, since probabilistic reasoning in DeepProbLog is performed through grounding, its computational complexity is akin to that of semantic conditioning where the set of variables is the Herbrand base of the Prolog program. An approached method for semantic conditioning on linear programs is proposed in [14].\n\nSemantic conditioning is \ud835\udd17s\u2062c:=(\ud835\udd0fs\u2062c,\u2111s\u2062c)assignsubscript\ud835\udd17normal-snormal-csubscript\ud835\udd0fnormal-snormal-csubscript\u2111normal-snormal-c\\mathfrak{T}_{sc}:=(\\mathfrak{L}_{sc},\\mathfrak{I}_{sc})fraktur_T start_POSTSUBSCRIPT italic_s italic_c end_POSTSUBSCRIPT := ( fraktur_L start_POSTSUBSCRIPT italic_s italic_c end_POSTSUBSCRIPT , fraktur_I start_POSTSUBSCRIPT italic_s italic_c end_POSTSUBSCRIPT ) such that for any abstract logic \u2112:=(\ud835\udcaf,\ud835\udcc8)assign\u2112\ud835\udcaf\ud835\udcc8\\mathcal{L}:=(\\mathcal{T},\\mathscr{s})caligraphic_L := ( caligraphic_T , script_s ), finite set of variables \ud835\udc18\ud835\udc18\\mathbf{Y}bold_Y and theory T\u2208\ud835\udcaf\u2062(\ud835\udc18)normal-T\ud835\udcaf\ud835\udc18T\\in\\mathcal{T}(\\mathbf{Y})italic_T \u2208 caligraphic_T ( bold_Y ):\nAn other approach and one of the most popular in the literature is to use a multi-objective scheme to train the neural network on both labeled instances and semantic constraints: a regularization term measuring the consistency of the output of the neural network with the prior knowledge is added to the standard negative log-likelihood of the labels to steer the model towards producing scores that match the background knowledge. First introduced using fuzzy logics [18, 19, 20], a regularization technique based on probabilistic reasoning was introduced for propositional theories in [11].Semantic regularization (with coefficient \u03bb>0normal-\u03bb0\\lambda>0italic_\u03bb > 0) on an abstract logic \u2112:=(\ud835\udcaf,\ud835\udcc8)assign\u2112\ud835\udcaf\ud835\udcc8\\mathcal{L}:=(\\mathcal{T},\\mathscr{s})caligraphic_L := ( caligraphic_T , script_s ) is \ud835\udd17r\u03bb:=(\ud835\udd0fr\u03bb,\u2111r\u03bb)assignsuperscriptsubscript\ud835\udd17normal-rnormal-\u03bbsuperscriptsubscript\ud835\udd0fnormal-rnormal-\u03bbsuperscriptsubscript\u2111normal-rnormal-\u03bb\\mathfrak{T}_{r}^{\\lambda}:=(\\mathfrak{L}_{r}^{\\lambda},\\mathfrak{I}_{r}^{%\n\\lambda})fraktur_T start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_\u03bb end_POSTSUPERSCRIPT := ( fraktur_L start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_\u03bb end_POSTSUPERSCRIPT , fraktur_I start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_\u03bb end_POSTSUPERSCRIPT ) such that for any finite set of variables \ud835\udc18\ud835\udc18\\mathbf{Y}bold_Y:Finally, semantic conditioning at inference is derived from semantic conditioning, but only applies conditioning in the inference module (i.e. infers the most probable state that satisfies prior knowledge) while retaining the standard negative log-likelihood loss. It was introduced for propositional prior knowledge in [15].Semantic conditioning at inference is \ud835\udd17s\u2062c\u2062i:=(\ud835\udd0fs\u2062c\u2062i,\u2111s\u2062c\u2062i)assignsubscript\ud835\udd17normal-snormal-cnormal-isubscript\ud835\udd0fnormal-snormal-cnormal-isubscript\u2111normal-snormal-cnormal-i\\mathfrak{T}_{sci}:=(\\mathfrak{L}_{sci},\\mathfrak{I}_{sci})fraktur_T start_POSTSUBSCRIPT italic_s italic_c italic_i end_POSTSUBSCRIPT := ( fraktur_L start_POSTSUBSCRIPT italic_s italic_c italic_i end_POSTSUBSCRIPT , fraktur_I start_POSTSUBSCRIPT italic_s italic_c italic_i end_POSTSUBSCRIPT ) such that for any abstract logic \u2112:=(\ud835\udcaf,\ud835\udcc8)assign\u2112\ud835\udcaf\ud835\udcc8\\mathcal{L}:=(\\mathcal{T},\\mathscr{s})caligraphic_L := ( caligraphic_T , script_s ), finite set of variables \ud835\udc18\ud835\udc18\\mathbf{Y}bold_Y and theory T\u2208\ud835\udcaf\u2062(\ud835\udc18)normal-T\ud835\udcaf\ud835\udc18T\\in\\mathcal{T}(\\mathbf{Y})italic_T \u2208 caligraphic_T ( bold_Y ):As mentioned in Section 2.2, all three abstract neurosymbolic techniques defined in Section 3.3 rely on solving MPE and PQE problems. Unfortunately, in the general cases of propositional logic, answer set programming and linear programming, MPE and PQE are NP-hard and #P-hard respectively. This implies that scaling these probabilistic neurosymbolic techniques to large classification tasks (i.e. tasks with a large number of variables) on arbitrary prior knowledge requires an exponential amount of computing resources (under the assumption that P\u2260N\u2062P\ud835\udc43\ud835\udc41\ud835\udc43P\\neq NPitalic_P \u2260 italic_N italic_P) and is therefore not realistic. However, there are fragments of these universal logics for which probabilistic reasoning problems can be solved efficiently. Hence, it is crucial to understand more finely the computational complexity of probabilistic reasoning to assess the scalability and limitations of such techniques.We first mention in Section 4.1 two popular families of algorithms for solving PQE and MPE problems, based on graphical models and knowledge compilation respectively. Then, we analyze in Section 4.2 the complexity of several classes of MPE and PQE problems that are frequently encountered in the neurosymbolic literature.Graphical models allow to specify a family of distributions over a finite set of variables by means of a graph. The graph encodes a set of properties (e.g.\u00a0factorization, independence, etc.) shared by all distributions in the family. These properties can be exploited to produce compressed representations and efficient inference algorithms. In the context of probabilistic reasoning, the primal graph of a theory T\ud835\udc47Titalic_T specifies to which graphical model the family of exponential distributions conditioned on T\ud835\udc47Titalic_T belong. In particular, traditional algorithms for graphical models can be leveraged to solve PQE and MPE problems in time \ud835\udcaa\u2062(k\u20622\u03c4\u2062(T))\ud835\udcaa\ud835\udc58superscript2\ud835\udf0f\ud835\udc47\\mathcal{O}(k2^{\\tau(T)})caligraphic_O ( italic_k 2 start_POSTSUPERSCRIPT italic_\u03c4 ( italic_T ) end_POSTSUPERSCRIPT ) where k\ud835\udc58kitalic_k is the number of variables and \u03c4\u2062(T)\ud835\udf0f\ud835\udc47\\tau(T)italic_\u03c4 ( italic_T ) the treewidth of the primal graph of T\ud835\udc47Titalic_T. Such algorithms were used to implement semantic conditioning in [10].\nKnowledge compilation is the process of translating theories from a given representation logic (e.g.\u00a0conjunctive normal form) into a target logic that supports some operations efficiently. Several fragments of boolean circuits [21] were identified as suitable target logics. Decomposable Negational Normal Form (DNNF) circuits can solve MPE problems in linear time (in the size of the circuit) and deterministic-DNNF can solve PQE problems in linear time. Sentential Decision Diagrams (SDD) [22] is a PQE and MPE-tractable fragment of boolean circuits [21] that offers linear and polynomial negation, conjunction and disjunction. Besides, [22] shows that a propositional formula \u03ba\ud835\udf05\\kappaitalic_\u03ba in conjunctive normal form with k\ud835\udc58kitalic_k variables and a tree-width \u03c4\u2062(\u03ba)\ud835\udf0f\ud835\udf05\\tau(\\kappa)italic_\u03c4 ( italic_\u03ba ) has an equivalent compressed and trimmed SDD of size \ud835\udcaa\u2062(k\u20622\u03c4\u2062(\u03ba))\ud835\udcaa\ud835\udc58superscript2\ud835\udf0f\ud835\udf05\\mathcal{O}(k2^{\\tau(\\kappa)})caligraphic_O ( italic_k 2 start_POSTSUPERSCRIPT italic_\u03c4 ( italic_\u03ba ) end_POSTSUPERSCRIPT ). Due to these properties, SDD has become a standard target logic for probabilistic neurosymbolic systems [11, 12]. In this paper, we use the graphical representation in [22] to represent SDD nodes. For instance, the node represented in Figure 2 is equivalent to (\u03b11\u2227\u03b21)\u2228(\u03b12\u2227\u03b22)subscript\ud835\udefc1subscript\ud835\udefd1subscript\ud835\udefc2subscript\ud835\udefd2(\\alpha_{1}\\land\\beta_{1})\\lor(\\alpha_{2}\\land\\beta_{2})( italic_\u03b1 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2227 italic_\u03b2 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) \u2228 ( italic_\u03b1 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2227 italic_\u03b2 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) where \u03b11,\u03b21,\u03b12subscript\ud835\udefc1subscript\ud835\udefd1subscript\ud835\udefc2\\alpha_{1},\\beta_{1},\\alpha_{2}italic_\u03b1 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_\u03b2 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_\u03b1 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT are terminal nodes (i.e. literals, \u22a4top\\top\u22a4 or \u22a5bottom\\bot\u22a5) and \u03b22subscript\ud835\udefd2\\beta_{2}italic_\u03b2 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT is another SDD node (structured according to the same linear vtree).An abstract logic \u2112:=(\ud835\udcaf,\ud835\udcc8)assign\u2112\ud835\udcaf\ud835\udcc8\\mathcal{L}:=(\\mathcal{T},\\mathscr{s})caligraphic_L := ( caligraphic_T , script_s ) is concise if the size of its theories on finite sets of variables is polynomial in the number of variables. Edge-based (resp. vertice-based) logics (see Section 2.1) are concise by design: since \u03c2\ud835\udf0d\\varsigmaitalic_\u03c2 has to be injective, the number of edges (resp. vertices) in the graph cannot be larger than the number of variables. A logic is MPE-tractable (resp. PQE-tractable) if MPE problems (resp. PQE problems) for a theory T\u2208\ud835\udcaf\ud835\udc47\ud835\udcafT\\in\\mathcal{T}italic_T \u2208 caligraphic_T can be solved in time polynomial in the size of the theory. A logic is tractable if it is concise, MPE and PQE-tractable. Moreover, counting problems are known to be much harder in general than optimization problems [23]. Therefore, it is natural to look for logics which are concise, MPE-tractable and for which PQE is still #P-hard. We call them semi-tractable logics. Besides the theoretical interest, these logics have great relevance in the context of probabilistic neurosymbolic techniques, as some remain scalable on semi-tractable logics (e.g.\u00a0semantic conditioning at inference) while others do not (e.g.\u00a0semantic conditioning and semantic regularization).As mentioned earlier, a typical criteria to identify a tractable fragment is to show that it is bounded in tree-width. However, most prior knowledge commonly found in the neurosymbolic literature do not meet this criteria. Therefore, in this section, we analyze the complexity of MPE and PQE problems for several logics of unbounded tree-width. A summary of the results can be found in Table 1. We tackle hierarchical classification [10, 12] in Section 4.2.1. Fixed cardinal constraints (or k-subset constraints) are mentioned in [14] as an example of computationally hard constraints for PQE. However, we give tractability results for such constraints in Section 4.2.2. Simple path constraints are growing increasingly popular in the neurosymbolic literature [11, 24, 14, 12, 25]. We show in Section 4.2.3 that simple path constraints are intractable in general, but that restricting to acyclic graphs render them tractable, meaning that probabilistic techniques can scale on acyclic graphs. It is worth noting that most techniques in the field are illustrated on grid graphs, which contain cycles. Informed classification tasks with matching constraints can be found in [24, 25]. We show that matching theories are semi-tractable in Section 4.2.4.Hierarchical constraints (e.g.\u00a0a dog is an animal) are ubiquitous in artificial intelligence because we are used to organize concepts in taxonomies, so much so that hierarchical classification (i.e. a classification task where the set of output classes are organized in a hierarchy) is a field of research on its own.Hierarchical logic \u2112H:=(\ud835\udcaf\u2219,\ud835\udcc8H)assignsubscript\u2112\ud835\udc3bsubscript\ud835\udcaf\u2219subscript\ud835\udcc8\ud835\udc3b\\mathcal{L}_{H}:=(\\mathcal{T}_{\\bullet},\\mathscr{s}_{H})caligraphic_L start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT := ( caligraphic_T start_POSTSUBSCRIPT \u2219 end_POSTSUBSCRIPT , script_s start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ) is a vertex-based logic where a theory (G:=(V,E),\u03c2)\u2208\ud835\udcafH\u2062(\ud835\udc18)assign\ud835\udc3a\ud835\udc49\ud835\udc38\ud835\udf0dsubscript\ud835\udcaf\ud835\udc3b\ud835\udc18(G:=(V,E),\\varsigma)\\in\\mathcal{T}_{H}(\\mathbf{Y})( italic_G := ( italic_V , italic_E ) , italic_\u03c2 ) \u2208 caligraphic_T start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ( bold_Y ) accepts a state \ud835\udc32\u2208\ud835\udd39\ud835\udc18\ud835\udc32superscript\ud835\udd39\ud835\udc18\\mathbf{y}\\in\\mathbb{B}^{\\mathbf{Y}}bold_y \u2208 blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT (i.e. \ud835\udc32\u2208\ud835\udcc8H\u2062((G,\u03c2))\ud835\udc32subscript\ud835\udcc8\ud835\udc3b\ud835\udc3a\ud835\udf0d\\mathbf{y}\\in\\mathscr{s}_{H}((G,\\varsigma))bold_y \u2208 script_s start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ( ( italic_G , italic_\u03c2 ) )) if the vertices selected in V\ud835\udc49Vitalic_V respect the hierarchical constraints expressed in G\ud835\udc3aGitalic_G: if a vertex v\u2208V\ud835\udc63\ud835\udc49v\\in Vitalic_v \u2208 italic_V belong to an accepting state \ud835\udc32\ud835\udc32\\mathbf{y}bold_y (i.e. \u03c2\u2062(v)\u2208\ud835\udc32\ud835\udf0d\ud835\udc63\ud835\udc32\\varsigma(v)\\in\\mathbf{y}italic_\u03c2 ( italic_v ) \u2208 bold_y), then all its parents in G\ud835\udc3aGitalic_G also belong to \ud835\udc32\ud835\udc32\\mathbf{y}bold_y.A hierarchical theory (G:=(V,E),\u03c2)assign\ud835\udc3a\ud835\udc49\ud835\udc38\ud835\udf0d(G:=(V,E),\\varsigma)( italic_G := ( italic_V , italic_E ) , italic_\u03c2 ) of \u2112Hsubscript\u2112\ud835\udc3b\\mathcal{L}_{H}caligraphic_L start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT can be compiled in polynomial time in a 2-cnf formula:The fragment of hierarchical logic composed of theories (G:=(V,E),\u03c2)assign\ud835\udc3a\ud835\udc49\ud835\udc38\ud835\udf0d(G:=(V,E),\\varsigma)( italic_G := ( italic_V , italic_E ) , italic_\u03c2 ) where (V,E)\ud835\udc49\ud835\udc38(V,E)( italic_V , italic_E ) is a tree is tractable.\u03baHsubscript\ud835\udf05\ud835\udc3b\\kappa_{H}italic_\u03ba start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT defined in Equation 10 is a 2-cnf of treewidth 1. Therefore (H:=(V,Eh,Ee),\u03c2)assign\ud835\udc3b\ud835\udc49subscript\ud835\udc38\u210esubscript\ud835\udc38\ud835\udc52\ud835\udf0d(H:=(V,E_{h},E_{e}),\\varsigma)( italic_H := ( italic_V , italic_E start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ) , italic_\u03c2 ) can be compiled into an SDD of size polynomial in k\ud835\udc58kitalic_k.\n\u220eThe semantics of hierarchical logic can be changed to \ud835\udcc8H\u2062esubscript\ud835\udcc8\ud835\udc3b\ud835\udc52\\mathscr{s}_{He}script_s start_POSTSUBSCRIPT italic_H italic_e end_POSTSUBSCRIPT so that variables that have no common descendants in G\ud835\udc3aGitalic_G are considered mutually exclusive: if v\u2208V\ud835\udc63\ud835\udc49v\\in Vitalic_v \u2208 italic_V belong to an accepting state \ud835\udc32\ud835\udc32\\mathbf{y}bold_y (i.e. \ud835\udc32\u2208\ud835\udcc8H\u2062e\u2062((G,\u03c2))\ud835\udc32subscript\ud835\udcc8\ud835\udc3b\ud835\udc52\ud835\udc3a\ud835\udf0d\\mathbf{y}\\in\\mathscr{s}_{He}((G,\\varsigma))bold_y \u2208 script_s start_POSTSUBSCRIPT italic_H italic_e end_POSTSUBSCRIPT ( ( italic_G , italic_\u03c2 ) )) and u\u2208V\ud835\udc62\ud835\udc49u\\in Vitalic_u \u2208 italic_V is such that there is no directed path from u\ud835\udc62uitalic_u to v\ud835\udc63vitalic_v or from v\ud835\udc63vitalic_v to u\ud835\udc62uitalic_u, then u\ud835\udc62uitalic_u does not belong to \ud835\udc32\ud835\udc32\\mathbf{y}bold_y (i.e. \u03c2\u2062(u)\u2209\ud835\udc32\ud835\udf0d\ud835\udc62\ud835\udc32\\varsigma(u)\\not\\in\\mathbf{y}italic_\u03c2 ( italic_u ) \u2209 bold_y).Hierarchical logic with mutual exclusion assumptions \u2112H\u2062e:=(\ud835\udcafH,\ud835\udcc8H\u2062e)assignsubscript\u2112\ud835\udc3b\ud835\udc52subscript\ud835\udcaf\ud835\udc3bsubscript\ud835\udcc8\ud835\udc3b\ud835\udc52\\mathcal{L}_{He}:=(\\mathcal{T}_{H},\\mathscr{s}_{He})caligraphic_L start_POSTSUBSCRIPT italic_H italic_e end_POSTSUBSCRIPT := ( caligraphic_T start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT , script_s start_POSTSUBSCRIPT italic_H italic_e end_POSTSUBSCRIPT ) is tractable.Theories (G:=(V,E),\u03c2)assign\ud835\udc3a\ud835\udc49\ud835\udc38\ud835\udf0d(G:=(V,E),\\varsigma)( italic_G := ( italic_V , italic_E ) , italic_\u03c2 ) in \u2112H\u2062esubscript\u2112\ud835\udc3b\ud835\udc52\\mathcal{L}_{He}caligraphic_L start_POSTSUBSCRIPT italic_H italic_e end_POSTSUBSCRIPT are enumerable. For each vertex v\u2208V\ud835\udc63\ud835\udc49v\\in Vitalic_v \u2208 italic_V, the state that contains v\ud835\udc63vitalic_v and its ancestors is accepted by (G,\u03c2)\ud835\udc3a\ud835\udf0d(G,\\varsigma)( italic_G , italic_\u03c2 ). The null state is the only other state accepted by (G,\u03c2)\ud835\udc3a\ud835\udf0d(G,\\varsigma)( italic_G , italic_\u03c2 ). Therefore, accepted states can be enumerated in polynomial time in the size of the theory. This means that PQE can be solved by summing probabilities of satisfying states. Likewise, MPE can be solved by iterating through satisfying states to find the most probable one. Hence, hierarchical logic with mutual exclusion assumptions is tractable.\n\u220eFixed cardinal constraints can be used to represent a classification task which consists in picking a fixed amount of variables in a given list (e.g.\u00a0suggesting a given number of related products to a client).A fixed cardinal theory consists of a linear constraint \u27e8\ud835\udc181:k,\ud835\udfcf\u27e9=lsubscript\ud835\udc18:1\ud835\udc581\ud835\udc59\\langle\\mathbf{Y}_{1:k},\\mathbf{1}\\rangle=l\u27e8 bold_Y start_POSTSUBSCRIPT 1 : italic_k end_POSTSUBSCRIPT , bold_1 \u27e9 = italic_l which accepts states that contain exactly l\ud835\udc59litalic_l variables amongst a set of size k\ud835\udc58kitalic_k (ie. states \ud835\udc32\u2208\ud835\udd39\ud835\udc181:k\ud835\udc32superscript\ud835\udd39subscript\ud835\udc18:1\ud835\udc58\\mathbf{y}\\in\\mathbb{B}^{\\mathbf{Y}_{1:k}}bold_y \u2208 blackboard_B start_POSTSUPERSCRIPT bold_Y start_POSTSUBSCRIPT 1 : italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT such that |\ud835\udc32|=l\ud835\udc32\ud835\udc59|\\mathbf{y}|=l| bold_y | = italic_l).Figures 3 gives a recursive template (using the graphical representation in [22]) to compile in polynomial time (create all decision nodes and connect them appropriately) a fixed cardinal theory \u27e8\ud835\udc181:k,\ud835\udfcf\u27e9=lsubscript\ud835\udc18:1\ud835\udc581\ud835\udc59\\langle\\mathbf{Y}_{1:k},\\mathbf{1}\\rangle=l\u27e8 bold_Y start_POSTSUBSCRIPT 1 : italic_k end_POSTSUBSCRIPT , bold_1 \u27e9 = italic_l into a concise SDD.Circuit Clksuperscriptsubscript\ud835\udc36\ud835\udc59\ud835\udc58C_{l}^{k}italic_C start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT has size polynomial in k\ud835\udc58kitalic_k.Circuit Clksuperscriptsubscript\ud835\udc36\ud835\udc59\ud835\udc58C_{l}^{k}italic_C start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT contains nodes (Cij)1\u2264j\u2264k,0\u2264i\u2264jsubscriptsuperscriptsubscript\ud835\udc36\ud835\udc56\ud835\udc57formulae-sequence1\ud835\udc57\ud835\udc580\ud835\udc56\ud835\udc57(C_{i}^{j})_{1\\leq j\\leq k,0\\leq i\\leq j}( italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ) start_POSTSUBSCRIPT 1 \u2264 italic_j \u2264 italic_k , 0 \u2264 italic_i \u2264 italic_j end_POSTSUBSCRIPT with 6666 wires each, meaning that the size of the circuit is in \ud835\udcaa\u2062(k2)\ud835\udcaasuperscript\ud835\udc582\\mathcal{O}(k^{2})caligraphic_O ( italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ).\n\u220eFor any number of variables k\u22651\ud835\udc581k\\geq 1italic_k \u2265 1 and 0\u2264l\u2264k0\ud835\udc59\ud835\udc580\\leq l\\leq k0 \u2264 italic_l \u2264 italic_k the circuit rooted in Clksuperscriptsubscript\ud835\udc36\ud835\udc59\ud835\udc58C_{l}^{k}italic_C start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT is a SDD (and even an OBDD) structured according to the right linear vtree on \ud835\udc181:ksubscript\ud835\udc18normal-:1\ud835\udc58\\mathbf{Y}_{1:k}bold_Y start_POSTSUBSCRIPT 1 : italic_k end_POSTSUBSCRIPT.Proof by recurrence on k\ud835\udc58kitalic_k starting at k=1\ud835\udc581k=1italic_k = 1:Initialization for k=1\ud835\udc581k=1italic_k = 1: C11superscriptsubscript\ud835\udc3611C_{1}^{1}italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT and C01superscriptsubscript\ud835\udc3601C_{0}^{1}italic_C start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT only contains variable Y1subscript\ud835\udc4c1Y_{1}italic_Y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and thus is structured according to the right linear vtree on Y1subscript\ud835\udc4c1Y_{1}italic_Y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT.Heredity from k\ud835\udc58kitalic_k to k+1\ud835\udc581k+1italic_k + 1: for all 0\u2264l\u2208k+10\ud835\udc59\ud835\udc5810\\leq l\\in k+10 \u2264 italic_l \u2208 italic_k + 1, Clk+1superscriptsubscript\ud835\udc36\ud835\udc59\ud835\udc581C_{l}^{k+1}italic_C start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k + 1 end_POSTSUPERSCRIPT has primes {Yk+1,\u00ac\u2062Yk+1}subscript\ud835\udc4c\ud835\udc581subscript\ud835\udc4c\ud835\udc581\\{Y_{k+1},\\neg Y_{k+1}\\}{ italic_Y start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT , \u00ac italic_Y start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT } and subs {Cl\u22121k,Clk}superscriptsubscript\ud835\udc36\ud835\udc591\ud835\udc58superscriptsubscript\ud835\udc36\ud835\udc59\ud835\udc58\\{C_{l-1}^{k},C_{l}^{k}\\}{ italic_C start_POSTSUBSCRIPT italic_l - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT , italic_C start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT } if l>0\ud835\udc590l>0italic_l > 0 and {C0k,\u22a5}superscriptsubscript\ud835\udc360\ud835\udc58bottom\\{C_{0}^{k},\\bot\\}{ italic_C start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT , \u22a5 } if l=0\ud835\udc590l=0italic_l = 0 which are all structured according to the right linear vtree on \ud835\udc181:ksubscript\ud835\udc18:1\ud835\udc58\\mathbf{Y}_{1:k}bold_Y start_POSTSUBSCRIPT 1 : italic_k end_POSTSUBSCRIPT, hence Clk+1superscriptsubscript\ud835\udc36\ud835\udc59\ud835\udc581C_{l}^{k+1}italic_C start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k + 1 end_POSTSUPERSCRIPT is structured according to the right linear vtree on \ud835\udc18i:k+1subscript\ud835\udc18:\ud835\udc56\ud835\udc581\\mathbf{Y}_{i:k+1}bold_Y start_POSTSUBSCRIPT italic_i : italic_k + 1 end_POSTSUBSCRIPT.\u220eThe circuit Clksuperscriptsubscript\ud835\udc36\ud835\udc59\ud835\udc58C_{l}^{k}italic_C start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT with k\u22651\ud835\udc581k\\geq 1italic_k \u2265 1 and 0\u2264l\u2264k0\ud835\udc59\ud835\udc580\\leq l\\leq k0 \u2264 italic_l \u2264 italic_k accepts a state \ud835\udc32\u2208\ud835\udd39\ud835\udc18i:k\ud835\udc32superscript\ud835\udd39subscript\ud835\udc18normal-:\ud835\udc56\ud835\udc58\\mathbf{y}\\in\\mathbb{B}^{\\mathbf{Y}_{i:k}}bold_y \u2208 blackboard_B start_POSTSUPERSCRIPT bold_Y start_POSTSUBSCRIPT italic_i : italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT iff it contains exactly l\ud835\udc59litalic_l variables (ie. |\ud835\udc32|=l\ud835\udc32\ud835\udc59|\\mathbf{y}|=l| bold_y | = italic_l).Proof by recurrence on k\ud835\udc58kitalic_k starting at k=1\ud835\udc581k=1italic_k = 1:Initialization for k=1\ud835\udc581k=1italic_k = 1: C11superscriptsubscript\ud835\udc3611C_{1}^{1}italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT only accepts y1=1subscript\ud835\udc6611y_{1}=1italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 1 and C01superscriptsubscript\ud835\udc3601C_{0}^{1}italic_C start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT only accepts y1=0subscript\ud835\udc6610y_{1}=0italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.Heredity from k\ud835\udc58kitalic_k to k+1\ud835\udc581k+1italic_k + 1, for 0\u2264l\u2264k0\ud835\udc59\ud835\udc580\\leq l\\leq k0 \u2264 italic_l \u2264 italic_k and \ud835\udc32\u2208\ud835\udd39\ud835\udc18i:k+1\ud835\udc32superscript\ud835\udd39subscript\ud835\udc18:\ud835\udc56\ud835\udc581\\mathbf{y}\\in\\mathbb{B}^{\\mathbf{Y}_{i:k+1}}bold_y \u2208 blackboard_B start_POSTSUPERSCRIPT bold_Y start_POSTSUBSCRIPT italic_i : italic_k + 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT:if l=0\ud835\udc590l=0italic_l = 0: C0k+1\u2062(\ud835\udc32)=1superscriptsubscript\ud835\udc360\ud835\udc581\ud835\udc321C_{0}^{k+1}(\\mathbf{y})=1italic_C start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k + 1 end_POSTSUPERSCRIPT ( bold_y ) = 1 iff yk+1=0subscript\ud835\udc66\ud835\udc5810y_{k+1}=0italic_y start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = 0 and C0k\u2062(\ud835\udc32i:k)=1superscriptsubscript\ud835\udc360\ud835\udc58subscript\ud835\udc32:\ud835\udc56\ud835\udc581C_{0}^{k}(\\mathbf{y}_{i:k})=1italic_C start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( bold_y start_POSTSUBSCRIPT italic_i : italic_k end_POSTSUBSCRIPT ) = 1, which means C0k+1superscriptsubscript\ud835\udc360\ud835\udc581C_{0}^{k+1}italic_C start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k + 1 end_POSTSUPERSCRIPT accepts \ud835\udc32\ud835\udc32\\mathbf{y}bold_y iff:if l>0\ud835\udc590l>0italic_l > 0: Clk+1\u2062(\ud835\udc32)=1superscriptsubscript\ud835\udc36\ud835\udc59\ud835\udc581\ud835\udc321C_{l}^{k+1}(\\mathbf{y})=1italic_C start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k + 1 end_POSTSUPERSCRIPT ( bold_y ) = 1 in only two cases:if yk+1=0subscript\ud835\udc66\ud835\udc5810y_{k+1}=0italic_y start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = 0 and Clk\u2062(\ud835\udc32i:k)=1superscriptsubscript\ud835\udc36\ud835\udc59\ud835\udc58subscript\ud835\udc32:\ud835\udc56\ud835\udc581C_{l}^{k}(\\mathbf{y}_{i:k})=1italic_C start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( bold_y start_POSTSUBSCRIPT italic_i : italic_k end_POSTSUBSCRIPT ) = 1, which means we have:if yk+1=1subscript\ud835\udc66\ud835\udc5811y_{k+1}=1italic_y start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = 1 and Cl\u22121k\u2062(\ud835\udc32i:k)=1superscriptsubscript\ud835\udc36\ud835\udc591\ud835\udc58subscript\ud835\udc32:\ud835\udc56\ud835\udc581C_{l-1}^{k}(\\mathbf{y}_{i:k})=1italic_C start_POSTSUBSCRIPT italic_l - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( bold_y start_POSTSUBSCRIPT italic_i : italic_k end_POSTSUBSCRIPT ) = 1, which means we have:\u220eThe task of Warcraft Shortest Path [24] is to learn how to predict the shortest path on a map from a picture of the map. Such a task can be viewed as an informed classification task where the set of output variables correspond to edges in a graph and prior knowledge informs us that only states representing simple paths in the graph are satisfying answers.Simple path logic \u2112\u2219\u2063\u2192\u2219:=(\ud835\udcaf\u2192,\ud835\udcc8\u2219\u2063\u2192\u2219)assignsubscript\u2112\u2219\u2192absent\u2219subscript\ud835\udcaf\u2192subscript\ud835\udcc8\u2219\u2192absent\u2219\\mathcal{L}_{\\bullet\\to\\bullet}:=(\\mathcal{T}_{\\to},\\mathscr{s}_{\\bullet\\to%\n\\bullet})caligraphic_L start_POSTSUBSCRIPT \u2219 \u2192 \u2219 end_POSTSUBSCRIPT := ( caligraphic_T start_POSTSUBSCRIPT \u2192 end_POSTSUBSCRIPT , script_s start_POSTSUBSCRIPT \u2219 \u2192 \u2219 end_POSTSUBSCRIPT ) is an edge-based logic where a theory (G:=(V,E),\u03c2)assign\ud835\udc3a\ud835\udc49\ud835\udc38\ud835\udf0d(G:=(V,E),\\varsigma)( italic_G := ( italic_V , italic_E ) , italic_\u03c2 ) accepts a state \ud835\udc32\u2208\ud835\udd39\ud835\udc18\ud835\udc32superscript\ud835\udd39\ud835\udc18\\mathbf{y}\\in\\mathbb{B}^{\\mathbf{Y}}bold_y \u2208 blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT if the edges selected in E\ud835\udc38Eitalic_E (ie. {e\u2208E|\u03c2\u2062(e)\u2208\ud835\udc32}conditional-set\ud835\udc52\ud835\udc38\ud835\udf0d\ud835\udc52\ud835\udc32\\{e\\in E|\\varsigma(e)\\in\\mathbf{y}\\}{ italic_e \u2208 italic_E | italic_\u03c2 ( italic_e ) \u2208 bold_y }) form a s-t simple path in G\ud835\udc3aGitalic_G, i.e. if the selected edges forms an acyclic path from a source of the graph to a target (or sink) of the graph.\n\nSimple path logic is MPE and PQE-intractable.Solving MPE for a simple path theory (G,\u03c2)\ud835\udc3a\ud835\udf0d(G,\\varsigma)( italic_G , italic_\u03c2 ) is equivalent to finding a shortest path in G\ud835\udc3aGitalic_G with real weights on the edges, which is known to be NP-hard by a polynomial reduction from the Hamiltonian path problem [26]. Likewise, counting the number of simple paths of a graph G\ud835\udc3aGitalic_G is known to be #P-hard [27] and has a trivial polynomial reduction to solving PQE for a simple path theory (G,\u03c2)\ud835\udc3a\ud835\udf0d(G,\\varsigma)( italic_G , italic_\u03c2 ).\n\u220eA simple path theory (D,\u03c2)\ud835\udc37\ud835\udf0d(D,\\varsigma)( italic_D , italic_\u03c2 ) is said acyclic if D\ud835\udc37Ditalic_D is a Directed Acyclic Graph (DAG). We say that (D,\u03c2)\ud835\udc37\ud835\udf0d(D,\\varsigma)( italic_D , italic_\u03c2 ) is topological if \u03c2\ud835\udf0d\\varsigmaitalic_\u03c2 is a topological ordering of the edges: for any path, the order of edges in the path is a sub-order of the topological order. For a vertex u\u2208V\ud835\udc62\ud835\udc49u\\in Vitalic_u \u2208 italic_V, we will note \u03c2m\u2062(u)subscript\ud835\udf0d\ud835\udc5a\ud835\udc62\\varsigma_{m}(u)italic_\u03c2 start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ( italic_u ) the index of the first incident edge to u\ud835\udc62uitalic_u and \u03c2M\u2062(u)subscript\ud835\udf0d\ud835\udc40\ud835\udc62\\varsigma_{M}(u)italic_\u03c2 start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ( italic_u ) the index of the last outgoing edge of u\ud835\udc62uitalic_u. We also note Djsuperscript\ud835\udc37\ud835\udc57D^{j}italic_D start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT the graph that contains edges (ei)1\u2264i\u2264jsubscriptsubscript\ud835\udc52\ud835\udc561\ud835\udc56\ud835\udc57(e_{i})_{1\\leq i\\leq j}( italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT 1 \u2264 italic_i \u2264 italic_j end_POSTSUBSCRIPT and all vertices that are endpoints of those edges. Finally, we can assume without loss of generality that D\ud835\udc37Ditalic_D only contains a single source and target. If not: chose one source vertex s\ud835\udc60sitalic_s, delete all the others and reconnect their outgoing edges to s\ud835\udc60sitalic_s, and do the same with a target vertex t\ud835\udc61titalic_t. This operation can be done in polynomial type and does not change the semantic of the theory (i.e. accepted states are left unchanged).Figures 4 and 5 represent recursive templates that allow us to compile in polynomial time (create all decision nodes and connect them appropriately) an acyclic and topological simple path theory (D:=(V,E),\u03c2)assign\ud835\udc37\ud835\udc49\ud835\udc38\ud835\udf0d(D:=(V,E),\\varsigma)( italic_D := ( italic_V , italic_E ) , italic_\u03c2 ) into a concise SDD rooted in node Ctksuperscriptsubscript\ud835\udc36\ud835\udc61\ud835\udc58C_{t}^{k}italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT.\n\nCircuit Ctksuperscriptsubscript\ud835\udc36\ud835\udc61\ud835\udc58C_{t}^{k}italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT has size polynomial in the number of edges in k\ud835\udc58kitalic_k.Circuit Ctksuperscriptsubscript\ud835\udc36\ud835\udc61\ud835\udc58C_{t}^{k}italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT has at most |V|\u00d7k=\ud835\udcaa\u2062(k3)\ud835\udc49\ud835\udc58\ud835\udcaasuperscript\ud835\udc583|V|\\times k=\\mathcal{O}(k^{3})| italic_V | \u00d7 italic_k = caligraphic_O ( italic_k start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ) decision nodes with 6666 wires each, meaning that the size of the circuit is in \ud835\udcaa\u2062(k3)\ud835\udcaasuperscript\ud835\udc583\\mathcal{O}(k^{3})caligraphic_O ( italic_k start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ).\n\u220eNotice that every node Cvisuperscriptsubscript\ud835\udc36\ud835\udc63\ud835\udc56C_{v}^{i}italic_C start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT such that such that i>\u03c3M\u2062(v)\ud835\udc56subscript\ud835\udf0e\ud835\udc40\ud835\udc63i>\\sigma_{M}(v)italic_i > italic_\u03c3 start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ( italic_v ) will not appear in the circuit rooted in Ctksuperscriptsubscript\ud835\udc36\ud835\udc61\ud835\udc58C_{t}^{k}italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT. Similarly, every node Cvisuperscriptsubscript\ud835\udc36\ud835\udc63\ud835\udc56C_{v}^{i}italic_C start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT such that i<\u03c3m\u2062(v)\ud835\udc56subscript\ud835\udf0e\ud835\udc5a\ud835\udc63i<\\sigma_{m}(v)italic_i < italic_\u03c3 start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ( italic_v ) is equivalent to \u22a5bottom\\bot\u22a5 and can thus be trimmed of the circuit. Therefore, the size of the trimmed circuit is much smaller that 2\u2062k32superscript\ud835\udc5832k^{3}2 italic_k start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT.For each vertex v\ud835\udc63vitalic_v and edge eisubscript\ud835\udc52\ud835\udc56e_{i}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, the circuit rooted in Cvisuperscriptsubscript\ud835\udc36\ud835\udc63\ud835\udc56C_{v}^{i}italic_C start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT is a SDD (and even an OBDD) structured according to the right linear vtree on \ud835\udc181:isubscript\ud835\udc18normal-:1\ud835\udc56\\mathbf{Y}_{1:i}bold_Y start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT.Proof by recurrence on i\ud835\udc56iitalic_i:Initialization for i=1\ud835\udc561i=1italic_i = 1: for all vertices v\ud835\udc63vitalic_v, Cv1superscriptsubscript\ud835\udc36\ud835\udc631C_{v}^{1}italic_C start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT only contains variable Y1subscript\ud835\udc4c1Y_{1}italic_Y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and thus is structured according to the right linear vtree on Y1subscript\ud835\udc4c1Y_{1}italic_Y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT.Heredity from i\ud835\udc56iitalic_i to i+1\ud835\udc561i+1italic_i + 1: for all vertices u\ud835\udc62uitalic_u, Cvisuperscriptsubscript\ud835\udc36\ud835\udc63\ud835\udc56C_{v}^{i}italic_C start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT has primes {Yi,\u00ac\u2062Yi}subscript\ud835\udc4c\ud835\udc56subscript\ud835\udc4c\ud835\udc56\\{Y_{i},\\neg Y_{i}\\}{ italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , \u00ac italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } and subs {Cui\u22121,Cvi\u22121,\u22a5}u\u2208V,(u,v)\u2208Esubscriptsuperscriptsubscript\ud835\udc36\ud835\udc62\ud835\udc561superscriptsubscript\ud835\udc36\ud835\udc63\ud835\udc561bottomformulae-sequence\ud835\udc62\ud835\udc49\ud835\udc62\ud835\udc63\ud835\udc38\\{C_{u}^{i-1},C_{v}^{i-1},\\bot\\}_{u\\in V,(u,v)\\in E}{ italic_C start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i - 1 end_POSTSUPERSCRIPT , italic_C start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i - 1 end_POSTSUPERSCRIPT , \u22a5 } start_POSTSUBSCRIPT italic_u \u2208 italic_V , ( italic_u , italic_v ) \u2208 italic_E end_POSTSUBSCRIPT which are all structured according to the right linear vtree on \ud835\udc181:i+1subscript\ud835\udc18:1\ud835\udc561\\mathbf{Y}_{1:i+1}bold_Y start_POSTSUBSCRIPT 1 : italic_i + 1 end_POSTSUBSCRIPT, hence Cvisuperscriptsubscript\ud835\udc36\ud835\udc63\ud835\udc56C_{v}^{i}italic_C start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT is structured according to the right linear vtree on \ud835\udc181:isubscript\ud835\udc18:1\ud835\udc56\\mathbf{Y}_{1:i}bold_Y start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT.\u220eAssume that edges follow a topological order. If for i>1\ud835\udc561i>1italic_i > 1 \ud835\udc32\u2208\ud835\udd39\ud835\udc181:i\ud835\udc32superscript\ud835\udd39subscript\ud835\udc18normal-:1\ud835\udc56\\mathbf{y}\\in\\mathbb{B}^{\\mathbf{Y}_{1:i}}bold_y \u2208 blackboard_B start_POSTSUPERSCRIPT bold_Y start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT represents a path s\u2192vnormal-\u2192\ud835\udc60\ud835\udc63s\\to vitalic_s \u2192 italic_v in Disuperscript\ud835\udc37\ud835\udc56D^{i}italic_D start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT and yi=1subscript\ud835\udc66\ud835\udc561y_{i}=1italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1, then ei=(u,v)subscript\ud835\udc52\ud835\udc56\ud835\udc62\ud835\udc63e_{i}=(u,v)italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ( italic_u , italic_v ) for some u\u2208Di\u22121\ud835\udc62superscript\ud835\udc37\ud835\udc561u\\in D^{i-1}italic_u \u2208 italic_D start_POSTSUPERSCRIPT italic_i - 1 end_POSTSUPERSCRIPT.We note ei=(u,w)subscript\ud835\udc52\ud835\udc56\ud835\udc62\ud835\udc64e_{i}=(u,w)italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ( italic_u , italic_w ). We first show that u\u2208Di\u22121\ud835\udc62superscript\ud835\udc37\ud835\udc561u\\in D^{i-1}italic_u \u2208 italic_D start_POSTSUPERSCRIPT italic_i - 1 end_POSTSUPERSCRIPT:\nif u=s\ud835\udc62\ud835\udc60u=sitalic_u = italic_s, then u\u2208D1\u2282Di\u22121\ud835\udc62superscript\ud835\udc371superscript\ud835\udc37\ud835\udc561u\\in D^{1}\\subset D^{i-1}italic_u \u2208 italic_D start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT \u2282 italic_D start_POSTSUPERSCRIPT italic_i - 1 end_POSTSUPERSCRIPT.if u\u2260s\ud835\udc62\ud835\udc60u\\neq sitalic_u \u2260 italic_s, since \ud835\udc32\ud835\udc32\\mathbf{y}bold_y represents a path s\u2192v\u2192\ud835\udc60\ud835\udc63s\\to vitalic_s \u2192 italic_v, there is an edge ej=(r,u)subscript\ud835\udc52\ud835\udc57\ud835\udc5f\ud835\udc62e_{j}=(r,u)italic_e start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = ( italic_r , italic_u ) with j<i\ud835\udc57\ud835\udc56j<iitalic_j < italic_i, which means that u\u2208Di\u22121\ud835\udc62superscript\ud835\udc37\ud835\udc561u\\in D^{i-1}italic_u \u2208 italic_D start_POSTSUPERSCRIPT italic_i - 1 end_POSTSUPERSCRIPT.Now let\u2019s show that w=v\ud835\udc64\ud835\udc63w=vitalic_w = italic_v reasoning by the absurd. Let\u2019s assume that w\u2260v\ud835\udc64\ud835\udc63w\\neq vitalic_w \u2260 italic_v, then since \ud835\udc32\ud835\udc32\\mathbf{y}bold_y represents a path s\u2192v\u2192\ud835\udc60\ud835\udc63s\\to vitalic_s \u2192 italic_v, there is an edge el=(q,v)subscript\ud835\udc52\ud835\udc59\ud835\udc5e\ud835\udc63e_{l}=(q,v)italic_e start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT = ( italic_q , italic_v ) with l<i\ud835\udc59\ud835\udc56l<iitalic_l < italic_i. Hence, there is a path from the end point w\ud835\udc64witalic_w of eisubscript\ud835\udc52\ud835\udc56e_{i}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to the start point q\ud835\udc5eqitalic_q of elsubscript\ud835\udc52\ud835\udc59e_{l}italic_e start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT with l<i\ud835\udc59\ud835\udc56l<iitalic_l < italic_i, which is in contradiction with edges following a topological order.Therefore we can conclude that ei=(u,v)subscript\ud835\udc52\ud835\udc56\ud835\udc62\ud835\udc63e_{i}=(u,v)italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ( italic_u , italic_v ) for some u\u2208Di\u22121\ud835\udc62superscript\ud835\udc37\ud835\udc561u\\in D^{i-1}italic_u \u2208 italic_D start_POSTSUPERSCRIPT italic_i - 1 end_POSTSUPERSCRIPT.\n\u220eThe circuit Csisuperscriptsubscript\ud835\udc36\ud835\udc60\ud835\udc56C_{s}^{i}italic_C start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT with 1\u2264i\u2264k1\ud835\udc56\ud835\udc581\\leq i\\leq k1 \u2264 italic_i \u2264 italic_k only accepts the null state \ud835\udfce\u2208\ud835\udd39\ud835\udc181:i0superscript\ud835\udd39subscript\ud835\udc18normal-:1\ud835\udc56\\mathbf{0}\\in\\mathbb{B}^{\\mathbf{Y}_{1:i}}bold_0 \u2208 blackboard_B start_POSTSUPERSCRIPT bold_Y start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT.Proof by recurrence on i\ud835\udc56iitalic_i:Initialization for i=1\ud835\udc561i=1italic_i = 1: Cs1\u2062(y1)=1superscriptsubscript\ud835\udc36\ud835\udc601subscript\ud835\udc6611C_{s}^{1}(y_{1})=1italic_C start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) = 1 iff y1=0subscript\ud835\udc6610y_{1}=0italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0 by definition (see Figure 3(c)).Heredity from i\ud835\udc56iitalic_i to i+1\ud835\udc561i+1italic_i + 1: s\ud835\udc60sitalic_s has no incoming edge in D\ud835\udc37Ditalic_D (because it is a source vertex), hence Csi+1superscriptsubscript\ud835\udc36\ud835\udc60\ud835\udc561C_{s}^{i+1}italic_C start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT accepts \ud835\udc32\u2208\ud835\udd39\ud835\udc181:i+1\ud835\udc32superscript\ud835\udd39subscript\ud835\udc18:1\ud835\udc561\\mathbf{y}\\in\\mathbb{B}^{\\mathbf{Y}_{1:i+1}}bold_y \u2208 blackboard_B start_POSTSUPERSCRIPT bold_Y start_POSTSUBSCRIPT 1 : italic_i + 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT iff yi+1=0subscript\ud835\udc66\ud835\udc5610y_{i+1}=0italic_y start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT = 0 and Csi\u2062(\ud835\udc321:i)=1superscriptsubscript\ud835\udc36\ud835\udc60\ud835\udc56subscript\ud835\udc32:1\ud835\udc561C_{s}^{i}(\\mathbf{y}_{1:i})=1italic_C start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( bold_y start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT ) = 1. Which means by the recurrence hypothesis that \ud835\udc321:i=\ud835\udfcesubscript\ud835\udc32:1\ud835\udc560\\mathbf{y}_{1:i}=\\mathbf{0}bold_y start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT = bold_0 and therefore \ud835\udc32=0\ud835\udc320\\mathbf{y}=0bold_y = 0.\u220eThe circuit Cvisuperscriptsubscript\ud835\udc36\ud835\udc63\ud835\udc56C_{v}^{i}italic_C start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT with 1\u2264i\u2264k1\ud835\udc56\ud835\udc581\\leq i\\leq k1 \u2264 italic_i \u2264 italic_k and v\u2208Di\u2216s\ud835\udc63superscript\ud835\udc37\ud835\udc56\ud835\udc60v\\in D^{i}\\setminus sitalic_v \u2208 italic_D start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT \u2216 italic_s accepts a state \ud835\udc32\u2208\ud835\udd39\ud835\udc181:i\ud835\udc32superscript\ud835\udd39subscript\ud835\udc18normal-:1\ud835\udc56\\mathbf{y}\\in\\mathbb{B}^{\\mathbf{Y}_{1:i}}bold_y \u2208 blackboard_B start_POSTSUPERSCRIPT bold_Y start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT iff it is a path s\u2192vnormal-\u2192\ud835\udc60\ud835\udc63s\\to vitalic_s \u2192 italic_v in Disuperscript\ud835\udc37\ud835\udc56D^{i}italic_D start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT. In particular Ctksuperscriptsubscript\ud835\udc36\ud835\udc61\ud835\udc58C_{t}^{k}italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT represents \ud835\udcbbD\u2062[s\u2192t]subscript\ud835\udcbb\ud835\udc37delimited-[]normal-\u2192\ud835\udc60\ud835\udc61\\mathscr{f}_{D[s\\to t]}script_f start_POSTSUBSCRIPT italic_D [ italic_s \u2192 italic_t ] end_POSTSUBSCRIPT.We will proceed by recurrence on i\ud835\udc56iitalic_i, first showing that all accepted states by Cvisuperscriptsubscript\ud835\udc36\ud835\udc63\ud835\udc56C_{v}^{i}italic_C start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT are paths s\u2192v\u2192\ud835\udc60\ud835\udc63s\\to vitalic_s \u2192 italic_v in Disuperscript\ud835\udc37\ud835\udc56D^{i}italic_D start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT then showing that only them are accepted.Initialization for i=1\ud835\udc561i=1italic_i = 1: D1superscript\ud835\udc371D^{1}italic_D start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT only contains the vertices s\ud835\udc60sitalic_s and v\ud835\udc63vitalic_v such that e1=(s,v)subscript\ud835\udc521\ud835\udc60\ud835\udc63e_{1}=(s,v)italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = ( italic_s , italic_v ) and Cv1superscriptsubscript\ud835\udc36\ud835\udc631C_{v}^{1}italic_C start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT accepts exactly y1=1subscript\ud835\udc6611y_{1}=1italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 1 which is the only path s\u2192v\u2192\ud835\udc60\ud835\udc63s\\to vitalic_s \u2192 italic_v in D1superscript\ud835\udc371D^{1}italic_D start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT.Heredity from i\ud835\udc56iitalic_i to i+1\ud835\udc561i+1italic_i + 1:Assume a state \ud835\udc32\u2208\ud835\udd39\ud835\udc181:i+1\ud835\udc32superscript\ud835\udd39subscript\ud835\udc18:1\ud835\udc561\\mathbf{y}\\in\\mathbb{B}^{\\mathbf{Y}_{1:i+1}}bold_y \u2208 blackboard_B start_POSTSUPERSCRIPT bold_Y start_POSTSUBSCRIPT 1 : italic_i + 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT represents a path s\u2192v\u2192\ud835\udc60\ud835\udc63s\\to vitalic_s \u2192 italic_v in Di+1superscript\ud835\udc37\ud835\udc561D^{i+1}italic_D start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT and note:if yi+1=1subscript\ud835\udc66\ud835\udc5611y_{i+1}=1italic_y start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT = 1, then according to Lemma 9 ei+1=(u,v)subscript\ud835\udc52\ud835\udc561\ud835\udc62\ud835\udc63e_{i+1}=(u,v)italic_e start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT = ( italic_u , italic_v ) for some u\u2208Di\ud835\udc62superscript\ud835\udc37\ud835\udc56u\\in D^{i}italic_u \u2208 italic_D start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPTif u=s\ud835\udc62\ud835\udc60u=sitalic_u = italic_s: then \ud835\udc321:i=\ud835\udfcesubscript\ud835\udc32:1\ud835\udc560\\mathbf{y}_{1:i}=\\mathbf{0}bold_y start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT = bold_0 and by Lemma 10 Cvi+1\u2062(\ud835\udc32)=Csi\u2062(\ud835\udc321:i)=1superscriptsubscript\ud835\udc36\ud835\udc63\ud835\udc561\ud835\udc32superscriptsubscript\ud835\udc36\ud835\udc60\ud835\udc56subscript\ud835\udc32:1\ud835\udc561C_{v}^{i+1}(\\mathbf{y})=C_{s}^{i}(\\mathbf{y}_{1:i})=1italic_C start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT ( bold_y ) = italic_C start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( bold_y start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT ) = 1if u\u2260s\ud835\udc62\ud835\udc60u\\neq sitalic_u \u2260 italic_s: then \ud835\udc321:isubscript\ud835\udc32:1\ud835\udc56\\mathbf{y}_{1:i}bold_y start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT represents a path s\u2192u\u2192\ud835\udc60\ud835\udc62s\\to uitalic_s \u2192 italic_u in Disuperscript\ud835\udc37\ud835\udc56D^{i}italic_D start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT and by the recurrence hypothesis Cvi+1\u2062(\ud835\udc32)=Cui\u2062(\ud835\udc321:i)=1superscriptsubscript\ud835\udc36\ud835\udc63\ud835\udc561\ud835\udc32superscriptsubscript\ud835\udc36\ud835\udc62\ud835\udc56subscript\ud835\udc32:1\ud835\udc561C_{v}^{i+1}(\\mathbf{y})=C_{u}^{i}(\\mathbf{y}_{1:i})=1italic_C start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT ( bold_y ) = italic_C start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( bold_y start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT ) = 1if yi+1=0subscript\ud835\udc66\ud835\udc5610y_{i+1}=0italic_y start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT = 0, then \ud835\udc321:isubscript\ud835\udc32:1\ud835\udc56\\mathbf{y}_{1:i}bold_y start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT represents a path s\u2192v\u2192\ud835\udc60\ud835\udc63s\\to vitalic_s \u2192 italic_v in Disuperscript\ud835\udc37\ud835\udc56D^{i}italic_D start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT and by the recurrence hypothesis Cvi+1\u2062(\ud835\udc32)=Cvi\u2062(\ud835\udc321:i)=1superscriptsubscript\ud835\udc36\ud835\udc63\ud835\udc561\ud835\udc32superscriptsubscript\ud835\udc36\ud835\udc63\ud835\udc56subscript\ud835\udc32:1\ud835\udc561C_{v}^{i+1}(\\mathbf{y})=C_{v}^{i}(\\mathbf{y}_{1:i})=1italic_C start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT ( bold_y ) = italic_C start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( bold_y start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT ) = 1.Assume a state \ud835\udc32\u2208\ud835\udd39\ud835\udc181:i+1\ud835\udc32superscript\ud835\udd39subscript\ud835\udc18:1\ud835\udc561\\mathbf{y}\\in\\mathbb{B}^{\\mathbf{Y}_{1:i+1}}bold_y \u2208 blackboard_B start_POSTSUPERSCRIPT bold_Y start_POSTSUBSCRIPT 1 : italic_i + 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT is accepted by Cvi+1superscriptsubscript\ud835\udc36\ud835\udc63\ud835\udc561C_{v}^{i+1}italic_C start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT:if yi+1=1subscript\ud835\udc66\ud835\udc5611y_{i+1}=1italic_y start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT = 1, then according to Lemma 9 ei+1=(u,v)subscript\ud835\udc52\ud835\udc561\ud835\udc62\ud835\udc63e_{i+1}=(u,v)italic_e start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT = ( italic_u , italic_v ) for some u\u2208Di\ud835\udc62superscript\ud835\udc37\ud835\udc56u\\in D^{i}italic_u \u2208 italic_D start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPTif u=s\ud835\udc62\ud835\udc60u=sitalic_u = italic_s: then Csi\u2062(\ud835\udc321:i)=Cvi+1\u2062(\ud835\udc32)=1superscriptsubscript\ud835\udc36\ud835\udc60\ud835\udc56subscript\ud835\udc32:1\ud835\udc56superscriptsubscript\ud835\udc36\ud835\udc63\ud835\udc561\ud835\udc321C_{s}^{i}(\\mathbf{y}_{1:i})=C_{v}^{i+1}(\\mathbf{y})=1italic_C start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( bold_y start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT ) = italic_C start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT ( bold_y ) = 1, hence by Lemma 10 \ud835\udc321:i=\ud835\udfcesubscript\ud835\udc32:1\ud835\udc560\\mathbf{y}_{1:i}=\\mathbf{0}bold_y start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT = bold_0 and \ud835\udc32\ud835\udc32\\mathbf{y}bold_y represents a path s\u2192v\u2192\ud835\udc60\ud835\udc63s\\to vitalic_s \u2192 italic_v in Di+1superscript\ud835\udc37\ud835\udc561D^{i+1}italic_D start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT.if u\u2260s\ud835\udc62\ud835\udc60u\\neq sitalic_u \u2260 italic_s: then Cui\u2062(\ud835\udc321:i)=Cvi+1\u2062(\ud835\udc32)=1superscriptsubscript\ud835\udc36\ud835\udc62\ud835\udc56subscript\ud835\udc32:1\ud835\udc56superscriptsubscript\ud835\udc36\ud835\udc63\ud835\udc561\ud835\udc321C_{u}^{i}(\\mathbf{y}_{1:i})=C_{v}^{i+1}(\\mathbf{y})=1italic_C start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( bold_y start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT ) = italic_C start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT ( bold_y ) = 1, hence by the recurrence hypothesis \ud835\udc321:isubscript\ud835\udc32:1\ud835\udc56\\mathbf{y}_{1:i}bold_y start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT represents a path s\u2192u\u2192\ud835\udc60\ud835\udc62s\\to uitalic_s \u2192 italic_u in Disuperscript\ud835\udc37\ud835\udc56D^{i}italic_D start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT and by adding ei+1subscript\ud835\udc52\ud835\udc561e_{i+1}italic_e start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT \ud835\udc32\ud835\udc32\\mathbf{y}bold_y represents a path s\u2192v\u2192\ud835\udc60\ud835\udc63s\\to vitalic_s \u2192 italic_v in Di+1superscript\ud835\udc37\ud835\udc561D^{i+1}italic_D start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT.if yi+1=0subscript\ud835\udc66\ud835\udc5610y_{i+1}=0italic_y start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT = 0, then Cui\u2062(\ud835\udc321:i)=Cui+1\u2062(\ud835\udc32)=1superscriptsubscript\ud835\udc36\ud835\udc62\ud835\udc56subscript\ud835\udc32:1\ud835\udc56superscriptsubscript\ud835\udc36\ud835\udc62\ud835\udc561\ud835\udc321C_{u}^{i}(\\mathbf{y}_{1:i})=C_{u}^{i+1}(\\mathbf{y})=1italic_C start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( bold_y start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT ) = italic_C start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT ( bold_y ) = 1, hence by the recurrence hypothesis \ud835\udc321:isubscript\ud835\udc32:1\ud835\udc56\\mathbf{y}_{1:i}bold_y start_POSTSUBSCRIPT 1 : italic_i end_POSTSUBSCRIPT represents a path s\u2192v\u2192\ud835\udc60\ud835\udc63s\\to vitalic_s \u2192 italic_v in Disuperscript\ud835\udc37\ud835\udc56D^{i}italic_D start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT and by not adding edge ei+1subscript\ud835\udc52\ud835\udc561e_{i+1}italic_e start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT \ud835\udc32\ud835\udc32\\mathbf{y}bold_y represents a path s\u2192v\u2192\ud835\udc60\ud835\udc63s\\to vitalic_s \u2192 italic_v in Di+1superscript\ud835\udc37\ud835\udc561D^{i+1}italic_D start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT.\u220eFinally, we can state our main result:The fragment of simple path logic composed of acyclic theories is tractable.First, it is concise because it is an edge-based logic. Let\u2019s assume an acyclic simple path theory (G,\u03c2)\ud835\udc3a\ud835\udf0d(G,\\varsigma)( italic_G , italic_\u03c2 ). If needed, merge source vertices and target vertices like mentioned above. Since a topological order of the edges in a DAG can be compiled in a polynomial time, we can compute \u03f1italic-\u03f1\\varrhoitalic_\u03f1 such that (G,\u03f1)\ud835\udc3aitalic-\u03f1(G,\\varrho)( italic_G , italic_\u03f1 ) is acyclic and topological. We compile (G,\u03f1)\ud835\udc3aitalic-\u03f1(G,\\varrho)( italic_G , italic_\u03f1 ) into a concise SDD Ctksuperscriptsubscript\ud835\udc36\ud835\udc61\ud835\udc58C_{t}^{k}italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT as seen above. We rewrite leaf nodes Yisubscript\ud835\udc4c\ud835\udc56Y_{i}italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to Y\u03c2\u2218\u03f1\u22121\u2062(i)subscript\ud835\udc4c\ud835\udf0dsuperscriptitalic-\u03f11\ud835\udc56Y_{\\varsigma\\circ\\varrho^{-1}(i)}italic_Y start_POSTSUBSCRIPT italic_\u03c2 \u2218 italic_\u03f1 start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( italic_i ) end_POSTSUBSCRIPT in the SDD. The new circuit is still a concise SDD, structured according to the right linear vtree on (Y\u03c2\u2218\u03f1\u22121\u2062(i))1\u2264i\u2264ksubscriptsubscript\ud835\udc4c\ud835\udf0dsuperscriptitalic-\u03f11\ud835\udc561\ud835\udc56\ud835\udc58(Y_{\\varsigma\\circ\\varrho^{-1}(i)})_{1\\leq i\\leq k}( italic_Y start_POSTSUBSCRIPT italic_\u03c2 \u2218 italic_\u03f1 start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( italic_i ) end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT 1 \u2264 italic_i \u2264 italic_k end_POSTSUBSCRIPT and is equivalent to the theory (G,\u03c2)\ud835\udc3a\ud835\udf0d(G,\\varsigma)( italic_G , italic_\u03c2 ). Because SDD is MPE and PQE-tractable, we have that acyclic simple path logic is concise, MPE and PQE-tractable. Therefore, acyclic simple path logic is tractable.\n\u220eMatching problems naturally arise in artificial intelligence when one wants to find the best pairing between various entities (e.g.\u00a0individuals, tasks, resources, etc.). A matching task can be thought of as an informed classification task where classes correspond to edges in a graph and prior knowledge tells us that only states that represent a matching (i.e. a set of non-adjacent edges) are accepted.Matching logic \u2112m:=(\ud835\udcaf\u2192,\ud835\udcc8m)assignsubscript\u2112\ud835\udc5asubscript\ud835\udcaf\u2192subscript\ud835\udcc8\ud835\udc5a\\mathcal{L}_{m}:=(\\mathcal{T}_{\\to},\\mathscr{s}_{m})caligraphic_L start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT := ( caligraphic_T start_POSTSUBSCRIPT \u2192 end_POSTSUBSCRIPT , script_s start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) is an edge-based logic where a theory (G:=(V,E),\u03c2)assign\ud835\udc3a\ud835\udc49\ud835\udc38\ud835\udf0d(G:=(V,E),\\varsigma)( italic_G := ( italic_V , italic_E ) , italic_\u03c2 ) accepts a state \ud835\udc32\u2208\ud835\udd39\ud835\udc18\ud835\udc32superscript\ud835\udd39\ud835\udc18\\mathbf{y}\\in\\mathbb{B}^{\\mathbf{Y}}bold_y \u2208 blackboard_B start_POSTSUPERSCRIPT bold_Y end_POSTSUPERSCRIPT if the edges selected in E\ud835\udc38Eitalic_E correspond to a matching in G\ud835\udc3aGitalic_G (i.e. no vertex in V\ud835\udc49Vitalic_V has two adjacent edges in {e\u2208E|\u03c2\u2062(e)\u2208\ud835\udc32}conditional-set\ud835\udc52\ud835\udc38\ud835\udf0d\ud835\udc52\ud835\udc32\\{e\\in E|\\varsigma(e)\\in\\mathbf{y}\\}{ italic_e \u2208 italic_E | italic_\u03c2 ( italic_e ) \u2208 bold_y }). Notice that matching logics ignore the direction of edges.Matching theories can be compiled in 2-cnf formulas with:Matching logic is semi-tractable.Matching logic is concise by design because it is en edge-based logic. It is MPE-tractable by reduction to finding a maximum weight-sum matching, which is known to be tractable [28]. It is however not PQE-tractable as shown in [29].\n\u220eIn this paper, we introduced an abstract formalism for informed supervised classification tasks and techniques (Section 3). To the best of our knowledge, we are the first to provide such a general view that capture prior knowledge expressed in any abstract logic (e.g.\u00a0propositional logic, answer set programming or linear programming). We build upon this formalism to re-frame three abstract neurosymbolic techniques based on probabilistic reasoning and relate them to various papers that defined them in the context of specific logics. Finally, we study the computational complexity of probabilistic reasoning for several families of prior knowledge commonly found in the neurosymbolic literature (Section 4). We showed for instance that some families of logical constraints that were thought computationally hard are actually scalable, while others that are frequently used in toy datasets to evaluate probabilistic neurosymbolic techniques are not. We also established that there are logics for which some techniques are tractable (e.g.\u00a0semantic conditioning at inference) while others are not (e.g.\u00a0semantic conditioning and semantic regularization), introducing a new criteria of comparison for probabilistic neurosymbolic techniques.Future directions for our research include a sharper understanding of semi-tractable logics and their practical consequences for informed classification tasks. We would like to explore approximate methods for probabilistic neurosymbolic techniques, in the case where prior knowledge is intractable. Finally, we are also interested in expanding our formalism beyond supervised classification to semi-supervised and weakly-supervised settings.",
    "2": "How are emotions formed? Through extensive debate and the promulgation of diverse theories , the theory of constructed emotion has become prevalent in recent research on emotions. According to this theory, an emotion concept refers to a category formed by interoceptive and exteroceptive information associated with a specific emotion. An emotion concept stores past experiences as knowledge and can predict unobserved information from acquired information. Therefore, in this study, we attempted to model the formation of emotion concepts using a constructionist approach from the perspective of the constructed emotion theory. Particularly, we constructed a model using multilayered multimodal latent Dirichlet allocation , which is a probabilistic generative model. We then trained the model for each subject using vision, physiology, and word information obtained from multiple people who experienced different visual emotion-evoking stimuli. To evaluate the model, we verified whether the formed categories matched human subjectivity and determined whether unobserved information could be predicted via categories.\nThe verification results exceeded chance level, suggesting that emotion concept formation can be explained by the proposed model.\nEmotions play a central role in human psychology and behavior, and they are deeply connected to all aspects of our daily lives, including decision making, human relationships, and health. However, despite emotions playing such an important role in the lives of people, their formation process remains unclear. In this study, we approached this problem from a constructionist approach, based on constructed emotion theory [6], which has exhibited considerable influence on recent emotion-related research.Constructed emotion theory posits that emotions are formed by integrating information from interoception, which refers to sensations related to the internal environment of the body such as internal organs, and exteroception, which refers to sensations experienced outside the body such as sight, hearing, smell, taste, and touch [12, 6, 29]. Barrett [6] stated that, \u201cin every waking moment, your brain uses past experience, organized as concepts, to guide your actions and give your sensations meaning. When the concepts involved are emotion concepts, your brain constructs instances of emotion.\u201d Researchers have expressed and debated varied perspectives related to emotions; however, most researchers believe that emotion concepts are acquired through experience [29].In this case, how are emotion concepts acquired? In this study, based on constructed emotion theory, we considered emotion concepts to be categories formed from interoceptive and exteroceptive information associated with specific emotions. When people receive a stimulus, they form a basic emotional state called a core affect based on the interoception of the stimulus [28, 27, 22]. Then, the brain integrates core affect and exteroceptive information, and further categorizes the integrated sensory information through emotion concepts acquired from past experience [6, 7, 29, 22]. Instances of specific emotions such as sadness and anger are generated from the concepts formed in this process. Here, emotion concepts do not refer to independent categories with clear boundaries. To begin with, emotion concepts are not fixed and are constantly changing. When the brain receives certain sensory information, it attempts to identify the cause. Particularly, the brain attempts to identify a reasonable combination of past sensory information (past experiences) that resulted in an emotion concept to identify where the specifically acquired information was generated. Thus, emotion concepts are stochastically composed of multiple dynamically changing categories [6, 5, 11, 36].Why must people form emotion concepts? We present an encounter with a dog at the side of a street as an example. When a person sees a dog, the person generates a certain type of physiological reaction, such as an increase in heart rate. A core affect is formed from this interoceptive information, which is integrated with visual and auditory information that is obtained related to the dog, such as the facial expression, size, and barking action of the dog. Categorization is then performed by matching the core affect with pre-existing emotion concepts, which have been formed by past experiences. Therefore, if a person was bitten by a similar dog in the past, then the emotion of \u201cfear\u201d is generated by the emotion concept that reflects that experience. Simultaneously, the person predicts tactile information such as \u201cpain\u201d and visual information such as \u201cred blood\u201d that are associated with the emotion of \u201cfear\u201d and attempts to perform evasive actions. Therefore, emotion concepts not only assign meaning to sensory information and generate emotional instances, but also aid in predicting unobserved information from the acquired information and prescribing behavior [6, 5]. This formation of categories for predicting unobserved information is an important function of emotion concepts and is one reason why people form emotion concepts.However, not all people experience the same emotion in response to the same stimulus. In the above example, if the observed dog is similar to a dog owned by the person, then they would probably not experience the emotion of \u201cfear.\u201d This is because, even if people have the same core affect state, their past experiences (happy memories of playing with their own dog) differ, resulting in the categorization of different emotion concepts. The results of this categorization update the existing emotion concept. The influence of this type of top-down processing based on emotion concepts produces the individual differences in emotion in response to the same external stimulus.As explained previously, interoception and exteroception are integrated to form an emotion concept, and the brain generates emotions and predicts unobserved information through this concept. However, little progress has been made in examining the types of information processes that actually occur when the interoceptive and exteroceptive information are integrated.\nResearch has also been conducted in the field of symbol emergence in robotics to form concepts using multimodal categorization for examining the concept formation process exhibited by people [23, 16, 21, 3, 14, 2]. Multimodal categorization in symbol-emergent robotics is the idea of learning categories that are similar to human sensations from multimodal information without supervision [23]. The learned categories correspond to concepts, and the prediction of unobserved information based on concepts is considered to be understanding. Concept formation using multimodal categorization has previously focused on externally observable aspects such as objects and places. Nakamura et al. [23] constructed a robot with sensors that can acquire visual, auditory, and tactile information, and they showed that object concepts could be acquired by categorizing this information. Moreover, Hagiwara et al. [16] showed that a robot can predict location names and categories in a manner similar to human predictions based on vision, position, and word information. Miyazawa et al. [21] used multilayered multimodal latent Dirichlet allocation (mMLDA) [3] to propose an integrated cognitive architecture for robots to simultaneously learn behavior and language, and they demonstrated the effectiveness of the model through experiments using an actual robot. However, human mental states such as emotions have barely been addressed in this field. Ohmori et al. [25] conducted multimodal concept formation using physiological signals based on the eating task performed by humans as an approach to cognitive structures including emotions. However, the number of stimuli and subjects in this study were limited, and the data may not have been sufficiently controlled.Therefore, in this study, we applied multimodal categorization to emotion concept formation and attempted to model it. Particularly, we constructed a model using mMLDA [3], which is a probabilistic generative model. We then trained a model for each subject using multimodal data obtained when emotion-evoking stimuli were presented to people, and we verified the validity of the model by comparing the categories formed by the model with subjective emotional reports. We also used the formed categories to investigate the prediction of unobserved information, which is an important element of the concept. We used the above probabilistic generative model to express emotion concept formation based on constructed emotion theory,\nand we believe this constructive approach toward emotions may yield a clue to the elucidation of human emotions.Here, we describe multimodal latent Dirichlet allocation (MLDA), which is the basis of the mMLDA. Subsequently, we provide an overview of the model used in this research and its implementation method.MLDA [23] is a generative model based on latent Dirichlet allocation (LDA) [8] that can form concepts from multimodal information. Here, a \u201cconcept\u201d refers to a category obtained by grouping different types of inputs. Fig. 1 shows the graphical model of MLDA when the number of modality information is two. In the figure, z\ud835\udc67zitalic_z represents the conceptual category of observed information w*\u2208{w1,w2}superscript\ud835\udc64superscript\ud835\udc641superscript\ud835\udc642w^{*}\\in\\{w^{1},w^{2}\\}italic_w start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT \u2208 { italic_w start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_w start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT }, which is generated from a multinomial distribution with parameters \u03b8\ud835\udf03\\thetaitalic_\u03b8, \u03d5*\u2208{\u03d51,\u03d52}superscriptitalic-\u03d5superscriptitalic-\u03d51superscriptitalic-\u03d52\\phi^{*}\\in\\{\\phi^{1},\\phi^{2}\\}italic_\u03d5 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT \u2208 { italic_\u03d5 start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_\u03d5 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT }, respectively. They are expressed using the following equations:Moreover, \u03b1\ud835\udefc\\alphaitalic_\u03b1, \u03b2*\u2208{\u03b21,\u03b22}superscript\ud835\udefdsuperscript\ud835\udefd1superscript\ud835\udefd2\\beta^{*}\\in\\{\\beta^{1},\\beta^{2}\\}italic_\u03b2 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT \u2208 { italic_\u03b2 start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_\u03b2 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT } represent the hyperparameters of the Dirichlet distribution that generate \u03b8\ud835\udf03\\thetaitalic_\u03b8 and \u03d5*superscriptitalic-\u03d5\\phi^{*}italic_\u03d5 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT, respectively, which can be expressed as follows:\nThe categorizations in MLDA involve estimating a concept z\ud835\udc67zitalic_z from the obtained observed information w*superscript\ud835\udc64w^{*}italic_w start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT and optimizing it through parameters \u03b8\ud835\udf03\\thetaitalic_\u03b8 and \u03d5*superscriptitalic-\u03d5\\phi^{*}italic_\u03d5 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT. By repeatedly updating parameters for the observed information, past experiences can be accumulated as new concepts. Gibbs sampling [15] was used for this estimation. The estimation of concept z\ud835\udc67zitalic_z using Gibbs sampling is expressed by the following equation [21]:Here, \ud835\udc7e\ud835\udc7e\\bm{W}bold_italic_W is a set of modality information, Wmsuperscript\ud835\udc4a\ud835\udc5aW^{m}italic_W start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT denotes the number of dimensions of the m\ud835\udc5amitalic_mth modality information, and \\\\\\backslash\\ indicates the exclusion of information. Moreover, nm,wm,k,jsubscript\ud835\udc5b\ud835\udc5asuperscript\ud835\udc64\ud835\udc5a\ud835\udc58\ud835\udc57n_{m,w^{m},k,j}italic_n start_POSTSUBSCRIPT italic_m , italic_w start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_k , italic_j end_POSTSUBSCRIPT denotes the number of times that category k\ud835\udc58kitalic_k is assigned to feature wmsuperscript\ud835\udc64\ud835\udc5aw^{m}italic_w start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT of m\ud835\udc5amitalic_mth modality information in the j\ud835\udc57jitalic_jth observed information,\nand \ud835\udc81\\m\u2062i\u2062jsuperscript\ud835\udc81\\absent\ud835\udc5a\ud835\udc56\ud835\udc57\\bm{Z}^{\\backslash{mij}}bold_italic_Z start_POSTSUPERSCRIPT \\ italic_m italic_i italic_j end_POSTSUPERSCRIPT is the set of all concepts excluding the concept category zm\u2062i\u2062jsubscript\ud835\udc67\ud835\udc5a\ud835\udc56\ud835\udc57z_{mij}italic_z start_POSTSUBSCRIPT italic_m italic_i italic_j end_POSTSUBSCRIPT that is assigned to the i\ud835\udc56iitalic_ith information of modality m\ud835\udc5amitalic_m in the j\ud835\udc57jitalic_jth observed information. If j\ud835\udc57jitalic_j is not added, it means the number of occurrences in all data, not the j\ud835\udc57jitalic_jth data. Furthermore, nk,j\\m\u2062i\u2062jsuperscriptsubscript\ud835\udc5b\ud835\udc58\ud835\udc57\\absent\ud835\udc5a\ud835\udc56\ud835\udc57n_{k,j}^{\\backslash{mij}}italic_n start_POSTSUBSCRIPT italic_k , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \\ italic_m italic_i italic_j end_POSTSUPERSCRIPT, nm,wm,k\\m\u2062i\u2062jsuperscriptsubscript\ud835\udc5b\ud835\udc5asuperscript\ud835\udc64\ud835\udc5a\ud835\udc58\\absent\ud835\udc5a\ud835\udc56\ud835\udc57n_{m,w^{m},k}^{\\backslash{mij}}italic_n start_POSTSUBSCRIPT italic_m , italic_w start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \\ italic_m italic_i italic_j end_POSTSUPERSCRIPT, and nm,k\\m\u2062i\u2062jsuperscriptsubscript\ud835\udc5b\ud835\udc5a\ud835\udc58\\absent\ud835\udc5a\ud835\udc56\ud835\udc57n_{m,k}^{\\backslash{mij}}italic_n start_POSTSUBSCRIPT italic_m , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \\ italic_m italic_i italic_j end_POSTSUPERSCRIPT denote the number of times category k\ud835\udc58kitalic_k is assigned among the j\ud835\udc57jitalic_jth observed information, excluding the information that is currently considered, number of times\nthat category k\ud835\udc58kitalic_k is assigned to feature wmsuperscript\ud835\udc64\ud835\udc5aw^{m}italic_w start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT of modality m\ud835\udc5amitalic_m\n, and total number of categories k\ud835\udc58kitalic_k assigned with modality m\ud835\udc5amitalic_m, respectively. This is repeated until n*subscript\ud835\udc5bn_{*}italic_n start_POSTSUBSCRIPT * end_POSTSUBSCRIPT converges; moreover, the values at the time of convergence are used to find \u03b8\ud835\udf03\\thetaitalic_\u03b8 and \u03d5*superscriptitalic-\u03d5\\phi^{*}italic_\u03d5 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT using the following equations:where K\ud835\udc3eKitalic_K denotes the total number of categories.Moreover, the learned model can be used to predict the categories of the unobserved data. If the observed information of each modality of actually observed data is \ud835\udc98o\u2062b\u2062ssubscript\ud835\udc98\ud835\udc5c\ud835\udc4f\ud835\udc60\\bm{w}_{obs}bold_italic_w start_POSTSUBSCRIPT italic_o italic_b italic_s end_POSTSUBSCRIPT, then the concept z^^\ud835\udc67\\hat{z}over^ start_ARG italic_z end_ARG can be estimated as follows:The category that maximizes this z^^\ud835\udc67\\hat{z}over^ start_ARG italic_z end_ARG is the category of the actually observed data. This is expressed as follows:This can also be used to predict unobserved modality information. The unobserved modality w\ud835\udc64witalic_w is predicted by the following equation:where P\u2062(\u03b8|\ud835\udc98o\u2062b\u2062s)\ud835\udc43conditional\ud835\udf03subscript\ud835\udc98\ud835\udc5c\ud835\udc4f\ud835\udc60P(\\theta|\\bm{w}_{obs})italic_P ( italic_\u03b8 | bold_italic_w start_POSTSUBSCRIPT italic_o italic_b italic_s end_POSTSUBSCRIPT ) in Eqs. (8) and (10) can be obtained by determining \u03b8\ud835\udf03\\thetaitalic_\u03b8 using the aforementioned Gibbs sampling.\nThus, MLDA can be used to form concepts from unsupervised multimodal information. However, MLDA can only handle a single concept and cannot express relationships between concepts. Therefore, in this study, we constructed an mMLDA model, which is an extension of MLDA.mMLDA [3] is a generative model that probabilistically expresses relationships between multiple concepts by connecting LDA and the above-mentioned MLDA in a hierarchical manner. Multiple LDAs and MLDAs can be prepared as lower layers corresponding to each piece of information, and MLDA can be arranged as an upper layer to integrate them to efficiently express the categories (i.e., relationships between concepts). In this study, we set up interoception and exteroception modules as the lower layers. Human exteroception and interoception are inherently diverse; however, in this study, we used vision and word information as exteroception and physiology information such as electrodermal activity and heartbeat waveforms as interoception.Fig. 2 shows a graphical model of the emotion concept formation model that was constructed based on the aforementioned ideas. In the figure, z*\u2208{zI,zV,zW}superscript\ud835\udc67superscript\ud835\udc67\ud835\udc3csuperscript\ud835\udc67\ud835\udc49superscript\ud835\udc67\ud835\udc4az^{*}\\in\\{z^{I},z^{V},z^{W}\\}italic_z start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT \u2208 { italic_z start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT , italic_z start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT , italic_z start_POSTSUPERSCRIPT italic_W end_POSTSUPERSCRIPT }, where zIsuperscript\ud835\udc67\ud835\udc3cz^{I}italic_z start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT, zVsuperscript\ud835\udc67\ud835\udc49z^{V}italic_z start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT, and zWsuperscript\ud835\udc67\ud835\udc4az^{W}italic_z start_POSTSUPERSCRIPT italic_W end_POSTSUPERSCRIPT represent the concepts of interoception, vision, and word, respectively; z\ud835\udc67zitalic_z represents the integrated concept that captures the relationships among these concepts. These are generated from a multinomial distribution with parameters \u03b8*\u2208{\u03b8I,\u03b8V,\u03b8W}superscript\ud835\udf03superscript\ud835\udf03\ud835\udc3csuperscript\ud835\udf03\ud835\udc49superscript\ud835\udf03\ud835\udc4a\\theta^{*}\\in\\{\\theta^{I},\\theta^{V},\\theta^{W}\\}italic_\u03b8 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT \u2208 { italic_\u03b8 start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT , italic_\u03b8 start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT , italic_\u03b8 start_POSTSUPERSCRIPT italic_W end_POSTSUPERSCRIPT } and \u03b8\ud835\udf03\\thetaitalic_\u03b8. Moreover, w*\u2208{wI1,wI2,wV,wW}superscript\ud835\udc64superscript\ud835\udc64subscript\ud835\udc3c1superscript\ud835\udc64subscript\ud835\udc3c2superscript\ud835\udc64\ud835\udc49superscript\ud835\udc64\ud835\udc4aw^{*}\\in\\{w^{I_{1}},w^{I_{2}},w^{V},w^{W}\\}italic_w start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT \u2208 { italic_w start_POSTSUPERSCRIPT italic_I start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT , italic_w start_POSTSUPERSCRIPT italic_I start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT , italic_w start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT , italic_w start_POSTSUPERSCRIPT italic_W end_POSTSUPERSCRIPT } is the observed information related to physiology (wI1superscript\ud835\udc64subscript\ud835\udc3c1w^{I_{1}}italic_w start_POSTSUPERSCRIPT italic_I start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT and wI2superscript\ud835\udc64subscript\ud835\udc3c2w^{I_{2}}italic_w start_POSTSUPERSCRIPT italic_I start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT), vision, and words, respectively. These are generated from a multinomial distribution with \u03d5*\u2208{\u03d5I1,\u03d5I2,\u03d5V,\u03d5W}superscriptitalic-\u03d5superscriptitalic-\u03d5subscript\ud835\udc3c1superscriptitalic-\u03d5subscript\ud835\udc3c2superscriptitalic-\u03d5\ud835\udc49superscriptitalic-\u03d5\ud835\udc4a\\phi^{*}\\in\\{\\phi^{I_{1}},\\phi^{I_{2}},\\phi^{V},\\phi^{W}\\}italic_\u03d5 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT \u2208 { italic_\u03d5 start_POSTSUPERSCRIPT italic_I start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT , italic_\u03d5 start_POSTSUPERSCRIPT italic_I start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT , italic_\u03d5 start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT , italic_\u03d5 start_POSTSUPERSCRIPT italic_W end_POSTSUPERSCRIPT }. Similar to MLDA, categorization involves estimating the parameters \u03b8\ud835\udf03\\thetaitalic_\u03b8, \u03b8*superscript\ud835\udf03\\theta^{*}italic_\u03b8 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT, and \u03d5*superscriptitalic-\u03d5\\phi^{*}italic_\u03d5 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT from the observed information w*superscript\ud835\udc64w^{*}italic_w start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT, and the Gibbs sampling was used for estimation.By using mMLDA, unobserved information that spans concepts can also be predicted.\nIn Fig. 2, \u03b1\ud835\udefc\\alphaitalic_\u03b1, \u03b1*\u2208{\u03b1I,\u03b1V,\u03b1W}superscript\ud835\udefcsuperscript\ud835\udefc\ud835\udc3csuperscript\ud835\udefc\ud835\udc49superscript\ud835\udefc\ud835\udc4a\\alpha^{*}\\in\\{\\alpha^{I},\\alpha^{V},\\alpha^{W}\\}italic_\u03b1 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT \u2208 { italic_\u03b1 start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT , italic_\u03b1 start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT , italic_\u03b1 start_POSTSUPERSCRIPT italic_W end_POSTSUPERSCRIPT }, and \u03b2*\u2208{\u03b2I1,\u03b2I2,\u03b2V,\u03b2W}superscript\ud835\udefdsuperscript\ud835\udefdsubscript\ud835\udc3c1superscript\ud835\udefdsubscript\ud835\udc3c2superscript\ud835\udefd\ud835\udc49superscript\ud835\udefd\ud835\udc4a\\beta^{*}\\in\\{\\beta^{I_{1}},\\beta^{I_{2}},\\beta^{V},\\beta^{W}\\}italic_\u03b2 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT \u2208 { italic_\u03b2 start_POSTSUPERSCRIPT italic_I start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT , italic_\u03b2 start_POSTSUPERSCRIPT italic_I start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT , italic_\u03b2 start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT , italic_\u03b2 start_POSTSUPERSCRIPT italic_W end_POSTSUPERSCRIPT } represent the hyperparameters of the Dirichlet distribution that generate \u03b8\ud835\udf03\\thetaitalic_\u03b8, \u03b8*superscript\ud835\udf03\\theta^{*}italic_\u03b8 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT, and \u03d5*superscriptitalic-\u03d5\\phi^{*}italic_\u03d5 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT, respectively. Furthermore, M\ud835\udc40Mitalic_M denotes the total number of data, N*\u2208{NI,NV,NW}superscript\ud835\udc41superscript\ud835\udc41\ud835\udc3csuperscript\ud835\udc41\ud835\udc49superscript\ud835\udc41\ud835\udc4aN^{*}\\in\\{N^{I},N^{V},N^{W}\\}italic_N start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT \u2208 { italic_N start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT , italic_N start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT , italic_N start_POSTSUPERSCRIPT italic_W end_POSTSUPERSCRIPT } is the total amount of information in each modality, and K\ud835\udc3eKitalic_K and K*\u2208{KI,KV,KW}superscript\ud835\udc3esuperscript\ud835\udc3e\ud835\udc3csuperscript\ud835\udc3e\ud835\udc49superscript\ud835\udc3e\ud835\udc4aK^{*}\\in\\{K^{I},K^{V},K^{W}\\}italic_K start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT \u2208 { italic_K start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT , italic_K start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT , italic_K start_POSTSUPERSCRIPT italic_W end_POSTSUPERSCRIPT } are the number of categories for each concept, respectively. In this study, the hyperparameters \u03b1\ud835\udefc\\alphaitalic_\u03b1, \u03b1*superscript\ud835\udefc\\alpha^{*}italic_\u03b1 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT, and \u03b2*superscript\ud835\udefd\\beta^{*}italic_\u03b2 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT were set to 1.0.In this study, we assumed that a emotion concept is represented in the interoception category zIsuperscript\ud835\udc67\ud835\udc3cz^{I}italic_z start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT, which is influenced by exteroception. Core affect, which is the perception of interoception, is said to be expressed on a plane consisting of two axes (valence and arousal) [27]. Emotion concept formation is speculated to occur with this core affect as a basis and adding exteroceptive information to it. Therefore, in this model, the emotion concept is expressed in zIsuperscript\ud835\udc67\ud835\udc3cz^{I}italic_z start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT, which is a core affect influenced by exteroceptive information.Serket [24] was used for model implementation. Serket is a framework that enables the construction of cognitive models by connecting modules while each module maintains its programmatic independence. Serket can achieve inference with mMLDA based on multiple LDA and MLDA inferences and the message passing approach. Parameter updates are based on a single LDA or MLDA inference; therefore, the learning and prediction methods described in Section II-A can be used. Fig. 3 shows an overview of the model illustrated in Fig. 2 when it is implemented using Serket.\nFirst, the probabilities shown below can be computed by Gibbs sampling.\nHere, wj*subscriptsuperscript\ud835\udc64\ud835\udc57w^{*}_{j}italic_w start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT is the observed information of the j\ud835\udc57jitalic_jth data, zj*subscriptsuperscript\ud835\udc67\ud835\udc57z^{*}_{j}italic_z start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT is the latent variables of the j\ud835\udc57jitalic_jth data.\nNext, the values of the shared latent variables are inferred stochastically using a integrated concept model:Here, C\u2208{I,V,W}\ud835\udc36\ud835\udc3c\ud835\udc49\ud835\udc4aC\\in\\{I,V,W\\}italic_C \u2208 { italic_I , italic_V , italic_W } is the concept.\nThese probabilities are also represented by finite and discrete parameters, which can be communicated using the message passing approach.\nThese parameters are sent to each concept model, where the latent variables assigned to the modality information of the concept are determined using Gibbs sampling.\nwhere \ud835\udc7e\ud835\udc7e\\bm{W}bold_italic_W represents all the information, and \ud835\udc81\\m\u2062i\u2062jsuperscript\ud835\udc81\\absent\ud835\udc5a\ud835\udc56\ud835\udc57\\bm{Z}^{\\backslash{mij}}bold_italic_Z start_POSTSUPERSCRIPT \\ italic_m italic_i italic_j end_POSTSUPERSCRIPT represents a set of latent variables, except for the latent variable assigned to i\ud835\udc56iitalic_ith information of modality m\ud835\udc5amitalic_m of the j\ud835\udc57jitalic_jth observation.\nAll the latent variables were learned in a complementary manner.The experiment was divided into three phases: data collection, model learning, and validation. In the data collection phase, we obtained information on each modality from the subjects through a visual stimulation task. In the model learning phase, we preprocessed the information related to each modality, which was obtained in the subject experiment for model learning. In the validation phase, zIsuperscript\ud835\udc67\ud835\udc3cz^{I}italic_z start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT of the learned model was compared with the subjective emotional reports of the subjects. We also attempted to predict unobserved information in other modalities from observed information in a specific modality for unknown images.A subject-based experiment was conducted to collect data for model learning. Thirty-one subjects (14 males and 17 females with ages ranging from 20 to 40 years) were selected for the experiment. Sixty images that aroused emotion were extracted from the International Affective Picture System (IAPS) [18] and presented to the subjects. The physiological signals were obtained using an E4 wristband [30] and a myBeat WHS-1 wearable heart-rate sensor to obtain the electrodermal activity (EDA; 4 Hz) and heartbeat waveform (128 Hz), respectively. These data can be obtained along with time stamps using a designated dongle connected via Bluetooth. myBeat can measure the electrical activity of the heart at 128 Hz by being attached to the electrode pad developed for myBeat and attaching it near the heart. The data were obtained with a timestamp by the attached receiver for the repeated cycles of the experiment. Subsequently, the subjects were asked to complete an emotional self-evaluation scale to understand their emotional state regarding the images. IAPS publishes images as well as values of Self-Assessment Manikin (SAM) evaluation [9]. SAM is an emotional self-evaluation scale that uses a nine-point scale with illustrations for each of the three evaluation axes: valence, arousal, and dominance. The 60 images used in this experiment were selected so that this value was varied.Note that the physiological signals are acquired as information on interoceptors, which are the interoception sources, and we selected the physiological signals that are most often used in research and that are relatively less burdensome to subjects [19, 35]. We also used Google speech recognition [26] for obtaining word information, with the subjects being asked to provide voice input in Japanese regarding their feelings about the presented images.\nThen, subjects were asked to input individual words as much as possible.Moreover, SAM was used as an emotional self-evaluation scale for images. In the experiment, we obtained all values for valence, arousal, and dominance; however, dominance is difficult to teach, and subjects often chose it incorrectly. An experiment by Lang et al., who created SAM, showed that the correlation coefficients between pleasant and aroused emotions and the SAM valence and arousal values, respectively, were 0.96 and 0.95, respectively, whereas that between a dominant emotion and the SAM dominance value was 0.23, which is relatively low [9]. Lang et al. posited that this was due to the fact that the dominance value inherently represents the personal sense of domination of the subject in a particular situation; however, subjects often interpret this as a sense of domination of the object in the image. In practice, many similar cases were observed in the present experiment as well; consequently, the dominance value was not used in the analysis, and only the valence and arousal values were used.The durations of the experiments were set as follows: 75 s for one cycle, 6 s for image display, 50 s for word information input, 15 s for SAM response, and 4 seconds in total for the waiting time between each phase. A three-minute break was also included between each session. Moreover, before the start of the experiments, we conducted five minutes of resting state measurements and a practice session for the subjects to become used to basic operations. After the completion of the experiments, subjects were asked to complete a questionnaire regarding their personal characteristics. The experiment time, including these times, was 2.5\u20133 h per person. Of all the subjects in this study, 29 (13 males, 16 females) who did not have major physiological signal deficits were included in the analysis. Furthermore, for subjects who showed deficits in only some of their physiological signals, all other data excluding the deficient data were included in the analysis.The model shown in Fig. 2 was trained using the observed information that was obtained by converting the modality information obtained through experiments into vector representations. The information for each modality was processed as explained subsequently.\nWe used 6 s of physiological signals during image presentation as modality information for the interoception module. For data preprocessing, we applied a high-pass filter that cuts frequencies below 0.05 Hz of the EDA, and the data was then smoothed using a moving average over a one-second period. Standardization was also conducted using experimental data. For the heartbeat waveform, the R-R interval (RRI) was calculated using \u201cfindpeaks,\u201d which is a peak detection algorithm in MATLAB from MathWorks. However, the RRI obtained using this method is not equally spaced; therefore, it was converted to equal intervals using the Akima algorithm for one-dimensional interpolation (sampling interval: 128 Hz) [1] and standardized in the same manner as the EDA data. Furthermore, each preprocessed data was compressed using a vector-quantized variational autoencoder (VQ-VAE) [34] and converted into features. VQ-VAE is a model that extends the variational autoencoder and can express latent variables in a discrete manner. In this study, each model was trained in advance using the physiological signals that were obtained during rest and practice. Each dataset was represented by a 11,383-dimensional latent variable using an encoder, and a discrete representation of the latent variable was obtained by comparing it with the 128 codebooks of these pretrained models. The frequency information of this discrete representation was used as observation information for mMLDA. The mean reconstruction errors for the EDA and RRI data after preprocessing were 0.048 and 0.0033, respectively.For the vision information, we used a 1536-dimensional vector obtained by converting the 60 IAPS images used in the experiment into features using Inception-Resnet-v2 [31]. Inception-Resnet-v2 is a convolutional neural network that has achieved high performance in image recognition and feature extraction tasks. In this study, we used the pretrained model published by Szgedy et al. [31] and used the intermediate layer when the 60 images used in the experiment as input were used as observed information.For the word information, we used a bag-of-words representation of the content entered by the subject during the 50 s of word information input. A bag-of-words representation is a vectorization of information related to frequency of occurrence of words, and the resulting 3626-dimensional frequency information was used as word observation.The number of LDA categories had to be manually determined in advance. The mMLDA used in this study is an extended model of LDA; therefore, the category K\ud835\udc3eKitalic_K of the integrated concept and category K*superscript\ud835\udc3eK^{*}italic_K start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT of each concept had to be determined in advance.The number of categories KIsuperscript\ud835\udc3e\ud835\udc3cK^{I}italic_K start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT for MLDA, which is the interoception category, was set to four. Various models have been proposed to express emotions [13, 32, 27, 4, 33]; however, no consensus has been reached on any of them [20]. Therefore, in this study, the emotion concept, which is considered to be represented in the interoception category, follows the idea that core affect is expressed on a plane consisting of two axes, valence and arousal [27], and we assumed that this category is similar to the space formed by these two axes. Then, we set the number of categories as four, which is the number of quadrants in this space.The number of LDA categories KVsuperscript\ud835\udc3e\ud835\udc49K^{V}italic_K start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT and KWsuperscript\ud835\udc3e\ud835\udc4aK^{W}italic_K start_POSTSUPERSCRIPT italic_W end_POSTSUPERSCRIPT, which are the exteroceptive concepts, is set as 4 and 34, respectively, after training an LDA model with only vision information for each subject, conducting a grid search using the log-likelihood function in the range of 4\u201360, and considering the mode of all subjects. Moreover, the number of categories K\ud835\udc3eKitalic_K in MLDA corresponding to an integrated concept was set to 28 after training the MLDA for each subject using the learning results for each concept as observed information, and similarly conducting a grid search using the log-likelihood function in the range of 4\u201360.The model was trained using two different methods for comparing the subjective emotional reports of the subjects and predicting unobserved information. To compare the reports, as a comparison target, we prepared the proposed model where some information was intentionally deleted. Fig. 4 shows four graphical models, including the proposed model (mMLDA). We used the observed information for the 60 images used in the experiment to first train LDA with only EDA, LDA with only RRI, and MLDA with only EDA and RRI, after which the mMLDA model shown in Fig. 2 was trained. To predict the information, we divided the 60 images used in the experiment into 45 learning data and 15 test data, and we used the learning data to train the mMLDA model. Subsequently, we used the learned model and test data to predict\neach information based on other information.\nThis training was conducted for each subject in both cases. Thus, models were trained for the total number of subjects.The model learning results were evaluated from two perspectives: similarity to subjective emotional reports and prediction accuracy of unobserved information.We verified whether the interoception categories learned by the model were similar to the subjective emotional reports of people. The SAM responses of the subjects were used for the subjective emotional reports. Fig. 5 shows the evaluation of the 60 images used in the experiment, with the vertical and horizontal axes denoting the mean valence and arousal values for all subjects, respectively. This was used as a basis to allocate each quadrant to a category, with the data centroid as the origin. Images were then divided into and classified under the four categories of \u201cpleasant + arousal,\u201d \u201cpleasant + sleepiness,\u201d \u201cunpleasant + arousal,\u201d and \u201cunpleasant + sleepiness.\u201d The number of datasets in each category were 8, 26, 18, and 6, respectively. We then calculated the Rand index between the learning categories in the model of each individual and categories labeled based on the SAM results (SAM categories). The Rand index measures the similarity between two clusters, taking a value between 0 and 1, where 00 indicates that the two clusters do not match in any pair, and 1111 indicates that the clusters match completely. The Rand index for each model (LDA, MLDA, mMLDA) was calculated for the number of subjects; therefore, the differences between the models were tested using the Wilcoxon signed-rank test. The significance level \u03b1\ud835\udefc\\alphaitalic_\u03b1 was set at 0.050.050.050.05, and the significance level was corrected by the number of tests using the Bonferroni correction.We also verified whether emotion concepts were expressed in the categories constructed by mMLDA by comparing the above-mentioned Rand index with chance level. The chance level is the assumption that the four categories were randomly created when calculating the Rand index. The chance level was determined to be 0.560.560.560.56.We used the mMLDA model to determine the prediction accuracy. The 60 images used in the experiment were divided into 45 learning data and 15 test data, and the learning data were used for training to predict unobserved modality information based on a set of modality information in the test data. The numbers of learning and test data for the different categories were set to (5, 23, 13, and 4) and (3, 5, 5, and 2), respectively, after considering the overall number ratio.\nWe used one set of modality information of test data as the observed information (all other modality information were set as 0).\nFor the prediction accuracy index, we determined the Kullback\u2013Leibler (KL) divergence between the predicted distribution and the actual distribution.\nThe predicted distribution means the distribution of the probability of other unobserved modality information obtained by making a prediction from observed information using the learned model.\nThe actual distribution means the distribution of the actual unobserved modality information normalized\nto sum to 1.Moreover, emotional words such as \u201canger\u201d and \u201cjoy\u201d are known to play a particularly important role in concept formation in language [10]. Therefore, for word information, we defined words included in the Japanese evaluation polarity dictionary [17] as emotional words, and we used the following two values: the KL divergence between the predicted and actual distributions for all audio-input words, and among them, the KL divergence between the predicted and actual distributions were reconstructed using only the 1,028 emotional words that were actually uttered. For example, when predicting the word information of only emotional words from physiological information, the actual distribution of emotional words can be expressed as \ud835\udc98n\u2062o\u2062r\u2062mWsubscriptsuperscript\ud835\udc98\ud835\udc4a\ud835\udc5b\ud835\udc5c\ud835\udc5f\ud835\udc5a\\bm{w}^{W}_{norm}bold_italic_w start_POSTSUPERSCRIPT italic_W end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n italic_o italic_r italic_m end_POSTSUBSCRIPT, as shown in the following equation.The KL divergence is an index that measures the difference between two probability distributions, and a smaller value indicates a higher similarity between the distributions. Moreover, we set a chance level in the same manner as when verifying the similarity with subjective emotional reports, and we evaluated the prediction accuracy by examining the occurrence of a significant difference from the chance level. For the chance level, the predicted distribution was assumed to be a uniform distribution, and we used the similarity between the uniform and actual distributions of words calculated using Eq. (III-D2). We also used the paired t-test as the testing method.First, for the similarity with subjective emotional reports, Fig. 6 shows the comparison results of the Rand index for each model, and Table I lists the mean values and standard deviations. In the comparison between each LDA and MLDA (Fig. 6(a)), the Rand index of MLDA was significantly higher (vs EDA:P=3.7\u00d710\u22129<0.01/2\ud835\udc433.7superscript1090.012P=3.7\\times 10^{-9}<0.01/2italic_P = 3.7 \u00d7 10 start_POSTSUPERSCRIPT - 9 end_POSTSUPERSCRIPT < 0.01 / 2, vs RRI:P=1.0\u00d710\u22125<0.01/2\ud835\udc431.0superscript1050.012P=1.0\\times 10^{-5}<0.01/2italic_P = 1.0 \u00d7 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT < 0.01 / 2). This suggests that using multiple sets of physiological information is better as an emotion concept formation. Moreover, a comparison between MLDA and mMLDA did not show a significant difference (P=0.73>0.05\ud835\udc430.730.05P=0.73>0.05italic_P = 0.73 > 0.05). Thus, in terms of category similarity, the influence of the presence of vision and word information is unclear. Next, we compared the Rand index with the chance level to determine whether the interoceptive sensation categories constructed by mMLDA could be considered as emotion concepts. The mean Rand index of mMLDA was 0.750.750.750.75, which was higher than the chance level (0.560.560.560.56). This suggests that an emotion concept was formed in this study that is more in line with the subjective emotional reports of people than a random emotion concept.Fig. 7 shows an example of the emotion concept space of a subject. This refers to a space in which the probability of belonging to each of the four categories, expressed by zIsuperscript\ud835\udc67\ud835\udc3cz^{I}italic_z start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT in Fig. 2, is compressed into two dimensions using t-distributed Stochastic Neighbor Embedding and visualized. As shown in Fig. 7(a), the category \u201cunpleasant + arousal\u201d can be confirmed in the conceptual space created by LDA (EDA), at the bottom-right side of the figure; however, the results corresponding to the other three categories are mixed together. In contrast, in Fig. 7(b), the categories are marginally separated in LDA (RRI); however, the results for \u201cpleasant + sleepiness\u201d were mixed with those of the other categories. In Fig. 7(c), the four categories are clustered separately for MLDA, and high-accuracy classification was achieved. Furthermore, in Fig. 7(d), the categories are clearly separated in mMLDA. Moreover, data for images belonging to \u201cpleasant,\u201d \u201cunpleasant, \u201d \u201carousal,\u201d and \u201csleepiness\u201d are clustered at the bottom-right, top-left, bottom-left, and top-right sides, respectively, indicating the valence and arousal axes. Qualitative evaluation of the space of 29 participants showed that 10 subjects visually displayed the valence and arousal axes on the conceptual space of mMLDA, which was higher than the 4 subjects for the MLDA.Next, to determine the prediction accuracy of unobserved information, we used the model trained with learning data, and show the results after comparing the KL divergence for the case of the predicted distribution when unobserved modality information was predicted based on one set of modality information in the test data with the KL divergence for the case of a uniform distribution in Figs. 8, 9, and 10. The mean and standard deviation values of the results are listed in Table II. For the EDA prediction (Fig. 8(a)), the KL divergence for the case of the predicted distribution from the vision and word information was significantly smaller than that for the case of a uniform distribution (P=6.7\u00d710\u22127<0.01/3\ud835\udc436.7superscript1070.013P=6.7\\times 10^{-7}<0.01/3italic_P = 6.7 \u00d7 10 start_POSTSUPERSCRIPT - 7 end_POSTSUPERSCRIPT < 0.01 / 3 and P=2.8\u00d710\u22127<0.01/3\ud835\udc432.8superscript1070.013P=2.8\\times 10^{-7}<0.01/3italic_P = 2.8 \u00d7 10 start_POSTSUPERSCRIPT - 7 end_POSTSUPERSCRIPT < 0.01 / 3, respectively). Similarly, for the RRI prediction (Fig. 8(b)), the KL divergence for the case of the predicted distribution from the vision and word information was significantly smaller that for the case of a uniform distribution (P=5.9\u00d710\u22125<0.01/3\ud835\udc435.9superscript1050.013P=5.9\\times 10^{-5}<0.01/3italic_P = 5.9 \u00d7 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT < 0.01 / 3 and P=2.8\u00d710\u22124<0.01/3\ud835\udc432.8superscript1040.013P=2.8\\times 10^{-4}<0.01/3italic_P = 2.8 \u00d7 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT < 0.01 / 3, respectively). This showed that unobserved physiological information can be predicted using the learned categories. No significant difference was observed between vision and word information in either EDA or RRI (P=0.38<0.05/3\ud835\udc430.380.053P=0.38<0.05/3italic_P = 0.38 < 0.05 / 3 and P=0.27<0.05/3\ud835\udc430.270.053P=0.27<0.05/3italic_P = 0.27 < 0.05 / 3, respectively).In the vision information prediction (Fig. 9), no significant difference was observed between the predicted distribution from physiology and word information and the uniform distribution (P=0.26>0.05/3\ud835\udc430.260.053P=0.26>0.05/3italic_P = 0.26 > 0.05 / 3 and P=0.041>0.05/3\ud835\udc430.0410.053P=0.041>0.05/3italic_P = 0.041 > 0.05 / 3, respectively). Similarly, no significant difference was observed between the physiology and word information (P=0.062<0.05/3\ud835\udc430.0620.053P=0.062<0.05/3italic_P = 0.062 < 0.05 / 3). Therefore, predicting vision information from unobserved data using physiology and word information using this model is likely to be difficult.\nFor the word information prediction (Fig. 10(a)), the KL divergence for the case of the predicted distribution from physiology and vision information was significantly smaller than that for the case of a uniform distribution (P=5.3\u00d710\u22127<0.01/3\ud835\udc435.3superscript1070.013P=5.3\\times 10^{-7}<0.01/3italic_P = 5.3 \u00d7 10 start_POSTSUPERSCRIPT - 7 end_POSTSUPERSCRIPT < 0.01 / 3 and P=6.7\u00d710\u22127<0.01/3\ud835\udc436.7superscript1070.013P=6.7\\times 10^{-7}<0.01/3italic_P = 6.7 \u00d7 10 start_POSTSUPERSCRIPT - 7 end_POSTSUPERSCRIPT < 0.01 / 3, respectively). This shows that unobserved word information can be predicted using the learned model. Moreover, even when reconstructing using only the emotional word results for the prediction results (Fig. 10(b)), the KL divergence was significantly smaller for each when compared to the case of the uniform distribution (P=3.0\u00d710\u22126<0.01/3\ud835\udc433.0superscript1060.013P=3.0\\times 10^{-6}<0.01/3italic_P = 3.0 \u00d7 10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT < 0.01 / 3 and P=4.2\u00d710\u22126<0.01/3\ud835\udc434.2superscript1060.013P=4.2\\times 10^{-6}<0.01/3italic_P = 4.2 \u00d7 10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT < 0.01 / 3, respectively). No significant difference was observed between physiology and vision information (P=0.21<0.05/3\ud835\udc430.210.053P=0.21<0.05/3italic_P = 0.21 < 0.05 / 3 and P=0.67<0.05/3\ud835\udc430.670.053P=0.67<0.05/3italic_P = 0.67 < 0.05 / 3, respectively).Moreover, Fig. 11 shows the prediction results for word information by category, and Table III lists the mean and standard deviation values for each category. Fig. 11 shows that the difference from the uniform distribution is greatest in the category of \u201cpleasant + sleepiness\u201d for both physiology and vision information. This suggests that the prediction accuracy for images belonging to the \u201cpleasant + sleepiness\u201d category was high. Note that statistical analysis was not possible owing to the small sample size for each category.In terms of similarity with subjective emotional reports, no significant difference was observed between MLDA and mMLDA.\nHowever, as shown in Fig. 7, for certain subjects, the valence and arousal axes clearly appeared in the concept space. When actually investigating the physiological reaction data of the subjects shown in Fig. 7, the EDA of the images belonging to the \u201cunpleasant + arousal\u201d category at the bottom-right side of Fig. 7(a) was elevated as compared to the others (Figs. 12(a, c, e, g)). EDA is known to have a correlation with arousal level [19], and this may have resulted in the response in the \u201cunpleasant + arousal\u201d category. Moreover, Fig. 7(b) shows that the results are clustered to a certain extent in each category. In the case of these subjects, RRI gradually decreased for \u201cunpleasant + arousal\u201d and remained flat or gradually increased for the other categories (Figs. 12(b, d, f, h)). This suggests that the categories were formed based on the magnitude and amplitude of this fluctuation as well as heart rate. The MLDA categories that were affected by these two physiological reactions clustered together by category, as shown in Fig. 7(c), and the Rand index was also significantly increased. Furthermore, the addition of vision and word information may have resulted in two-dimensional orthogonal axes appearing as shown in Fig. 7(d) as a result of understanding the relationships between categories based on the external environment and past experiences of individuals. The number of subjects in which orthogonal axes appeared in such a concept space was greater in mMLDA than in MLDA, suggesting that both interoceptive and exteroceptive information exhibited an influence on concept formation.Regarding the prediction of physiology and word information, Figs. 8 and 10 show that unobserved physiology and word information can be predicted from other modality information. This was not possible with the MLDA in the present study and is a result that demonstrates the superiority of mMLDA.Furthermore, the results of the KL divergence based on the category in the word information prediction (Fig. 11) showed that the KL divergence of the \u201cpleasant + sleepiness\u201d category was the smallest. Fig. 13 shows the numbers of emotional words uttered per image and unique emotional words uttered by category. The figure exhibits a relatively large difference between the number of emotional words uttered per image and number of unique emotional words uttered per image in the \u201cpleasant + sleepiness\u201d category. This signifies that each subject repeatedly input the same emotional words for images belonging to the \u201cpleasant + sleepiness\u201d category. This may have enabled the learning of the co-occurrence of words and increased the accuracy of word information in unobserved data. However, the accuracy of the \u201cunpleasant + arousal\u201d category, which similarly exhibited a large difference between the numbers of emotional words uttered per image and unique emotional words uttered, was not as high as the other categories. As shown in Figs. 12(e) and (f), the \u201cunpleasant + arousal\u201d category exhibited the largest physiological reaction, which may have had an influence; however, in the present study, the amount of data was small, and must be increased to conduct a more detailed analysis.In this study, 45 images were used as the learning data; however, increasing the sample size may enable more accurate predictions. Moreover, although physiological information could be predicted from vision information, vision information could not be predicted from physiological information. This is speculated to be due to the fact the vision has numerous feature dimensions (1536 dimensions), making it difficult to predict vision information.\nMoreover, the fact that physiological information could be predicted indicates that, although physiological reactions that occur in response to a specific stimulus differ in magnitude and reaction type for each individual, the reaction is relatively consistent within an individual, and the physiological reactions evoked by different visual information can be similar in terms of features.As these results, the proposed model can be used in robots to acquire emotion concepts as well as objects and places, and is expected to be applied to cases such as understanding the emotions of others in communication situations.Finally, a limitation of this study is that the individuality of emotion concepts cannot be evaluated. The evaluation in this study involved setting four categories using valence and arousal to compare the subjective evaluation of the emotions of subjects. However, the emotion concepts of an individual are inherently not fixed in nature, but rather are shaped by the experiences of each person. Therefore, not everyone experiences the four categories defined in valence and arousal. Nevertheless, as emotions are shared in society, there are similarities with the evaluation categories set in this study. In the future, we may be able to discuss the different categories each individual possesses using mMLDA to set the number of higher-likelihood emotion concept topics and analyzing each topic. However, currently, no method exits to prove whether this truly matches the emotion concept of each individual; therefore, continued discussion will be needed.In this study, we investigated an emotion concept formation model based on an integrated categorization of interoceptive, exteroceptive, and word information in image stimuli. We used an mMLDA model, which was trained using human data when image stimuli were presented. The results showed that the model matched over 70%percent7070\\%70 % of the categories that were subjectively reported by the subjects. This result exceeded the chance level; therefore, we concluded that emotion concept formation can be explained by the proposed model. Moreover, predictions of the unobserved information using a model trained using the learning data exceeded the chance level; therefore, this model could predict unobserved information, which is an important function as a concept. This model can be used to acquire emotion concepts as well as objects and places, and is expected to be applied for the understanding of the emotions of others in communication situations. In the future, we plan to study models related to exteroception other than visual perception and evaluate the individuality of emotion concepts.We would like to thank Professor Takayuki Nagai for useful discussions.\nThis work supported in part by JSPS KAKENHI JP21H04420, JP22H05082 and JST, ACT-X JPMJAX21AL, Japan.\nThis work involved human subjects or animals in its research. Approval of all ethical and experimental procedures and protocols was\ngranted by the Research Ethics Committee, Nara Institute of Science and Technology.",
    "3": "Knowledge graphs are useful tools to organize, recommend and sort data. Hierarchies in knowledge graphs provide significant benefit in improving understanding and compartmentalization of the data within a knowledge graph. This work leverages large language models to generate and augment hierarchies in an existing knowledge graph. For small (<<<100,000 node) domain-specific KGs, we find that a combination of few-shot prompting with one-shot generation works well, while larger KG may require cyclical generation. We present techniques for augmenting hierarchies, which led to coverage increase by 98% for intents and 99% for colors in our knowledge graph.Knowledge graphs (KG) are widely used in industry to understand user behavior and provide contextual recommendations (figure 1) and search results. At Adobe, we utilize a knowledge graph to understand users\u2019 creative intent and recommend Adobe assets based on the intent. While the original knowledge graph had over 12000 intent nodes and over 100000 nodes in total, the original taxonomy was mostly flat, lacking substantial hierarchies that could amplify the semantic significance between nodes and drive additional intent-based recommendation use cases.In this work, we present a novel approach to automatically generate intricate graph hierarchies in KGs by leveraging neural transformers. We enhance the structure of our graph by generating hierarchies for both intent (what an Adobe user wants to accomplish, e.g.\u00a0create a child\u2019s birthday card or a banner for their cafe\u2019s website) and color node types, resulting in a significant increase in hierarchy coverage: 98% for intent and 99% for color. Hierarchies have key benefits to our users.\nOrganizational Structure: Hierarchical relationships makes it easier to navigate and comprehend the KG. Hierarchies help maintain order and provide a clear understanding of how different concepts are related to each other.\nSemantic Relationships: Rich intent hierarchies allow us to capture the semantic relationships between concepts. They also help us unlock key features, such as powering browse and SEO relationships.\nScalability and Flexibility: Top level categories allow for easier addition of new intents without disrupting the overall structure as the KG grows.Knowledge graphs are widely used in industry in a variety of roles, from providing social media recommendations (Ying et\u00a0al., 2018; Ugander et\u00a0al., 2011) to providing entity linking and semantic information between concepts (Dong et\u00a0al., 2014; Stokman and Vries, 1988). With recent improvements in attention-based networks, specifically large transformers (OpenAI, 2023; Touvron et\u00a0al., 2023), there has been academic focus towards grounding language models with KGs (Pan et\u00a0al., 2023), thereby providing semantic reasoning and generation based on the KG information. Recent works also investigate automated generation and completion of KGs using large transformer models (Meyer et\u00a0al., 2023; Carta et\u00a0al., 2023). They utilize language models like ChatGPT to add new nodes to subsets of the graph.\nWhile most works focus on adding additional nodes to KGs, our work focuses on augmenting the semantic relationships of existing nodes in the graph using large transformer models. We generate rich hierarchies and associations inside the graph, something that is novel to the field.First we create top level (L1subscript\ud835\udc3f1L_{1}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT) categories for a specific class of nodes (e.g.\u00a0intent or color). L1subscript\ud835\udc3f1L_{1}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT categories can be selected by domain experts or by a language model. They need to be broad and expansive, as our aim is to transition from a general intent to a more specific one, progressing through multiple levels. We created the L1subscript\ud835\udc3f1L_{1}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT intent nodes by examining Adobe Express frequent queries and their intents, Adobe Stock content categories, and the Google open-source product type taxonomy (Google, [n.\u2009d.]). This resulted in 26 L1subscript\ud835\udc3f1L_{1}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT intent categories (e.g.\u00a0Business and Industry, Travel, Shopping, Beauty and Wellness, Health). These overlap with standard taxonomies but comprise a subset relevant to Adobe Stock and Express users.\nAfter establishing the L1subscript\ud835\udc3f1L_{1}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT categories, a classifier module assigns all KG nodes to one or more L1subscript\ud835\udc3f1L_{1}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT categories. Then a generator module enhances the existing hierarchy (if any) with the newly added nodes. Finally, a scalable pipeline auto-ingests the new hierarchies into the KG and queries the KG at inference time. To generate our hierarchies, we utilize two modules (Figure 3):Classifier Module: The classifier module takes all nodes to be added to hierarchy and classifies them into one or more of the L1subscript\ud835\udc3f1L_{1}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT candidate classes. We found large language models to be better at few-shot classification than their smaller variants.Generator Module: The generator module runs in a loop for each L1subscript\ud835\udc3f1L_{1}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT category. The generator takes the existing hierarchy for that L1subscript\ud835\udc3f1L_{1}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT category (just the L1subscript\ud835\udc3f1L_{1}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT node if no hierarchy exists) and adds all the candidate nodes to generate the updated category hierarchy. We found one-shot hierarchy generation using large language models to be the best approach (\u00a73.2).In order to use the classification module, we do few-shot learning in which we provide the language model (GPT4 with a 32K context (OpenAI, 2023)) with a few classification examples and a strict prompt. A sample prompt we used is \u201cYou are a taxonomist, classify the given node to one or more of the provided categories. If you think the category should be its own thing, return Other. Please return a dictionary every time.\u201d\nWith the prompt, we provide a few sample nodes, the categories and an output prediction. Based on a few rounds of samples, we then provide the true candidates for classification to the model. Similar to other approaches in the industry (Parnami and Lee, 2022; Song et\u00a0al., 2022), we see a significant boost of 12% in accuracy by doing few-shot learning compared to zero-shot classification.Once the candidates are categorized, we experimented with two techniques in the generation module to create the updated hierarchy. Cyclical Generation: Generate each level of the category in a loop. This means that L2subscript\ud835\udc3f2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT level nodes are added first, then L3subscript\ud835\udc3f3L_{3}italic_L start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT and so on. This is needed when the existing hierarchy is large and cannot fit in the model context.\nOne-Shot Generation: All candidate nodes are added to the hierarchy in a single pass, without any cyclical generation. We found that this approach produced better results with smaller (<<<100,000 node) taxonomies.Cyclical Generation\nIn the cyclical generation approach, we follow a cyclical pattern to generate each level of the hierarchy for an L1subscript\ud835\udc3f1L_{1}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT category and its children.From the candidate set of nodes at level Lisubscript\ud835\udc3f\ud835\udc56L_{i}italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, classify nodes that belong to that level. Utilize the existing nodes at that level (i.e.\u00a0any nodes already present in the hierarchy) to help the language model via a few-shot approach.The nodes not categorized to be part of level Lisubscript\ud835\udc3f\ud835\udc56L_{i}italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT will become part of lower levels (Li+1..nL_{i+1..n}italic_L start_POSTSUBSCRIPT italic_i + 1 . . italic_n end_POSTSUBSCRIPT).Pass the remaining nodes as well as the existing hierarchy to the generator module to create the new updated hierarchy for level Lisubscript\ud835\udc3f\ud835\udc56L_{i}italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. The generator module will attempt to categorize and place each of the remaining nodes under one of the Lisubscript\ud835\udc3f\ud835\udc56L_{i}italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT nodes.For each of the Lisubscript\ud835\udc3f\ud835\udc56L_{i}italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT nodes and their hierarchy, repeat the process in a recursive manner to fine-tune the hierarchies. The process stops when either a specified depth (Lisubscript\ud835\udc3f\ud835\udc56L_{i}italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) is reached or all nodes have been added to the hierarchy.While the cyclical approach is useful, especially for larger graphs, we saw several drawbacks with it when generating our intent hierarchies.Error Propagation: LLMs can hallucinate or generate incorrect structured content. Having multiple steps in the generation process can lead to error propagation through the chain. This is the biggest issue with a recursive approach.The Other Conundrum: LLMs are bad at placing nodes into the \u201cOther\u201d category. This means that most nodes were assigned into a level\u2019s category (Li\ud835\udc56{}_{i}start_FLOATSUBSCRIPT italic_i end_FLOATSUBSCRIPT) rather than being placed into the Other category (to be a part of the L3 and lower levels).Order Importance: Whether we pass nodes in a batch or one at a time for classification, the order of nodes plays a huge difference in the categorizing. For example, if \u201cbirthday party\u201d was categorized as an Li\ud835\udc56{}_{i}start_FLOATSUBSCRIPT italic_i end_FLOATSUBSCRIPT first and then the node \u201cbirthday\u201d is shown to the LLM, it often incorrectly categorizes \u201cbirthday\u201d as a child of \u201cbirthday party\u201d due to their similarities. Additional checks and another overall pass is required to fix the categorizations. One-shot generation (see below) alleviates these issues.One-Shot Generation\nIn the one-shot generation process, we provide all the nodes to be added as well as the full existing hierarchy to the LLM and allow it to generate the L2subscript\ud835\udc3f2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, L3subscript\ud835\udc3f3L_{3}italic_L start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT and lower categories with just a few examples provided. This approach worked well when there was an existing, partial hierarchy in place to guide the language model\u2019s intuition. If a large number of candidates need to be added to the hierarchies, batched generations followed by an overall pass where the language model has the chance to correct any errors is utilized. For our domain-specific use cases, we found full, one-shot generation to be more viable since our taxonomy is relatively small (<15000absent15000<15000< 15000 intent nodes and <3000absent3000<3000< 3000 color nodes).Updating the Graph For new intents, the above approach is used to integrate them into the KG. When creating intents for a new domain (e.g.\u00a0Adobe app tools), a subgraph is generated for the new domain and then merged into the existing KG. One algorithm improvement, suggested by an anonymous reviewer, is to examine each subgraph in the generated graph and query the LLM as a third step if it looks good. This uses the LLM for evaluation and updating, instead of generating.Key Failure CasesOne of the key failure cases we saw with using LLM for hierarchical generation is confusion of similar semantic nodes. As shown in Fig. 5, LLMs can sometimes get confused with related concepts and ascribe a parent-child relationship. While mom dad and romantic message are related to the concept of love, they should be children of marriage. We found few-shot prompting and review pass-throughs (asking LLM to find flaws in the hierarchy once generated) to be the most effective techniques in preventing and correcting these errors.Statistics on the hierarchical KG generated using one-shot generation and few-shot prompting are summarized below (\u2018Lower\u2019 indicates nodes in L5subscript\ud835\udc3f5L_{5}italic_L start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT or below categories). We evaluated the KG hierarchies through a human-in-the-loop approach. We provided a graphical interface to identify nodes that are incorrectly positioned and to offer suggestions for enhancement. We engaged 16 Adobe-internal domain experts to review the hierarchies within each L1subscript\ud835\udc3f1L_{1}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT category for both intent and color nodes. The hierarchies were found to be relevant >>>95% of the time. Lower levels were spot-checked for accuracy. Identified errors were then manually corrected.\nThe ultimate evaluation will be in leveraging the KG hierarchies in search and recommendation features. The non-hierarchical graph already provides related search style links between Express SEO pages (figure 1) and powers null and low recovery in Adobe Express by mapping queries and templates to intents. These use cases will be enhanced by using the hierarchy to provide additional links, to type the links, and to provide back-off through the hierarchy. The Express SEO color pages represent the first user-facing application of the hierarchical graph.",
    "4": "We study the effectiveness of a simple approach to develop a small base language model (LM) starting from an existing large base LM: first inherit a few transformer blocks from the larger LM, and then train this smaller model on a very small subset (0.1%) of the raw pretraining data of the larger model. We call our simple recipe \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune and first demonstrate it for building a small base LM with 1.5B parameters using 1B tokens (and a starting few layers of larger LM of 3B parameters); we do this using a single A6000 GPU for less than half a day. Across 9 diverse evaluation datasets as well as the MMLU benchmark, the resulting model compares favorably to publicly available base models of 1B-2B size, some of which have been trained using 50-1000 times more tokens.We investigate \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune in a slightly different setting where we train small LMs utilizing larger LMs and their full pre-training dataset. Here we show that smaller LMs trained utilizing some of the layers of GPT2-medium (355M) and GPT-2-large (770M) can effectively match the val loss of their bigger counterparts when trained from scratch for the same number of training steps on OpenWebText dataset with 9B tokens. We analyze \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune with extensive experiments and demonstrate it efficacy on diverse settings. Our code is available at https://github.com/sanyalsunny111/LLM-Inheritune.Pre-training Small Base LMs with Fewer Tokens\n\n\n\nSunny Sanyal\u2020\u2020thanks: The corresponding author can be reached out at sanyal.sunny@utexas.edu.\n\nUT Austin\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\n\nSujay Sanghavi\n\nUT Austin\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\n\nAlexandros G. Dimakis\n\nUT Austin\n\n Scaling large language models (LLMs) is very challenging both in terms of the required computation resources but also in terms of the size and quality of the available datasets. The standard pre-training recipe involves working with multi-billion parameter models and training with trillion(s) of tokens Kaplan et\u00a0al. (2020); Hoffmann et\u00a0al. (2022); Peiyuan\u00a0Zhang and Lu (2023); Touvron et\u00a0al. (2023b). In this paper we investigate what can be achieved when pre-training small base language models (LMs) of size 1.5B parameters with just 1 billion tokens of pre-training data. A base LM is a decoder style LLM that is solely trained for next token prediction without any subsequent instruction tuning, RLHF (reinforcement learning with human feedback), etc. In this paper, we focus on crafting/developing small base LMs from existing large reference LMs, when we have only a very small fraction of the pre-training data of the reference model available in public. This setting is important because there are many large models for which the weights are available, but the full pre-training datasets are not. Our method, is very simple: we keep the first few transformer blocks (layers) of the large reference LM and perform pre-training on the small available dataset, iterating for multiple epochs over the data. Our method is efficient in data and compute: the training was performed with 1 A6000 GPU in less than 12 hours (half the time (Geiping and Goldstein, 2022) used for BERT pre-training from scratch). Next we thoroughly analyzed our recipe in three different settings 1) where we start from large and more performant reference models, 2) we train model of different sizes with the available 1B data, 3) we trained our 1.5 billion parameter model using slightly larger subsets (1-5%) of pre-training data, ranging from 10 to 50 billion tokens. We then compared its performance with that of a model trained on 1B tokens of available data, but repeated for multiple epochs.In summary, our key findings are as follows.We introduce a pre-training technique called \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune. This method involves training a small size target model, extracted from a larger reference LM. We use OpenLLaMA-3B as our reference model and utilize few of the transformer blocks (layers) to obtain our target model of size 1.5B. Subsequently the target model was further trained on 1\u2062B1\ud835\udc351B1 italic_B of training data for 8888 epochs. Our small base LM achieves 89% of average downstream accuracy on 9 different datasets and 94% of MMLU (5-shot) score compared to the reference model which is double in size. Further, our small model achieves comparable performance on average downstream accuracy of 9 tasks and MMLU 5-shot score compared to publicly available baseline models pre-trained with 50-300 times more data as shown in Figure 1. We empirically analyze \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune with a 50B subset of pre-train data and larger reference models to demonstrate the generality of our approach.We analyze \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune in a slightly different setting where we assume full access to the pre-training data.\nWe observe that a much smaller target model can be extracted if we use the full pre-training data. We ran controlled experiments with GPT2-large and GPT2-medium LLMs. Utilizing \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune we show that for GPT2-large we can keep 50% of the layers and 45% of parameters while for a GPT2-medium, we keep 33% layers and 28% parameters without compromising the validation loss (log perplexity) as shown in Table 6. Intriguingly, we also observe that these smaller models derived with \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune exhibit lower validation loss to their same-sized counterparts are trained from scratch for 2x the number of training steps. Moreover, these smaller models derived with \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune exhibit a similar convergence pattern to their larger counterparts, as depicted in Figure 5.In this section, we delve into the details of our proposed pre-training approach, \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune. Here we discuss \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune methodology in a setting where a small fraction of the pre-training data is available along with an existing large base LM. Later, we also investigate \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune where we assume full access to the pre-training data in Section 4.We assume that there exists a pre-trained reference model \u2133refsubscript\u2133ref\\mathcal{M}_{\\text{ref}}caligraphic_M start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT , comprising k\ud835\udc58kitalic_k layers, represented by \u03b8ref={\u03b80,\u03b81,\u2026,\u03b8k\u22121}subscript\ud835\udf03refsubscript\ud835\udf030subscript\ud835\udf031\u2026subscript\ud835\udf03\ud835\udc581\\theta_{\\text{ref}}=\\{\\theta_{0},\\theta_{1},\\ldots,\\theta_{k-1}\\}italic_\u03b8 start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT = { italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_\u03b8 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_\u03b8 start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT } trained with \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}_{\\text{train}}caligraphic_D start_POSTSUBSCRIPT train end_POSTSUBSCRIPT, however this full training data is unavailable and we only have a very small subset \ud835\udc9f^trainsubscript^\ud835\udc9ftrain\\hat{\\mathcal{D}}_{\\text{train}}over^ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT train end_POSTSUBSCRIPT. Moreover we assume that the distribution is preserved such that \ud835\udc9f^train\u223c\ud835\udc9ftrainsimilar-tosubscript^\ud835\udc9ftrainsubscript\ud835\udc9ftrain\\hat{\\mathcal{D}}_{\\text{train}}\\sim\\mathcal{D}_{\\text{train}}over^ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT train end_POSTSUBSCRIPT \u223c caligraphic_D start_POSTSUBSCRIPT train end_POSTSUBSCRIPT. We make no special efforts in selecting a high quality subset rather \ud835\udc9f^trainsubscript^\ud835\udc9ftrain\\hat{\\mathcal{D}}_{\\text{train}}over^ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT train end_POSTSUBSCRIPT is just a random subset of all the domains of \ud835\udc9ftrainsubscript\ud835\udc9ftrain{\\mathcal{D}}_{\\text{train}}caligraphic_D start_POSTSUBSCRIPT train end_POSTSUBSCRIPT.We initialize the target model \u2133tgtsubscript\u2133tgt\\mathcal{M}_{\\text{tgt}}caligraphic_M start_POSTSUBSCRIPT tgt end_POSTSUBSCRIPT, with the first n\ud835\udc5bnitalic_n layers of the \u2133refsubscript\u2133ref\\mathcal{M}_{\\text{ref}}caligraphic_M start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT; thus the weights of \u2133tgtsubscript\u2133tgt\\mathcal{M}_{\\text{tgt}}caligraphic_M start_POSTSUBSCRIPT tgt end_POSTSUBSCRIPT are {\u03b80,\u03b81,\u2026,\u03b8n\u22121}subscript\ud835\udf030subscript\ud835\udf031\u2026subscript\ud835\udf03\ud835\udc5b1\\{\\theta_{0},\\theta_{1},\\ldots,\\theta_{n-1}\\}{ italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_\u03b8 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_\u03b8 start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT }. The prediction head and token embeddings are also inherited from \u2133r\u2062e\u2062fsubscript\u2133\ud835\udc5f\ud835\udc52\ud835\udc53\\mathcal{M}_{ref}caligraphic_M start_POSTSUBSCRIPT italic_r italic_e italic_f end_POSTSUBSCRIPTfor the given number E\ud835\udc38Eitalic_E of epochs on the available training data \ud835\udc9f^trainsubscript^\ud835\udc9ftrain\\hat{\\mathcal{D}}_{\\text{train}}over^ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT train end_POSTSUBSCRIPT.We now outline the setup and the main results for our first setting where using a very small fraction of pre-training data i.e. just 1B tokens (1000x small compared to original training data) to craft a 1.5B target LM which is competitive with existing base LMs both trained from scratch or derived using existing large base LMs of similar size.We utilize a 1B subset111https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample of the Redpajama v1 dataset Computer (2023) which originally consists of 1T tokens from 7 different domains namely common crawl, c4, wikipedia, books, arxiv papers, github and stackexcahnge. We combine the dataset in the proportions suggested in LLaMA by Touvron et\u00a0al. (2023a), which is also the same set for proportions that was used for pretraining our reference LM: OpenLLAMA-3B. Our pre-training data thus constitutes a randomly selected 0.1% subset of OpenLLAMA-3B\u2019s training dataset. Next for an extended evaluation to analyze the effects of \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune while using a larger proportion of the same data we randomly sample 50B subset (i.e. 5% subset) from Redpajama v1 dataset.We use OpenLLaMA-3B version1 Geng and Liu (2023) as the reference model for our main results to efficacy of the \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune recipe. This model has k=26\ud835\udc5826k=26italic_k = 26 layers; we report comparisons for a choice of n=13\ud835\udc5b13n=13italic_n = 13. That is, our model derived using \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune has 13 transformer blocks222We also include a study of the performance for other choices of n\ud835\udc5bnitalic_n in Figure 2.. We train for 8 epochs (where each epoch uses all the 1B tokens) with a batch size of 131K tokens per batch. The choice of training epochs are made based on analysis done in Figure 4. We use lit-gpt framework AI (2023) for training all small base LMs discussed in this paper. Further discussions on the training hyper-parameters can be found in the appendix. For an extended evaluation of our results we also employ larger reference LMs namely LLaMA2-7B Touvron et\u00a0al. (2023b) and OpenLLaMA-7B version 1 Geng and Liu (2023).Table 2 describes the models we compare against; all of them are publicly available checkpoints of similar size to our 1.5B derived model, following related prior work Javaheripi et\u00a0al. (2023); Xia et\u00a0al. (2023); Li et\u00a0al. (2023); Peiyuan\u00a0Zhang and Lu (2023). However there are a few differences between them in how they are pre-trained.We use pre-training data as a key criteria for selecting the baseline models. We pre-dominantly select publicly available baseline models based on their number of parameters and pre-training data i.e. redpajama. As the quality of the pre-training data plays a key role in model development. For the purpose of fair comparison we choose MPT-1.3B Author (s), Sheared LLaMA=1.3B Xia et\u00a0al. (2023) trained with redpajama. Next we also include models OPT-1.3B Zhang et\u00a0al. (2022) and Pythia-1.3B Biderman et\u00a0al. (2023) as these models are pre-trained with dataset which are known to be close with redpajama. Sheared LLaMA-1.3B is developed with targeted structural pruning with 0.4B tokens and continual pre-training using 50B data. Pruning with training has shown to have some added benefits towards overall generalization Jin et\u00a0al. (2022). Therefore it\u2019s a bit unfair to directly compare a pruning and continual training method with our recipe which can be better classified as a model initialization technique. However both Shearing and \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune start training using an existing model and because of that we anyway make the comparison while treating it as a special case that we discuss separately in Section 3.2.In this study, we use few-shot accuracy particularly 0-shot and 5-shot accuracy on multiple different downstream tasks to measure the quality of base LMs. This is a standard practice of evaluating pre-trained LLMs and has been done in several prior works Brown et\u00a0al. (2020); Touvron et\u00a0al. (2023a); Javaheripi et\u00a0al. (2023); Biderman et\u00a0al. (2023). Our evaluation methodology spans downstream tasks across four distinct genres: commonsense reasoning, natural language understanding, factuality, and natural language inference. We evaluate commonsense reasoning using 0-shot performance on 5 diverse datasets\u2013PIQA Bisk et\u00a0al. (2020), BOOLQ Clark et\u00a0al. (2019), WINOGRANDE Sakaguchi et\u00a0al. (2020), WINOGRAD Kocijan et\u00a0al. (2020) and LOGIQA Liu et\u00a0al. (2020). We evaluate language understanding using 5-shot performance on massive multitask language understanding dataset (MMLU) Hendrycks et\u00a0al. (2020). Factuality is a key concern in LLMs and we evaluate factuality using 0-shot performance on TruthfulQA Lin et\u00a0al. (2022) data. Finally we evaluate language inference capabilities using 0-shot performance on MNLI Bowman et\u00a0al. (2015), QNLI Wang et\u00a0al. (2018) and WNLI Wang et\u00a0al. (2018) datasets. For the entire evaluation we use the lm eval harness framework Gao et\u00a0al. (2023).\nWe provide a visual summary of our main results in Figure 1, accompanied by a comprehensive evaluation of performance, broken down by tasks, in Table 3.We compare against base LMs of size similar to ours but which have been trained from scratch. As seen in Table 2, this lack of advantage is reflected in the remarkably larger number of tokens they are trained on. We show this comparison mainly to illustrate that when one is fortunate enough to have a large reference base model and a subset of it\u2019s pre-training data, inheriting the target size base LM with a subset of the layers of the reference LM makes the training remarkably more sample efficient than pre-training from scratch. From Table 2, it is evident that our 1.5B model, developed using \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune, excels in 7 out of 10 individual tasks. It achieves a score of 90% or higher compared to the reference language model, which is twice its size and trained with 1000 times more data, or it outperforms at least two other base LMs of similar size trained with 50-300 times more data. Scores deemed favorable by us have been highlighted in bold. Next we compare our small LM with MPT-1.3B333https://huggingface.co/mosaicml/mpt-1b-redpajama-200b model trained from scratch with 200B tokens of redpajama and observe that we match 97% accuracy in all 9 downstream tasks and matches the MMLU (5-shot) score. To this end we compare with 2 small base LMs (OPT-1.3B and Pythia-1.3B) trained with large data corpses other than redpajama and show that we outperform both in MMLU (5-shot) score and perform comparably in other 9 datasets.Our training recipe can be better classified as an initialization technique than a pruning technique. There are some major differences of our recipe with pruning. a) Pruning does not always require re-training Li et\u00a0al. (2020); Xia et\u00a0al. (2022), whereas any initialization based recipe requires training. However, some prior works opt to continue train after pruning as an additional step, such as shearing Xia et\u00a0al. (2023). b) Typically, pruning techniques adhere to the \"train large, then compress\" paradigm Li et\u00a0al. (2020), which requires starting from larger models, whereas \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune is a training recipe that pre-trains a model from ground up, utilizing a better initialization than random. c) Targeted structural pruning is a carefully designed technique based on some weight update criteria hence it requires a lot of compute to perform, Shearing Xia et\u00a0al. (2023) took 8 GPUs, 0.4B tokens and 3k steps for just pruning. Later it also continually trains the pruned model for 50B tokens using 16 GPUs. Our recipe performs training with 1 A6000 GPU for less than 12 hours using a total 1B data to develop a small base LM. Overall although it\u2019s not fair to compare a targeted structural pruning technique with an initialization recipe but for the sake of completeness we still perform the comparison. Here in Table 2 we observe that \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune shows competitive scores compared to Sheared LLaMA-1.3B in MMLU, Winograd, Boolq, QNLI and truthfulqa (i.e. 5/10 tasks) despite being trained with 50 times less data and much larger compute requirement.For the results in the previous section we only considered a single choice n=k/2\ud835\udc5b\ud835\udc582n=k/2italic_n = italic_k / 2 i.e. half the layers \u2013 for the size of the smaller model. In this section we do a preliminary investigation into \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune with different choices of n\ud835\udc5bnitalic_n (but the same 1B token dataset). All models in this section also use the OpenLLAMA-3B as the large pretrained reference model, and also share the same choices for all training hyperparameters \u2013 the only thing changing is the choice of n\ud835\udc5bnitalic_n.We developed 8 different submodels with n={4,6,8,10,13,16,18,20}\ud835\udc5b4681013161820n=\\{4,6,8,10,13,16,18,20\\}italic_n = { 4 , 6 , 8 , 10 , 13 , 16 , 18 , 20 }. Figure 2 depicts the MMLU (5-shot) score as a function of n\ud835\udc5bnitalic_n. As can be expected, we see a positive-sloping trend line. The submodel with 20 layers, exhibits a slight decrease in performance, potentially due to data overfitting issues due to increase in model size. The training details of all these submodels remain consistent with the target 1.5B small base LM and mentioned in appendix. We leave a more comprehensive investigation on the choice of n\ud835\udc5bnitalic_n \u2013 including possibly varying both n\ud835\udc5bnitalic_n and the number of training tokens jointly, and evaluating on a more exhaustive set of tasks \u2013 as future work.We further analyze \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune to see the impact of it\u2019s performance when more tokens are available. Initially for the main results we limited ourselves to 1B (i.e. 0.1%) tokens from the 1T pre-training data, here we use a 50B subset (i.e. 5%) of the pre-train data. Moreover we also extend this study to include larger base LMs of 7B parameters as reference models, employing OpenLLaMA-7B and LLaMA2-7B as reference models. For the purpose of this study we do not repeat the tokens from our 50B subset. As shown in Figure 3, we observe that there is clear improvement in overall MMLU (5-shot) score with more data. Additionally it is interesting to see that 1.5B (or 1.6B models) developed with \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune using larger reference models show even greater improvements when fed with 50B subset of non repetitive data (i.e fresh tokens). We present a Table 4 using Figure 3 to show the best MMLU (5-shot) scores achieved using different reference LMs. For developing our small base LMs using larger reference LMs we use n\ud835\udc5bnitalic_n=7 (i.e. 7 layers). The training details are mentioned in supplementary materials.We ran ablations (refer Figure 4) to choose the total number of epochs (multiple passes over the data) and observe that repetition when training our 1.5B (or 1.6B) LM is helpful particularly for MMLU. We also observe that the for an average of all the 9 other datasets (i.e. except MMLU) peaks it\u2019s performance at 5 epochs and then deteriorates. Some prior works have studied this phenomenon that the scaling of downstream tasks with data is not always linear Biderman et\u00a0al. (2023).Next we tackle the question \u2013 whether one should re-use 1B tokens for multiple epochs or use the same number of fresh tokens? Some prior works have recommended that if you have a reasonably large size dataset one can repeat it upto 4 epochs Muennighoff et\u00a0al. (2023). In our study we observe that one can safely re-use 1B tokens upto 10-20 epochs as shown in Table 5. We emphasis that this phenomenon needs a through investigation in itself and we defer this to future work. The models discussed in Table are saved checkpoints during a single training run and not the final model unless otherwise specified.In this section we study the performance of \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune in a slightly different setting from that of the previous section. In particular, in this section we look at the case where we not only have the larger reference base model \u2133r\u2062e\u2062fsubscript\u2133\ud835\udc5f\ud835\udc52\ud835\udc53\\mathcal{M}_{ref}caligraphic_M start_POSTSUBSCRIPT italic_r italic_e italic_f end_POSTSUBSCRIPT, but we also have the entire dataset that was used to train \u2133r\u2062e\u2062fsubscript\u2133\ud835\udc5f\ud835\udc52\ud835\udc53\\mathcal{M}_{ref}caligraphic_M start_POSTSUBSCRIPT italic_r italic_e italic_f end_POSTSUBSCRIPT.For this setup we investigate if the smaller base LMs we make can match the performance of the larger reference LM. Note that this is in contrast to what was observed in the previous (different) setup of Section 3, where the smaller models had significantly worse performance than their corresponding reference models.For the experiments in this section,\ndue to limited compute resources, we used less than billion size reference models GPT2-large, and GPT2-medium; and the 9B token OpenWebText as our dataset. Table 1 provides details on these models. For the description of the experiment, we will just focus on GPT-2 large; the same process was followed for GPT-2 Medium.We first split this dataset into a training set \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}_{\\text{train}}caligraphic_D start_POSTSUBSCRIPT train end_POSTSUBSCRIPT and a smaller validation subset \ud835\udc9fvalsubscript\ud835\udc9fval\\mathcal{D}_{\\text{val}}caligraphic_D start_POSTSUBSCRIPT val end_POSTSUBSCRIPT.We then train the full 36-layer GPT-2 large model on the entire training set \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}_{\\text{train}}caligraphic_D start_POSTSUBSCRIPT train end_POSTSUBSCRIPT for certain number of training steps (T\ud835\udc47Titalic_T) and evaluate its validation loss on \ud835\udc9fvalsubscript\ud835\udc9fval\\mathcal{D}_{\\text{val}}caligraphic_D start_POSTSUBSCRIPT val end_POSTSUBSCRIPT which we call benchmark val loss. Here by validation loss we mean the standard log-perplexity (refer Table 6).We then apply \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune v2 (refer Algorithm 2) with the following inputs: the reference model given as input is the (now trained) 36-layer model, the smaller model\u2019s intended number of layers was n=18\ud835\udc5b18n=18italic_n = 18, the dataset is the full \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}_{\\text{train}}caligraphic_D start_POSTSUBSCRIPT train end_POSTSUBSCRIPT, the benchmark val loss and the number of steps of training was the same as that used in step (2) above for the reference model. This gives us our 18-layer model; we evaluate the validation loss of this model.If the validation loss of the our model in step (3) is worse than that of the reference model in step (2), we re-do step (3) but now with the value of n\ud835\udc5bnitalic_n increased by 2. That is, we progressively explore bigger sizes for the inherited model until we achieve parity in validation loss with the original reference model.Separately and for comparison, we also trained a randomly initialized but other wise identical 18-layer model for the same number of epochs as the models in steps (2) and (3).Table 6 illustrates that a smaller target LM with a few layers can be extracted without losing performance in the validation loss from the GPT2 family. Moreover, we also show that the target LM crafted using a larger reference LM through \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune demonstrates comparable zero-shot performance on two relevant downstream tasks. Next, our results show that the target LM developed using \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune surpasses a similar-sized model in performance, particularly when the target LM is trained from scratch. This result also aligns with our results in limited data setting (refer Section 3) where we show that small base LM derived with \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune matches the downstream performance of similar size models trained from scratch on large datasets. Interestingly our target model trained with \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune for 100K steps outperforms a same size model pre-trained from scratch for double the number of training steps i.e. 200K steps. Through the perspective of convergence, some prior works have made some connections of over-parameterization with faster convergence Bengio et\u00a0al. (2005); Vaswani et\u00a0al. (2018). In Figure 5 we show that the LMs derived with \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune although smaller compared to their reference models still converge as fast as their large size reference models.Next we ran extensive experiments with GPT2-medium (355M) to better understand the impact of initialization on transformer blocks on both generalization (val loss) and convergence speed during the pre-training phase. It is important to note that GPT2 models has a parameterized layernorm (LM). Previously in Table 6 we observed that our 16 layer GPT2-medium model with \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune matches the val loss of the 24 layer GPT2-medium. We include the 24 and 16 layer GPT2-medium model from scratch and also our 16 layer model. Next we trained 4 new GPT2-medium 16 layer variant model while initializing different parts of the transformer blocks with weights from 24 layer GPT2-medium trained from scratch. We initialize the transformer blocks with 1) attention ((key, query, value and projection) and the layernorm weights (attn w/ layernorm), 2) attention and mlp weights without the layernorm (attn+mlp w/o layernorm), 3) mlp weights with the layernorm (mlp w/ layernorm), and 4) \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune without residual connections. We emphasis that \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune performs initialization which inherits weights of attention and mlp weights with the layernorm (attn+mlp w/ layernorm). In Table 7 we observe that models trained with attention and mlp weights has shown the best performance agnostic to the layer norm initialization. The convergence patterns of these models can be seen in Figure 6. Therefore there is a clear advantage of initialization of both attention and mlp weights. Next we also make a surprising observation that initializing either the attention or mlp weights resulted in very similar improvements in both the speed of convergence and the final validation loss.Small base LMs are typically trained from scratch with trillion size data which takes a huge compute budget and time. For instance, tinyllama-1B Peiyuan\u00a0Zhang and Lu (2023) a scaled down version of LLaMA2 models Touvron et\u00a0al. (2023b) is independently pre-trained from scratch with 3T tokens for 90 days utilizing 16 GPUs. Efforts have also been made to create small base LMs using high-quality synthetic data, as demonstrated in the training of the Phi models Javaheripi et\u00a0al. (2023). Despite the use of high-quality synthetic data, these models still require training with trillions of tokens. In this paper we argue that small base LMs can be trained in a data efficient way (also faster) if trained with better initialization and with multiple passes over the data. Furthermore, our pre-training dataset consists of 1B tokens randomly sampled from redpajama, without any additional efforts in data curation.In settings outside of LLMs, such as in small neural networks (NNs) and convolutional neural networks (CNNs), pruning weights\u2014especially from the width of pre-trained networks post training has proven to be highly effective Han et\u00a0al. (2015); Li et\u00a0al. (2016) in reducing memory usage through sparsity. These models can be pruned by up to approximately 90% LeCun et\u00a0al. (1989); Frankle and Carbin (2019) without compromsing their test performance. However, to our knowledge, there is no existing works in LLM scale that showed a similar result. This is likely because the functions these large base LMs aim to learn during pre-training are significantly more complex than those previously demonstrated in CNNs and NNs. Additionally we have provided a detailed discussion of \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune with some other pruning methodologies in Section 3.2.\nSome prior knowledge distillation (KD) works have initialized the student network with few layers from the teacher network and then distilled the student using full pre-training data. It is important to note that these works Turc et\u00a0al. (2020); Sanh et\u00a0al. (2019) employed this initialization has a trick embedded in their training details and not a main focus of their work. These studies typically explored the impact of KD in scenarios where the model was fine-tuned for a specific task, also they did not analyze how much of the improvement came from the initialization technique as opposed to the distillation process itself. Furthermore these works also used the full pre-train data used to train the teacher for distilling the small student model. In our work we provide \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune as algorithm and deeply analyzed it in a low data regime and full data regime. Next we also showed improvements in multi task pre-train setup. Finally KD is extremely compute intensive and \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune is very efficient in compute as we ground-up train a small base LM using 1 GPU for less than 12 hours.Some recent works Xu et\u00a0al. (2024) in transformers that uses weight initialization for faster fine-tuning for vision setting. However there is an evidence Trockman and Kolter (2023) that shows attention patterns of vision and language pre-trained models are different and hence initialization method that works for vision setting may not necessarily works in language setting as well. Next another recent work Liu et\u00a0al. (2023) achieves better domain specific performance using weights from an existing model. However this work have also used high quality synthetic data for finetuning. Our paper makes no claims of high quality data rather we sample a small random subset of an pre-training data used to train an existing Large base LM.In this section we discuss some of the key implications of our work.Pre-training a small base LM of 1-2B parameters from scratch is extremely expensive. For instance mpt-1.3B base LM is pre-trained with 440 A100 GPUs for half a day, while the Pythia-1.4B base LM Biderman et\u00a0al. (2023) utilized 64 A100-40GB GPUs for 4830 GPU hours. Similarly, TinyLLaMA-1.1B model Peiyuan\u00a0Zhang and Lu (2023) was pre-trained using 16 A100 GPUs for 3 months. Our 1.5B (1B data variant) LM shows competitive performance despite being trained with 1 A6000 GPU for less than 12 hours. Typically small base LMs are finetuned for a specific task before deployment and are not used in it\u2019s base form. With \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune we present a really easy and cheap way for developing a small base LM to be later finetuned before deployment.Typically small variants of large base LMs are pre-trained using the same pre-training data Peiyuan\u00a0Zhang and Lu (2023); Groeneveld et\u00a0al. (2024). Our recipe introduces a new perspective of identifying sufficient depth without losing any generalization on the held out set (pre-train val loss). Next we also show that even with a small fraction of pre-train data (randomly sampled) and few initial layers of the large base LM one can develop a small base LM. Therefore our \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune recipe has the potential to become the naive baseline for any pre-training pipeline aiming to develop a smaller variant of a large base LM.The architectural choices particularly the total number of layers used for an LLM is largely taken arbitrarily. In Section 4 we present \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune v2 (refer Algorithm 2) which can be utilized to identify sufficient depth of a particular model without loosing on pre-train validation loss. The usage of our recipe can go beyond depth.The \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune recipe has some limitations. The key limitation of our recipe is that it can only change the number of transformer blocks/layers and that restricts the architectural design space of selecting the hidden size and attention head for a custom small base LM. Our method is likely to be very sensitive to the quality of the training dataset, since it is so small. Finally, the selection of which blocks to keep and how to select the training dataset and hyper-parameters have not been explored in our work but could offer some benefits.In conclusion, this study demonstrates the efficacy of our Inheritune method for pre-training small base language models using significantly fewer data tokens and computational resources than traditional methods. We proposed a very simple way of creating a smaller LM using a large reference model and a small training set. We also analyzed a slightly different setting where \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune is employed with the full pre-training dataset. In that case, the validation loss achieved by GPT2-large with 100K training steps on OpenWebText 9B tokens can be matched using our GPT2 variant with half the layers. Furthermore, we presented similar results utilizing GPT2-medium with 24 layers and our variant of GPT2-medium with 16 layers. Our proposed recipe provides a new perspective on simple methods to make small base LMs from an existing large pre-trained model and a small subset of available pre-training data.Supplementary MaterialsA: Frequently Asked Questions.B: Implementation Details\nPhi-1.5 is trained on GPT-4 generated high quality synthetic data which neither full or even a small fraction is not made publicly available. Since data quality is paramount to LLM pe-training we restricted ourselves to model trained with redpajama-1T for fairness. We showed pythia because the data distribution of pile and redpajama is similar. The key idea we demonstrate here is that one can make a performant small base model utilizing an existing model and it\u2019s a small subset of pre-training data.We have compared \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune with targeted structural pruning recipe called Shearing Xia et\u00a0al. (2023) and also provided a detailed discussion in Section 3.2 about why it is bit unfair to directly compare any pruning methodology to \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune which we believe can be best described as a model initialization technique as we ground up pre-train a model. Let\u2019s understand this with an example, let us assume that we want a scaled down or smaller version of OpenLLaMA-7B model and as an academic researcher we can only manage 1 GPU with 20GB memory. The OpenLLaMA-7B model takes 17 GB of memory to load on that particular GPU. Now pruning is virtually impossible as it requires a lot of compute and memory (for iterations of pruning steps). In such setting \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune can be used to pre-train an scaled down version with very little compute requirements. Given one assumes a lot of available compute with access to the full pre-train data \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune may also help in determining the sufficient depth of a model as shown in Section 4.We show that with a little training compute 1 GPU for less than 12 hours (half the time assumed by Geiping and Goldstein (2022) for BERT pre-training) one can develop a performant 1.5B base LM using just 1B data, which compares favorably across widely used pre-training datasets/benchmark(s). Next we show that less deeper base LMs can be pre-trained while using \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune (with upto 50% parameter reduction for GPT2-large model) without compromising the pre-train val loss. Some key implications of our work are discussed in Section 6.Although we don\u2019t make any specific claim that \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune is parameter efficient. Some prior parameter efficient methods Hu et\u00a0al. (2021); Malladi et\u00a0al. (2023) are designed to reduce the memory requirement during training. Unlike these prior works \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune reduces both trainable and total model parameters and thus makes training more parameter efficient. We defer a through investigation of \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune for parameter efficient adaptation to future.We present our main results with Our-1.5B model trained with an existing OpenLLaMA version 1 Geng and Liu (2023) and 1 B tokens randomly sampled from 1T redpajama version1 data. The hyper-parameters related to this model is provided in Table 8. It is important to note that our claim that we only use 1 GPU for less than 12 hours to train Our-1.5 B model is specific to models derived using \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune with 1B data. Next we also train multiple sub-models as shown in Figure 2 the training details remains consistent with that of the initial model discussed earlier. However we observe that increasing the number of layers in a sub-model also increase the training time.We also trained our 1.5B model with larger subsets of data as shown in Figure 3.It is important to note that all the intermediate tokens until 50B are intermediate checkpoints of a single training run. Some of the key hyper-parameters of our training runs are discussed in Table 9. We have also trained three variants of small base LMs utilizing 3 different reference base LMs namely OpenLLaMA-3B, OpenLLaMA-7B and LLaMA2-7B. For target LMs developed with OpenLLaMA-3B we use n\ud835\udc5bnitalic_n=13 i.e. 13 layers. For target LMs developed using reference LMs of 7B parameters we use n\ud835\udc5bnitalic_n=7 i.e. 7 layers. The training hyper-parameters remains consistent across all the models trained with 50B subset of the pre-train data.For an exploratory analysis we also trained GPT2-medium and GPT2-large models as shown in Table 6. The GPT2-medium model with 24 layers and also the inheritune variant with lesser layers are all trained with a 50K tokens per batch. The GPT2-large model and it\u2019s variants developed using \ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\ud835\udda8\ud835\uddc7\ud835\uddc1\ud835\uddbe\ud835\uddcb\ud835\uddc2\ud835\uddcd\ud835\uddce\ud835\uddc7\ud835\uddbe\\mathsf{Inheritune}sansserif_Inheritune are trained with 16K tokens per batch. The training details are given in Table 10. The GPT2 models are trained on a single node consisting of 3 A100 with 40 GB memory.",
    "5": "The conversational search task aims to enable a user to resolve information needs via natural language dialogue with an agent. In this paper, we aim to develop a conceptual framework of the actions and intents of users and agents explaining how these actions enable the user to explore the search space and resolve their information need. We outline the different actions and intents, before discussing key decision points in the conversation where the agent needs to decide how to steer the conversational search process to a successful and/or satisfactory conclusion. Essentially, this paper provides a conceptualization of the conversational search process between an agent and user, which provides a framework and a starting point for research, development and evaluation of conversational search agents.A conversational search agent needs to support the user in finding, exploring, and understanding the possible options and information objects that are available \u2014 which will help to satisfy the user\u2019s information need. While past work has started to tease out different actions that users and agents perform and respond to during the conversational search process, there has been little work on formalizing these actions and decisions. Thus the goal of this paper is to develop a conceptual framework of different actions and intents, along with the key decision points within the conversation. Our aim is to make these tasks explicit in order to formalize the research, development and evaluation of conversational search agents. To this end, we first examine\nthe key actions and intents identified in past work, and enumerate these along with others that can be naturally inferred from a simulated conversational context, before discussing the key decisions that the agent needs to make in order to advance the conversation to a satisfactory or successful end.To ground the proposed framework we first provide a brief overview of conversational agents in general. Then, we focus in on conversational search agents and describe what actions various proponents suggest that they need to be able to perform and accomplish.Liu et al. (Zhanyi\u00a0Liu, 2017) provide a high-level overview of six abilities a conversational agent needs to possess:filtering out superfluous information e.g. fillers, pauses, false starts;determining an appropriate system response and the need for sophisticated decision making mechanism (e.g. should the agent show results, show a hierarchy, ask a follow up query?, etc.);answer aggregation to present a summary answer rather than a just ranked list of results;conversation management that considers and maintains the search goals, conversational history and current state of the agent\u2019s understanding;general knowledge that the agent should have about external world in order to efficiently exploit contextual information and correctly process the user\u2019s query;and personality and moral responsibility to respond to emotional and sensitive queries.Similarly, Allen et al. \u00a0(Allen\net\u00a0al., 2001) suggest that a conversational agent needs to be aware of the current state of the conversation, and continuously update its representation given the user\u2019s responses in order to generate the best response. A key action that an agent needs to perform is elicitation\u00a0(Christakopoulou et\u00a0al., 2016), where the agent needs to learn about the user\u2019s preferences. Then the agent needs to be able to help the user to efficiently explore the search space \u00a0(Winterboer et\u00a0al., 2011). Demberg et al. \u00a0(Demberg\net\u00a0al., 2011) suggests that agents also need to be able to cope with both under and over-constrained requests.In developing a conversational agent, Moore\u00a0(Moore, 2018) suggests that an agent needs to have strategies for repair and disengagement. For example, the agent needs to be able to repeat information or ask for information to be repeated in case of misunderstanding, as well as recognize when the conversation is closing, correcting the recipient and dealing with insults. Bohus and Rudnicky (Bohus and\nRudnicky, 2005) also point out that such agents need to be able to handle misunderstandings (and be able to ask for a statement to be rephrased etc., but also that agents needs to be able to explain their current state (i.e. report what they understand)) - so that misunderstandings can be corrected. Moore\u00a0(Moore, 2018) postulates that it is crucial for an agent to be able to perform such actions to exhibit basic conversational competence.Various taxonomies of conversational actions have been developed in the context of dialogue management. For instance, Henderson et al.\u00a0(Henderson\net\u00a0al., 2013) in their \u2018Dialogue State Tracking Challenge\u2019 list: affirm, confirm-domain, negate and repeat among the examples of agent actions and deny, inform and request as some of the users actions. In Clark and Schaefer\u00a0(Clark and\nSchaefer, 1989), they present several methods for establishing common ground in conversation, these include: continued attention, next contribution, acknowledgement, demonstration and display. These have also been formalized as part of an ISO standard for semantic annotation of dialogues, ISO 24617\u00a0(Bunt\net\u00a0al., 2010), more commonly referred to as dialogue act markup language (DiAML). While the above actions and taxonomies can be applied to any general dialogue, in this paper, we focus on actions and intents that are relevant to the conversational search process.More specifically related to conversational search, Radlinski and Craswell (Radlinski and\nCraswell, 2017) provide a theoretical framework, that puts forward five properties that a search agent needs to have in order to be conversational. The properties are:user revealment where the user discloses to the agent their information needs,agent revealment where the agent reveals what the agent understands, what actions it can perform, and what options are available to the user,\nmixed initiative where both the agent and the user can initiative and direct the conversation,memory where the agent tracks and manages the state of the conversation, the user\u2019s information need, etc., and,set retrieval where agent needs to be able to work with, manipulate and explain the sets of options/objects which are retrieved given the conversational context.The above properties are required so that the agent can facilitate search by helping user to formulate their information need and build expectations regarding its capabilities. During the search process, the agent takes initiative and uses memory to retain information relevant to the query. Before presenting results back to user, the agent needs to reason about the utility of retrieved information and decide on what to present to the user.Trippas et al. (Trippas et\u00a0al., 2018) provide a different high-level formalization of the conversational search process distinguishing three phases: (1) query formulation, (2) search results exploration and (3) query re-formulation. They note that during each phase, the agent needs to elicit details of the information need and obtain feedback on the results, and then use this to inform subsequent actions. They suggest that key activities that an agent should be able to perform include: listing and summarizing results and result pages, and requesting feedback from the user.Recently, we have seen a number of studies exploring how people interact with conversational search agents (where the agent is either a human with a search engine, or a simulated \u201cWizard of Oz\u201d agent, e.g. (Vtyurina et\u00a0al., 2017; Vtyurina and\nFourney, 2018; Wolters et\u00a0al., 2009; Trippas et\u00a0al., 2017).\nIn these studies they focus on ascertaining how people behave and interact with the agent, without imposing a defined interaction policy regarding how the \u201cagent\u201d should act\u00a0(McDuff et\u00a0al., 2017). Instead, participants are provided with search tasks and roles but not explicitly instructed on how to complete them. Therefore, while they do not explicitly focus on specifying the actions taken by the agent (e.g. wizard or mediator) the conversational search logs could be useful in identifying common actions - and could be used to provide evidence to support the proposed framework.The main goal of this paper is to abstract out key actions and decisions points that manifest during the conversational search process. In order to do this, we will walk-through possible ways in which an agent and user may converse during the search process and draw out particular tasks that the agent needs to perform either to respond to the user or engage with the user.Assumptions We take a focused view of the conversational search setting, where we assume that our hypothesized agent wants to:help the user: the agent is cooperative, seeks to serve the user\u2019s interests, is not adversarial and does not seek to maximize its own benefit,minimize the conversational effort: the agent does not seek to waste time with idle chit-chat,\nand seeks to avoid burdening the user\u2019s cognitive capacity by asking excessive questions, or presenting irrelevant options, etc.,\nand,maximize the range/number of relevant options provided to the user: the agent wants to provide the user with an understanding of the search space so the user can make an informed choice, as well as guide the user via continued exploration the search space subject to the time the user has available and the user\u2019s cognitive limits.Ultimately, we assume that the agent wants to: (i) help the user to resolve their information need, and/or (ii) help the user to understand the space of options available to the user. Depending on the domain of the conversational search agent, it could enable the search over very specific resources (e.g. flight, flowers, etc.) or be more general and enable ad-hoc and exploratory search of any kind of information objects (e.g. news, holidays, reviews, places, recipes, etc.).Now, imagine that we have a user, with a goal in mind - maybe the goal is well-defined, precise and fixed (e.g. a known destination, time/date, etc.), or maybe it is loosely defined, vague and flexible (e.g. planning a holiday). This is unknown to the agent - and so the agent will need to discover what the user wants. The user will also have particular preferences. But again, we assume that these are, initially, unknown to the agent and will be revealed through the conversation(s).While we will provide specific examples using natural language, it is also worth noting that we assume that the agent has sufficient capabilities to understand and respond to the requests of the user. Note that for the purposes of the simulated conversations we assume that the agent has the capacity to perfectly understand the user\u2019s utterances and requests, and can provide meaningful and relevant responses. We acknowledge that these are both very much open challenges that have yet to be fully addressed. Of concern here, is the second of these challenges i.e. a \u201cmeaningful response\u201d which requires the agent needs to decide on what action(s) to take at any particular point in the conversational search process.Examples Below we have two examples of conversations between users and agents, where the task is ad-hoc and exploratory in nature. The first considers the context of exploring holiday options, while the second is in the context of current affairs. These contexts present tasks that are open ended, ongoing and that evolve over the conversation and time.During each of the above conversations, while in different contexts, we can start to see actions that are common to both, which can be abstracted out more generally. For example, in both cases, (i) a user discloses part of their information need, (ii) the agent reveals some details about the options available (given the partial information need), the agent elicits more details regarding the user\u2019s information need, (iii) the user refines or expands their information need, (iv) the agent hypothesizes about alternative information needs and then suggests other options, and so forth. Of course, a user (and an agent) will communicate their intent to the other party through natural language (e.g. an utterance or chat). Another more general challenge is to identify from the natural language the intent and action that the user wants to communicate, and conversely how to express the intent and action of the agent to the user.In the next two subsections, we provide a non-exhaustive list of the different kinds of actions users and agents might perform during the conversational search process (as shown in Table\u00a01), before considering the different decisions agents will need to make during in the conversation. This list should be seen as a starting point\nfor conceptualizing agent-human interactions for conversational search. In creating this list we have attempted to represent the main conversational search actions previously observed and discussed in the literature, as well as other actions that can be naturally inferred from the conversational context. For simplicity we will provide examples based around planning a holiday, though the same kinds of actions generalize to other search scenarios.Information Need Pathways Before listing the different actions, we will assume that the conversational search process revolves around the agent trying to help the user resolve their information need. As such the agent will need to maintain a representation of the user\u2019s information need as it evolves over time.Given a point in the conversation, we refer to this representation as the Current Information Need (CIN). It encapsulates what the agent has understood (and has modelled) given the preceding conversation. Given the CIN, the agent can retrieve the list of related information objects (i.e. the list of destinations, reviews, tours, etc that are returned by the query formulated based on the CIN). The agent will then use this list to inform and direct the subsequent actions.\nOver the course of the conversation, the CIN will change and evolve, and so the agent will also need to keep track of these Past Information Needs (PINs) and the corresponding set of associated objects. This is because the user may wish to refer back to a previous point in the conversation, or the agent may need to explain how they came to a particular point in the conversation.In addition to the user created information needs, the agent itself, may generate Alternative Information Needs (AINs) based on CIN and PINs. AINs are generated to provide other recommendations to the user that the user may not have explicitly considered, or not yet asked about. Again, these AINs have a corresponding space of associated information objects. The agent will also need to record what parts of the search space of objects has been revealed to the user over the course of the conversation.During the course of the conversational search process, a user will perform particular actions that will progress the search in different ways. There are two main types of user actions, i.e. those that: (i) change the state of the information need, and those that: (ii) are related to the space of information options/objects (w.r.t the CIN, PINs or AINs).Reveal Actions. At various points in the conversation, the user will disclose details regarding their information need either voluntarily, or in response to an agent\u2019s question, which will then be used to update the CIN, e.g.:Of course, there are cases where the user chooses not to disclose information regarding their information need/preferences, either because the user is unsure, or doesn\u2019t want to (i.e. Disclose - Unsure or Disclose - Not ). However, for the search to move forward (or change direction), the user will need to communicate some preferences to the agent.Since information needs are not fixed requirements will evolve during the course of the search. Consequently, the user may wish to revise and/or refine an existing specified criteria given the CIN.\nHere we assume that an information need is composed of a number of criteria e.g. the holiday destination, the places to visit, the type of holiday, when, duration, which airports to fly to, who is travelling, etc.In the first example, the user revises their CIN by changing the date, but also adds in another criterion, by asking for an evening departure. Whereas in the second example, the user now decides to further revise their CIN and disclose their budget preference.\nExpand. Rather than constraining the search through a refinement, a user may wish to entertain more possibilities by generalizing or removing the criteria provided.In the first example, the user now wants to consider both Italy and Spain as possibilities, and then to also consider more expensive holidays. In each case, the search space is now larger (though it may not mean that the number of options available increases).Inquire Actions. Rather than update or modify the CIN, a common set of related actions involves inquiring about the space of options available given the CIN (or PINs/AINs). As illustrated in the example below, a user may ask for a list of the different options (List), a summary of the different options (Summarize), a selection of different options (Subset), a comparison between options (Compare) or for options that are similar to the current set of options (Similar).Navigate Actions. As previously mentioned, conceptually any particular information need has a related set of information objects/options associated with it. The user inquire actions are request to reveal part of this space in some way i.e. given the set (e.g. all hotels in Montepulciano in Tuscany), in most cases, perform some ordering to form a list (sort by rating/price/etc.), make a selection (pick the best/cheapest/etc.), and then reveal/compare/etc. Consequently, the user may want to navigate within the list in various ways. For example, Repeat to revisit the options already revealed, Back to go back to previous options, More to learn about more options in the list, etc.During the conversational search process, options will be encountered by the user, that they will want to take Note of - that is mark or save in some way, in order to refer back to or consider at a later point in time, and so the user may communicate to the agent something like \u201cThat hotel could be a possibility.\u201d or \u201cSave that hotel for later.\u201d Given the set of noted options/objects the user will want to revisit this list of options and ask for operations to be performed on this particular set e.g. inquire and navigate.\nInterrupt Action. There will be points in the conversation, for example, when the agent is reciting a long list of items, asking irrelevant questions, etc. when the user will want to interrupt or stop the agent from continuing the conversational such direction. This may be for various reasons - to inspect an item in a list, change the CIN, etc. This interruption request will invariably be coupled with another action/intent, e.g. \u201cStop! For the previous hotel, what are the reviews like?\u201d.Interrogate Actions. A higher order action of the user is the intent to interrogate the state and understanding of the agent, where the user might like to: (i) know what the agent knows about their information need and the assumptions it has made so far (i.e. Understand) , and/or (ii) have the agent explain why particular items are being shown, why particulars suggestions are being made, etc. (i.e. Explain).\nClosing Actions. Finally, the user may choose to end the conversational search process, which could end in various states such as, moving to the task completion stage (\u201cOk, great, I will book that flight/hotel/car/tour/etc.\u201d Complete. Alternatively, the user may leave with some indication of returning back or without (\u201cOk, let me think about it - I\u2019ll get back to you.\u201d, or \u201cthanks for your help, bye.\u201d), or by not responding anymore.So far we have explored what actions a user might take when interacting with a conversational search agent. Given these user actions, the agent will need to respond or act accordingly in order to deal with the various requests/responses.Inquire Actions. A core action that the agent needs to perform is to Elicit the user\u2019s information need, by asking the user about various criteria to scope and reduce the possible search space. For very constrained domains this will be based around a template i.e. slot filling, but the bigger challenge is to infer the criteria based on the conversational context and domain. The agent will also need to elicit the user\u2019s preferences and constraints.\nThat is, given a particular criterion how flexible is the criteria? Do they have particular preferences (e.g. \u201cUser: I\u2019d prefer a sunny beach holiday.\u201d) or do they have hard constraints? (e.g. \u201cUser: I definitely do want to go somewhere cold!\u201d).\nSo the elicit activity includes: Elicit Criteria the different criteria/conditions of the information need, and Elicit Constraints the flexibility of the specified criteria/condition. The related task the agent will have to perform is to Extract criteria and conditions that have arisen during the conversation.Following on from eliciting criteria and preferences is a need to Clarify what the user meant. For example, the agent may want check its understanding due to: miscommunication, i.e. the user was not heard properly (\u201cDid you say you wanted somewhere that is cold?\u201d), or to obtain further specificity (\u201cWhat do you mean by cold? Less than 20 degrees Celsius?\u201d).Reveal Actions. In terms of system revealment, the agent will need to disclose information about what it has found given the CIN/PINs/AINs. Either in response to a user list request, or because the agent decides to offer suggestions, the agent will at some point need toShow what items it has found by describing the items it has retrieved.Given the objects the agent will also need to be able to Summarize or aggregate them to provide the user with an overview of the search space. In the example below we contrast the agent listing the objects vs. summarizing the them available given the user\u2019s information need. The agent will need to decide whether it is more appropriate to list objects or summarize (see next subsection).Agent: I\u2019ve found a number of possible tours around the wine yards. One leaves at 8.30am for 100 pounds, another is at 1.30pm for 75 pounds, and the last one is at 4pm for 139 pounds. [List]Agent: Tours range from 75 to 139 pounds, and leave in the morning, afternoon and early evening. [ Summarize]Other list-based operations that the agent will need to perform include comparing different items in the list, i.e. Compare, and to manipulate Subsets, for example, performing operations that will sort, and select some of the objects given a specified criteria (i.e. \u201cUser: What is the cheapest tour?\u201d. The agent will also need to be able to find Similar objects given a particular list objects.Agent: The cheapest tour is for 75 pounds and leaves at 1.30pm, while the evening tour leaves at 4pm and includes a three course dinner, but is more expensive at 139 pounds. [Compare]Traverse Actions. Corresponding to the user navigate actions, the agent will need to remember where the user is in the list, and how to update where they are in the search space given their actions (i.e. to Repeat, move Back, provide Move options, etc., and Record when the user shows interest in particular options)Suggest Actions. At some point in the conversation the agent may either be asked, or may want to make a suggestion regarding the options available. So the agent will need to be able to Recommend a particular options/object given the CIN. Alternatively, the agent may want to Hypothesize about the CIN and generate what-if information needs i.e. (AINs) such as:\nwhat-if they could go to a different country/region/place, what if they could go on different dates, etc. These Alternative Information Needs (AINs) may lead to a different space of options which might (or might not) be of relevance to the user. The subtle distinction here is whether the suggestion is based on what is currently being discussed (i.e. the CIN) , or whether it is based on some variation (i.e. an AIN).Explain Actions. The agent will need to be able to report its understanding of the CIN to respond to a user\u2019s interrogation or to provide context to the user. Further, it will also need to be able to justify and reason why it took a particular course of action, made a particular suggestions, etc.Error and Finalization Actions.\nWhen an agent gets confused or misunderstands what the user wants it will need to recover smoothly. This may be by updating the information need to reflect what the user actually wanted, or to go back in the conversation where they both share a common understanding of the search space. The agent will also need to handle requests/actions that are outside of the agents scope or capacity. For example, when a user about medical conditions when it is designed to help plan holidays, etc.End. At some point, the conversation will draw to a close - and it may or may not be resumed. For example, the user might have decided on the various aspects of their holiday (and thus will transition to the booking agent). On the other hand, the user might revisit or continue exploring the space of options, before and even during the holiday itself. The agent will therefore need to track and persist the conversation, and decide how to respond depending on the context.So far we have enumerated different actions that users and agents may perform during the conversational search process - and shown in Table\u00a01 how they relate to one another. In this subsection, we aim to discuss in more detail the different decisions and tasks that the agent needs to perform to facilitate the conversation.Agent Dialog Policy. The overarching decision that the agent needs to perform is to decide on how to respond/initiate given the the preceding conversation and the user\u2019s request/response. The agent must decide on what action or actions to take in order to provide a useful and meaningful response that drives the conversation forward (w.r.t the user\u2019s goals). Assuming a turn-based dialogue model, where the user initiates the dialogue (e.g., \u201cI\u2019m looking to go on holidays to Italy.\u201d), what action(s) should the agent take \u2014 at the high level \u2014 the agent can: inquire, reveal, traverse, suggest or explain. The Agent\u2019s dialog policy will define how the agent will act and behave based on the context of the conversation. However, some actions are going to be better than others \u2014 better in the sense that they advance the user towards their goal in some way, and do so in an efficient manner. Essentially the agent needs to decide what action(s) it should take next. For example, the agent could: (a) inquire-elicit, (b) reveal-list, (c) reveal-summarize, (d) explain-report, etc. Crafting and/or inferring the dialog policy is an open challenge and core task of a conversational search agent.\nBelow, we describe a number of the lower level tasks that will be part of such a policy.Intent Identification and Extraction Task. A key task performed during each turn is to extract out the intent(s) of the user given their utterance.\nFor example, given the user\u2019s initial statement above, the agent needs to identify the intent of the user, in this case they are revealing and disclosing their information need i.e. Reveal-Disclose. Given this intention, the agent then needs to extract out the disclosed criteria and represent this within the current information need, i.e.. the CIN models that the user wants to go on holiday to Italy. Being able to infer and extract the intention will be a core task that any conversational agent will need to be able to perform accurately.Inquire-Elicit Question Selection Task. For a given CIN, the search space may be quite large or have many different aspects/facets which represent different ways to explore and narrow down the search space. For example, given the current example, there are hundreds of thousands of possible holiday options. Thus, an agent will need to decide what question to ask the user in order to narrow down the search space to help refine the CIN. Of course, there is a range of questions that the agent could ask. For example, where in Italy, when they would like to travel, what they want to do, how much they are willing to spend, who will be going, etc. Given the agent\u2019s objectives, the agent will most likely want to ask a question that reduces the search space (such that answer from the user will result in the number of options associated with the updated information need less than the number of options associated with the previous information need).\nHowever, there may be cases when/if the information need is over specified, and so the agent will need to ask question which requires the user to expand their information need. A key challenge here is to select a question that efficiently decomposes the search space, but is also contextually relevant i.e. makes sense to the user, and maximizes the user\u2019s understanding of the search space.\nInquire-Clarify Requirement Clarification Task. A related task that the agent will need to perform is to decide whether it should seek a clarification, given the previous utterance (and what the agent has inferred and extracted from it). This may arise in a number of situations: the agent doesn\u2019t understand what the user said, or the utterance doesn\u2019t make sense in some way, or the information provided is under specified. For example, let\u2019s assume the agent asks the user when they would like to travel, and the user responds, \u201cOn the 4th\u201d. Does the user mean \u201cMay the 4th\u201d, \u201cJune the 4th\u201d, etc. it is ambiguous. The agent must decide whether to: (i) leave it unspecified, (ii) impute the missing details (assume they meant \u201cMay the 4th, 2018\u201d, or (iii) ask a clarifying question e.g.\u201cDo you mean the 4th of May, 2018 ?\u201d. Depending on how much the agent understands of the domain, and the given context, will determine how much the agent can assume or impute given the underspecified criteria. If the agent doesn\u2019t make any assumptions, and keeps asking clarifying questions, then the conversation may get unnecessarily bogged down in details. On the other hand, if too many assumptions are made, the agent\u2019s representation of the information need may significantly differ from the user\u2019s representation.\nThis will mean that there may be a trade-off between the efficiency of the conversation and the accuracy of the information need as the agent has to decide between how important it is to clarify and how risky it is to infer or impute the underspecified or missing details.Elicit-Reveal Decision Task. At some point during the conversation, the agent will need to decide whether it should: (i) continue eliciting requirements from the user, or (ii) given the CIN reveal options to the user (in some manner). For example, let\u2019s assume that the CIN is that the user wants to visit Nice in France, and wants to know about the different museums and galleries in the area. To go through and list all options, for most use cases, is not going to be a very efficient, and quite possibly quite cognitively taxing on the user. Conversely, if the CIN is so overly specified that only a few or no options are available, then eliciting further requirements is also inefficient and pointless. At some point, the time it takes to elicit will be similar to the time it takes to reveal. For example, it may be more efficient and useful if agent reveals: \u201cThere are variety of museums and galleries depending on what your interested in, specific artists, local and living artists, modern art, popular or period exhibitions\u201d rather asking the user about the different aspects, one at a time. Finding the balance between eliciting and revealing is a key decision that the agent will have to make, which will again influence how efficient and effective the agent is in helping the user explore the space of available options.Reveal Task. Once the agent decides to reveal options, it then needs to decide how to reveal the objects available to the user, i.e. should the agent: list, summarize, compare, etc. Again how the options are revealed will impact the direction and efficiency of the conversation. For example, providing a long listing of the options will take longer and be more cognitively taxing than providing a summary of the set of options, so that the user can drill down. A further key challenge here is to provide a compact, descriptive and useful summary, listing or comparison of the objects that focuses on the most salient features that are relevant to the task at hand i.e. helping the user understand the possible space of options available so that they can further refine/expand their information need accordingly.Suggest Task. During the conversation, the agent may decide to make suggestions based on the current or past information needs. As such the agent needs to generate hypothetical i.e. alternate information needs. For example, the user may have narrowed down the search space to a handful of museums to visit. The agent might then decide to provide alternatives, such as other related museums, ones that are close to the others, or museums that are similar but rated more highly. This presents yet another conversational trade-off \u2014 suggestions help provide the user with a better understanding of the space of options (i.e. maximizes exploration), but at the expense of increasing the conversational effort. If the agent keeps suggesting alternative options, the conversation will be protracted and drawn out, frustrating the user especially if non-relevant options are been recommended. However, the user may be rather dissatisfied if they later learn that a better option was available to them that the agent had not suggested.Report Task. During the conversation, the user may explicitly ask the agent to report their understanding of the CIN, however, the agent may also decide to voluntarily disclose what it understands. One one hand, reporting the state will provide conversational awareness ensuring that the user and agent are on talking about the same thing, but it will elongate the conversation and increase the conversational effort. On the other hand, if the agent does not report its understanding, it risks straying from what the user actually wants. So two challenge that arises here is how to efficiently report the agent\u2019s understanding, and when to report its understanding to the user.Above we described a number of tasks that the agent is faced with in processing and dealing with the user\u2019s requests/responses given the conversation. This is only a subset of core tasks that agents need to be able to perform. By breaking down the conversational process into these tasks, we can focus more specifically on the different decisions that agent needs to make and consider how these actions can be evaluated separately.In this work, we enumerate key aspects of the information action space for users and agents during the conversational search process. We created a simple conceptual framework for conversational search by providing a set of possible actions that agents need to perform and the key decisions that they have to make during the conversational search process.While the conceptual framework is an incomplete specification, it provides a starting point for the development of complete interaction model, that is based on previous work. By outlining these different actions together we begin to obtain a clearer picture of the nature of the conversational search process \u2013 and in doing so we have identified various trade-offs between different actions. For example, the contrasting objectives to minimize effort while also maximizing exploration of relevant objects.In this paper, we have not discussed nor specified how to implement the actions or decisions that the agent needs to perform. This is very much an open problem. However, the conceptual representation helps to draw attention to these key decisions and actions, so that implementations of the interaction model will allow aspects of the conceptual framework to be tested empirically, and identify which actions/intents are critical and which ones may be discarded or refined.We hope that the proposed framework will lead to a more principled approach to the development of conversational search agent because the community will be able to focus on th key actions and intents as well as the process (decision points) and design evaluation tasks to examine them in detail. As the sophistication of agents develop, so too will the space of actions capable of being performed. To support each action and intent is a challenge in its own right - for example - how should the agent reveal options/objects, summarize a set of options, etc., and when should the agent elicit, reveal, explain, hypothesize, etc. So while, we make no claim that this is a definitive conceptual framework. Its goal is to serve as a starting point for deeper discussion, such as:What other actions, intents and decisions should be considered?How can we best represent the interaction between the agent and user in such a framework?How can we more formally represent and model the interaction?And, how can we model the state of the information needs (the current information need, the possible information needs, etc.) and their influence on the search space?Going beyond the workshop we will look to how we can empirically validate the framework using conversational search logs with humans (e.g.\u00a0(McDuff et\u00a0al., 2017)) and how we can develop a software based framework to support our conceptualization.",
    "6": "Based on one million arXiv papers submitted from May 2018 to January 2024, we assess the textual density of ChatGPT\u2019s writing style in their abstracts by means of a statistical analysis of word frequency changes. Our model is calibrated and validated on a mixture of real abstracts and ChatGPT-modified abstracts (simulated data) after a careful noise analysis. We find that ChatGPT is having an increasing impact on arXiv abstracts, especially in the field of computer science, where the fraction of ChatGPT-revised abstracts is estimated to be approximately 35%, if we take the output of one of the simplest prompts, \u201crevise the following sentences\u201d, as a baseline. We conclude with an analysis of both positive and negative aspects of the penetration of ChatGPT into academics\u2019 writing style.Since its official release on November 30, 2022, ChatGPT (Chat Generative Pre-trained Transformer) has impacted many aspects of our lives, and academic writing has not been immune. While ChatGPT does increase productivity and may help scientific discovery (Noy & Zhang, 2023; AI4Science & Quantum, 2023), we must be wary of its potential risks and the possibility of negative impacts. A large number of papers have already explored the advantages and disadvantages of large language models (LLMs) (Kasneci et\u00a0al., 2023); here, we focus on ChatGPT, which is being very widely used (reaching an unprecedented 100 million active users three months after its release) and is considered to be one of the two major milestones of language models along with GPT-4. (Zhao et\u00a0al., 2023; von Garrel & Mayer, 2023)While there is already a corpus of current research on using ChatGPT in academia (Casal & Kessler, 2023; Lingard et\u00a0al., 2023; Fergus et\u00a0al., 2023; Lund et\u00a0al., 2023), to our knowledge only a handful of works have attempted to quantify its impact on the whole academic community. As this article was being finalized, two preprints appeared that addressed related questions: one focuses on AI conferences peer reviews (Liang et\u00a0al., 2024a) and, even more recently, analyzes scientific papers(Liang et\u00a0al., 2024b). They claim that the usage of LLMs is evident in AI conference reviews and scientific writings, especially in computer science papers. Within the broad field of academic writing and publishing, we chose the abstracts of articles as the focus of this work, as they have a relatively uniform format across disciplines, are supposed to condense an entire research article and thus are often highly polished, and can be considered short articles of pure text, not involving pictures nor tables.ChatGPT is of course able to generate abstracts directly given a suitable prompt\u00a0(Luo et\u00a0al., 2023), and studies have shown that identifying such abstracts is not easy even if they remain unedited by humans (Gao et\u00a0al., 2023; Cheng et\u00a0al., 2023) \u2013 watermarking being a possible strategy to enable such identification (Kirchenbauer et\u00a0al., 2023). Determining whether a given few sentences were generated by ChatGPT is difficult, but determining that millions of sentences were influenced by ChatGPT is statistically feasible, as we demonstrate here. We analyzed the fingerprints of ChatGPT on scientific abstracts as a function of time in order to tease out a statistical signature, rather than attempting to detect whether a given abstract was generated or polished with ChatGPT.In fact, that the abstract of a paper shows what we call the \u201cChatGPT style\u201d does not necessarily mean that the authors directly utilized ChatGPT to generate or modify it. It is also possible that the authors used ChatGPT in another context and that, as a result, their writing habits were influenced by the ChatGPT style \u2013 not a remote possibility.\nIt is worth considering in this context that reading and writing in English is more difficult for non-native English academics (Amano et\u00a0al., 2023). Before ChatGPT was released, the pros and cons of other tools were discussed, such as Google Translate (Mundt & Groves, 2016) and Grammarly (Fitria, 2021), but ChatGPT has a much wider range of application scenarios \u2013 not to mention, a much higher flexibility.We have seen similar AI-induced seismic shifts in the past: after AlphaGo (Silver et\u00a0al., 2017) shocked the world, professional Go players have begun training with AI, and the sport of Go has been profoundly changed as a result (Kang et\u00a0al., 2022). AlphaFold brings new opportunities for life science research (Varadi & Velankar, 2023) and ChatGPT has also been used for data extraction in materials science. (Polak & Morgan, 2023). A similar story may be happening with academic writing, especially for researchers whose first language is not English (Hwang et\u00a0al., 2023). This paper is a first effort at establishing whether this is the case.\narXiv dataset: The metadata of arXiv papers are provided by Kaggle (arXiv.org submitters, 2024). Because the abstracts in this dataset are updated when authors submit changes, we used the first version in 2024 (version 161) as well as the last version before the ChatGPT era (version 105). Our observations and analysis are based on one million arXiv articles submitted from May 2018 to January 2024.English word frequency: Google Ngram dataset is chosen for comparison and reference (Michel et\u00a0al., 2011). Specifically, we used the freely available mirrors on Kaggle (http://kaggle.com/datasets/wheelercode/english-word-frequency-list) covering word frequencies from the 1800s to 2019 as established from Google Books.We approach the problem by analyzing how the frequency of words changes after ChatGPT has been deployed, so we define the change factor in the frequency of word i\ud835\udc56iitalic_i, Risubscript\ud835\udc45\ud835\udc56R_{i}italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, as follows:where fi\u2062(t)subscript\ud835\udc53\ud835\udc56\ud835\udc61f_{i}(t)italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t ) is the count of word i\ud835\udc56iitalic_i during the time period t\ud835\udc61titalic_t. We divided the 1 million abstracts into 100 uneven time-periods, each encompassing 10,000 abstracts.Figure 1 illustrates that most of the words with the largest change rate in the time period considered (generally, an increase) in the abstracts are related to hot research topics of the last few years, such as \u201cCovid-19\u201d, \u201cLLMs\u201d, \u201cAI\u201d. But this is not the case for all words with the largest growth in terms of frequency. Indeed, the frequency of some non-specialized words also starts to skyrocket in early 2023, as presented in Figure 2. How could the frequencies of words like \u201csignificant\u201d grow significantly together?\nAnother striking example is the frequency change of the words \u201care\u201d and \u201cis\u201d, as depicted in Figure 3. The counts in 10,000 abstracts of these two words were quite stable before 2023. However, the frequency of these two terms has dropped by more than 10% in 2023.Because the average length of abstracts tends to grow over time, we also considered normalizing frequencies to abstract length. The corresponding figures are displayed in the Appendix, and show similar trends.These examples, anecdotal as they are, may represent the tip of the iceberg of a wider and growing phenomenon: the rapid increase in the usage of ChatGPT. The rise and fall in frequency of specific technical nouns may well be related to the changing popularity of certain research topics, but that a research trend is responsible for the change in usage of adjectives appears implausible \u2013 even less so for words like \u201cis\u201d and \u201care\u201d.We wanted to be more specific about the impact of ChatGPT on articles from different disciplines, so we examined arXiv abstracts from different categories separately. The one million arXiv articles were divided into 20 periods in this part in order to increase the number of articles per period and reduce estimation error, which is not the same as the previous part. The identifier numbers of the first and last arXiv articles corresponding to each period are given in the Appendix.Previous studies have shown that ChatGPT has its own linguistic style (AlAfnan & MohdZuki, 2023), and that likely includes the frequency of some words. Although there is no direct way to investigate ChatGPT\u2019s word preference, we can ask ChatGPT to polish or rewrite real, pre-2023 abstracts, and use the resulting simulation data to calculate the estimated frequency change rate r^i\u2062jsubscript^\ud835\udc5f\ud835\udc56\ud835\udc57\\hat{r}_{ij}over^ start_ARG italic_r end_ARG start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT of word i\ud835\udc56iitalic_i in category j\ud835\udc57jitalic_j:where qi\u2062jdsubscriptsuperscript\ud835\udc5e\ud835\udc51\ud835\udc56\ud835\udc57q^{d}_{ij}italic_q start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT represents the word frequency of real abstracts in the dataset and q~i\u2062jdsubscriptsuperscript~\ud835\udc5e\ud835\udc51\ud835\udc56\ud835\udc57\\tilde{q}^{d}_{ij}over~ start_ARG italic_q end_ARG start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT means the frequency after ChatGPT processing. We have no way of knowing the real usage scenarios of ChatGPT, so some simple prompts were used, for example,\u201cRevise the following sentences:\u201dGPT-3.5 was utilized in our simulations for 10,000 abstracts in period 14 (April 2022 to July 2022), although it may have different word preferences than the more recent GPT-4. Many words have different frequencies before and after ChatGPT processing, such as the words \u201cis\u201d, \u201care\u201d, and \u201csignificant\u201d that we mentioned earlier. For simplicity, the results of the 4 categories with the highest number of articles are shown in Table 1 and the rest parts in this paper, namely cs (computer science), math (mathematics), astro (astrophysics), and cond-mat (condensed matter).This corroborates the hypothesis, formulated earlier, that the drop in the frequency of these two words observed in real abstracts in 2023 may have been caused by ChatGPT.In the meantime, we also defined the word frequency change in all abstracts from year t\u22121\ud835\udc611t-1italic_t - 1 to year t\ud835\udc61titalic_t, Ri\u2062j,tsubscript\ud835\udc45\ud835\udc56\ud835\udc57\ud835\udc61R_{ij,t}italic_R start_POSTSUBSCRIPT italic_i italic_j , italic_t end_POSTSUBSCRIPT:where Fi\u2062j,tsubscript\ud835\udc39\ud835\udc56\ud835\udc57\ud835\udc61F_{ij,t}italic_F start_POSTSUBSCRIPT italic_i italic_j , italic_t end_POSTSUBSCRIPT represent frequency of word i\ud835\udc56iitalic_i per arXiv abstract in category j\ud835\udc57jitalic_j in year t\ud835\udc61titalic_t.Only words with a frequency larger than 0.1 times per abstract before ChatGPT processing are plotted in Figure 4 and Figure 5. The correlation coefficient between the word frequency change in arXiv abstracts and our estimated ChatGPT-induced word frequency change is very small in all four categories of abstracts, as shown in\nFigure 4.However, Figure 5 presents a totally different pattern, where r^i\u2062jsubscript^\ud835\udc5f\ud835\udc56\ud835\udc57\\hat{r}_{ij}over^ start_ARG italic_r end_ARG start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT and Ri\u2062j,2023subscript\ud835\udc45\ud835\udc56\ud835\udc572023R_{ij,2023}italic_R start_POSTSUBSCRIPT italic_i italic_j , 2023 end_POSTSUBSCRIPT are strongly correlated, especially in computer science abstracts. Although many words seem insensitive to ChatGPT, we can still see a positive correlation for some words in this figure, even among the other categories.Taken together, our consideration point to ChatGPT as one of the important reasons, possibly even the main reason, for the recent word frequency change in abstracts. Our next step is to start by modeling ChatGPT impact, as well as estimating the impact based on real data and simulations.Imagine different scenarios of using ChatGPT in scientific writing: a researcher might simply use it to correct grammatical errors, another employs it for translating native sentences into English, and yet another one wants it to polish their draft in English very purposefully. In theory, each of these use cases contributes the same proportion of ChatGPT usage. But, as is well known, different prompts will lead to different outputs, which means different word frequency changes.\nTherefore, we use the more neutral term \u201cChatGPT impact\u201d instead of \u201cproportion\u201d in our estimation part.We start with a simple model, ignoring noise and variability for this subsection. Suppose that the frequency of word i\ud835\udc56iitalic_i for abstracts in subject category j\ud835\udc57jitalic_j changes from fi\u2062j*subscriptsuperscript\ud835\udc53\ud835\udc56\ud835\udc57f^{*}_{ij}italic_f start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT to f~i\u2062j*subscriptsuperscript~\ud835\udc53\ud835\udc56\ud835\udc57\\tilde{f}^{*}_{ij}over~ start_ARG italic_f end_ARG start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT after being processed by ChatGPT, when it\u2019s used as a means to polish and improve the abstract (if not to fully generate it). The corresponding word change rate is defined asSuppose that f\u00afi\u2062j\u2062(t)subscript\u00af\ud835\udc53\ud835\udc56\ud835\udc57\ud835\udc61\\bar{f}_{ij}(t)over\u00af start_ARG italic_f end_ARG start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_t ) is the word frequency for word i\ud835\udc56iitalic_i in category j\ud835\udc57jitalic_j at time period t\ud835\udc61titalic_t, this can be written as:where \u03b7j\u2062(t)subscript\ud835\udf02\ud835\udc57\ud835\udc61\\eta_{j}(t)italic_\u03b7 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ) denotes the proportion of abstracts in category j\ud835\udc57jitalic_j affected by ChatGPT, and fi\u2062j*\u2062(t)subscriptsuperscript\ud835\udc53\ud835\udc56\ud835\udc57\ud835\udc61f^{*}_{ij}(t)italic_f start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_t ) represents the original evolution in word frequency without ChatGPT.This model is highly idealized: we have to additionally consider the effects of noise (such as randomness inside ChatGPT), uncertainty in word usage evolution without ChatGPT, and the epistemic uncertainty in how users actually prompt ChatGPT.We now consider the noise terms, which might be modelled in many different ways.For example, we denote the word frequency for word i\ud835\udc56iitalic_i in category j\ud835\udc57jitalic_j by fi\u2062jdsubscriptsuperscript\ud835\udc53\ud835\udc51\ud835\udc56\ud835\udc57{f}^{d}_{ij}italic_f start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT, which represents the word frequency observed in the data:where \u03b4i\u2062j\u2062(\u22c5)subscript\ud835\udeff\ud835\udc56\ud835\udc57\u22c5\\delta_{ij}(\\cdot)italic_\u03b4 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( \u22c5 ) represents noise and word usage variability which are not directly related to the internal parameters of ChatGPT.Thus, we can defineandIn this case, the equation corresponding to Eq. (5) iswhere the function Ci\u2062j\u2062(\u22c5)subscriptC\ud835\udc56\ud835\udc57\u22c5\\mathrm{C}_{ij}(\\cdot)roman_C start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( \u22c5 ) means the frequency after ChatGPT process.We assume that the noise for word i\ud835\udc56iitalic_i due to ChatGPT processing can be represented as \u03f5i\u2062j\u2062(\u22c5)subscriptitalic-\u03f5\ud835\udc56\ud835\udc57\u22c5\\epsilon_{ij}(\\cdot)italic_\u03f5 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( \u22c5 ) and \u03f5i\u2062js\u2062(\u22c5)superscriptsubscriptitalic-\u03f5\ud835\udc56\ud835\udc57\ud835\udc60\u22c5\\epsilon_{ij}^{s}(\\cdot)italic_\u03f5 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( \u22c5 ), then Eq. (2) and Eq. (4) are related byTherefore,whereThen, Eq. (9) \u2013 representing the difference in word frequency before and after ChatGPT processing \u2013 can be rewritten aswherewhere \u03b4i\u2062j\u2032\u2062(\u22c5)subscriptsuperscript\ud835\udeff\u2032\ud835\udc56\ud835\udc57\u22c5\\delta^{\\prime}_{ij}(\\cdot)italic_\u03b4 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( \u22c5 ) follows the same distribution as \u03b4i\u2062j\u2062(\u22c5)subscript\ud835\udeff\ud835\udc56\ud835\udc57\u22c5\\delta_{ij}(\\cdot)italic_\u03b4 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( \u22c5 ).\nIt should be noted that gi\u2062j\u2062(t)subscript\ud835\udc54\ud835\udc56\ud835\udc57\ud835\udc61g_{ij}(t)italic_g start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_t ) includes only ChatGPT-related noise \u03f5i\u2062j\u2062(\u22c5)subscriptitalic-\u03f5\ud835\udc56\ud835\udc57\u22c5\\epsilon_{ij}(\\cdot)italic_\u03f5 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( \u22c5 ) and \u03f5i\u2062js\u2062(\u22c5)subscriptsuperscriptitalic-\u03f5\ud835\udc60\ud835\udc56\ud835\udc57\u22c5\\epsilon^{s}_{ij}(\\cdot)italic_\u03f5 start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( \u22c5 ), however \u03bei\u2062j\u2062(t)subscript\ud835\udf09\ud835\udc56\ud835\udc57\ud835\udc61\\xi_{ij}(t)italic_\u03be start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_t ) contains \u03b4i\u2062j\u2062(\u22c5)subscript\ud835\udeff\ud835\udc56\ud835\udc57\u22c5\\delta_{ij}(\\cdot)italic_\u03b4 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( \u22c5 ) and \u03b4i\u2062j\u2032\u2062(\u22c5)subscriptsuperscript\ud835\udeff\u2032\ud835\udc56\ud835\udc57\u22c5\\delta^{\\prime}_{ij}(\\cdot)italic_\u03b4 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( \u22c5 ) that are unrelated to ChatGPT.In many data analysis applications, more data point (in our case, using a larger number of words) means better estimates. But in our case, the effect of noise is different for each data point (word), and choosing wisely which words to include can improve our estimates.For simplicity, we defineFor abstracts in category j\ud835\udc57jitalic_j, we use the words in the subset Ijsubscript\ud835\udc3c\ud835\udc57I_{j}italic_I start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT (whose determination is discussed below), of numerosity njsubscript\ud835\udc5b\ud835\udc57n_{j}italic_n start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT. In order to estimate \u03b7j\u2062(t)subscript\ud835\udf02\ud835\udc57\ud835\udc61\\eta_{j}(t)italic_\u03b7 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ), we can use the quadratic loss functionIf we ignored the dependency of gi\u2062j\u2062(t)subscript\ud835\udc54\ud835\udc56\ud835\udc57\ud835\udc61g_{ij}(t)italic_g start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_t ) and \u03bei\u2062j\u2062(t)subscript\ud835\udf09\ud835\udc56\ud835\udc57\ud835\udc61\\xi_{ij}(t)italic_\u03be start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_t ) on \u03b7j\u2062(t)subscript\ud835\udf02\ud835\udc57\ud835\udc61\\eta_{j}(t)italic_\u03b7 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ), the estimate of ChatGPT impact would simply be given by Ordinary Least Squares (OLS) asHowever, since gi\u2062j\u2062(t)subscript\ud835\udc54\ud835\udc56\ud835\udc57\ud835\udc61g_{ij}(t)italic_g start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_t ) also depends on \u03b7j\u2062(t)subscript\ud835\udf02\ud835\udc57\ud835\udc61\\eta_{j}(t)italic_\u03b7 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ) and \u03bei\u2062jsubscript\ud835\udf09\ud835\udc56\ud835\udc57\\xi_{ij}italic_\u03be start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT contains \u03b7j\u2062(t)subscript\ud835\udf02\ud835\udc57\ud835\udc61\\eta_{j}(t)italic_\u03b7 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ) as described in Eq. (15) and Eq. (16), we need to make additional assumptions to progress further.Case 1: if the effect of \u03b7j\u2062(t)subscript\ud835\udf02\ud835\udc57\ud835\udc61\\eta_{j}(t)italic_\u03b7 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ) on \u03bei\u2062j\u2062(t)subscript\ud835\udf09\ud835\udc56\ud835\udc57\ud835\udc61\\xi_{ij}(t)italic_\u03be start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_t ) can be ignored compared to other terms, e.g., the following simple scenario,One can also derive the approximation below:where \u03b4i\u2062j\u2062(*)subscript\ud835\udeff\ud835\udc56\ud835\udc57\\delta_{ij}(*)italic_\u03b4 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( * ) is a random variable with zero mean and variance much smaller than \u03b7j\u2062(t)\u2062fi\u2062j*\u2062(t)subscript\ud835\udf02\ud835\udc57\ud835\udc61subscriptsuperscript\ud835\udc53\ud835\udc56\ud835\udc57\ud835\udc61\\eta_{j}(t)f^{*}_{ij}(t)italic_\u03b7 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ) italic_f start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_t ), and its derivative with respect to \u03b7j\u2062(t)subscript\ud835\udf02\ud835\udc57\ud835\udc61\\eta_{j}(t)italic_\u03b7 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ) is negligible compared to fi\u2062j*\u2062(t)subscriptsuperscript\ud835\udc53\ud835\udc56\ud835\udc57\ud835\udc61f^{*}_{ij}(t)italic_f start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_t ).Therefore, the loss function under this assumption is:Thus,If we require a minimum by setting \u2202Lj,t,g\u2062(\u03b7j)\u2202\u03b7j=0subscript\ud835\udc3f\ud835\udc57\ud835\udc61\ud835\udc54subscript\ud835\udf02\ud835\udc57subscript\ud835\udf02\ud835\udc570\\frac{\\partial L_{j,t,g}(\\eta_{j})}{\\partial\\eta_{j}}=0divide start_ARG \u2202 italic_L start_POSTSUBSCRIPT italic_j , italic_t , italic_g end_POSTSUBSCRIPT ( italic_\u03b7 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) end_ARG start_ARG \u2202 italic_\u03b7 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG = 0, we obtain a new estimate \u03b7^jg\u2062(t)subscriptsuperscript^\ud835\udf02\ud835\udc54\ud835\udc57\ud835\udc61\\hat{\\eta}^{g}_{j}(t)over^ start_ARG italic_\u03b7 end_ARG start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ), which is equal to the OLS \u03b7^j\u2062(t)subscript^\ud835\udf02\ud835\udc57\ud835\udc61\\hat{\\eta}_{j}(t)over^ start_ARG italic_\u03b7 end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ) in Eq. (19) corrected for bias and noise,But without knowing the distribution of \u03f5i\u2062j\u2062(\u22c5)subscriptitalic-\u03f5\ud835\udc56\ud835\udc57\u22c5\\epsilon_{ij}(\\cdot)italic_\u03f5 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( \u22c5 ) and \u03f5i\u2062js\u2062(\u22c5)subscriptsuperscriptitalic-\u03f5\ud835\udc60\ud835\udc56\ud835\udc57\u22c5\\epsilon^{s}_{ij}(\\cdot)italic_\u03f5 start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( \u22c5 ), we have no way of estimating the value of this bias, so we assume that \u03f5i\u2062j\u2062(fi\u2062j)\u223c\ud835\udca9\u2062(0,fi\u2062j\u2062\u03c3i\u2062j,\u03f52)similar-tosubscriptitalic-\u03f5\ud835\udc56\ud835\udc57subscript\ud835\udc53\ud835\udc56\ud835\udc57\ud835\udca90subscript\ud835\udc53\ud835\udc56\ud835\udc57superscriptsubscript\ud835\udf0e\ud835\udc56\ud835\udc57italic-\u03f52\\epsilon_{ij}(f_{ij})\\sim\\mathcal{N}(0,f_{ij}\\sigma_{ij,\\epsilon}^{2})italic_\u03f5 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ) \u223c caligraphic_N ( 0 , italic_f start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT italic_\u03c3 start_POSTSUBSCRIPT italic_i italic_j , italic_\u03f5 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) and \u03f5i\u2062js\u2062(fi\u2062j)\u223c\ud835\udca9\u2062(0,fi\u2062j\u2062\u03c3i\u2062j,\u03f52)similar-tosubscriptsuperscriptitalic-\u03f5\ud835\udc60\ud835\udc56\ud835\udc57subscript\ud835\udc53\ud835\udc56\ud835\udc57\ud835\udca90subscript\ud835\udc53\ud835\udc56\ud835\udc57superscriptsubscript\ud835\udf0e\ud835\udc56\ud835\udc57italic-\u03f52\\epsilon^{s}_{ij}(f_{ij})\\sim\\mathcal{N}(0,f_{ij}\\sigma_{ij,\\epsilon}^{2})italic_\u03f5 start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ) \u223c caligraphic_N ( 0 , italic_f start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT italic_\u03c3 start_POSTSUBSCRIPT italic_i italic_j , italic_\u03f5 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ), e.g., \u03f5i\u2062j\u2062(1)\u223c\ud835\udca9\u2062(0,\u03c3i\u2062j,\u03f52)similar-tosubscriptitalic-\u03f5\ud835\udc56\ud835\udc571\ud835\udca90superscriptsubscript\ud835\udf0e\ud835\udc56\ud835\udc57italic-\u03f52\\epsilon_{ij}(1)\\sim\\mathcal{N}(0,\\sigma_{ij,\\epsilon}^{2})italic_\u03f5 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( 1 ) \u223c caligraphic_N ( 0 , italic_\u03c3 start_POSTSUBSCRIPT italic_i italic_j , italic_\u03f5 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ), then we can obtain an expression for \u03f5i\u2062j\u03b7\u2062(q,f,t)subscriptsuperscriptitalic-\u03f5\ud835\udf02\ud835\udc56\ud835\udc57\ud835\udc5e\ud835\udc53\ud835\udc61\\epsilon^{\\eta}_{ij}(q,f,t)italic_\u03f5 start_POSTSUPERSCRIPT italic_\u03b7 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_q , italic_f , italic_t ):Therefore, all terms on the right-hand side of Eq. (23) are zero-mean noise, except for the last one:Removing the items with zero means, we getAnd the bias part is expressed asSome insights can be gained from the results above. As by definition \u03b7j\u2062(t)\u22650subscript\ud835\udf02\ud835\udc57\ud835\udc610\\eta_{j}(t)\\geq 0italic_\u03b7 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ) \u2265 0, the estimate \u03b7^j\u2062(t)subscript^\ud835\udf02\ud835\udc57\ud835\udc61\\hat{\\eta}_{j}(t)over^ start_ARG italic_\u03b7 end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ) given by Eq. (19) tends to be biased high in our model. The value of r^i\u2062jsubscript^\ud835\udc5f\ud835\udc56\ud835\udc57\\hat{r}_{ij}over^ start_ARG italic_r end_ARG start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT plays a role in the minimization of bias, as it only appears in the denominator in Eq. (29). Similarly, if the value of r^i\u2062jsubscript^\ud835\udc5f\ud835\udc56\ud835\udc57\\hat{r}_{ij}over^ start_ARG italic_r end_ARG start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT is similar for different words, then larger values of qi\u2062jdsubscriptsuperscript\ud835\udc5e\ud835\udc51\ud835\udc56\ud835\udc57q^{d}_{ij}italic_q start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT and fi\u2062j*subscriptsuperscript\ud835\udc53\ud835\udc56\ud835\udc57f^{*}_{ij}italic_f start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT will reduce the bias, as seen from Eq.\u00a0(28) \u2013 therefore, we should consider including preferentially in our analysis words with larger values of qi\u2062jdsubscriptsuperscript\ud835\udc5e\ud835\udc51\ud835\udc56\ud835\udc57q^{d}_{ij}italic_q start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT, fi\u2062j*subscriptsuperscript\ud835\udc53\ud835\udc56\ud835\udc57f^{*}_{ij}italic_f start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT and r^i\u2062jsubscript^\ud835\udc5f\ud835\udc56\ud835\udc57\\hat{r}_{ij}over^ start_ARG italic_r end_ARG start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT. Considering that the value of \u03b7j\u2062(t)subscript\ud835\udf02\ud835\udc57\ud835\udc61\\eta_{j}(t)italic_\u03b7 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ) affects the bias as well, which is not simply linear, we are led to consider adaptive or iterative criteria for word choice, which will in general depend on the true (and unknown) value of \u03b7j\u2062(t)subscript\ud835\udf02\ud835\udc57\ud835\udc61\\eta_{j}(t)italic_\u03b7 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ).Case 2: Gaussian distribution for \u03b4i\u2062j\u2062(fi\u2062j)subscript\ud835\udeff\ud835\udc56\ud835\udc57subscript\ud835\udc53\ud835\udc56\ud835\udc57\\delta_{ij}(f_{ij})italic_\u03b4 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ), e.g., \u03b4i\u2062j\u2062(fi\u2062j)\u223c\ud835\udca9\u2062(0,fi\u2062j\u2062\u03c3i\u2062j2)similar-tosubscript\ud835\udeff\ud835\udc56\ud835\udc57subscript\ud835\udc53\ud835\udc56\ud835\udc57\ud835\udca90subscript\ud835\udc53\ud835\udc56\ud835\udc57superscriptsubscript\ud835\udf0e\ud835\udc56\ud835\udc572\\delta_{ij}(f_{ij})\\sim\\mathcal{N}(0,f_{ij}\\sigma_{ij}^{2})italic_\u03b4 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ) \u223c caligraphic_N ( 0 , italic_f start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT italic_\u03c3 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ), which is justified empirically in the Appendix, Figure 14 and Figure 16.\nAs a result,Therefore, we can define gi\u2062jc\u2062(t)subscriptsuperscript\ud835\udc54\ud835\udc50\ud835\udc56\ud835\udc57\ud835\udc61g^{c}_{ij}(t)italic_g start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_t ) and \u03bei\u2062jc\u2062(t)subscriptsuperscript\ud835\udf09\ud835\udc50\ud835\udc56\ud835\udc57\ud835\udc61\\xi^{c}_{ij}(t)italic_\u03be start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_t ):As \u03bei\u2062jc\u2062(t)subscriptsuperscript\ud835\udf09\ud835\udc50\ud835\udc56\ud835\udc57\ud835\udc61\\xi^{c}_{ij}(t)italic_\u03be start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_t ) doesn\u2019t depend on \u03b7j\u2062(t)subscript\ud835\udf02\ud835\udc57\ud835\udc61\\eta_{j}(t)italic_\u03b7 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ), the loss function under this assumption is:And we will get a complex expression for the bias part like Eq. (23), which gives us similar conclusions. (Some calculations are in the Appendix.)Finding criteria for selecting the words that are included in the frequency change analysis greatly reduces the computational complexity compared to trying different word combinations. Our analysis of noise models gives some insights into these criteria, such as qi\u2062jdsubscriptsuperscript\ud835\udc5e\ud835\udc51\ud835\udc56\ud835\udc57q^{d}_{ij}italic_q start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT and r^i\u2062jsubscript^\ud835\udc5f\ud835\udc56\ud835\udc57\\hat{r}_{ij}over^ start_ARG italic_r end_ARG start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT.\nUnfortunately, we cannot know the true value of fi\u2062j*\u2062(t)subscriptsuperscript\ud835\udc53\ud835\udc56\ud835\udc57\ud835\udc61f^{*}_{ij}(t)italic_f start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_t ) in the ChatGPT era, but we can replace it with the estimation f^i\u2062j*\u2062(t)subscriptsuperscript^\ud835\udc53\ud835\udc56\ud835\udc57\ud835\udc61\\hat{f}^{*}_{ij}(t)over^ start_ARG italic_f end_ARG start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_t ) based on the word frequency before ChatGPT was introduced. As our objective is to identify the words that ChatGPT \u201clikes\u201d (or \u201cdislikes\u201d) to use compared to academic researchers on average, we assume that the frequencies of these words should remain stable without ChatGPT, i.e., we take the average of the pre-ChatGPT periods before T0subscript\ud835\udc470T_{0}italic_T start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT as following:Considering that the noise in real data is likely highly complex, we did not estimate the variance of \u03f5i\u2062j\u2062(\u22c5)subscriptitalic-\u03f5\ud835\udc56\ud835\udc57\u22c5\\epsilon_{ij}(\\cdot)italic_\u03f5 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( \u22c5 ). Instead, we used ChatGPT to process additional abstracts (on top of those used to estimate ri\u2062jsubscript\ud835\udc5f\ud835\udc56\ud835\udc57r_{ij}italic_r start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT), and used the resulting frequencies as calibration for the bias and noise.In order to verify the theoretical and practical validity of our approach, we used calibrations and tests, with ChatGPT-processed abstracts mixed with real abstracts.As previous analyses have demonstrated, with the goal of reducing bias in estimation, different selected words are likely to correspond to the different (unknown) ground truth value of \u03b7j\u2062(t)subscript\ud835\udf02\ud835\udc57\ud835\udc61\\eta_{j}(t)italic_\u03b7 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ). Therefore, we construct N\ud835\udc41Nitalic_N different sets of abstract data for calibration, Dnsubscript\ud835\udc37\ud835\udc5bD_{n}italic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT, with its correspond mixed ratio of ChatGPT-processed abstracts, \u03b7nsubscript\ud835\udf02\ud835\udc5b\\eta_{n}italic_\u03b7 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT, asand similarly for test data Tn\u2032subscript\ud835\udc47superscript\ud835\udc5b\u2032T_{n^{\\prime}}italic_T start_POSTSUBSCRIPT italic_n start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT and \u03b7n\u2032\u2032subscriptsuperscript\ud835\udf02\u2032superscript\ud835\udc5b\u2032\\eta^{\\prime}_{n^{\\prime}}italic_\u03b7 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT,And for one pair of (Dn,\u03b7n)subscript\ud835\udc37\ud835\udc5bsubscript\ud835\udf02\ud835\udc5b(D_{n},\\eta_{n})( italic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_\u03b7 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) and a specific word choice requirement qksubscript\ud835\udc5e\ud835\udc58q_{k}italic_q start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT (for example, ri\u2062j*>0.1subscriptsuperscript\ud835\udc5f\ud835\udc56\ud835\udc570.1r^{*}_{ij}>0.1italic_r start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT > 0.1 and r^i\u2062j+1r^i\u2062j2<0.1+10.12subscript^\ud835\udc5f\ud835\udc56\ud835\udc571subscriptsuperscript^\ud835\udc5f2\ud835\udc56\ud835\udc570.11superscript0.12\\frac{\\hat{r}_{ij}+1}{\\hat{r}^{2}_{ij}}<\\frac{0.1+1}{0.1^{2}}divide start_ARG over^ start_ARG italic_r end_ARG start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT + 1 end_ARG start_ARG over^ start_ARG italic_r end_ARG start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT end_ARG < divide start_ARG 0.1 + 1 end_ARG start_ARG 0.1 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG), the efficiency can be defined aswhere \u03b7^n\u2062(Dn,qk)subscript^\ud835\udf02\ud835\udc5bsubscript\ud835\udc37\ud835\udc5bsubscript\ud835\udc5e\ud835\udc58\\hat{\\eta}_{n}(D_{n},q_{k})over^ start_ARG italic_\u03b7 end_ARG start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_q start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) is the estimate of \u03b7nsubscript\ud835\udf02\ud835\udc5b\\eta_{n}italic_\u03b7 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT using Eq. (19) and the words set Ijsubscript\ud835\udc3c\ud835\udc57I_{j}italic_I start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT can be derived from qksubscript\ud835\udc5e\ud835\udc58q_{k}italic_q start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, denoted Ij\u2062(qk)subscript\ud835\udc3c\ud835\udc57subscript\ud835\udc5e\ud835\udc58I_{j}(q_{k})italic_I start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )\nFor a given set of qksubscript\ud835\udc5e\ud835\udc58q_{k}italic_q start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT (examples can be found in the Appendix), we are looking for the best one minimizing e\u2062(Dn,\u03b7n,qk)\ud835\udc52subscript\ud835\udc37\ud835\udc5bsubscript\ud835\udf02\ud835\udc5bsubscript\ud835\udc5e\ud835\udc58e(D_{n},\\eta_{n},q_{k})italic_e ( italic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_\u03b7 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_q start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ), denoted q\u2062(Dn,\u03b7n)\ud835\udc5esubscript\ud835\udc37\ud835\udc5bsubscript\ud835\udf02\ud835\udc5bq(D_{n},\\eta_{n})italic_q ( italic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_\u03b7 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ), which is the calibration part.For the test data Tn\u2032subscript\ud835\udc47superscript\ud835\udc5b\u2032T_{n^{\\prime}}italic_T start_POSTSUBSCRIPT italic_n start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT, the estimate of \u03b7n\u2032subscript\ud835\udf02superscript\ud835\udc5b\u2032\\eta_{n^{\\prime}}italic_\u03b7 start_POSTSUBSCRIPT italic_n start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT is calculated from Eq. (19) with different Ijsubscript\ud835\udc3c\ud835\udc57I_{j}italic_I start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT, based on different q\u2062(Dn,\u03b7n)\ud835\udc5esubscript\ud835\udc37\ud835\udc5bsubscript\ud835\udf02\ud835\udc5bq(D_{n},\\eta_{n})italic_q ( italic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_\u03b7 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) obtained in the calibration procedure.Because of the goal of the calibration, word choice may well actually introduce a new bias to neutralize the original bias, so that the estimate is not necessarily higher in the test results than the ground truth.To calibrate the choice of set Ijsubscript\ud835\udc3c\ud835\udc57I_{j}italic_I start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT, we use different mixing ratios, in proportion to the value of \u03b7j\u2062(t)subscript\ud835\udf02\ud835\udc57\ud835\udc61\\eta_{j}(t)italic_\u03b7 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ). In addition, we only consider the 10,000 words with the highest frequency in the Google Ngram dataset.We continue our simulations based on GPT-3.5. As the training data for GPT-3.5 is up to September 2021, abstracts submitted later than this time are considered: 20,000 abstracts in period 13 to estimate ri\u2062jsubscript\ud835\udc5f\ud835\udc56\ud835\udc57r_{ij}italic_r start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT, 10,000 abstracts in period 12 for calibration, and 10,000 abstracts in period 14 for testing.We used the first 10 periods before ChatGPT was introduced, to estimate fi\u2062j*\u2062(t)subscriptsuperscript\ud835\udc53\ud835\udc56\ud835\udc57\ud835\udc61f^{*}_{ij}(t)italic_f start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_t ), as they weren\u2019t influenced by ChatGPT, which means T0=10subscript\ud835\udc47010T_{0}=10italic_T start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 10 and #\u2062{t\u2264T0}=10#\ud835\udc61subscript\ud835\udc47010\\#\\{t\\leq T_{0}\\}=10# { italic_t \u2264 italic_T start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT } = 10 in Eq. (34).We take {\u03b7n}={0,0.05,1,\u2026,0.45,0.5}subscript\ud835\udf02\ud835\udc5b00.051\u20260.450.5\\{\\eta_{n}\\}=\\{0,0.05,1,\\dots,0.45,0.5\\}{ italic_\u03b7 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } = { 0 , 0.05 , 1 , \u2026 , 0.45 , 0.5 } and m=1\ud835\udc5a1m=1italic_m = 1, which means N=#\u2062{(Dn,\u03b7n)}=11\ud835\udc41#subscript\ud835\udc37\ud835\udc5bsubscript\ud835\udf02\ud835\udc5b11N=\\#\\{(D_{n},\\eta_{n})\\}=11italic_N = # { ( italic_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_\u03b7 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) } = 11.\nThen the 11 Ijsubscript\ud835\udc3c\ud835\udc57I_{j}italic_I start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT (with possible repetitions), obtained from mixed data with 11 corresponding \u03b7nsubscript\ud835\udf02\ud835\udc5b\\eta_{n}italic_\u03b7 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT of period 12, were used for \u03b7n\u2032superscriptsubscript\ud835\udf02\ud835\udc5b\u2032\\eta_{n}^{\\prime}italic_\u03b7 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT estimation in the test data (period 14). Other parameters can be found in the Appendix.The results using the same prompt for generating calibration and test data are shown in Figure 6, with injected mixed ratio (i.e., ChatGPT impact) \u03b7n\u2032superscriptsubscript\ud835\udf02\ud835\udc5b\u2032\\eta_{n}^{\\prime}italic_\u03b7 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT from 0 to 0.5. It is clear that when the calibration and test sets are mixed in the same ratio, word combinations that achieve better estimates on the calibration set generally work better on the test set, as well.Unlike in Figure 6 where we normalized the word frequency by the total number of abstracts, we normalized it by the total number of words for one period in Figure 7. The trends remain similar, albeit different in detail.Because one may use a wide variety of prompts in practical applications, we also evaluated the robustness of our approach by adopting a different prompt for generating the test data than the one we used for calibration. The corresponding results in Figure 8 use the following prompt:\u201dPlease rewrite the following paragraph from an academic paper:\u201dIn this example, we add the word \u201cplease\u201d and make it clear that this comes from an \u201cacademic paper\u201d, replacing \u201crevise\u201d with \u201crewrite\u201d.Although the quantitative results of our tests were not as good as before, the errors were still small at lower mixed ratios, which also illustrates the robustness of our method. This is understandable because in data generated with different prompts, not all of our previous assumptions hold, and the estimate of r^i\u2062jsubscript^\ud835\udc5f\ud835\udc56\ud835\udc57\\hat{r}_{ij}over^ start_ARG italic_r end_ARG start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT on ri\u2062jsubscript\ud835\udc5f\ud835\udc56\ud835\udc57r_{ij}italic_r start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT in our model may be biased. We can also note that most of our estimates in Figure 8 are on the high side relative to the ground truth, most likely because we use a more precise prompt for the test data here, making the frequency change rate of the relevant words higher.The estimates of ChatGPT impact on the real data are shown in Figure 9 and Figure 10. Based on our calibration results, we chose 11 words set Ijsubscript\ud835\udc3c\ud835\udc57I_{j}italic_I start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT for different injected values of \u03b7nsubscript\ud835\udf02\ud835\udc5b\\eta_{n}italic_\u03b7 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT. According to the results of the first estimation about \u03b7j\u2062(t)subscript\ud835\udf02\ud835\udc57\ud835\udc61\\eta_{j}(t)italic_\u03b7 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ), we found the three values of \u03b7nsubscript\ud835\udf02\ud835\udc5b\\eta_{n}italic_\u03b7 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT that were closest to the mean of the first estimation and used their optimal word set Ijsubscript\ud835\udc3c\ud835\udc57I_{j}italic_I start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT in the calibration procedure for a second estimation, leading to the triangle points shown in the figures.Despite mild differences in the estimates under the two different normalizations, the conclusions are essentially the same. Our estimates on \u03b7j\u2062(t)subscript\ud835\udf02\ud835\udc57\ud835\udc61\\eta_{j}(t)italic_\u03b7 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ) hover around 0 until 2023, which gives reassurance of the reliability of our methodology. More and more abstracts are being influenced by ChatGPT, especially in the cs category, starting from December 2022, after the release of ChatGPT.Our estimate indicates that the density of ChatGPT style texts of the most recent time period in this category is around 35%, when we use the results of one simple prompt, \u201crevise the following sentences\u201d, as a baseline. By contrast, we detected a much smaller uptick in ChatGPT impact in math, while astro and cond-mat both reach values between 10% and 20%, approximately.It is important to note that our ChatGPT impact here is a relative value that corresponds to the change in word frequency from the use of simple prompts. More precise prompts, both in reality and in simulation, could potentially lead to an impact value greater than 1.Is ChatGPT transforming academics\u2019 writing style? An important question before these discussions is the evaluation of the actual penetration of the usage of ChatGPT in academic writing \u2013 without a quantitative estimate, the debate is founded on anecdotal evidence.We have demonstrated here that a simple statistical analysis of word frequency is sufficient to detect and analyze the impact of ChatGPT in arXiv abstracts, which is easily extendable to other subjects and to the complete text of articles. We found convincing evidence of a change in word frequency after ChatGPT\u2019s release, consistent with predictions obtained from simulating ChatGPT\u2019s impact from possible users\u2019 prompts. The most enthusiastic community (among the four we investigated) in terms of ChatGPT adoption appears to be that of computer scientists, a result that is perhaps unsurprising. Mathematicians, by contrast, are the least keen.Our estimates are founded on a population level and based on the output of simple prompts. Using more precise prompts, it is entirely possible to achieve abstracts that are more ChatGPT-like texts than our simulations. In addition, in the real world people might use large language models other than ChatGPT to revise articles, which may have similar but not identical word preferences to ChatGPT.Another central take-away from this article is that we can monitor the impact of ChaGPT on academic writing by using simple and transparent statistical methods (e.g., word frequencies) rather than black-box GPT detectors. At the same time, we also believe that better estimates can be made by more rigorous analysis, such as considering more complex noise terms.The debate around the usage of generative models such as ChatGPT in academic writing is multi-faceted: from fears of lowering rigour due to \u201challucinations\u201d to uncertainty about the actual sources of AI-produced text. It is however indisputable that tools such as ChatGPT also have positive impacts: they help non-English native writers to improve the quality and flow of their text, as well as to translate into English from their mother tongue or vice versa. In this sense, generative AI is a great leveller, and as such it is a welcome addition to the academic\u2019s toolbox. What we need to be wary of is its use in fully generative mode, without expert human supervision \u2013 something that we have not addressed in this paper.We are aware that our methods can be further improved. For example, our results follow from analyzing a set of words selected based on the value of qi\u2062jdsubscriptsuperscript\ud835\udc5e\ud835\udc51\ud835\udc56\ud835\udc57q^{d}_{ij}italic_q start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT and r^i\u2062jsubscript^\ud835\udc5f\ud835\udc56\ud835\udc57\\hat{r}_{ij}over^ start_ARG italic_r end_ARG start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT. It is actually possible to fine-tune this criterium for a more accurate word selection, which would theoretically give better results, but would be more computationally expensive. Similarly, trying a larger range of prompts should theoretically result in better estimates. We are more interested in the density of ChatGPT style texts and its relative value (comparisons between categories and over time) than in establishing how many people are using ChatGPT \u2013 this can be estimated with the help of questionnaires, and it is not possible to get an accurate estimate only based on simulated data.As our results have shown, ChatGPT is having an increasing impact on academic publications. This trend is hard to avoid, and we need to adapt gradually. With the increasing influx of young researchers, especially non-native English speakers, tools based on large language models, represented by ChatGPT, are transforming academic writing, at least for some disciplines. Even if you refuse to use them, you are likely to be influenced indirectly.Formally, arXiv has 8 categories in total: physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems science, economics. The first 3 categories contribute the vast majority of arXiv articles, around 91% among the 1 million articles. Hence, we divided the physics papers into sub-categories: astrophysics, condensed matter, high energy physics, etc. The four categories (computer science, mathematics, astrophysics, condensed matter) we selected account for 70% of the total number of articles. To avoid repetition, we also only count the first category of the article for those that have multiple categories (cross-postings).model: gpt-3.5-turbo-1106temperature: 0.7seed: 1106top_p: 0.21qi\u2062jd1subscriptsuperscript\ud835\udc5e\ud835\udc51\ud835\udc56\ud835\udc57\\frac{1}{q^{d}_{ij}}divide start_ARG 1 end_ARG start_ARG italic_q start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT end_ARG: 10, 20, 30, 40, 50, 60, 70, 80, 100, 150, 200, 500r^i\u2062jsubscript^\ud835\udc5f\ud835\udc56\ud835\udc57\\hat{r}_{ij}over^ start_ARG italic_r end_ARG start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT: 0.1, 0.15, 0.2, 0,3, 0.4, 0.5, 0.6, 0.7, 0.8 (corresponding value of r^i\u2062j+1r^i\u2062j2subscript^\ud835\udc5f\ud835\udc56\ud835\udc571subscriptsuperscript^\ud835\udc5f2\ud835\udc56\ud835\udc57\\frac{\\hat{r}_{ij}+1}{\\hat{r}^{2}_{ij}}divide start_ARG over^ start_ARG italic_r end_ARG start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT + 1 end_ARG start_ARG over^ start_ARG italic_r end_ARG start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT end_ARG)For example, when we take 1qi\u2062jd<101subscriptsuperscript\ud835\udc5e\ud835\udc51\ud835\udc56\ud835\udc5710\\frac{1}{q^{d}_{ij}}<10divide start_ARG 1 end_ARG start_ARG italic_q start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT end_ARG < 10 and r^i\u2062j+1r^i\u2062j2<0.1+10.12subscript^\ud835\udc5f\ud835\udc56\ud835\udc571subscriptsuperscript^\ud835\udc5f2\ud835\udc56\ud835\udc570.11superscript0.12\\frac{\\hat{r}_{ij}+1}{\\hat{r}^{2}_{ij}}<\\frac{0.1+1}{0.1^{2}}divide start_ARG over^ start_ARG italic_r end_ARG start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT + 1 end_ARG start_ARG over^ start_ARG italic_r end_ARG start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT end_ARG < divide start_ARG 0.1 + 1 end_ARG start_ARG 0.1 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG for abstracts in computer science, the words that satisfy the conditions are: \u2019the\u2019, \u2019is\u2019, \u2019for\u2019, \u2019by\u2019, \u2019be\u2019, \u2019this\u2019, \u2019are\u2019, \u2019i\u2019, \u2019at\u2019, \u2019which\u2019, \u2019an\u2019, \u2019have\u2019, \u2019but\u2019, \u2019we\u2019, \u2019all\u2019, \u2019they\u2019, \u2019one\u2019, \u2019has\u2019, \u2019their\u2019, \u2019other\u2019, \u2019there\u2019, \u2019more\u2019, \u2019new\u2019, \u2019any\u2019, \u2019these\u2019, \u2019time\u2019, \u2019than\u2019, \u2019some\u2019, \u2019only\u2019, \u2019two\u2019, \u2019into\u2019, \u2019them\u2019, \u2019our\u2019, \u2019under\u2019, \u2019first\u2019, \u2019most\u2019, \u2019then\u2019, \u2019over\u2019, \u2019work\u2019, \u2019where\u2019, \u2019many\u2019, \u2019through\u2019, \u2019well\u2019, \u2019how\u2019, \u2019even\u2019, \u2019while\u2019, \u2019however\u2019, \u2019high\u2019, \u2019given\u2019, \u2019present\u2019, \u2019large\u2019, \u2019research\u2019, \u2019different\u2019, \u2019set\u2019, \u2019study\u2019, \u2019important\u2019, \u2019several\u2019, \u2019e\u2019, \u2019further\u2019, \u2019including\u2019, \u2019often\u2019, \u2019provide\u2019, \u2019due\u2019, \u2019using\u2019, \u2019better\u2019, \u2019various\u2019, \u2019problem\u2019, \u2019show\u2019, \u2019problems\u2019, \u2019design\u2019, \u2019proposed\u2019, \u2019g\u2019, \u2019across\u2019, \u2019approach\u2019, \u2019existing\u2019, \u2019compared\u2019, \u2019task\u2019, \u2019learn\u2019, \u2019improve\u2019, \u2019achieve\u2019, \u2019novel\u2019, \u2019domain\u2019, \u2019demonstrate\u2019, \u2019introduce\u2019, \u2019propose\u2019, \u2019prediction\u2019.And when 1qi\u2062jd<501subscriptsuperscript\ud835\udc5e\ud835\udc51\ud835\udc56\ud835\udc5750\\frac{1}{q^{d}_{ij}}<50divide start_ARG 1 end_ARG start_ARG italic_q start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT end_ARG < 50 and r^i\u2062j+1r^i\u2062j2<0.8+10.82subscript^\ud835\udc5f\ud835\udc56\ud835\udc571subscriptsuperscript^\ud835\udc5f2\ud835\udc56\ud835\udc570.81superscript0.82\\frac{\\hat{r}_{ij}+1}{\\hat{r}^{2}_{ij}}<\\frac{0.8+1}{0.8^{2}}divide start_ARG over^ start_ARG italic_r end_ARG start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT + 1 end_ARG start_ARG over^ start_ARG italic_r end_ARG start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT end_ARG < divide start_ARG 0.8 + 1 end_ARG start_ARG 0.8 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG, the words are: \u2019i\u2019, \u2019would\u2019, \u2019so\u2019, \u2019some\u2019, \u2019what\u2019, \u2019out\u2019, \u2019work\u2019, \u2019very\u2019, \u2019because\u2019, \u2019much\u2019, \u2019good\u2019, \u2019way\u2019, \u2019great\u2019, \u2019here\u2019, \u2019since\u2019, \u2019might\u2019, \u2019last\u2019, \u2019end\u2019, \u2019means\u2019, \u2019having\u2019, \u2019thus\u2019, \u2019above\u2019, \u2019give\u2019, \u2019e\u2019, \u2019further\u2019, \u2019far\u2019, \u2019find\u2019, \u2019although\u2019, \u2019show\u2019, \u2019n\u2019, \u2019help\u2019, \u2019together\u2019, \u2019particular\u2019, \u2019whose\u2019, \u2019issue\u2019, \u2019according\u2019, \u2019addition\u2019, \u2019usually\u2019, \u2019art\u2019, \u2019especially\u2019, \u2019respect\u2019, \u2019works\u2019, \u2019shows\u2019, \u2019g\u2019, \u2019makes\u2019, \u2019hard\u2019, \u2019significant\u2019, \u2019run\u2019, \u2019address\u2019, \u2019particularly\u2019, \u2019idea\u2019, \u2019consider\u2019, \u2019includes\u2019, \u2019built\u2019, \u2019adopted\u2019, \u2019obtain\u2019, \u2019establish\u2019, \u2019useful\u2019, \u2019leading\u2019, \u2019performed\u2019, \u2019create\u2019, \u2019named\u2019, \u2019conducted\u2019, \u2019resulting\u2019, \u2019hence\u2019, \u2019findings\u2019, \u2019towards\u2019, \u2019prove\u2019, \u2019build\u2019, \u2019perform\u2019, \u2019moreover\u2019, \u2019describe\u2019, \u2019besides\u2019, \u2019demonstrated\u2019, \u2019via\u2019, \u2019presents\u2019, \u2019mainly\u2019, \u2019fail\u2019, \u2019namely\u2019, \u2019allowing\u2019, \u2019demonstrate\u2019, \u2019advances\u2019, \u2019suffer\u2019, \u2019overcome\u2019, \u2019introduce\u2019, \u2019accurately\u2019, \u2019identifying\u2019, \u2019enhance\u2019, \u2019crucial\u2019, \u2019etc\u2019, \u2019utilize\u2019, \u2019demonstrates\u2019, \u2019additionally\u2019, \u2019focuses\u2019, \u2019motivated\u2019, \u2019characterize\u2019.Abstracts in the cs category among the first 500,000 articles were divided into groups in chronological order, with the same number in each group. We counted the number of occurrences of each word within each group, and calculated the variance between the different groups. This was repeated as a function of the number of abstracts included in each group, and the results are shown in Figure 13.Then we also analyzed the variance-to-mean ratio (defined as the variance of the sum of a word\u2019s counts divided by the mean of the sum) and the coefficient of variation (defined as the standard deviation of the sum divided by the mean of the sum) for the 12 most frequent words, as shown in Figure 14 and Figure 15, and the variance-mean ratio of further words as in Figure 16.We observe that, at least for a subset of the words considered here, the variance-to-mean ratios are essentially on the same scale (although there are words that do not follow this pattern). Therefore, a simple Gaussian distributionwhich corresponds to case 2, seems to be a reasonable approximation.As in case 1, we set \u2202Lj,t,gc\u2062(\u03b7j)\u2202\u03b7j=0subscriptsuperscript\ud835\udc3f\ud835\udc50\ud835\udc57\ud835\udc61\ud835\udc54subscript\ud835\udf02\ud835\udc57subscript\ud835\udf02\ud835\udc570\\frac{\\partial L^{c}_{j,t,g}(\\eta_{j})}{\\partial\\eta_{j}}=0divide start_ARG \u2202 italic_L start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j , italic_t , italic_g end_POSTSUBSCRIPT ( italic_\u03b7 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) end_ARG start_ARG \u2202 italic_\u03b7 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG = 0 to obtain the new estimate \u03b7^jg\u2062(t)subscriptsuperscript^\ud835\udf02\ud835\udc54\ud835\udc57\ud835\udc61\\hat{\\eta}^{g}_{j}(t)over^ start_ARG italic_\u03b7 end_ARG start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ) corrected for bias and noise,whereThe bias part is also expressed asAlso with the same assumptions for \u03f5i\u2062j\u2062(\u22c5)subscriptitalic-\u03f5\ud835\udc56\ud835\udc57\u22c5\\epsilon_{ij}(\\cdot)italic_\u03f5 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( \u22c5 ) and \u03f5i\u2062js\u2062(\u22c5)subscriptsuperscriptitalic-\u03f5\ud835\udc60\ud835\udc56\ud835\udc57\u22c5\\epsilon^{s}_{ij}(\\cdot)italic_\u03f5 start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( \u22c5 ), \u03f5i\u2062j\u2062(fi\u2062j)\u223c\ud835\udca9\u2062(0,fi\u2062j\u2062\u03c3i\u2062j,\u03f52)similar-tosubscriptitalic-\u03f5\ud835\udc56\ud835\udc57subscript\ud835\udc53\ud835\udc56\ud835\udc57\ud835\udca90subscript\ud835\udc53\ud835\udc56\ud835\udc57superscriptsubscript\ud835\udf0e\ud835\udc56\ud835\udc57italic-\u03f52\\epsilon_{ij}(f_{ij})\\sim\\mathcal{N}(0,f_{ij}\\sigma_{ij,\\epsilon}^{2})italic_\u03f5 start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ) \u223c caligraphic_N ( 0 , italic_f start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT italic_\u03c3 start_POSTSUBSCRIPT italic_i italic_j , italic_\u03f5 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) and \u03f5i\u2062js\u2062(fi\u2062j)\u223c\ud835\udca9\u2062(0,fi\u2062j\u2062\u03c3i\u2062j,\u03f52)similar-tosubscriptsuperscriptitalic-\u03f5\ud835\udc60\ud835\udc56\ud835\udc57subscript\ud835\udc53\ud835\udc56\ud835\udc57\ud835\udca90subscript\ud835\udc53\ud835\udc56\ud835\udc57superscriptsubscript\ud835\udf0e\ud835\udc56\ud835\udc57italic-\u03f52\\epsilon^{s}_{ij}(f_{ij})\\sim\\mathcal{N}(0,f_{ij}\\sigma_{ij,\\epsilon}^{2})italic_\u03f5 start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ) \u223c caligraphic_N ( 0 , italic_f start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT italic_\u03c3 start_POSTSUBSCRIPT italic_i italic_j , italic_\u03f5 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ). then we can obtain an expression for \u03f5i\u2062j\u03b7\u2062(q,f,t)subscriptsuperscriptitalic-\u03f5\ud835\udf02\ud835\udc56\ud835\udc57\ud835\udc5e\ud835\udc53\ud835\udc61\\epsilon^{\\eta}_{ij}(q,f,t)italic_\u03f5 start_POSTSUPERSCRIPT italic_\u03b7 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_q , italic_f , italic_t ),and its derivative,\nCombining the above equations, we can get similar conclusions as in case 1.We also define a change factor in the frequency of word i\ud835\udc56iitalic_i, Ri\u2032superscriptsubscript\ud835\udc45\ud835\udc56\u2032{R_{i}}^{\\prime}italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT, as follows:where fi\u2032\u2062(t)superscriptsubscript\ud835\udc53\ud835\udc56\u2032\ud835\udc61{f_{i}}^{\\prime}(t)italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_t ) is the count of word i\ud835\udc56iitalic_i in period t\ud835\udc61titalic_t, normalized to the same value of \u2211ifi\u2062(t)subscript\ud835\udc56subscript\ud835\udc53\ud835\udc56\ud835\udc61\\sum_{i}f_{i}(t)\u2211 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t ) for all periods t\ud835\udc61titalic_t.The total number of words in all abstracts of the first period is used as a base to normalize the frequency of words in the other periods, and the corresponding results are shown in Figure 17, Figure 18, and Figure 19.",
    "7": "Purpose: Automatic quantification of longitudinal changes in PET scans for lymphoma patients has proven challenging, as residual disease in interim-therapy scans is often subtle and difficult to detect. Our goal was to develop a longitudinally-aware segmentation network (LAS-Net) that can quantify serial PET/CT images for pediatric Hodgkin lymphoma patients. \nMaterials and Methods: This retrospective study included baseline (PET1) and interim (PET2) PET/CT images from 297 patients enrolled in two Children\u2019s Oncology Group clinical trials (AHOD1331 and AHOD0831). LAS-Net incorporates longitudinal cross-attention, allowing relevant features from PET1 to inform the analysis of PET2. Model performance was evaluated using Dice coefficients for PET1 and detection F1 scores for PET2. Additionally, we extracted and compared quantitative PET metrics, including metabolic tumor volume (MTV) and total lesion glycolysis (TLG) in PET1, as well as qPET and \u0394\u0394\\Deltaroman_\u0394SUVmax in PET2, against physician measurements. We quantified their agreement using Spearman\u2019s \u03c1\ud835\udf0c\\rhoitalic_\u03c1 correlations and employed bootstrap resampling for statistical analysis. \nResults: LAS-Net detected residual lymphoma in PET2 with an F1 score of 0.606 (precision/recall: 0.615/0.600), outperforming all comparator methods (P<0.01). For baseline segmentation, LAS-Net achieved a mean Dice score of 0.772. In PET quantification, LAS-Net\u2019s measurements of qPET, \u0394\u0394\\Deltaroman_\u0394SUVmax, MTV and TLG were strongly correlated with physician measurements, with Spearman\u2019s \u03c1\ud835\udf0c\\rhoitalic_\u03c1 of 0.78, 0.80, 0.93 and 0.96, respectively. The performance remained high, with a slight decrease, in an external testing cohort.\nConclusion: LAS-Net achieved high performance in quantifying PET metrics across serial scans, highlighting the value of longitudinal awareness in evaluating multi-time-point imaging datasets.Keywords\u2002Quantitative PET \u00a0\u22c5\u22c5\\cdot\u22c5\nLongitudinal Analysis \u00a0\u22c5\u22c5\\cdot\u22c5\nDeep Learning \u00a0\u22c5\u22c5\\cdot\u22c5\nImage SegmentationAmong pediatric cancers, Hodgkin lymphoma (HL) is a highly curable malignancy (1), with 5-year survival exceeding 90%percent\\%% for patients receiving combination chemotherapy, radiation, or combined treatment (2). Despite this, pediatric patients face a significant risk of long-term side effects from therapeutic toxicities. Emerging evidence suggests that early responders to treatment may benefit from de-escalated therapies (3). Several clinical trials have used response assessment on interim Fluorodeoxyglucose (18F-FDG) PET scans after two cycles of chemotherapy for risk stratification (2,4). Currently, PET response is assessed using visual evaluation criteria, such as Deauville scores (DSs) (5). Compared to the qualitative assessment, quantitative PET metrics have shown promise in guiding lymphoma treatment strategies (6,7). However, its use often relies on manual lesion segmentation, which is difficult and time-consuming, and has been limited to clinical trial settings. Deep learning (DL) algorithms have the potential to overcome this limitation and enable automatic PET analysis.There have been extensive studies using DL to segment lymphoma (8\u201311) and extract quantitative metrics (12\u201314) in PET scans. However, existing algorithms focus on quantifying baseline tumor burden, overlooking the important role of interim PET in response assessment. Compared to baseline PET, analyzing interim PET poses significant challenges, as tumor uptake is often subtle and difficult to differentiate from confounding physiologic or inflammatory FDG activity. Physicians typically rely on cross comparison with baseline PET to identify residual lymphoma, but methods for incorporating this information to interim PET analysis remain underexplored.In this study, we aimed to develop a longitudinally-aware segmentation network (LAS-Net) for automatic quantification of serial PET/CT images, facilitating PET-adaptive therapy for pediatric HL patients. Central to our design is a dual-branch architecture: one branch dedicated to segmenting lymphoma in baseline PET, while the other detects residual lymphoma in interim PET. The model was trained using PET/CT images from multiple centers as part of a phase 3 clinical trial. To assess the performance of our method, we evaluated its detection performance in interim PET and its segmentation performance in baseline PET. Furthermore, we extracted various quantitative PET metrics and quantified their agreement with physician measurements. We compared LAS-Net to other methods, including those with and without the integration of baseline PET information. Lastly, we performed external testing using data from another multi-center clinical trial of pediatric HL.This retrospective study included patients from two Children\u2019s Oncology Group (COG) clinical trials: AHOD1331 (ClinicalTrials.gov number, NCT02166463) (2) and AHOD0831 (NCT01026220) (4). Both are phase 3 trials of pediatric patients aged 2-21 diagnosed with high-risk HL. The AHOD1331 trial assessed the utility of incorporating Brentuximab Vedotin with chemotherapy while the AHOD0831 trial evaluated the effects of combination chemotherapy together with radiation therapy. Baseline and interim FDG PET/CT images were gathered and transferred from IROC Rhode Island to our institution under data use agreements. Retrospective analysis was approved by institutional review board with no requirement of additional consent from patients. Of the 600 patients enrolled in the AHOD1331 trial, 200 with complete PET/CT datasets were randomly selected and used as our internal cohort. Among the 166 patients from the AHOD0831 trial, 97 had complete PET/CT datasets, and these were used for external testing.For the AHOD1331 dataset, three experienced nuclear medicine (NM) physicians provided lesion-level annotations for both baseline and interim PET using a semi-automated workflow (LesionID, MIM Software, Cleveland, Ohio), following a multi-reader adjudication process. One physician (M.S.) labelled all 200 cases while the other physicians (S.B.P. and S.Y.C.) each adjudicated 100 of the cases, refining the annotations by adding, deleting, or modifying contours as necessary. All segmented lesions were labeled according to physician confidence (non-equivocal or equivocal). Annotators were trained using a labeling guide (described in Appendix S1).For the AHOD0831 dataset, PET images for each patient were annotated by one of two NM physicians (J.K. and I.L.) on Mirada XD (Oxford, UK) software as part of a prior research study (12,15). Table 1 summarizes the characteristics of these two datasets.We designed LAS-Net with a dual-branch architecture to accommodate baseline and interim PET/CT images, as illustrated in Figure 1A. One branch exclusively processes baseline PET (PET1) and predicts the corresponding lesion masks. The other branch focuses on interim PET (PET2), but also utilizes information extracted from the PET1 branch to generate masks of residual lymphoma. This architecture enables our model to gather useful information from PET1 to inform and improve the analysis of subsequent scans. Meanwhile, it ensures a one-way information flow, preventing PET2 information from influencing PET1 analysis.Like many segmentation networks, LAS-Net was adapted from a UNet-like architecture. It is based on 3D SwinUNETR (16), a state-of-the-art (SOTA) model comprising a Swin Transformer (17) encoder and a convolutional neural network (CNN) decoder. In LAS-Net, each convolutional block is a stack of two convolution units (3\u00d7\\times\u00d73\u00d7\\times\u00d73 convolution sub-layers, instance normalization, leaky ReLU) with a residual connection. Beyond these components, we have introduced two critical mechanisms to allow information from the PET1 branch to influence the PET2 branch. One is the longitudinally-aware window attention (LAWA) on the encoder side, and the other is the longitudinally-aware attention gate (LAAG) on the decoder side.Figure 1B illustrates the structure of the LAWA module. Compared to the standard Swin Transformer block (17), this module introduces a window-based multi-head cross-attention (W-MCA) layer with a window size of 7\u00d7\\times\u00d77\u00d7\\times\u00d77 in the PET2 branch. The W-MCA takes the query vectors from PET2 features and the key and value vectors from PET1 features. It computes the attention matrix of the query and key using scaled dot product, allowing the model to dynamically allocate focus based on the relevance of regions across PET1 and PET2. The value vectors are then reweighted by this attention matrix and added to input PET2 features.Figure 1C presents the design of the LAAG module. Similar to the original attention gate (18), the LAAG module processes inputs from both the prior layer and skip connections, generating attention coefficients. To enable additional longitudinal awareness, we concatenate the attention coefficients derived from PET1 and PET2 and convolve them with a learnable 7\u00d7\\times\u00d77\u00d7\\times\u00d77 kernel to refine the PET2 attention coefficients. This CNN-based cross-attention gate allows the LAAG module to select PET2 features using information from the PET1 branch.LAS-Net operates on 112\u00d7\\times\u00d7112\u00d7\\times\u00d7112 patches from co-registered baseline and interim PET/CT images. Except for the longitudinal cross-attention components, all other weights in the model are shared between the PET1 and PET2 branches. The model was jointly optimized for PET1 and PET2 lesion segmentation using a compound loss, comprised of cross-entropy and Dice loss. Models were trained and evaluated through fivefold cross-validation (N=40 in each test fold). Implementation details can be found in Appendices S2-3.In baseline PET analysis, we evaluated model performance using the Dice coefficient, false positive volume (FPV), and false negative volume (FNV) per patient. The quantitative metrics computed for PET1 scans (definitions in Appendix S4) included metabolic tumor volume (MTV) (19), total lesion glycolysis (TLG) (20), maximum lesion standardized uptake value (SUVmax), maximum tumor dissemination (Dmax) (21), maximum distance between the lesion and the spleen (Dspleen) (22) and the number of lesions. Since interim PET analysis primarily involves SUVmax or SUVpeak measurements (23), accurate tumor segmentation is not needed. Consequently, for PET2 scans, we evaluated our model\u2019s performance using detection F1 scores, precision, and recall. Lesions detected by the model that were considered as equivocal by the physicians were not counted as false positives (FPs) or true positives (TPs) in our evaluation. We also extracted quantitative PET2 metrics from model predictions, including SUVmax, percentage difference between baseline and interim SUVmax (\u0394\u0394\\Deltaroman_\u0394SUVmax), qPET (23), and the number of residual lesions. Notably, \u0394\u0394\\Deltaroman_\u0394SUVmax and qPET have been demonstrated to have predictive potential for patient prognosis (23\u201325). The agreement between automated PET metrics and physician measurements was quantified by Spearman\u2019s \u03c1\ud835\udf0c\\rhoitalic_\u03c1 correlations.We compared the performance of LAS-Net to other models trained on our dataset, including DynUNet (26,27), SegResNet (28) and SwinUNETR (16). No longitudinal cross-attention was incorporated into these models\u2019 architectures. We also evaluated Clinical Knowledge-Driven Hybrid Transformer (CKD-Trans) (29) and Spatial-Temporal Transformer (ST-Trans) (30), both of which integrated information from PET1 into PET2 analysis using cross-attention. Notably, CKD-Trans and ST-Trans were initially developed for tumor segmentation in multiparametric MRI. Table 2 summarizes key differences among these models.Furthermore, we implemented a previous technique (15,31) that used deformable registration between PET1 and PET2 scans to reduce FPs in PET2 lesion masks. Specifically, segmentation masks predicted for PET1 are propagated to PET2 using deformable registration, and then PET2 contours that do not overlap with PET1 contours are excluded. In our work, we refer to this technique as \u201cmask propagation through deformable registration\u201d (MPDR). Quantitative results were reported both with and without MPDR. Additionally, we conducted ablation studies to assess the effectiveness of individual components in LAS-Net.DSs serve as an internationally accepted scoring system for assessing treatment response in interim PET. Two types of thresholds (DS3-DS5 positive or DS4-DS5 positive) are typically used to categorize patients into adequate or inadequate response classes, depending on the clinical context (32). Although our model was not trained to output patient-level DSs, we can estimate DSs by converting extracted qPET values to DSs using the qPET criterion (23). This indirect method allowed for a comparison of model-predicted DSs and physician-assigned DSs. The level of agreement was quantified by the F1 score and the Kappa index.\nThe 95%percent\\%% confidence intervals (CIs) for our results were derived using nonparametric bootstrap resampling (33) with 10,000 repetitive trials. The difference between two data groups was statistically significant at 0.05 when one group exceeded the other in 95%percent\\%% of trials.The COG clinical trial data is archived in NCTN Data Archive. Our algorithm was implemented using the Auto3dSeg pipeline in Monai (27). The code and models are available in the open-source project: https://github.com/xtie97/LAS-Net.Figure 2A shows the comparison of lesion detection performance in PET2 across all evaluated models. LAS-Net detected residual lymphoma with an F1 score of 0.606 (95%percent\\%%CI, 0.528, 0.674). Applying MPDR to predicted interim masks increased LAS-Net\u2019s precision (0.615 to 0.667), but at the cost of reduced recall (0.600 to 0.481). This suggests that MPDR may filter out true positive lesions, including new lesions that are not present in the baseline scan. Overall, the use of MPDR did not improve the detection performance of LAS-Net (F1 without vs. with MPDR: 0.606 vs. 0.558, P=0.22). Conversely, all comparator models benefited from MPDR, with ST-Trans (with MPDR) achieving the highest F1 score (0.446, 95%percent\\%%CI, 0.346, 0.538) of the comparator methods. Nevertheless, it was statistically inferior (P=0.005) to LAS-Net in identifying residual lesions. In terms of quantitative PET2 metrics, LAS-Net consistently outperformed other methods (Figure 2B), with Spearman\u2019s \u03c1\ud835\udf0c\\rhoitalic_\u03c1 correlations of 0.79 (95%percent\\%%CI, 0.70, 0.86) for SUVmax, 0.80 (95%percent\\%%CI, 0.72 0.86) for \u0394\u0394\\Deltaroman_\u0394SUVmax, 0.78 (95%percent\\%%CI, 0.70, 0.85) for qPET and 0.64 (95%percent\\%%CI, 0.54, 0.72) for the number of lesions. \u0394\u0394\\Deltaroman_\u0394SUVmax and qPET had exact matches in values for 55%percent\\%% (110/200) and 69.5%percent\\%% (139/200) of cases, respectively.For automatic PET1 analysis (Figure 3), LAS-Net attained a mean Dice score of 0.772 (95%percent\\%%CI, 0.752, 0.791), with average FNV of 10.80 ml (95%percent\\%%CI, 8.53, 13.46) and FPV of 9.68 ml (95%percent\\%%CI, 7.50, 12.40) per patient. It demonstrated comparable performance to the best model, DynUNet, which had a Dice score of 0.779 (95%percent\\%%CI, 0.758, 0.797, P=0.32). Among the PET1 metrics extracted by LAS-Net, MTV, TLG and SUVmax exhibited high correlations with the values measured by physicians (\u03c1\ud835\udf0c\\rhoitalic_\u03c1=0.93 for MTV, 0.96 for TLG, 0.90 for SUVmax). No significant differences were observed across the four evaluated models for these metrics. For the distance-based metrics, Dmax and Dspleen, LAS-Net showed moderate correlations (\u03c1\ud835\udf0c\\rhoitalic_\u03c1=0.62 for Dmax, 0.70 for Dspleen) with physician measurements, indicating the challenges of detecting individual lesions at the farthest distances.Scatter plots in Figure 4 visualize the agreement between PET metrics assessed by physicians and those measured by LAS-Net.Figure 5 displays images from nine sample cases, each comprising baseline and interim lesion masks predicted by LAS-Net along with physician annotations. In cases A-F, LAS-Net successfully identified the residual lesions, including the hottest lesions (DS4 or DS5) as well as those with lower uptake (DS3). Notably, in case B, LAS-Net detected new lesions, not present on PET1, located near the neck and bladder. If MPDR was applied, these true positive lesions would be excluded, leading to an underestimation of SUVmax and qPET.In scenarios with multiple dispersed PET2 lesions (cases G-H), LAS-Net had difficulties in accurately identifying all lesions. Additionally, LAS-Net occasionally identified FP lesions in negative cases (case I), especially when the residual SUVs were close to the mediastinum uptake. For baseline lymphoma segmentation, LAS-Net performed consistently well at delineating bulky diseases. Nonetheless, it was less effective in detecting small lesions situated at a distance from the primary disease sites, which was true for all comparator methods.To assess the benefits of integrating longitudinal awareness into the model architecture, we compared the predictions of LAS-Net with those of DynUNet in Figure 6. Without applying MPDR, the PET2 FPs predicted by DynUNet significantly affected the accuracy of automated PET2 metrics. Especially in case D, DynUNet mistakenly identified brown fat uptake as residual lymphoma.Table 3 presents DS classification results. If grouping cases into two categories \u2013 scores of DS 1, 2 vs. DS 3, 4, 5 \u2013 LAS-Net attained an F1 score of 0.752 (precision/recall: 0.687/0.836) and Cohen\u2019s kappa of 0.630, outperforming (P<0.05) the top comparator, ST-Trans (with MPDR), which had 0.660 for F1 and 0.501 for Cohen\u2019s kappa. If grouping based on DS of 1, 2 and 3 vs. DS 4 and 5, LAS-Net achieved an F1 score of 0.633 (precision/recall: 0.500/0.867) and Cohen\u2019s kappa of 0.549, and was superior to other evaluated methods.The results of ablation studies are shown in Table 4. We found that both LAWA and LAAG modules for longitudinal cross-attention improved lesion detection performance in PET2. Also, the inclusion of the PET1 branch and the combined PET1 and PET2 training enhanced the model\u2019s capability to quantify PET2 scans. The choice of registration methods between PET1 and PET2 did not impact model performance. When input baseline and interim PET/CT images were co-registered using rigid registration, the performance was slightly worse but not significantly different from that achieved with deformable registration (P=0.22 for F1 scores).We applied LAS-Net, trained on all AHOD1331 data, to the external AHOD0831 dataset. The detection F1 score in PET2 was 0.525 (95%percent\\%%CI, 0.456, 0.582) and the Dice score in PET1 was 0.684 (95%percent\\%%CI, 0.655, 0.711). Regarding quantitative PET metrics, the Spearman\u2019s \u03c1\ud835\udf0c\\rhoitalic_\u03c1 correlations between LAS-Net predictions and physician measurements showed a slight decrease: 0.70 for PET2 \u0394\u0394\\Deltaroman_\u0394SUVmax, 0.69 for PET2 qPET, 0.87 for PET1 MTV and 0.89 for PET1 TLG. Detailed results along with example cases are provided in Appendix S5.In this study, we introduced a novel deep-learning-based method (LAS-Net) for longitudinal analysis of serial PET/CT images in pediatric HL patients. Our approach was different from prior methods in two aspects. First, it used longitudinal cross-attention to extract baseline PET information for improved analysis of interim PET. Second, it adopted a dual-branch architecture to enable automatic quantification of both baseline and interim scans. Through comparative and ablation studies, we validated the effectiveness of our approach using data from two multi-center clinical trials, highlighting its potential to deliver rapid and consistent assessment of PET tumor burden and response.Existing DL algorithms for detecting lymphoma lesions have been limited to analyzing PET1 scans without the ability to quantify PET2 for response assessment and outcome prediction. This limitation is primarily due to the challenge of detecting residual lymphoma in PET2, which often has low FDG uptake. It is even a difficult task for expert physicians, and they usually rely on PET1 (i.e., viewing PET1 and PET2 side-by-side) to identify residual lymphoma. Our method was intended to fill this gap by integrating longitudinal cross-attention mechanisms into the architecture. While previous research has leveraged prior PET data for interim image denoising (34) and response classification (35), our work distinguishes itself by incorporating longitudinal awareness to improve the analysis of multi-time-point imaging datasets.To develop a model for longitudinal response assessment in PET scans, we chose to jointly optimize our model for PET1 and PET2 analysis. This substantially improved model performance in identifying residual lesions in PET2, as evidenced by the ablation study. However, our model\u2019s PET1 segmentation performance was no better than other SOTA models trained on PET1 scans. This was expected, as the PET1 branch should not, in principle, benefit from the PET2 branch.The quantitative PET metrics that we investigated have been demonstrated to be better than visual criteria at guiding lymphoma treatment (6,7). For PET1, we found that MTV and TLG were the metrics most accurately quantified by the DL model. They are also the most time-consuming metrics for physicians to measure. Newly proposed distance-based metrics (Dmax, Dspleen) were harder for accurate quantification, because a single FP or FN can have a large impact on the values of these metrics. For PET2, we focused on measuring SUVmax, qPET, and the response metric \u0394\u0394\\Deltaroman_\u0394SUVmax, as these have been associated with patient outcome in previous studies (23\u201325). Detecting residual lymphoma on PET2 was very challenging for models that did not use longitudinal information, and this was reflected in their poor F1 scores and their performance in PET2 quantification. Even with MPDR, these models were inferior to LAS-Net.This study has several limitations. First, our training pipeline did not involve any pretraining or semi-supervised techniques. Such approaches may allow us to use unlabeled data, but whether they could enhance model performance remains to be answered. Second, we focused on quantitative PET metrics (MTV, qPET, etc.). Future research will aim to associate these metrics with patient outcome. Third, the labeling process for our external dataset differed from that used for our internal dataset. It is unclear if the performance drop in external testing is attributed to dataset shift, or different annotation quality. Fourth, our current model only operates at two imaging time points. In future work, we hope to develop a unified framework that can process PET/CT images across all time points. Lastly, we only evaluated our algorithm in the cohorts of pediatric HL patients. Whether it is applicable to other diseases or populations requires further investigation.In conclusion, our study introduced a longitudinally-aware segmentation network to address the challenges of automatic quantification of serial PET scans. The proposed method demonstrated significantly improved lesion detection performance in interim PET without sacrificing the model\u2019s ability to segment lymphoma in baseline PET. This technology opens opportunities to identify predictive imaging biomarkers that can lead to more effective PET-adaptive therapies.We sincerely thank Drs. Jihyun Kim and Inki Lee for annotating the AHOD0831 dataset in our previous study.We acknowledge funding support from Imaging and Radiology Oncology Core Rhode Island (U24CA180803), Biomarker, Imaging and Quality of Life Studies Funding Program (BIQSFP), NIH (U10CA098543), NCTN Operations Center Grant (U10CA180886), NCTN Statistics &\\&& Data Center Grants (U10CA180899 and U10CA098413), QARC (CA29511), IROC RI (U24CA180803), and St. Baldrick\u2019s Foundation.Research reported in this publication was also supported by the National Institute Of Biomedical Imaging And Bioengineering of the National Institutes of Health under Award Number R01EB033782, by the Department of Defense under Award Number W81XWH-22-1-0336, and by GE Healthcare.Disclaimer: The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.Mauz-K\u00f6rholz C, Metzger ML, Kelly KM, et al. Pediatric Hodgkin Lymphoma. JCO. 2015;33(27):2975\u20132985. doi: http://doi.org/10.1200/JCO.2014.59.4853.Castellino SM, Pei Q, Parsons SK, et al. Brentuximab Vedotin with Chemotherapy in Pediatric High-Risk Hodgkin\u2019s Lymphoma. N Engl J Med. 2022;387(18):1649\u20131660. doi: http://doi.org/10.1056/NEJMoa2206660.Friedman DL, Chen L, Wolden S, et al. Dose-Intensive Response-Based Chemotherapy and Radiation Therapy for Children and Adolescents With Newly Diagnosed Intermediate-Risk Hodgkin Lymphoma: A Report From the Children\u2019s Oncology Group Study AHOD0031. JCO. 2014;32(32):3651\u20133658. doi: http://doi.org/10.1200/JCO.2013.52.5410.Kelly KM, Cole PD, Pei Q, et al. Response-adapted therapy for the treatment of children with newly diagnosed high risk Hodgkin lymphoma (AHOD0831): a report from the Children\u2019s Oncology Group. Br J Haematol. 2019;187(1):39-48. http://doi.org/10.1111/bjh.16014.Cheson BD, Fisher RI, Barrington SF, et al. Recommendations for Initial Evaluation, Staging, and Response Assessment of Hodgkin and Non-Hodgkin Lymphoma: The Lugano Classification. JCO. 2014;32(27):3059\u20133067. doi: http://doi.org/10.1200/JCO.2013.54.8800.Rogasch JMM, Hundsdoerfer P, Hofheinz F, et al. Pretherapeutic FDG-PET total metabolic tumor volume predicts response to induction therapy in pediatric Hodgkin\u2019s lymphoma. BMC Cancer. 2018;18(1):521. doi: http://doi.org/10.1186/s12885-018-4432-4.Okuyucu K, Ozayd\u0131n S, Alagoz E, et al. Prognosis estimation under the light of metabolic tumor parameters on initial FDG-PET/CT in patients with primary extranodal lymphoma. Radiology and Oncology. 2016;50(4):360\u2013369. doi: http://doi.org/10.1515/raon-2016-0045.Weisman AJ, Kieler MW, Perlman SB, et al. Convolutional Neural Networks for Automated PET/CT Detection of Diseased Lymph Node Burden in Patients with Lymphoma. Radiology: Artificial Intelligence. 2020;2(5):e200016. doi: http://doi.org/10.1148/ryai.2020200016.Constantino CS, Leoc\u00e1dio S, Oliveira FPM, et al. Evaluation of Semiautomatic and Deep Learning\u2013Based Fully Automatic Segmentation Methods on [18F]FDG PET/CT Images from Patients with Lymphoma: Influence on Tumor Characterization. J Digit Imaging. 2023;36(4):1864\u20131876. doi: http://doi.org/10.1007/s10278-023-00823-y.Huang L, Ruan S, Decazes P, Den\u0153ux T. Lymphoma segmentation from 3D PET-CT images using a deep evidential network. International Journal of Approximate Reasoning. 2022;149:39\u201360. doi: http://doi.org/10.1016/j.ijar.2022.06.007.Hu H, Shen L, Zhou T, Decazes P, Vera P, Ruan S. Lymphoma Segmentation in PET Images Based on Multi-view and Conv3D Fusion Strategy. 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI). Iowa City, IA, USA: IEEE; 2020. p. 1197\u20131200. doi: http://doi.org/10.1109/ISBI45749.2020.9098595.Weisman AJ, Kim J, Lee I, et al. Automated quantification of baseline imaging PET metrics on FDG PET/CT images of pediatric Hodgkin lymphoma patients. EJNMMI Phys. 2020;7(1):76. doi: http://doi.org/10.1186/s40658-020-00346-3.Yousefirizi F, Klyuzhin IS, O JH, et al. TMTV-Net: fully automated total metabolic tumor volume segmentation in lymphoma PET/CT images \u2014 a multi-center generalizability analysis. Eur J Nucl Med Mol Imaging. 2024; doi: http://doi.org/10.1007/s00259-024-06616-x.Blanc-Durand P, J\u00e9gou S, Kanoun S, et al. Fully automatic segmentation of diffuse large B cell lymphoma lesions on 3D FDG-PET/CT for total metabolic tumour volume prediction using a convolutional neural network. Eur J Nucl Med Mol Imaging. 2021;48(5):1362\u20131370. doi: http://doi.org/10.1007/s00259-020-05080-7.Weisman AJ, Lee I, Im H, et al. Machine learning-based assignment of Deauville scores is comparable to interobserver variability on interim FDG PET/CT images of pediatric lymphoma patients. Journal of Nuclear Medicine 61,1434\u20131434 (2020).Hatamizadeh A, Nath V, Tang Y, Yang D, Roth H, Xu D. Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images. arXiv; 2022. http://arxiv.org/abs/2201.01266. Accessed February 8, 2023Liu Z, Lin Y, Cao Y, et al. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. 2021 IEEE/CVF International Conference on Computer Vision (ICCV). Montreal, QC, Canada: IEEE; 2021. p. 9992\u201310002. doi: http://doi.org/10.1109/ICCV48922.2021.00986.Oktay O, Schlemper J, Folgoc LL, et al. Attention U-Net: Learning Where to Look for the Pancreas. arXiv; 2018. doi: http://doi.org/10.48550/arXiv.1804.03999. Accessed February 8, 2023Im H-J, Bradshaw T, Solaiyappan M, Cho SY. Current Methods to Define Metabolic Tumor Volume in Positron Emission Tomography: Which One is Better? Nucl Med Mol Imaging. 2018;52(1):5\u201315. doi: http://doi.org/10.1007/s13139-017-0493-6.Larson SM, Erdi Y, Akhurst T, et al. Tumor Treatment Response Based on Visual and Quantitative Changes in Global Tumor Glycolysis Using PET-FDG Imaging. The Visual Response Score and the Change in Total Lesion Glycolysis. Clin Positron Imaging. 1999;2(3):159-171. doi: http://doi.org/10.1016/s1095-0397(99)00016-3.Cottereau A-S, Nioche C, Dirand A-S, et al. 18 F-FDG PET Dissemination Features in Diffuse Large B-Cell Lymphoma Are Predictive of Outcome. J Nucl Med. 2020;61(1):40\u201345. doi: http://doi.org/10.2967/jnumed.119.229450.\nGirum KB, Cottereau A-S, Vercellino L, et al. Tumor Location Relative to the Spleen Is a Prognostic Factor in Lymphoma Patients: A Demonstration from the REMARC Trial. J Nucl Med. 2024;65(2):313\u2013319. doi: http://doi.org/10.2967/jnumed.123.266322.Hasenclever D, Kurch L, Mauz-K\u00f6rholz C, et al. qPET \u2013 a quantitative extension of the Deauville scale to assess response in interim FDG-PET scans in lymphoma. Eur J Nucl Med Mol Imaging. 2014;41(7):1301\u20131308. doi: http://doi.org/10.1007/s00259-014-2715-9.Santos FM, Marin JFG, Lima MS, et al. Impact of baseline and interim quantitative PET parameters on outcomes of classical Hodgkin Lymphoma. Ann Hematol. 2023; doi: http://doi.org/10.1007/s00277-023-05461-6.Yang S, Qiu L, Huang X, Wang Q, Lu J. The prognostic significance of \u0394\u0394\\Deltaroman_\u0394SUVmax assessed by PET/CT scan after 2 cycles of chemotherapy in patients with classic Hodgkin\u2019s lymphoma. Ann Hematol. 2020;99(2):293\u2013299. doi: http://doi.org/10.1007/s00277-019-03892-8.Isensee F, Jaeger PF, Kohl SAA, Petersen J, Maier-Hein KH. nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nat Methods. 2021;18(2):203\u2013211. doi: http://doi.org/10.1038/s41592-020-01008-z.Cardoso MJ, Li W, Brown R, et al. MONAI: An open-source framework for deep learning in healthcare. arXiv; 2022. http://arxiv.org/abs/2211.02701. Accessed March 1, 2023.Myronenko A. 3D MRI brain tumor segmentation using autoencoder regularization. arXiv; 2018. http://arxiv.org/abs/1810.11654. Accessed February 19, 2024Lin J, Lin J, Lu C, et al. CKD-TransBTS: Clinical Knowledge-Driven Hybrid Transformer With Modality-Correlated Cross-Attention for Brain Tumor Segmentation. IEEE Trans Med Imaging. 2023;42(8):2451\u20132461. doi: http://doi.org/10.1109/TMI.2023.3250474.Zhang J, Cui Z, Shi Z, et al. A robust and efficient AI assistant for breast tumor segmentation from DCE-MRI via a spatial-temporal framework. Patterns. 2023;4(9):100826. doi: http://doi.org/10.1016/j.patter.2023.100826.Huff DT, Santoro-Fernandes V, Chen S, et al. Performance of an automated registration-based method for longitudinal lesion matching and comparison to inter-reader variability. Phys Med Biol. 2023;68(17):175031. doi: http://doi.org/10.1088/1361-6560/acef8f.Barrington SF, Kluge R. FDG PET for therapy monitoring in Hodgkin and non-Hodgkin lymphomas. Eur J Nucl Med Mol Imaging. 2017;44(S1):97\u2013110. doi: http://doi.org/10.1007/s00259-017-3690-8.Smith L, Tanabe LK, Ando RJ nee, et al. Overview of BioCreative II gene mention recognition. Genome Biol. 2008;9(S2):S2. doi: http://doi.org/10.1186/gb-2008-9-s2-s2.Wang Y-R (Joyce), Qu L, Sheybani ND, et al. AI Transformers for Radiation Dose Reduction in Serial Whole-Body PET Scans. Radiology: Artificial Intelligence. 2023;5(3):e220246. doi: http://doi.org/10.1148/ryai.220246.Joshi A, Eyuboglu S, Huang S-C, et al. OncoNet: Weakly Supervised Siamese Network to automate cancer treatment response assessment between longitudinal FDG PET/CT examinations. arXiv; 2021. http://arxiv.org/abs/2108.02016. Accessed January 9, 2024.Details of our labeling approach can be found in the labeling guide available at https://github.com/xtie97/lymphoma_labeling_guide. In short, lymphoma detection in the internal AHOD1331 cohort was facilitated by a customized MIM LesionID workflow. A standardized uptake value (SUV) and volume (2 ml) threshold was first used to pre-identify regions of high FDG uptake based on the PERCIST criteria (1). Then the annotator deleted regions of interest (ROIs) that did not contain tumors. Any tumor regions that were missed by pre-labelling (such as lesions <<< 2 ml) were manually added by the annotator using the PET Edge+ tool in the MIM software. For any liver and osseous/bone marrow involvement, only focal diseases were identified. Splenic lesions were considered if focal uptake was present or diffuse uptake was higher than 1.5 of the liver SUV. When it was unclear whether a lesion was lymphoma or physiological, it was classified as \u201cequivocal\u201d. After the first annotator labeled all cases, the second annotator (one of two senior nuclear medicine physicians) reviewed and edited the contours as necessary. Given the absence of a universally-accepted approach for delineation of tumor boundaries (2), we performed an internal calibration study to evaluate various PET thresholding methods against a set of physician-drawn contours. Consequently, we used a union of SUV >>> 2.5 and SUV >>> 40%percent\\%% of SUVmax within the lesion ROIs to create final segmentation masks. To analyze interim PET scans, the annotator compared baseline and interim PET side-by-side and used PET Edge+ to add residual tumors. Any tumor that was considered Deauville score 3-5 had an associated contour.In the external AHOD0831 cohort, the annotator placed large ROIs around areas containing disease using Mirada XD software, excluding diffused osseous/bone marrow involvement and regions with physiological uptake. Then a union of SUV >>> 2.5 and SUV >>> 40%percent\\%% of SUVmax was applied within each lesion ROI for segmentation of the lymphoma disease. For interim PET scans, the annotator manually added the residual lesions that had FDG activity above mediastinum uptake.\nIn both internal and external cohorts, we resampled PET and CT images to a voxel size of 3\u00d7\\times\u00d73\u00d7\\times\u00d73 mm using trilinear interpolation. Labels were resampled to the same voxel size via nearest neighbor interpolation. To reduce the spatial discrepancies across longitudinal imaging data, we registered the baseline scans to the interim CT using deformable transformation. We used ANTsPy (0.4.2), a python library for medical image registration. Considering that there is a lot of background information near the edge, we cropped the PET and CT volumes using bounding boxes determined by a SUV threshold of 0.2. PET SUVs and CT Hounsfield units (HUs) were then linearly scaled from [0,30] to [0,1] and from [-150, 250] to [0,1], respectively. During training, equivocal and non-equivocal lesions were combined and used as the ground truth mask.We first concatenated the PET and CT images as two channels for model input and then cropped random patches of 112\u00d7\\times\u00d7112\u00d7\\times\u00d7112 centered on the areas of lesion class with a probability of 0.8 (0.2 for background). To alleviate the over-fitting problem, we applied the data augmentation techniques to training data, including random affine transformation (rotation between -25 and 25 degrees, axis flip for all dimensions, zoom between 0.8 to 1.2), Gaussian noise and Gaussian blur.\nWe jointly optimized the baseline and interim PET branches, using the following loss function:Where x1subscript\ud835\udc651x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, x2subscript\ud835\udc652x_{2}italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT denote baseline PET/CT (PET1) and interim PET/CT (PET2). y1subscript\ud835\udc661y_{1}italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, y2subscript\ud835\udc662y_{2}italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT denote reference baseline and interim lesion masks. g1subscript\ud835\udc541g_{1}italic_g start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, g2subscript\ud835\udc542g_{2}italic_g start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT denote the PET1 and PET2 branches of the model. Note that g1subscript\ud835\udc541g_{1}italic_g start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT solely depends on PET1 while g2subscript\ud835\udc542g_{2}italic_g start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT takes inputs from both PET1 and PET2. For each branch, the loss is an unweighted sum of cross-entropy (CE) loss and Dice loss, which has proven effective in various segmentation tasks (3). To enable the model to learn joint feature representations from both time points, except for the longitudinal cross-attention, all other components in the model are shared between the two branches.The models were trained using the AdamW optimizer (4), with an initial learning rate of 10\u22124superscript10410^{-4}10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT, weight decay regularization of 10\u22125superscript10510^{-5}10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT, and a cosine annealing scheduler. We set the batch size to 3 per GPU and trained the models for 300 epochs on 2 NVIDIA A100 GPUs. The learning environment requires the following Python (3.8.8) libraries: PyTorch (1.13.0), Monai (1.3.0).During inference, we generated lesion masks for baseline and interim PET scans separately. For baseline mask prediction, original PET1 scans were used as input for the PET1 branch, while PET2 branch inputs were set to zeros. This ensures that baseline predictions solely depend on the initial scan data. However, interim mask predictions require both PET1 and PET2 scans as input, with PET1 being deformable-registered to PET2. We employed the sliding window method with an overlap rate of 0.625 and blended outputs of overlapping patches using Gaussian weighting.Figure E1 shows the scatter plots comparing quantitative PET metrics measured by our model and by physicians in the external AHOD0831 dataset. Figure E2 presents six sample cases from the AHOD0831 dataset, each comprising model predictions and reference physician annotations.Wahl RL, Jacene H, Kasamon Y, Lodge MA. From RECIST to PERCIST: Evolving Considerations for PET response criteria in solid tumors. J Nucl Med. 2009;50 Suppl 1(Suppl 1):122S-50S. doi: http://doi.org/10.2967/jnumed.108.057307.Mart\u00edn-Saladich Q, Reyn\u00e9s-Llompart G, Sabat\u00e9-Llobera A, Palomar-Mu\u00f1oz A, Domingo-Dom\u00e8nech E, Cort\u00e9s-Romera M. Comparison of different automatic methods for the delineation of the total metabolic tumor volume in I-II stage Hodgkin Lymphoma. Sci Rep. 2020;10(1):12590. doi: http://doi.org/10.1038/s41598-020-69577-9.Ma J, He Y, Li F, Han L, You C, Wang B. Segment anything in medical images. Nat Commun. 2024;15(1):654. doi: http://doi.org/10.1038/s41467-024-44824-z.Loshchilov I, Hutter F. Decoupled Weight Decay Regularization. arXiv; 2019. http://arxiv.org/abs/1711.05101. Accessed August 31, 2023.Wasserthal J, Breit H-C, Meyer MT, et al. TotalSegmentator: Robust Segmentation of 104 Anatomic Structures in CT Images. Radiology: Artificial Intelligence. Radiological Society of North America; 2023;5(5):e230024. doi: http://doi.org/10.1148/ryai.230024.",
    "8": "(eccv)                Package eccv Warning: Package \u2018hyperref\u2019 is loaded with option \u2018pagebackref\u2019, which is *not* recommended for camera-ready versionReferring image segmentation is a challenging task that involves generating pixel-wise segmentation masks based on natural language descriptions. Existing methods have relied mostly on visual features to generate the segmentation masks while treating text features as supporting components. This over-reliance on visual features can lead to suboptimal results, especially in complex scenarios where text prompts are ambiguous or context-dependent. To overcome these challenges, we present a novel framework VATEX to improve referring image segmentation by enhancing object and context understanding with Vision-Aware Text Feature. Our method involves using CLIP to derive a CLIP Prior that integrates an object-centric visual heatmap with text description, which can be used as the initial query in DETR-based architecture for the segmentation task. Furthermore, by observing that there are multiple ways to describe an instance in an image, we enforce feature similarity between text variations referring to the same visual input by two components: a novel Contextual Multimodal Decoder that turns text embeddings into vision-aware text features, and a Meaning Consistency Constraint to ensure further the coherent and consistent interpretation of language expressions with the context understanding obtained from the image. Our method achieves a significant performance improvement on three benchmark datasets RefCOCO, RefCOCO+ and G-Ref. Code is available at: https://nero1342.github.io/VATEX_RISReferring image segmentation (RIS) is an emerging new task in computer vision that predicts pixel-wise segmentation of visual objects in images from natural language cues.\nCompared to traditional segmentation \u00a0[3, 49, 42, 44, 51, 14, 47], RIS allows user-friendly interactions via text prompts to select and control the segmentation results, which is useful in various applications, especially interactive applications for image editing and robotics.A particular technique to solve RIS is to obtain a robust alignment between language and vision.\nPerforming such an alignment presents significant challenges due to the nature of languages, which are highly ambiguous and context-dependent. Early alignment approaches\u00a0[25, 53, 35, 11] in RIS either used bottom-up methods, merging vision and language features in early fusion and using an FCN as a decoder to produce object masks, or top-down methods, which first identify objects in the image and use the expression as the grounding criterion to select the best-matched result.Recent approaches\u00a0[52, 9, 30] are based on transformers that learn the interaction between vision-text modalities followed by a standard encoder-decoder process to produce pixel-level segmentation results. However, this interaction still remains sufficiently underexplored in referring image segmentation. Specifically, the cross-modal interaction mainly occurs only in one way: keeping text features unchanged and utilizing them to enhance visual features.\nAs a result, previous methods fail to identify the target object and generate precise segmentation for complex and challenging text expressions. For example, in Figure\u00a01(a), although LAVT can easily segment the well-defined \"orange-striped fish\" and specific location information \"on the left\" in the first expression, LAVT struggles with the object \"clownfish\" and picks the turtle that is \"facing towards the camera\" in the second expression.\nWe hypothesize that gradually putting the visual information into text features and performing the full bidirectional vision-text interactions can capture fine-grained semantic information of these modalities. This encourages us to focus more on vision-aware text features for object and context understanding.\nFirst, for object understanding, we proposed integrating visual cues into text features in the query initialization process, facilitated by the visual-text embedding provided by CLIP\u00a0[38].\nWe propose using the alignment between CLIP-Image and CLIP-Text embeddings to generate an object-centric visual heatmap that can be subsequently used as an initial mask for our model. By integrating this heatmap into the original text feature, we combine both linguistic and spatial information cues during the query initialization phase.\nThis query initialization method transfers the knowledge from the pre-trained CLIP model and allows the model to localize the referred object correctly, even in the challenging case where the expression contains \"unseen\" category (e.g. clownfish).\nSecond, for context understanding, we introduce a Contextual Multimodal Decoder (CMD) to further exploit the superior interaction between visual and text modalities, especially the vision-to-language interaction. CMD aims to enhance text features by using contextual information obtained from the visual features and to bring the semantic-aware textual information back to visual features in a hierarchical architecture. While we can use the ground truth mask annotations as a direct learning signal to supervise the language-to-vision features, the opposite interaction is implicitly learned without any learning signal.\nBy observing that there are multiple ways to describe an instance based on the context provided by the image, we propose the Meaning Consistency Constraint to enforce consistency of vision-aware text features across different expressions referring to the same instance in an image. The vision-to-language interaction can explicitly learn through this extra in-context learning signal, resulting in a profound, coherent, and contextual understanding in the feature space.We present our novel framework Vision-Aware TEXt for Referring Image Segmentation (VATEX), which achieves superior performance on all splits of the RefCOCO, RefCOCO+, and G-Ref datasets, surpassing the current state of the art for each dataset by 3\u22124%3percent43-4\\%3 - 4 %. We also achieve state-of-the-art performance on both the validation split of Ref-YouTube-VOS and Ref-DAVIS 2017 with 65.465.4\\mathbf{65.4}bold_65.4 \ud835\udca5&\u2131\ud835\udca5\u2131\\mathcal{J}\\&\\mathcal{F}caligraphic_J & caligraphic_F.\nFigure\u00a01(b) demonstrates the differences in the complexity of the query sentences between three RIS benchmark datasets and the quantitative result of our proposed method compared to SOTA methods for RIS, which is LAVT\u00a0[52] and VG-LAW\u00a0[40]. In summary, our key contributions are as follows.We propose a novel DETR-based framework for referring image segmentation, specially designed to utilize vision-aware text features.We present a CLIP-based prior, a Contextual Multimodal Decoder, and a Meaning Consistency Constraint to address the complexities of cross-modal understanding and language ambiguities.Through extensive experiments on three referring image segmentation benchmarks (RefCOCO, RefCOCO+, G-Ref) and two referring video segmentation benchmarks (Ref-YouTube-VOS, Ref-DAVIS17), our approach sets the new state-of-the-art.Referring image segmentation\u00a0[15] aims to generate pixel-wise segmentation masks for referred objects in images given a natural language expression.\nEarly works\u00a0[29, 16, 12] proposed to extract visual and linguistic features independently from convolutional and recurrent neural networks, respectively, and then concatenating these features to create multimodal features for decoding final segmentation results.\nIn recent works\u00a0[24, 55, 52, 8, 10, 1], transformer-based multimodal encoders have been designed to fuse visual and text features, capturing the interaction between vision and language information in the early stage. VG-LAW\u00a0[40] utilizes language-adaptive weights to dynamically adjust the visual backbone, enabling expression-specific feature extraction for better mask prediction while JMCELN\u00a0[17] learns contextual embeddings and progressively aligns predictions for more accurate image segmentation. PolyFormer\u00a0[30], on the other hand, treats this task as a sequential polygon generation process.Query Initialization. The DETR (DEtection TRansformer) framework\u00a0[2] has achieved impressive performance in object detection by directly transforming the task of object detection into a set prediction problem. Building upon DETR, several works have focused on improving the query initialization process for better performance. Deformable DETR\u00a0[56] proposes a deformable transformer architecture to refine object queries, while DAB-DETR\u00a0[31] directly uses the bounding box coordinate in the image to improve query initialization. In the field of referring segmentation, ReferFormer\u00a0[46] extracts the word embeddings from the referring expression and treats them as the initial query for the framework. Our CLIP Prior elevates this approach by incorporating a CLIP-generated heatmap, enriching the textual features with spatial context during query initialization. This enriched query leads to improved performance in the subsequent segmentation stages.Contrastive Learning is pivotal in advancing vision-language tasks\u00a0[4, 5, 13, 48], enhancing model performance by distinguishing similarities and differences in visual and textual data. CLIP\u00a0[38] employed a contrastive loss on an extensive image-text dataset. CRIS\u00a0[45] leveraged text and pixel-level contrastive learning while VLT\u00a0[9] applied masked contrastive learning to refine visual features across diverse expressions. Unlike previous approaches\u00a0[45, 9] that solely focus on improving visual qualities by raw linguistic information, our work utilizes contrastive learning to enhance the comprehension of varied expressions conditioned in a shared image context before using it to enrich the visual features. This ensures the accuracy and stability of mutual interaction between text and visual features, particularly through the comparison of vision-aware expressions related to objects in an image.Our framework is constructed by three main components, as demonstrated in\u00a0Figure\u00a02.\nFirst, we propose a CLIP Prior module to generate an object-centric visual heatmap that localizes the object of interest from the text expression, which can be subsequently used to initialize the object queries for the DETR-based method (Section\u00a03.1).\nNext, we propose to utilize cross-attention modules to interact between visual-text modalities in a hierarchical architecture via our Contextual Multimodal Decoder (Section\u00a03.2).\nDuring training, we leverage Meaning Consistency Constraint to harness vision-aware text features generated by CMD to obtain a meaningful and consistent feature space (Section\u00a03.3).\nWe further adopt a masked-attention transformer decoder\u00a0[6] to enhance the object queries through multiscale text-guided visual features.\nFinally, the enhanced object queries and the visual features from CMD are utilized to output segmentation masks (Section\u00a03.4).Mathematically, given an input image with the size of H\u00d7W\u00d73\ud835\udc3b\ud835\udc4a3H\\times W\\times 3italic_H \u00d7 italic_W \u00d7 3, we can obtain the multiscale visual feature maps \ud835\udcb1={Vi}i=14,Vi\u2208\u211dHi\u00d7Wi\u00d7Ciformulae-sequence\ud835\udcb1superscriptsubscriptsubscript\ud835\udc49\ud835\udc56\ud835\udc5614subscript\ud835\udc49\ud835\udc56superscript\u211dsubscript\ud835\udc3b\ud835\udc56subscript\ud835\udc4a\ud835\udc56subscript\ud835\udc36\ud835\udc56\\mathcal{V}=\\left\\{V_{i}\\right\\}_{i=1}^{4},V_{i}\\in\\mathbb{R}^{H_{i}\\times W_{%\ni}\\times C_{i}}caligraphic_V = { italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT , italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_H start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u00d7 italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u00d7 italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT from the Visual Encoder that captures the visual information in the data, where Hi,Wi,Cisubscript\ud835\udc3b\ud835\udc56subscript\ud835\udc4a\ud835\udc56subscript\ud835\udc36\ud835\udc56H_{i},W_{i},C_{i}italic_H start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT denote the height, width, and the channel dimension of Visubscript\ud835\udc49\ud835\udc56V_{i}italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Given the L\ud835\udc3fLitalic_L-word language expression as input, we use our Text Encoder to encode it into word-level text features fw\u2208\u211dL\u00d7Csubscript\ud835\udc53\ud835\udc64superscript\u211d\ud835\udc3f\ud835\udc36f_{w}\\in\\mathbb{R}^{L\\times C}italic_f start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_L \u00d7 italic_C end_POSTSUPERSCRIPT with C\ud835\udc36Citalic_C as the channel dimension.\nOur visual and text features will be further processed as described in the following sections.To accurately locate the object of interest in the image from a text expression, it is essential to identify the visual region that is the most closely related to the expression. In previous work, query-based methods typically convert only textual information from the natural language into object queries\u00a0[46] or learn the target object representation implicitly through multi-modal transformers\u00a0[8]. Our goal is to leverage the exceptional alignment between visual and text features in the CLIP\u00a0[38] model to generate the heatmap explicitly and embed that positional prior in the query.\nA naive approach of entering full text expressions and images into the CLIP Model results in poor localization accuracy due to the complex input sentence. CLIP is trained to map textual descriptions to visual features, enabling it to identify various objects and visual concepts. Due to the diversity of these visual concepts, only a few specific features are activated for a particular class or category while the majority of features remain inactive and become redundant\nfeatures. As the complexity of text prompts increases, there is a corresponding increase of redundant features within the feature space. When calculating the similarity between an image and a text query, these redundant features also contribute to the final output, causing noisy activations in the heatmap and degrading the model\u2019s performance, as illustrated in \u00a0Table\u00a05. An example is provided in\u00a0Figure\u00a03 where the heatmap covers irrelevant regions in the image with spot activation and does not target to segment the main object of the sentence.Additionally, the human approach to RIS does not involve parsing complex sentences entirely. Instead, we naturally break down a referring expression into its core components: the object of interest and its description with context information. Initially, the primary focus is on identifying what is the object mentioned in the expression (e.g. \"the bull\"). Following this, the search within the image is narrowed to objects that match the main object\u2019s category. The final step involves using the specific characteristics or contextual information described in the expression to pinpoint the target object.Inspired by this, we first extract the main noun phrase from the expression (e.g. the bull) in order to obtain a simplified text expression. Referring expressions in the RIS task are object-centric, which means that the main noun phrase can be considered as the main object of the sentence. We then convert the complex referring expression to a simple template-based sentence before passing it to the CLIP Encoder. In our implementation, we use \"A Photo of [Object]\" as our template as it is the most common prompt to describe an object\u00a0[54] in CLIP, where the resulting text feature is represented by \u2131textsubscript\u2131text\\mathcal{F}_{\\text{text}}caligraphic_F start_POSTSUBSCRIPT text end_POSTSUBSCRIPT. We found that this improves the accuracy of the heatmap in localizing the object of interest. In a separate flow, our input image goes through the CLIP Visual Encoder, resulting in features for multiple image tokens \u2131CLIP-Image\u2208\u211d(H16\u00d7W16+1)\u00d7Csubscript\u2131CLIP-Imagesuperscript\u211d\ud835\udc3b16\ud835\udc4a161\ud835\udc36\\mathcal{F}_{\\text{CLIP-Image}}\\in\\mathbb{R}^{\\left(\\frac{H}{16}\\times\\frac{W}%\n{16}+1\\right)\\times C}caligraphic_F start_POSTSUBSCRIPT CLIP-Image end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT ( divide start_ARG italic_H end_ARG start_ARG 16 end_ARG \u00d7 divide start_ARG italic_W end_ARG start_ARG 16 end_ARG + 1 ) \u00d7 italic_C end_POSTSUPERSCRIPT.While this template approach can efficiently localize regions of interest, it may lead to information loss due to oversimplification (e.g. focusing only on the bull in this scenario). However, its primary function is to narrow down the search space by localizing a region containing the object of interest, not necessarily finding out the exact object. To find the exact object of interest, the full-text expression needs to pass through the CMD and MCC for more comprehensive characteristics and contextual understanding.\nOur visual heatmap for the object of interest can be obtained by calculating the similarity between the visual and text features, then reshaping to image space and going through L2-normalization:As normal practice, positional prior from CLIP is embeded to the text features by changing the dimension of the similarity map from H16\u00d7W16+1\ud835\udc3b16\ud835\udc4a161\\frac{H}{16}\\times\\frac{W}{16}+1divide start_ARG italic_H end_ARG start_ARG 16 end_ARG \u00d7 divide start_ARG italic_W end_ARG start_ARG 16 end_ARG + 1 to C\ud835\udc36Citalic_C, then repeat it N\ud835\udc41Nitalic_N times together with the text features, then add these two to create initial object queries feature with N\ud835\udc41Nitalic_N queries, each with C\ud835\udc36Citalic_C-dimension, for embedding the positional prior and text information into the query feature.For a robust use of visual and text features in subsequent steps, we propose Contextual Multimodal Decoder (CMD) to produce multi-scale text-guided visual feature maps while enhancing contextual information from the image into word-level text features in a hierarchical design, see the architecture figure in the supplementary material.\nSpecifically, our CMD is based on a feature pyramid network architecture\u00a0[27], which has four levels. Each level transfers the semantic information from visual features to text features and then uses these vision-aware text features to update the visual features afterward via cross-modal attention.In the i\ud835\udc56iitalic_i-th level of CMD, given the input visual features Visubscript\ud835\udc49\ud835\udc56V_{i}italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and text features Fi\u22121wsubscriptsuperscript\ud835\udc39\ud835\udc64\ud835\udc561F^{w}_{i-1}italic_F start_POSTSUPERSCRIPT italic_w end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT, the multi-modal interactions are performed in two steps. First, a cross-attention that takes text features Fi\u22121wsubscriptsuperscript\ud835\udc39\ud835\udc64\ud835\udc561F^{w}_{i-1}italic_F start_POSTSUPERSCRIPT italic_w end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT as query and visual features Visubscript\ud835\udc49\ud835\udc56V_{i}italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as key and value is used to model the relationship of the text and visual information. Then it forms the vision-aware text features by associating them with current text features:where MHA\u2062(q,k,v)MHA\ud835\udc5e\ud835\udc58\ud835\udc63\\text{MHA}(q,k,v)MHA ( italic_q , italic_k , italic_v ) is the multi-head cross-attention module with query q\ud835\udc5eqitalic_q, key k\ud835\udc58kitalic_k, value v\ud835\udc63vitalic_v.Fiwsubscriptsuperscript\ud835\udc39\ud835\udc64\ud835\udc56F^{w}_{i}italic_F start_POSTSUPERSCRIPT italic_w end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is then treated as the key and value and Visubscript\ud835\udc49\ud835\udc56V_{i}italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is treated as the query in another multi-head cross-attention module to reinforce the alignment between the visual and text modalities and generate features Vi\u2032subscriptsuperscript\ud835\udc49\u2032\ud835\udc56V^{\\prime}_{i}italic_V start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.\nConsequently, Vi\u2032subscriptsuperscript\ud835\udc49\u2032\ud835\udc56V^{\\prime}_{i}italic_V start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is fused with the text-guided visual feature Fi\u22121vsubscriptsuperscript\ud835\udc39\ud835\udc63\ud835\udc561F^{v}_{i-1}italic_F start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT from the previous level i\u22121\ud835\udc561i-1italic_i - 1 followed by a Conv2d layer to obtain the text-guided visual feature Fivsubscriptsuperscript\ud835\udc39\ud835\udc63\ud835\udc56F^{v}_{i}italic_F start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Mathematically, the whole process is as follows:where Conv2d\u2062()Conv2d\\text{Conv2d}()Conv2d ( ) is the 2D convolutional layer, and Ups\u2062()Ups\\text{Ups}()Ups ( ) denotes upsampling Fi\u22121vsubscriptsuperscript\ud835\udc39\ud835\udc63\ud835\udc561F^{v}_{i-1}italic_F start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT to the size of Vi\u2032subscriptsuperscript\ud835\udc49\u2032\ud835\udc56V^{\\prime}_{i}italic_V start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.\nFor the first level with i=1\ud835\udc561i=1italic_i = 1, we skip F0vsubscriptsuperscript\ud835\udc39\ud835\udc630F^{v}_{0}italic_F start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and let F0w=fwsubscriptsuperscript\ud835\udc39\ud835\udc640subscript\ud835\udc53\ud835\udc64F^{w}_{0}=f_{w}italic_F start_POSTSUPERSCRIPT italic_w end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT, where fwsubscript\ud835\udc53\ud835\udc64f_{w}italic_f start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT is the word-level linguistic features extracted by the Text Encoder.\nIt is common for each object in an image to be described by different text expressions. Although the linguistic meaning of these descriptions may be different, they should convey the same semantic meaning when referencing that image (see two red expressions in \u00a0Figure\u00a04). According to this perspective, it\u2019s crucial for CMD to gradually comprehend contextual cues from visual features into textual representations and ensure consistent identification of target objects, where expressions referring to the same object yield identical representations. However, previous studies have often overlooked the relationship between expressions that pertain to the same instance.To delve deeper into this relationship and provide the explicit in-context learning signal for vision-aware text features within CMD, we propose Meaning Consistency Constraint (MCC), a sentence-level contrastive learning approach. MCC aims to learn meaningful and discriminative representations for different expressions while consistently pulling sentences referring to the same object close to each other.Unlike previous contrastive learning-based approaches\u00a0[9, 45], we focus on linguistic features that are enriched and conditioned by visual information. This can encourage CMD module to gradually learn how to produce richer text features and lead to the improvement of visual features in context understanding due to the bidirectional attention mechanism and hierarchical design of CMD module.Our contrastive learning pipeline is illustrated in \u00a0Figure\u00a04. During training, we construct a triplet of text expressions for each image. Each triplet comprises two sentences that refer to the same object (positive samples), along with a third sentence describing a different object (negative sample). We denote the positive samples by p1,p2subscript\ud835\udc5d1subscript\ud835\udc5d2p_{1},p_{2}italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and the negative sample by n\ud835\udc5bnitalic_n, respectively. With each sample x\ud835\udc65xitalic_x, we derive the sentence-level feature by averaging the vision-aware word-level textual features:where F4w\u2208RL\u00d7C,Fs\u2208RCformulae-sequencesubscriptsuperscript\ud835\udc39\ud835\udc644superscript\ud835\udc45\ud835\udc3f\ud835\udc36subscript\ud835\udc39\ud835\udc60superscript\ud835\udc45\ud835\udc36F^{w}_{4}\\in R^{L\\times C},F_{s}\\in R^{C}italic_F start_POSTSUPERSCRIPT italic_w end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT \u2208 italic_R start_POSTSUPERSCRIPT italic_L \u00d7 italic_C end_POSTSUPERSCRIPT , italic_F start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT \u2208 italic_R start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT.\nWe adopt the InfoNCE loss\u00a0[37] to ensure that linguistic features referring to the same object converge, while features of different objects diverge:\nwhere sim\u2062(p,n)=exp\u2062(Fs\u2062(p)\u22c5Fs\u2062(n))sim\ud835\udc5d\ud835\udc5bexp\u22c5subscript\ud835\udc39\ud835\udc60\ud835\udc5dsubscript\ud835\udc39\ud835\udc60\ud835\udc5b\\text{sim}(p,n)=\\text{exp}(F_{s}(p)\\cdot F_{s}(n))sim ( italic_p , italic_n ) = exp ( italic_F start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( italic_p ) \u22c5 italic_F start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( italic_n ) ) to calculate the exponential for cosine similarity of sentence-level obtained from the text expressions.Prediction Heads.\nWe adopt the masked-attention transformer decoder\u00a0[6] to update our initial query feature fosubscript\ud835\udc53\ud835\udc5cf_{o}italic_f start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT by using the multi-scale text-guided visual features {Fiv}i=13superscriptsubscriptsubscriptsuperscript\ud835\udc39\ud835\udc63\ud835\udc56\ud835\udc5613\\left\\{F^{v}_{i}\\right\\}_{i=1}^{3}{ italic_F start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT to obtain the final object queries Fo\u2208\u211dN\u00d7Csubscript\ud835\udc39\ud835\udc5csuperscript\u211d\ud835\udc41\ud835\udc36F_{o}\\in\\mathbb{R}^{N\\times C}italic_F start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_N \u00d7 italic_C end_POSTSUPERSCRIPT. The final object queries will directly predict the probability of the target object p^\u2208\u211dN^\ud835\udc5dsuperscript\u211d\ud835\udc41\\hat{p}\\in\\mathbb{R}^{N}over^ start_ARG italic_p end_ARG \u2208 blackboard_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT. The high-resolution segmentation mask s^\u2208RH4\u00d7W4\u00d7N^\ud835\udc60superscript\ud835\udc45\ud835\udc3b4\ud835\udc4a4\ud835\udc41\\hat{s}\\in R^{\\frac{H}{4}\\times\\frac{W}{4}\\times N}over^ start_ARG italic_s end_ARG \u2208 italic_R start_POSTSUPERSCRIPT divide start_ARG italic_H end_ARG start_ARG 4 end_ARG \u00d7 divide start_ARG italic_W end_ARG start_ARG 4 end_ARG \u00d7 italic_N end_POSTSUPERSCRIPT is produced by associating between object queries Fosubscript\ud835\udc39\ud835\udc5cF_{o}italic_F start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT with the last fine-grained text-guided visual features F4v\u2208\u211dH4\u00d7W4\u00d7Csubscriptsuperscript\ud835\udc39\ud835\udc634superscript\u211d\ud835\udc3b4\ud835\udc4a4\ud835\udc36F^{v}_{4}\\in\\mathbb{R}^{\\frac{H}{4}\\times\\frac{W}{4}\\times C}italic_F start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT divide start_ARG italic_H end_ARG start_ARG 4 end_ARG \u00d7 divide start_ARG italic_W end_ARG start_ARG 4 end_ARG \u00d7 italic_C end_POSTSUPERSCRIPT, which can be formulated as:Instance Matching.\n\nThe prediction set output from prediction heads is represented by y^={yi^}i=1N^\ud835\udc66superscriptsubscript^subscript\ud835\udc66\ud835\udc56\ud835\udc561\ud835\udc41\\hat{y}=\\left\\{\\hat{y_{i}}\\right\\}_{i=1}^{N}over^ start_ARG italic_y end_ARG = { over^ start_ARG italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT, where y^i={pi^,si^}subscript^\ud835\udc66\ud835\udc56^subscript\ud835\udc5d\ud835\udc56^subscript\ud835\udc60\ud835\udc56\\hat{y}_{i}=\\left\\{\\hat{p_{i}},\\hat{s_{i}}\\right\\}over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { over^ start_ARG italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG , over^ start_ARG italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG }.\nSince a text expression refers to only a specific object, we denote the ground truth object as y={pg\u2062t=1,sg\u2062t}\ud835\udc66subscript\ud835\udc5d\ud835\udc54\ud835\udc611subscript\ud835\udc60\ud835\udc54\ud835\udc61y=\\left\\{p_{gt}=1,s_{gt}\\right\\}italic_y = { italic_p start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT = 1 , italic_s start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT }. The best prediction y^\u03b4subscript^\ud835\udc66\ud835\udeff\\hat{y}_{\\delta}over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_\u03b4 end_POSTSUBSCRIPT can be found by a Hungarian algorithm\u00a0[23] by minimizing the matching cost in terms of probability and segmentation mask\u00a0[7, 6].Training.\nOur prediction y^\u03b4subscript^\ud835\udc66\ud835\udeff\\hat{y}_{\\delta}over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_\u03b4 end_POSTSUBSCRIPT is supervised by three losses. Firstly, the class loss \u2112c\u2062l\u2062ssubscript\u2112\ud835\udc50\ud835\udc59\ud835\udc60\\mathcal{L}_{cls}caligraphic_L start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT is binary cross entropy (BCE) loss to supervise the probability of referred object. Secondly, the mask loss \u2112m\u2062a\u2062s\u2062ksubscript\u2112\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58\\mathcal{L}_{mask}caligraphic_L start_POSTSUBSCRIPT italic_m italic_a italic_s italic_k end_POSTSUBSCRIPT is a combination of dice loss and BCE. Finally, our sentence-level contrastive loss \u2112m\u2062c\u2062csubscript\u2112\ud835\udc5a\ud835\udc50\ud835\udc50\\mathcal{L}_{mcc}caligraphic_L start_POSTSUBSCRIPT italic_m italic_c italic_c end_POSTSUBSCRIPT is used to enforce our Meaning Consistency Constraint. The total loss can be formulated as follows:where \u03b3c\u2062l\u2062s,\u03b3m\u2062a\u2062s\u2062k,\u03b3m\u2062c\u2062csubscript\ud835\udefe\ud835\udc50\ud835\udc59\ud835\udc60subscript\ud835\udefe\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58subscript\ud835\udefe\ud835\udc5a\ud835\udc50\ud835\udc50\\gamma_{cls},\\gamma_{mask},\\gamma_{mcc}italic_\u03b3 start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT , italic_\u03b3 start_POSTSUBSCRIPT italic_m italic_a italic_s italic_k end_POSTSUBSCRIPT , italic_\u03b3 start_POSTSUBSCRIPT italic_m italic_c italic_c end_POSTSUBSCRIPT are the scalar coefficients.Inference. In inference, our method aligns with the standard practice of using a single image or video with one text expression, and MCC only requires sampling positive and negative expressions in the training phase. During inference, the query with the highest probability score is selected as the target object for the final output.\nWe evaluate the performance of our model on three image datasets: RefCOCO\u00a0[18], RefCOCO+\u00a0[18], G-Ref\u00a0[36] and further evaluate the performance of our model on two video datasets: Ref-Youtube-VOS\u00a0[39] and Ref-DAVIS17\u00a0[19].For evaluation metrics, we calculate mIoU to measure the overlap between the predicted segmentation mask and the ground truth mask for image segmentation. We also calculate region similarity \ud835\udca5\ud835\udca5\\mathcal{J}caligraphic_J and contour accuracy \u2131\u2131\\mathcal{F}caligraphic_F, and their average \ud835\udca5&\u2131\ud835\udca5\u2131\\mathcal{J}\\&\\mathcal{F}caligraphic_J & caligraphic_F for video object segmentation.We use PyTorch to implement our method. During training, we freeze both the CLIP Visual and Text Encoder. Images are resized to a short side of 480. We set the coefficients for the losses as \u03b3c\u2062l\u2062s=2,\u03b3m\u2062a\u2062s\u2062k=5formulae-sequencesubscript\ud835\udefe\ud835\udc50\ud835\udc59\ud835\udc602subscript\ud835\udefe\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc585\\gamma_{cls}=2,\\gamma_{mask}=5italic_\u03b3 start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT = 2 , italic_\u03b3 start_POSTSUBSCRIPT italic_m italic_a italic_s italic_k end_POSTSUBSCRIPT = 5, and \u03b3m\u2062c\u2062c=2subscript\ud835\udefe\ud835\udc5a\ud835\udc50\ud835\udc502\\gamma_{mcc}=2italic_\u03b3 start_POSTSUBSCRIPT italic_m italic_c italic_c end_POSTSUBSCRIPT = 2, with the feature dimension C\ud835\udc36Citalic_C set to 256256256256. We train the network for 100,000100000100,000100 , 000 iterations on the RefCOCO(+/g) datasets with an initial learning rate of 10\u22124superscript10410^{-4}10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT and is reduced by a factor of 0.10.10.10.1 at the 2/3232/32 / 3 last iteration. For the Ref-Youtube-VOS dataset, we initialize the pre-trained weight from RefCOCO(+/g) and train the network for 100,000100000100,000100 , 000 iterations. Regarding the Ref-DAVIS17 dataset, we directly use the weight obtained from the Ref-Youtube-VOS dataset for inference.\nFor detailed information on each dataset and implementation, please see the supplementary material.Referring Image Segmentation.\nWe compare our proposed method with the state-of-the-art methods on three common datasets (RefCOCO, RefCOCO+, G-Ref). As illustrated in\u00a0Table\u00a01, our method outperforms the state-of-the-art methods by a large margin in all splits of different datasets in the standard setting. Particularly, for the RefCOCO dataset, our method outperforms the recent VG-LAW, with mIoU gains of 3.11%, 2.28%, and 3.95% across the three splits, respectively. For the more complex RefCOCO+ dataset, our method outperforms the previous top method, JMCELN, by mIoU margins of 3.03%, 1.72%, and 5.18% in the validation, test A, and test B sets, respectively. The G-Ref dataset presented the most significant challenge, with longer and more complicated language expressions. Despite this, our method achieved remarkable performance improvements of approximately 5% in mIoU on the validation and test sets, compared to the second-best method, VG-LAW, in the same setting.Referring Video Segmentation. Our model can be extended to video datasets with minor adaptations to handle temporal information. As shown in\u00a0Table\u00a03, VATEX outperforms the current SOTA methods VLT and ReferFormer on the same Video-Swin-B backbone by 1.6 and 3.8 \ud835\udca5&\u2131\ud835\udca5\u2131\\mathcal{J}\\&\\mathcal{F}caligraphic_J & caligraphic_F on the Ref-Youtube-VOS and Ref-DAVIS17 datasets, respectively.Ablation Study.\nWe conduct an ablation study on the validation sets of RefCOCO with Swin-B backbone and Ref-Youtube-VOS validation set with Video-Swin-B backbone to examine the impact of each proposed component in our model. The baseline follows the architecture of ReferFormer\u00a0[46] by using only languages as the initial query (removing CLIP Prior), while only using text-guided vision features and ignoring the vision-aware text features (removing CMD and MCC). As shown in\u00a0Table\u00a05, the combination of CLIP Prior, CMD, and MCC modules results in the best performance, showcasing a remarkable performance increase of up to 7.74% in mIoU on RefCOCO and 5.6% in \ud835\udca5&\u2131\ud835\udca5\u2131\\mathcal{J}\\&\\mathcal{F}caligraphic_J & caligraphic_F on Ref-Youtube-VOS. This outcome unequivocally attests to the remarkable effectiveness of our approach and underscores its significant impact. The full ablation study is shown in the supplementary material.CLIP Prior Analysis. As described in\u00a0Section\u00a03.1, our CLIP Prior relies on a template sentence to convert a complex sentence into a simplified sentence suitable for CLIP. The baseline which uses text features as initial query achieves 75.43% mIoU. We investigate the effects of using different templates and how these affect the final performance. Table\u00a05 demonstrates that using the complex sentence leads to a significant deterioration in performance at 1.09%. In this case, CLIP introduces noisy activation on various objects based on their discriminative characteristics within the complex sentence, which is harmful to the model. By using the template \u201cA Photo of [Object]\u201d, there is a notable improvement of 2.73% in mIoU. We conducted an additional experiment where we aggregated the text embeddings from 80 ImageNet prompts, which has a very minor performance improvement. This demonstrates that the template \u201cA Photo of [Object]\u201d remains a practical choice.\nRegarding the quality of the heatmap, Figure\u00a08 demonstrates the enhanced localization performance achieved when prompt templates are utilized, compared to the original expression. Without simplifying the sentence, the heatmap tends to encompass distinctive categories in images or mentioned in the sentence (e.g. obstacles, woman and blue shirts), or highlights the wrong object (e.g. wind-surfing boat). By reducing the complexity of the text expression, it can be seen that the activation of the object of interest becomes more accurate. For more visualization, please see the supplementary material.\nEffect of CMD and MCC on Vision-aware Text Features.\nTo study the impact of the CMD and MCC on text features, we calculate the similarity between sentence-level text features before and after processing through CMD. The average similarity results for all pairs of expressions referring to the same or different objects at each CMD level are depicted in\u00a0Figure\u00a06. Here, Level 0 represents the initial features, derived directly from the CLIP Embedding, while Level 4 indicates the final vision-aware text features of CMD.In the context of expressions referring to the same object, the initial similarity of text features stands at 68.88%percent68.8868.88\\%68.88 %, 65.82%percent65.8265.82\\%65.82 %, and 63.35%percent63.3563.35\\%63.35 % for RefCOCO, RefCOCO+, and G-Ref, respectively. Following just one CMD level, the similarities witness an approximate 7%percent77\\%7 % elevation across all datasets. The final vision-aware text features of the CMD process yield similarities of 81.29%percent81.2981.29\\%81.29 % in RefCOCO, 78.28%percent78.2878.28\\%78.28 % in RefCOCO+, and 76.19%percent76.1976.19\\%76.19 % in G-Ref.Regarding different objects, we compute the similarity between all expressions pertaining to different objects within the same image to showcase the influence of our CMD module on them. As illustrated in\u00a0Figure\u00a06(b), the similarity between the features of these expressions progressively decreases from 53.23%percent53.2353.23\\%53.23 %, 55.23%percent55.2355.23\\%55.23 %, 60.06%percent60.0660.06\\%60.06 % to 22.12%percent22.1222.12\\%22.12 %, 32.82%percent32.8232.82\\%32.82 %, 36.81%percent36.8136.81\\%36.81 % on RefCOCO, RefCOCO+, and G-Ref, respectively.These findings underscore the pivotal role of CMD in bolstering multimodal comprehension. Specifically, we reveal how CMD effectively improves the representation of text features that are conditioned on visual context, illustrating its significance in advancing the interaction between textual and visual information.Effect of MCC on Object segmentation mask. To quantify the ability to consistently segment various expressions for the same object and further validate the effectiveness of our proposed Meaning Consistency Constraint, we leverage an Object-centric Intersection over Union (Oc-IoU) score, which calculates the overlap and union area between ground truth and all segmentation predictions of the same object. Specifically, consider the i\ud835\udc56iitalic_i-th object with Kisubscript\ud835\udc3e\ud835\udc56K_{i}italic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT expressions referring to that object and the corresponding ground truth mask GTisubscriptGT\ud835\udc56\\text{GT}_{i}GT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Let Pijsuperscriptsubscript\ud835\udc43\ud835\udc56\ud835\udc57P_{i}^{j}italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT be the model\u2019s prediction for the j\ud835\udc57jitalic_j-th expression of the i\ud835\udc56iitalic_i-th object, where j=1..Ki\u00af.j=\\overline{1..\\;K_{i}}.italic_j = over\u00af start_ARG 1 . . italic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG . The Object-centric IoU can be formulated as follows:\nwhere N\ud835\udc41Nitalic_N is the total number of objects/instances in the datasets.\nTable\u00a03 provides the comparisons between our method and the state-of-the-art method LAVT in Oc-IoU on the validation set of three RIS benchmarks. As can be seen, our method outperforms LAVT in all three datasets.\nComparing the last two rows of\u00a0Table\u00a03, we can see that the MCC helps the model, especially CMD to enhance mutual information between textual and visual features to further provide more consistent and accurate segmentation. These results underscore the compelling efficacy of our Meaning Consistency Constraint in resolving language ambiguities, thus improving the segmentation performance.\nQualitative Analysis. We provide the visualization of our results in\u00a0Figure\u00a07. Our method can successfully segment objects in complex scenarios, such as the presence of multiple similar objects. For example, in the first column, we can localize the guy who sits in the chair instead of the man standing on the tennis court. In the second sample, VATEX can distinguish the sushi plate among several food dishes that LAVT cannot. In the fourth column, our model can not only identify the correct umbrella belonging to the guy in the light jacket but also segment a part of the shaft of the umbrella that the ground truth does not provide. With the expression \"bear child is hugging\" in the sixth image, LAVT can only segment the bear\u2019s head, but VATEX can output the whole annotation of the target bear.\nHowever, our method fails to segment objects that need to be counted and selected by their order or be described indirectly through another object, as we have not leveraged counting information and object interaction in our model.Another point worth mentioning is the differences in architecture design between VATEX and LAVT, with VATEX focusing on instance-based segmentation and LAVT on pixel-based segmentation. This will lead to smoother, more complete segmentation masks by VATEX. However, this strength becomes a weakness when the model misidentifies an instance, resulting in entirely incorrect segmentation masks, whereas LAVT\u2019s pixel-based approach can still partially identify the object in such situations.Our method is not without limitations.\nParticularly, our method does not exploit the interaction and relationship between different objects as well as the alignment between actions and expressions referring to them (see\u00a0Figure\u00a07 the last two columns). Consequently, situations involving counting (\"third from left\"), indirect descriptions (\"kid next to girl in pink pants\"), or actions (\"a woman walking to the left\") might lead to inaccurate predictions. Dealing with intra-frame object relationships and inter-frame information is vital for future work. Another line of future work is making RIS work on general scenarios (e.g. segment all the red-colored objects, segment all the text in the image) or more fine-grained segmentation (e.g. segment the eye of the owl).\nIt is also of great interest to investigate vision-aware text features with the SOTA vision-and-language model and to lift the referring expression segmentation task to the 3D domain.This paper introduces VATEX, which examines how vision-aware text features can enhance the performance of RIS. First, we proposed adding visual cues to text features during the query initialization process in CLIP-Prior. Second, we exploit the interaction between visual-text modalities, especially in vision-to-language direction (CMD). Following that, we propose the MCC module for maintaining a meaningful and consistent interpretation of various language phrases when referring to the same instance. In this approach, we take advantage of the dataset\u2019s characteristic of containing multiple expressions for a single instance, a detail often overlooked by other methods. Looking ahead, we aim to bolster the robustness of MCC by enriching the diversity of language expressions through large Vision-Language Models (VLMs). Preliminary results demonstrating the use of VLMs to generate and augment expressions are detailed in the supplementary material, highlighting the promising direction of our research.\nImproving Referring Image Segmentation using Vision-Aware Text Features \u2013 Supplementary Materials\nHai Nguyen-Truong E-Ro NguyenCo-first author Tuan-Anh VuCorresponding author Minh-Triet Tran Binh-Son Hua Sai-Kit YeungOur supplementary has 4 sections. Section\u00a07 shows additional information about datasets and training procedure. Section\u00a08 contains the additional experiments on RefCOCO(+/g), Ref-Youtube-VOS and Ref-DAVIS17. This section also illustrates and analyzes the performance of CLIP Prior, CMD and MCC in different situations. Section\u00a09 discusses about future work of VATEX, where we can enhance the expression\u2019s diversity using GPT-4(V).Image datasets. RefCOCO and RefCOCO+\u00a0[18] are two of the largest image datasets used for referring image segmentation. They contain 142,209 and 141,564 language expressions describing objects in images. RefCOCO+ is considered to be more challenging than RefCOCO, as it focuses on purely appearance-based descriptions. G-Ref\u00a0[36], or RefCOCOg, is another well-known dataset with 85,474 language expressions with more than 26,000 images. The language used in G-Ref is more complex and casual, with longer sentence lengths on average.Video datasets. Ref-YouTube-VOS\u00a0[39] and Ref-DAVIS17\u00a0[19] are well-known datasets for referring video object segmentation. Ref-YouTube-VOS contains 3978 video sequences with approximately 15000 referring expressions, while Ref-DAVIS17 consists of 90 high-quality video sequences. These datasets are used to evaluate the performance of algorithms that aim to identify a specific object within a video sequence based on natural language expressions.Our model is optimized using AdamW\u00a0[34] optimizer with the initial learning rate of 10\u22125superscript10510^{-5}10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT for the visual encoder and 10\u22124superscript10410^{-4}10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT for the rest. Our model comprises a total of nine Masked-Attention Transformer Decoder layers followed\u00a0[6]. We set the number of queries to 5\u00a0[46]. For the setting of training from classification weight from Imagenet on Ref-Youtube-VOS dataset, we train the model for 200,000200000200,000200 , 000 iteration with the learning drop at 140,000140000140,000140 , 000-th iteration. On Ref-DAVIS17[19], we directly report the results using the model trained on Ref-YouTube-VOS without finetuning.We have benchmarked our model, VATEX, using the ResNet-101 backbone, aligning it with CRIS and JMCELN for a more equitable comparison. This adaptation demonstrates VATEX\u2019s superior performance, achieving a 1.26% improvement on RefCOCO val and a significant 1.93% on RefCOCO testB over the current state-of-the-art methods.Further, to address comparisons with LAVT, we have experimented with CLIP as the text encoder, adhering to the official repository guidelines. This experiment revealed a performance decline of approximately 1% when substituting BERT with CLIP as the text encoder. This finding underscores the critical importance of using the CLIP Image Encoder together with the CLIP Text Encoder to maintain model performance. A similar trend was observed with ReferFormer, reinforcing our conclusion. Consequently, when compared to LAVT under the fair conditions in backbone, VATEX shows a substantial improvement, outperforming by 5.01%, 4.40%, and 5.62% on RefCOCO val, testA, and testB, respectively. This data confirms the effectiveness of our approach and the importance of consistent backbone usage for fair and accurate performance assessment.Table\u00a07 presents a detailed comparison between our model, VATEX, and other leading methods such as LAVT, ReLA, and CG-Former, highlighting VATEX\u2019s remarkable superiority across benchmarks, particularly in Pr@0.7 and Pr@0.9. Notably, VATEX exceeds CG-Former by 6.34% and ReLA by 10.12% at the Pr@0.9 benchmark, showcasing its exceptional ability to achieve complete and accurate segmentation of different instances within images. This performance is evidenced by its high precision scores, as well as through visualizations. To further enhance VATEX\u2019s mIoU, an improved matching function is essential for precisely aligning instances with user expressions, underscoring our next steps in improving VATEX\u2019s performance in the future.SeqTR, RefTR, and PolyFormer enhance their performance on the RefCOCO dataset by incorporating external datasets\u2014Visual Genome with 5.4M descriptions across over 33K categories, Flickr30k-entities with 158K descriptions, and the joint dataset RefCOCO(+/g) with 368K descriptions. Their papers indicate that using such external datasets for pretraining can improve performance by 8-10%.Compared to PolyFormer\u00a0[30], without using external pretraining dataset, VATEXRefCOCO demonstrates superior performance over PolyFormer-B, while VATEXRefCOCO+ and VATEXG-Ref achieve comparable results with [30] while using 42x and 69x smaller datasets respectively, with the exception of the RefCOCO+ test B. The performance\u2019s gap on RefCOCO+ Test B, which focuses on non-human objects described purely by their appearance (e.g. \"the porcelain throne,\" \"part of the bed occupied by a black pamphlet\"), could be attributed to the varied object categories covered during the pre-training phase with extensive external datasets.On the otherhand, VATEXjoint adopts a different strategy. By solely utilizing the RefCOCO(+/g) dataset, which is 16x smaller than the datasets used by PolyFormer, VATEXjoint with Swin-B backbone still achieves remarkable results. Specifically, VATEXjoint outperforms PolyFormer by 4-6% across all benchmarks, setting a new state-of-the-art result on the RefCOCO dataset.Table\u00a09 illustrates the quantitative performance between VATEX with generalist foundation models: Grounded-SAM[32][22], SEEM\u00a0[58] and X-Decoder\u00a0[57] in \u00a0Table\u00a09. For Grounded-SAM, we first use Grounding DINO to extract the bounding box prediction from the text prompt, then we feed that bounding box to SAM to obtain the final segmentation mask. For X-Decoder and SEEM, we directly use the report number on their official github***https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once/ with Focal-L backbones. While VATEX is trained on much smaller dataset sizes and smaller backbones, VATEXjoint still significantly outperforms Grounded-SAM with 14.34%, 15.65%, and 16.4% improvements on RefCOCO, RefCOCO+ and G-Ref, respectively. Compared with X-Decoder and SAM, which are trained and finetuned on RefCOCO(+/g) datasets, we also outperform them with approximately 2% with VATEX and 7.7% with VATEXjoint,The result for Ref-Youtube-VOS dataset is shown in Table\u00a011.\nAs can be seen, our method demonstrates superior performance, setting a new state-of-the-art for referring video object segmentation on the Ref-Youtube-VOS dataset with different backbones. In particular, our approach with the spatial-temporal backbone (e.g., Video-Swin\u00a0[33]) and pre-trained weights from the image dataset achieves the highest \ud835\udca5&\u2131\ud835\udca5\u2131\\mathcal{J}\\&\\mathcal{F}caligraphic_J & caligraphic_F score of 65.4%percent65.465.4\\%65.4 % among all other methods on the Ref-Youtube-VOS dataset, including VLT and ReferFormer.The results for Ref-DAVIS17 are shown in Table\u00a011. Similarly, our approach achieves competitive performance compared to other state-of-the-art methods in referring video object segmentation. Specifically, with backbones ResNet-50, our proposed model outperforms ReferForme and achieves slightly better results than RRVOS. Moreover, our method achieves the best performance among all methods with the Video-Swin-B backbone with a \ud835\udca5&\u2131\ud835\udca5\u2131\\mathcal{J}\\&\\mathcal{F}caligraphic_J & caligraphic_F score of 65.4%, which is 3.8% higher than the closest competitor VLT.To obtain the heatmap result, from the vector of shape (H16\u00d7W16+1,1)\ud835\udc3b16\ud835\udc4a1611\\left(\\frac{H}{16}\\times\\frac{W}{16}+1,1\\right)( divide start_ARG italic_H end_ARG start_ARG 16 end_ARG \u00d7 divide start_ARG italic_W end_ARG start_ARG 16 end_ARG + 1 , 1 ), we remove \"CLS\u201d token and reshape it into 2D heatmap of H16\u00d7W16\ud835\udc3b16\ud835\udc4a16\\frac{H}{16}\\times\\frac{W}{16}divide start_ARG italic_H end_ARG start_ARG 16 end_ARG \u00d7 divide start_ARG italic_W end_ARG start_ARG 16 end_ARG. For visualization purposes, we resize the original image to 960\u00d7960960960960\\times 960960 \u00d7 960, then pass it through CLIP-Image Encoder, resulting in a high-quality heatmap of size 60\u00d760606060\\times 6060 \u00d7 60. Notably, we only use a default input size of 224\u00d7224 during training. Regarding the quality of the heatmap, Figure\u00a08 demonstrates the comparison between the naive implementation and our prompt-based template. In the 3rd and 7th rows, it is evident that simplifying the sentence and employing prompt templates can aid in distinguishing the target object from the image, resulting in decreased localization errors.While CLIP Prior excels at localizing objects of interest, it can struggle in complex cases where the expression describes multiple instances within the same category and their relative positions (e.g. bottom right of Figure\u00a08). In these situations, the heatmap may encompass all objects within the category rather than the specific referred instances. However, CLIP Prior\u2019s core purpose is to narrow down the relevant region, not pinpoint the exact object. Identifying the precise instance will be handled later in the full-text prompt by the Transformer architecture, which can leverage additional context and relationships.Moreover, CLIP Prior can also help the model in cases when the referring expression contains out-of-vocabulary objects. By transferring the knowledge from CLIP and embedding the heatmap into the query initialization, the model can obtain a good segmentation mask based on the cues from CLIP Prior. Figure\u00a09 shows how CLIP Prior heatmap can help the model to localize the object in the early phase, thus improving the model\u2019s performance.It is easy to implement and incorperate CLIP Prior into query-based segmentation model. With CLIP-based models the heatmap can be generated without any additional computational cost through matrix multiplication and L2-normalization, as illustrated in Equation 1. For non-CLIP-based models like ReLA\u00a0[28], the BERT text encoder needs to be replaced with the CLIP-Text Encoder, and an additional CLIP-Image Encoder is employed to obtain the well aligned embeddings from the CLIP Model. The overall computational cost for ReLA, measured in FLOPS, is calculated as:with FLOPS(BERT) \u2248\\approx\u2248 3.42 GFLOPS and FLOPS(CLIP) \u2248\\approx\u2248 10.24 GFLOPS for a 20-word sentence and an 224x224 image.CLIP-based model in RIS. Adopting CLIP is a good practice taken by several previous methods, including CRIS, CM-MaskSD, and RIS-CLIP. However, to effectively use the aligned embedding from CLIP to obtain good results in referring segmentation is an open question. For example, although using powerful CLIP as the backbone, the SOTA CLIP-based method RIS-CLIP\u00a0[21] has a comparable performance with the SOTA Non-CLIP model VG-LAW\u00a0[40]. To analyze it, we take CRIS\u00a0[45] as a baseline for CLIP-based model. CRIS directly used the well-aligned embedding space between text and vision for RIS. However, the performance of this work is not good compared to others, as there are two concerns with relying solely on CLIP for referring image segmentation tasks:Frozen CLIP Model. CLIP model, trained on object-centric images, generates visual features focusing on semantic class meanings rather than instance-based details (see bird example in \u00a0Figure\u00a08). This limits the effectiveness of CLIP for instance-level tasks.Fine-tuning CLIP Model. Fine-tuning the CLIP model risks overfitting on training samples, thereby diminishing its ability to generalize features to novel classes.We found that learning from a visual backbone pre-trained on ImageNet and only utilizing frozen CLIP as a prior gave better performance on both instance-level segmentation and open-vocabulary segmentation nature of RIS.For a truly fair comparison, we provide our method w/o CLIP, which achieves 75.43, 67.38, and 68.12 mIoU, and we still outperform the SOTA LAVT (74.46, 65.81, and 63.34) and VG-LAW (75.05, 66.61 and 65.36) on RefCOCO(+/g) in the same setting.Table 13 presents an ablation study conducted on the validation set of RefCOCO and Ref-Youtube-VOS, evaluating the mIoU (mean Intersection over Union) and \ud835\udca5&\u2131\ud835\udca5\u2131\\mathcal{J}\\&\\mathcal{F}caligraphic_J & caligraphic_F, respectively of different model configurations. The study explores the impact of three components: CLIP Prior, CMD (Contextual Multimodal Decoder), and MCC (Meaning Consistency Constraint).The first row represents the baseline model with none of the studied components incorporated. The mIoU for this configuration is 70.42%percent70.4270.42\\%70.42 % mIoU and 59.8 \ud835\udca5&\u2131\ud835\udca5\u2131\\mathcal{J}\\&\\mathcal{F}caligraphic_J & caligraphic_F. In rows 2 to 4, the ablation study reveals that incorporating independently the CLIP Prior alone (row 2) and CMD (row 3) both contribute positively to the mIoU on the RefCOCO and \ud835\udca5&\u2131\ud835\udca5\u2131\\mathcal{J}\\&\\mathcal{F}caligraphic_J & caligraphic_F on Ref-YoutubeVOS validation set with an improvement of 1.53%,2.76%percent1.53percent2.761.53\\%,2.76\\%1.53 % , 2.76 % mIoU and 1.7%,2.1%percent1.7percent2.11.7\\%,2.1\\%1.7 % , 2.1 % \ud835\udca5&\u2131\ud835\udca5\u2131\\mathcal{J}\\&\\mathcal{F}caligraphic_J & caligraphic_F, whereas the introduction of the Meaning Consistency Constraint (MCC) alone (row 4) leads to a modest increase (only 0.30%percent0.300.30\\%0.30 % mIoU and 0.40.40.40.4 \ud835\udca5&\u2131\ud835\udca5\u2131\\mathcal{J}\\&\\mathcal{F}caligraphic_J & caligraphic_F), emphasizing the individual significance of each component in enhancing model performance. Although MCC alone has a modest impact, when combined with the CMD in row 7, there is a notable improvement of 4.7%percent4.74.7\\%4.7 % (mIoU of 75.175.175.175.1) and 3.3%percent3.33.3\\%3.3 % (\ud835\udca5&\u2131\ud835\udca5\u2131\\mathcal{J}\\&\\mathcal{F}caligraphic_J & caligraphic_F of 63.1). This synergy demonstrates that while MCC alone may not perform exceptionally, its collaboration with CMD effectively enhances model performance, aligning with our approach of leveraging enriched text features conditioned by visual information for improved mutual interaction. The final row represents the model with all components (CLIP Prior, CMD, and MCC) combined, achieving the highest mIoU of 78.16 (+7.74) and \ud835\udca5&\u2131\ud835\udca5\u2131\\mathcal{J}\\&\\mathcal{F}caligraphic_J & caligraphic_F of 65.4 (+5.6).\nFor a robust use of visual and text features in subsequent steps, we propose to fuse visual and text features using a Contextual Multimodal Decoder (CMD), which is designed to produce multi-scale text-guided visual feature maps while enhancing contextual information from the image into word-level text features in a hierarchical design as shown in Figure\u00a010. The process on each level of CMD is achieved by a Bi-directional Attention Transfer(BAT), which incorporates two cross-attention modules.In Figure 11, we provide a detailed analysis of the performance of our approach based on the length of language expressions. To do this, we followed the approach in [20] and split each validation set of the dataset into four groups based on the length of the language expressions (sentence length), with each group being roughly the same size. Our approach VATEX achieved outstanding results, outperforming most previous methods with large margins of over 4%percent44\\%4 % in IoU, except for the group of length 6-7 on RefCOCO, where the difference in performance was only 0.5%percent0.50.5\\%0.5 %. Notably, our approach demonstrated exceptional performance on the G-Ref dataset, surpassing the previous methods\u00a0[20] by a remarkable margin of more than 10%percent1010\\%10 % on each sentence length group. These results attest to the effectiveness of our approach in handling the challenges posed by diverse and complex referring expressions in the G-Ref dataset. Additionally, our approach displayed less performance deterioration across the range of sentence lengths on three datasets compared to others. Our findings suggest that our approach accurately identifies objects in complex and diverse visual scenes, rendering it a promising solution for real-world applications where accurate and efficient segmentation is crucial.\nOur method\u2019s utilization of diverse referring expressions for each object aligns with established best practices in text-image dataset annotation. This approach is widely accepted and implemented across several benchmark datasets. In scenarios where multiple expressions per object are unavailable, we have the flexibility to employ Large Language Models (LLMs) for enhancing expression diversity. This can be achieved either by augmenting existing expressions or generating new ones based on object masks, a technique successfully employed by datasets like RIS-CQ. Furthermore, we demonstrate a practical application of this approach through a sample that showcases how we can prompt ChatGPT to generate relevant expressions in Fig.\u00a012. This generation is based on factors like an object\u2019s position in the image, its relative position to other objects or people, and distinguishing attributes such as color or appearance.Figure 10 showcases two innovative prompting techniques for generating object descriptions. On the left, we demonstrate how combining an original image with its masked version can effectively prompt GPT-4 to generate detailed descriptions. The right side of Figure 10 highlights the application of the SOTA \u2019Set of Mark\u2019 (SoM\u2020\u2020\u2020https://github.com/microsoft/SoM\u00a0[50]) technique to enhance the capability of GPT-4(V) in acquiring deeper knowledge. SoM involves creating masks for each object in the image using SAM, each distinguished by a unique identifier. This marked image then serves as an input for GPT-4V, enabling it to respond to queries necessitating visual grounding with greater accuracy and relevance.\nIn Figure 13, we present additional visualization results for our approach. These results demonstrate that VATEX can successfully segment referred objects in a variety of scenarios, including complex expressions and scenes. To further illustrate our method\u2019s capabilities, we have also created a video demo that compares our approach to ReferFormer on Ref-Youtube-VOS. This video demo is provided as an attachment.\n",
    "9": "Visual question answering (VQA) is known as an AI-complete task as it requires understanding, reasoning, and inferring about the vision and the language content.\nOver the past few years, numerous neural architectures have been suggested for the VQA problem.\nHowever, achieving success in zero-shot VQA remains a challenge due to its requirement for advanced generalization and reasoning skills.\nThis study explores the impact of incorporating image captioning as an intermediary process within the VQA pipeline.\nSpecifically, we explore the efficacy of utilizing image captions instead of images and leveraging large language models (LLMs) to establish a zero-shot setting.\nSince image captioning is the most crucial step in this process, we compare the impact of state-of-the-art image captioning models on VQA performance across various question types in terms of structure and semantics.\nWe propose a straightforward and efficient question-driven image captioning approach within this pipeline to transfer contextual information into the question-answering (QA) model.\nThis method involves extracting keywords from the question, generating a caption for each image-question pair using the keywords, and incorporating the question-driven caption into the LLM prompt. We evaluate the efficacy of using general-purpose and question-driven image captions in the VQA pipeline.\nOur study highlights the potential of employing image captions and harnessing the capabilities of LLMs to achieve competitive performance on GQA under the zero-shot setting. Our code is available at https://github.com/ovguyo/captions-in-VQA.Visual Question Answering (VQA) is a complex multimodal task that demands a high-level understanding of several aspects, such as object and attribute identification, object localization, comprehension of the relationship between the image and the question, and reasoning about the context and the scene. The common steps of a typical VQA model involve generating embeddings of the image and the question using encoders for each, combining the image and question embeddings with a fusing module, and generating answers using a text generator or a classifier. For a general overview of the VQA techniques, the reader may refer to [34, 33].The inherent multimodal nature of the VQA problem is the primary factor contributing to its complexity.\nCombining different types of information, such as text and images, makes the model\u2019s training more complex, as the model must understand and utilize the connections and interactions between these different modalities.\nSeveral studies [18, 32, 28, 4, 15] propose an approach to tackle multimodality for the VQA problem.\nHowever, these methods indicate limitations in their capacity to adapt to new tasks, particularly in zero-shot settings.Recent advances in high-capacity large language models (LLMs) [5, 1, 36] have marked a dramatic milestone in the domain. LLMs are predominantly trained with millions (or billions) of parameters and utilized for processing textual data. LLMs show outstanding performance in a variety of natural language tasks.\nThe ongoing research challenge lies in extending the capabilities of LLMs to the intersection of different modalities, e.g., textual and visual data. Recently, GPT-4 [1] and Gemini [36] stand out as remarkable examples of multimodal LLMs, adept at successfully processing textual and visual modalities for various downstream tasks, including VQA. Several alternative approaches [19, 20, 22, 2, 7] have also been proposed in the realm of large-scale vision-language integration.\nThe challenge in multimodal training lies in the extensive computational and data costs required to align the representation spaces of vision and language.Some recent studies [11, 37, 42] delve into the potential of utilizing image captions with unimodal LLMs in the zero-shot VQA setting.\nOur study differs from these studies in the following aspects. Firstly, we focus on examining the representation capacity of image captions from various vision-language models on the VQA performance. Second, our study investigates whether image captions can be informative for specific types of questions by evaluating the results in structurally and semantically different questions. Within this scope, we also evaluate the influence of feeding LLMs with general-purpose and question-driven captions, and only the most relevant sentence in the caption during the QA stage.Numerous VQA datasets are available in the literature, including CLEVR [17], VQA [3], VQA 2.0 [9], OK-VQA [25], GQA [16]. Among these sets, although each serves various purposes effectively, GQA stands out for its emphasis on testing compositional and grounded reasoning abilities and its relatively diverse Q/A set. In this study, we conduct our experiments on the GQA dataset and focus on measuring performance on semantically and structurally different questions.We structure the VQA task into two fundamental components: image captioning and question-answering. The goal is to leverage the respective strengths of these tasks, aiming for a more thorough comprehension of both the visual content and the corresponding questions.\nWe carry out experiments with state-of-the-art vision-language models, including CogVLM [40], BLIP-2 [20], and FuseCap [31] to comprehend their scene representation capacity in the VQA pipeline.We outline our contributions as follows:We evaluate the image captioning performance of various vision-language models incorporating them with LLMs for zero-shot VQA, analyzing their effectiveness across various question types.We propose a straightforward question-driven captioning approach to better transfer the context into LLMs for question-answering.The rest of the paper is organized as follows. Section 2 reviews related works. Section 3 mentions the components of the proposed pipeline. In Section 4, we present the experiments designed for our study. Section 5 discusses evaluation results. Section 6 outlines the conclusions drawn from our findings and discusses potential avenues for future research.LLMs [5, 27, 38] trained on extensively rich web-scale corpus usually employ autoregressive methods to generate target tokens. LLMs demonstrate remarkable proficiency in processing and generating text with human-like characteristics. This attribute renders them suitable instruments for various language-related tasks, including question-answering, text generation, machine translation, etc.\nExpanding the scope of LLMs to include additional modalities results in the creation of multimodal LLMs [20, 1, 40, 36, 22], which boosts the performance for many downstream tasks including image captioning, visual question answering, text-to-image synthesis.The main challenge of the VQA domain comes from bridging the gap between visual understanding and natural language.\nNumerous studies have been proposed to tackle questions related to visual content.\nRelation Networks [32] involves employing a compact and straightforward neural network module that takes pairs of features as input and generates a score indicative of the relationship between these feature pairs.\nLXMERT [35] is a large-scale transformer model that fuses textual and visual representations with a cross-modality encoder.\nMDETR [18] is an end-to-end modulated detector which is an improved version of the object detection model DETR [6] by adding the capability of processing free-form texts.\nAlternatively, neuro-symbolic approaches in VQA have gained attention to enhance model interpretability. A neuro-symbolic approach in VQA combines two main parts: neural network modules for handling images and text modalities, and a symbolic reasoning module for managing logic and knowledge representation.\nNS-VQA [43] and NS-CL [24] use neural networks for scene parsing and dividing questions into program instructions, and propose a symbolic module executing the program instructions on the scene representation.\nAn alternative hybrid approach, ProTo [44], proposes program-guided transformers that use semantic and structural information of the programs being parsed from the questions by a sequence-to-sequence model.\nA recent approach, namely VisProg [12], generates program instructions from questions using LLMs and employs instructions on images benefiting from different modules for object detection, visual question answering, image classification, and segmentation. Recent large-scale multimodal approaches used for VQA are mentioned in Section 1 and Section 2.1.Image captioning aims to produce a caption describing visual content in natural language.\nConventional approaches in image captioning are based on attention and encoder-decoder structure [14, 41, 13].\nA typical image captioning model consists of an encoder for gathering visual cues and a textual decoder to produce the final caption.\nLike VQA, this requires bridging the gap between visual and natural language understanding.\nRecently, large-scale multimodal models [26, 19, 12, 40, 1, 36, 20] have resulted in notable enhancements in performance and demonstrated adaptability to various downstream applications, including image captioning.Question-answering (QA) models aim to provide contextually appropriate responses based on a document or text, often requiring an understanding of linguistic rules, syntax, and contextual nuances.\nRecent models in QA leverage transformer architectures and large-scale pre-training on diverse datasets [8, 23, 29, 5].The primary and most crucial element in the suggested pipeline is the creation of image captions with high visual representation capability. Image captions provide a summarized version of the visual content, and specific visual details may be lost, which could affect the VQA performance. We survey image captioning models, selecting ones that provide more detailed captions while taking into account our computational resource limitations. Consequently, we evaluate several zero-shot vision-language models, including CogVLM [40], FuseCap [31], and BLIP-2 OPT2.7\u2062b2.7\ud835\udc4f{}_{2.7b}start_FLOATSUBSCRIPT 2.7 italic_b end_FLOATSUBSCRIPT [20] by integrating them into the VQA pipeline. We employ both the chat and visual grounding variants of CogVLM, considering their potential performance impacts across different question types.\nVQA performance is assessed across various image captions according to structurally and semantically different question categories. More details about question categories are given in Section 4.1.Two approaches are utilized in this paper to generate captions. First, each image is captioned without considering the questions associated with it, which we refer to as \u201cgeneral-purpose captioning\u201d throughout the paper. However, general-purpose captions are designed to provide a broad description of the visual content, and they may lack the precision needed to address detailed and specific queries. Therefore, in our second approach, we create image captions for each image-question pair, a process we refer to as \u201cquestion-driven image captioning\u201d. For this purpose, KeyBERT [10] is employed to extract keywords from the questions. KeyBERT utilizes BERT-embeddings along with a basic cosine similarity measure to identify the most representative words that encapsulate the content of the entire text. Extracted keywords are fed into the image captioning model along with the corresponding image, as illustrated in Figure 1.We also investigate whether less relevant portions of an image caption could potentially introduce confusion or result in inaccurate answers for the QA model/LLM. Hence, in our analysis, we experiment with keeping only the most relevant sentence of the image caption and providing it to the LLM during the QA step. To achieve this, we utilize Sentence-BERT [30], specifically employing the MiniLM-L6 model111https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1, to extract the most relevant sentence from the image caption based on the given question.As in the pipeline shown in Figure 1, the QA model takes the image caption and the question as input, leveraging information from the image caption to generate an answer.\nDuring the QA step, we utilize GPT-3.5, recognized for its high zero-shot performance in QA benchmarks [39].\nDespite the superior performance of the more recent LLM, GPT-4, across various natural language tasks including QA, we choose not to use GPT-4 in our experiments to keep our pipeline cost-effective. In future works, the integration of higher-performing LLMs with the pipeline could be explored.We derive answers with an open-ended generation, specifically using GPT-3.5-turbo API provided by OpenAI. The answer size is restricted to a maximum of two words, aligning with the answer size distribution in the GQA dataset. Optimal prompts are given in the Section 4.5.We conduct experiments on the GQA [16] dataset, specifically the balanced version of the test-dev subset, comprising 12,5781257812,57812 , 578 questions. Each image in the dataset is linked to multiple questions, and the overall number of images included is 398398398398. This subset contains a diverse distribution of questions across various categories, with a primary focus on categorization based on structure and semantics. The structural type is determined by the final operation in the functional program of the question, encompassing categories such as verify, query, choose, logical, and compare. The semantic type specifies the primary focus of the question and includes categories like object, attribute, category, relation, and global.\nTable 1 presents an overview of question types, corresponding descriptions, and the respective number of questions in the GQA test-dev [16].To evaluate zero-shot VQA performance, we use the chat variation of CogVLM222https://huggingface.co/THUDM/cogvlm-chat-hf and BLIP-2 FlanT5X\u2062L\ud835\udc4b\ud835\udc3f{}_{XL}start_FLOATSUBSCRIPT italic_X italic_L end_FLOATSUBSCRIPT333https://huggingface.co/Salesforce/blip2-flan-t5-xl. CogVLM is an open-sourced pre-trained vision-language model with 10B visual and 7B language parameters. CogVLM outperforms many vision-language models, e.g., InstructBLIP [7] and LlaVA-1.5 [21], in VQA benchmarks. Due to our resource constraints with 16161616 GB VRAM, we apply 4444-bit quantization to CogVLM. BLIP-2 FlanT5X\u2062L\ud835\udc4b\ud835\udc3f{}_{XL}start_FLOATSUBSCRIPT italic_X italic_L end_FLOATSUBSCRIPT with 4.1B parameters also indicate high performance surpassing BLIP-2 OPT6.7\u2062B6.7\ud835\udc35{}_{6.7B}start_FLOATSUBSCRIPT 6.7 italic_B end_FLOATSUBSCRIPT and Flamingo [2] in VQA benchmarks. We employ BLIP-2 FlanT5X\u2062L\ud835\udc4b\ud835\udc3f{}_{XL}start_FLOATSUBSCRIPT italic_X italic_L end_FLOATSUBSCRIPT with F16 precision.We examine the VQA performance attributed to semantic and structural question types mentioned in Section 4.1. Image captions are obtained through the visual grounding444https://huggingface.co/THUDM/cogvlm-grounding-generalist-hf and chat555https://huggingface.co/THUDM/cogvlm-chat-hf variations of the CogVLM, FuseCap666https://github.com/RotsteinNoam/FuseCap, and BLIP-2 OPT2.7\u2062b2.7\ud835\udc4f{}_{2.7b}start_FLOATSUBSCRIPT 2.7 italic_b end_FLOATSUBSCRIPT777https://huggingface.co/Salesforce/blip2-opt-2.7b models. When determining the image captioning method, we pay attention to both its alignment with our resource capacity and its high performance in image captioning benchmarks. We employ 4-bit quantization to CogVLM and use F16 precision for BLIP-2 OPT2.7\u2062b2.7\ud835\udc4f{}_{2.7b}start_FLOATSUBSCRIPT 2.7 italic_b end_FLOATSUBSCRIPT.Before the evaluation, GPT-3.5 predictions undergo post-processing, which involves the removal of punctuation. During the evaluation process, we employ the accuracy metric, calculated as the ratio of correctly predicted answers to the total number of answers. Given that answers are derived through open-ended generation using LLMs and might include variations, we do not seek an exact match between the prediction and the ground truth. Instead, we evaluate semantic similarity using cosine similarity in a vector space with the threshold 0.700.700.700.70. If two strings are closely aligned in meaning, the prediction is accepted as correct; for example, accepting c\u2062o\u2062u\u2062c\u2062h\ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc50\u210ecouchitalic_c italic_o italic_u italic_c italic_h as correct for the label s\u2062o\u2062f\u2062a\ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc4esofaitalic_s italic_o italic_f italic_a. We determine the similarity threshold through manual observation of the results. At lower thresholds, we observe that predictions incorporating words related to each other, yet lacking identical meanings, are also considered correct. For instance, the similarity value between the words blue and brown is found to be 0.670.670.670.67. We additionally assess performance across higher cosine similarity thresholds, e.g. 0.80.80.80.8 and 0.90.90.90.9, and for exact matching (EM).A brief prompt, \u2018Describe the scene in this image\u2019 is supplied to the image captioning model to create general-purpose image captions. To create question-driven captions, \u2018Consider the keywords: [keywords]\u2019 is added to the prompt. In the QA stage, the LLM prompt involves \u2018Answer the question in a maximum of two words based on the text. Consider the type of question in your answer. For example, if it is a yes/no question, the answer should be yes or no. Text: [text], Question: [question]\u2019. We notice a positive impact on the results when we include an instruction in the prompt to consider the question type. In the decoding step for the answer generation, we set t\u2062e\u2062m\u2062p\u2062e\u2062r\u2062a\u2062t\u2062u\u2062r\u2062e\ud835\udc61\ud835\udc52\ud835\udc5a\ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc52temperatureitalic_t italic_e italic_m italic_p italic_e italic_r italic_a italic_t italic_u italic_r italic_e as 0.20.20.20.2, t\u2062o\u2062pp\ud835\udc61\ud835\udc5csubscript\ud835\udc5d\ud835\udc5dtop_{p}italic_t italic_o italic_p start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT as 1, and specify f\u2062r\u2062e\u2062q\u2062u\u2062e\u2062n\u2062c\u2062y\u2062_\u2062p\u2062e\u2062n\u2062a\u2062l\u2062t\u2062y\ud835\udc53\ud835\udc5f\ud835\udc52\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc66_\ud835\udc5d\ud835\udc52\ud835\udc5b\ud835\udc4e\ud835\udc59\ud835\udc61\ud835\udc66frequency\\_penaltyitalic_f italic_r italic_e italic_q italic_u italic_e italic_n italic_c italic_y _ italic_p italic_e italic_n italic_a italic_l italic_t italic_y and p\u2062r\u2062e\u2062s\u2062e\u2062n\u2062c\u2062e\u2062_\u2062p\u2062e\u2062n\u2062a\u2062l\u2062t\u2062y\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc52_\ud835\udc5d\ud835\udc52\ud835\udc5b\ud835\udc4e\ud835\udc59\ud835\udc61\ud835\udc66presence\\_penaltyitalic_p italic_r italic_e italic_s italic_e italic_n italic_c italic_e _ italic_p italic_e italic_n italic_a italic_l italic_t italic_y as 0.Table 2 summarizes our results and demonstrates that employing our suggested QD image captioning approach for VQA enhances performance across most question categories compared to general-purpose image captioning.\nAlso, Table 3 indicates that the QD image captioning approach utilizing the CogVLM-chat variant surpasses other image captioning methods in evaluations seeking both different cosine similarity thresholds and exact matching.Significant performance enhancements are evident in QD image captions, particularly in the verify category for yes/no questions, as well as attribute and category types primarily focused on identifying and describing a single object\u2019s properties. However, challenges arise in the object category often asking which of two objects exists in the frame. Particularly in this category of questions, despite the QD image captions containing relevant information, inaccuracies emerge due to the behavior of the QA model, as elaborated in Section 5.2.We also notice that the QD captioning emphasizing question keywords is linked to a performance decline in the global type questions. Global-type questions typically pertain to the overall content of an image. It suggests that the emphasis on question keywords in the caption negatively affects the model\u2019s ability to make inferences about the entire image. On the other hand, it is quite possible to give other answers to questions of this type that are meaningful and contextually correct but do not match the label. In most of the cases, we observe that GPT-3.5 predicts answers that could be correct but do not precisely match the expected label (see examples in Figure 3).In most question categories, the accuracy achieved by combining QD image captions with GPT-3.5 for VQA exceeds the performance of BLIP-2 FlanT5X\u2062L\ud835\udc4b\ud835\udc3f{}_{XL}start_FLOATSUBSCRIPT italic_X italic_L end_FLOATSUBSCRIPT in the zero-shot setting.\nHowever, all image captioning-based approaches indicate inferior performance compared to the CogVLM-chat model for VQA. We are intrigued to discover a notable disparity in performance when comparing the image captions extracted by the CogVLM-chat model and provided to LLM, in contrast to the VQA performance of the CogVLM-chat model, unlike the case with BLIP-2.Among the FuseCap, BLIP-2 OPT2.7\u2062b2.7\ud835\udc4f{}_{2.7b}start_FLOATSUBSCRIPT 2.7 italic_b end_FLOATSUBSCRIPT, CogVLM-chat, and CogVLM-visual grounding models, the most informative captions for VQA are obtained through the CogVLM-chat variant. The CogVLM-visual grounding variant indicates the highest performance only in object and logical question categories. This suggests that visual grounding models may provide an advantage in these question categories with their capacity to connect language queries to relevant visual elements and reason about object-related relationships.Limiting image captions to the most relevant sentence reduces the overall performance of the CogVLM-chat model, though the impact varies across question types, with verify, query, choose and relation types being more negatively affected. This suggests that sentences less directly related to the questions do not result in confusion or inaccuracies for LLM during the QA. Conversely, generating more comprehensive and context-rich image captions is necessary for optimal performance.Figure 2 and 3 feature examples of both correct and incorrect outcomes, where image captions are generated by the CogVLM-chat model using question-driven captioning and then fed to GPT-3.5 for answer prediction.When examining incorrect predictions based on question types, we discover some common issues.We notice that 27% of the incorrect predictions are related to yes/no questions. A closer look reveals that in 11% of the incorrectly answered yes/no questions, GPT-3.5 provides a response using a word other than yes or no.\nFor instance, when the provided caption is \u2018The image showcases a skateboarder in action, possibly performing a trick on a ramp. The skateboarder is wearing protective gear, including a helmet, knee pads, and elbow pads. The background features a clear blue sky, trees, and a building. The overall ambiance suggests an outdoor skateboarding event or practice session.\u2019, in response to the question \u2018Are there salt shakers or skateboards in the picture?\u2019 GPT-3.5\u2019s prediction is skateboards, while the ground-truth is yes.\nWe observe that most similar inaccuracies are associated with questions related to object and logical types, often connecting more than one object or attribute using conjunctions like and or or, as given in the example. We posit that this issue can be alleviated by crafting more effective prompts for GPT-3.5 or by employing a more powerful LLM for QA.We also assess the instances where the LLM fails to provide an answer based on the information present in the image caption. Specifically, we examine the occurrences of not mentioned and not visible responses from GPT-3.5. Our findings indicate that, for the best general-purpose image captioning model, GPT-3.5 is not able to respond to 1.7% of the questions. Notably, when employing question-driven captioning, this rate decreases to 0.5%.This study aims to develop a zero-shot VQA pipeline, leveraging LLMs with the inclusion of image captioning as an intermediate step, and evaluate its performance on the GQA benchmark. The proposed approach involves question-driven image captioning to transfer contextual information to the QA model. The study includes a thorough evaluation of zero-shot models for image captioning in the VQA context, comparing the impact of general-purpose and question-driven image captions in terms of various types of questions.\nOur comparative analysis suggests that incorporating question-driven image captions into the VQA process has a more favorable effect on overall performance, surpassing the VQA performance of BLIP-2. Future endeavors may explore the integration of larger-scale LLMs, e.g., GPT-4, to further enhance performance. Additionally, evaluating the pipeline in a few-shot setting could offer a more comprehensive comparison. To enhance transparency, replacing the QA model with an interpretable alternative, such as graph-based QA models, can be explored.This work is partially supported by Middle East Technical University Scientific Research Projects Coordination Unit (METU-BAP), under the project number ADEP-704-2024-11482.",
    "10": "\u00a9 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.In the realm of fashion object detection and segmentation for online shopping images, existing state-of-the-art fashion parsing models encounter limitations, particularly when exposed to non-model-worn apparel and close-up shots. To address these failures, we introduce FashionFail; a new fashion dataset with e-commerce images for object detection and segmentation. The dataset is efficiently curated using our novel annotation tool that leverages recent foundation models.\nThe primary objective of FashionFail is to serve as a test bed for evaluating the robustness of models.\nOur analysis reveals the shortcomings of leading models, such as Attribute-Mask R-CNN and Fashionformer.\nAdditionally, we propose a baseline approach using naive data augmentation to mitigate common failure cases and improve model robustness.\nThrough this work, we aim to inspire and support further research in fashion item detection and segmentation for industrial applications.\nThe dataset, annotation tool, code, and models are available at https://rizavelioglu.github.io/fashionfail/.Computer vision has impacted many fields across different industries, such as autonomous driving [1, 2], health care [3, 4] or retail [5, 6]. As fashion is undoubtedly one of the largest industries in the world, this field has also become a popular research field enabling many applications, including recommendation systems [7, 8, 9] or outfit synthesis [10, 11, 12].All methods used in fashion applications share a common reliance on successful fashion item detection. What poses a particular challenge in recognizing fashion items, as evidenced in Fashionpedia images\u00a0[13], is the vast diversity in garment appearances and the variability in poses exhibited by individuals wearing them. Moreover, the presence of cluttered backgrounds in certain images further compounds the difficulty of the detection task. Nevertheless, recent progress has yielded remarkable results for state-of-the-art models in accurately segmenting (and therefore detecting) fashion items in complex scenes\u00a0[13, 14]. These models have demonstrated the ability to reliably segment individual garment instances\u00a0(e.g.\u00a0jackets) along with their respective garment parts (e.g.\u00a0collar, sleeve, pocket, buttons, \u2026).On the contrary, e-commerce fashion images typically display one fashion item on a solid background. As the images of the items are clean, centered, and of high quality, one would expect that the same state-of-the-art models trained on Fashionpedia would perform similarly well on these supposedly simpler e-commerce images. However, in this work we demonstrate that state-of-the-art models face considerable difficulties in processing these clear images, leading to consistent failures. Particularly undesirable among these failure cases are false predictions with overly high confidence or no predictions at all, as depicted in Fig.\u00a01.\nWhile it is assumed that models perform \u201creasonably well if the apparel item is worn by a model\u201d [13], pointing out the lack of context as the sole reason for these failures, our findings indicate that the issue is not solely due to the absence of context but also the scale of the apparel items, cf.\u00a0Fig.\u00a02.\nMoreover, Fig.\u00a02b demonstrates that scale alone may not be sufficient for detection in all cases and that both context and scale are required in some instances. This observation challenges the assumption that such models are well-suited for e-commerce applications and highlights the need for specialized models tailored to unique characteristics of e-commerce fashion images.Since a robust detection of fashion items is the basis for various fashion applications, we argue that overcoming the mentioned shortcomings would unlock new possibilities for the development of applications within the fashion domain. In particular, addressing these detection failures is essential to advance the development of downstream fashion applications using computer vision.\nIn addition, given the substantial availability of fashion data, efficient labeling methods within this domain would make an ideal test environment for the development of methods in robust object detection and segmentation that generalize across other domains.In this work, we introduce the new dataset FashionFail along with an efficient data annotation pipeline. The dataset is designed to address the limitations of existing state-of-the-art fashion parsing models. FashionFail consists of a diverse set of online shopping images with categories that are compatible with the established Fashionpedia [13] ontology. We conduct a thorough evaluation of the detection task on both Fashionpedia and FashionFail with multiple state-of-the-art models.\nBy providing insights into specific failure modes, our goal is to transparently showcase the limitations of models and emphasize the necessity for further advancements in this research direction.The paper is structured as follows: in Section\u00a0II, we provide an overview of the fashion research landscape, outlining key areas, real-world applications, notable datasets, and models. In Section\u00a0III, we detail the dataset creation, which covers data scraping, pre-processing, automated annotation and filtering, and quality checks. Section\u00a0IV provides a concise dataset analysis and a comparison with an established existing dataset. Lastly, Section\u00a0V delves into our methodology, evaluation protocol, and an in-depth discussion of the results.The existing body of research in the fashion domain\nencompasses synthesis, recommendation, analysis, and detection, cf.\u00a0[17].\nIn the synthesis domain, research has explored topics such as style transfer and pose simulation to create novel fashion compositions\u00a0[18, 19, 20, 21]. Fashion recommendation systems\u00a0[22] have explored outfit compatibility and matching algorithms\u00a0[23, 24, 25, 26]. The analysis domain focused on attribute recognition and style learning\u00a0[27, 28]. Moreover, detection-based research includes landmark detection, item retrieval, and the task of fashion parsing (combining object detection and segmentation in fashion)\u00a0[29, 30, 31].For all the mentioned tasks the detection mechanisms form the basis enabling the downstream fashion applications\u00a0[32].\nIn our work, we aim to enhance the effectiveness and reliability of fashion-related tasks by explicitly focusing on fashion parsing, which is an essential aspect of fashion detection.\nApplications:\nNumerous companies use fashion detection in various real-world applications. Visual Search is a prominent application, evident in platforms such as Amazon\u2019s Shop The Look\u00a0[33], Meta\u2019s GrokNet\u00a0[34],\nMicrosoft\u2019s Bing\u00a0[35], Alibaba\u00a0[36], Ebay\u00a0[37], Zalando\u00a0[38], Pinterest\u2019s Complete The Look\u00a0[39] and Shop The Look\u00a0[40].\nFashion item retrieval is facilitated by models in Zalando\u00a0[41] and NAVER\u00a0[42].\nOutfit recommendation systems are implemented by major players such as Amazon\u00a0[23] and Alibaba\u00a0[43], providing users with personalized and stylish outfit suggestions.\nSynthesizing fashion is also an active research field [19, 18, 20], enabling users to virtually try on clothes before making a purchase.\nMethods:\nVarious methodologies address complex challenges in fashion parsing. Match R-CNN\u00a0[44] extends Mask R-CNN to DeepFashion2 [44], which includes garment detection, landmark estimation, instance segmentation, and consumer-to-shop retrieval.\nSimilarly, Attribute-Mask R-CNN\u00a0[13] extends Mask R-CNN by an additional prediction head\nand introduces a lightweight architecture for effective fashion object detection and segmentation, particularly in attribute prediction. This design mitigates computational demands, presenting a notable alternative in the landscape of fashion parsing methodologies.\nRecently, Fashionformer\u00a0[14] has been introduced and adopts a distinctive strategy that leverages a single Transformer-based model [45].\nEmploying an encoder-decoder framework similar to DETR [46], Fashionformer is jointly trained for instance segmentation and attribute recognition. Departing from multi-headed models, Fashionformer achieves state-of-the-art performance on Fashionpedia [13], establishing itself as the current state-of-the-art in the field.Datasets:\nModaNet\u00a0[47], DeepFashion2\u00a0[44], and Fashionpedia\u00a0[13] stand out as widely recognized datasets in fashion parsing. However, these datasets primarily consist of \u201cin the wild\u201d images \u2013 street photos of people wearing clothes. While beneficial for various tasks, such datasets may not perfectly align with the requirements of domains such as e-commerce, which often demand images with clean backgrounds and close-up shots.\nPolyvore Outfits\u00a0[48] or Pinterest CTL\u00a0[39] offer extensive datasets of collage-like images presenting outfits that encompass multiple objects. However, this unique format complicates the\nanalysis of single objects due to the low pixel resolution per object. Additionally, these datasets lack pixel-level object masks, a crucial component for certain applications that require detailed segmentation information.While existing datasets have been instrumental in advancing fashion research, their limitations are evident.\nA significant gap persists, with no datasets specifically tailored for the e-commerce setting, which is characterized by larger, single objects with clean backgrounds and no contextual elements.\nIn response to these challenges, we present FashionFail, a precisely curated dataset designed to complement the existing dataset landscape.\nFashion datasets often use images from free-license photo websites, which typically exhibit low resolution. For instance, Fashionpedia [13] training images have an average resolution of 755755755755 (width) \u00d7 986986986986 (height) pixels.\nConsequently, we opted to use images from adidas AG due to their superior image quality and alignment with Fashionpedia categories.\nIn the following, we detail the process of data collection and annotation, alongside basic information about the resulting dataset.\nA custom web crawler built with Scrapy\u00a0(https://scrapy.org)\nautomatically collected over 10,0001000010,00010 , 000 product entries, including images and descriptions, from Adidas website\u00a0(https://adidas.de)\n. To accelerate the results, we randomly sampled 2,50025002,5002 , 500 products.\nThe human annotator then manually filtered images based on three strict criteria: (1) presence of multiple objects or instances of the same object, (2) visibility of any part of the human body, and (3) extreme close-ups hindering category determination from the image, as shown in\u00a0Fig.\u00a03.\nImages meeting any criterion were excluded to ensure pure, clean, and informative e-commerce product images devoid of contextual information, in contrast to Fashionpedia. The filtering process was efficient, taking less than a second per image using our simple tool, resulting in a total of 5,79557955,7955 , 795 images.Expert annotation is often a laborious and time-consuming task, particularly for pixel-wise label annotations, which are fine-grained and expensive\u00a0[52]. Hence, we developed a novel method for automatically annotating our collected data. \u00a0Fig.\u00a04 illustrates the complete pipeline.\nWe employed GPT-3.5\u00a0[49] (text-davinci-003) to annotate the Fashionpedia category by prompting it with the product description obtained by scraping, as described in Section\u00a0III-A.\nTo annotate the coordinates of the bounding box, we utilized Grounding DINO\u00a0[50], prompting it with the text \u201can object\u201d alongside the product image. Note that we chose a generic text prompt instead of using the category derived in the preceding step as otherwise Grounding DINO would encounter the same difficulties as state-of-the-art fashion parsing models in detecting single fashion items within our collected e-commerce data.\nFinally, we employed Segment Anything\u00a0(SAM)\u00a0[51] to generate segmentation masks, prompting it with the previously collected bounding box coordinates and the product image.\nSubsequently, any encountered anomaly, such as a sample without a label annotation or multiple box annotations, was automatically filtered, which resulted in a total of 2,68226822,6822 , 682 images.\nThis comprehensive approach not only automates the annotation process but also addresses specific challenges associated with different annotation types, resulting in a highly efficient and accurate annotation pipeline.Following the automated stages, a manual quality review is conducted where human annotators use our straightforward interface to flag any invalid samples, as illustrated in\u00a0Fig.\u00a05. These tagged samples are excluded in case of incorrect class labels, or inaccurate box or mask annotations. Human annotators only need to review the automatically annotated samples. This combination of automation and human oversight optimizes the efficiency and accuracy of the annotation process.FashionFail is aligned with practical applications in the fashion domain, particularly in clothing recommendation systems and e-commerce, where primary items play a central role. Therefore, the dataset focuses on primary clothing, such as jackets, pants, and shoes. In alignment with the Fashionpedia ontology, categories falling under \u2018garment parts\u2019, \u2018closures\u2019, and \u2018decorations\u2019 super-categories are excluded. This eliminates categories such as sleeve, pocket, applique, and others. Additionally, due to an insufficient number of images, categories such as sweater, cape, tie, belt, and leg warmer were removed. The refined dataset comprises 22 categories, outlined in\u00a0Fig.\u00a06.Through our automated annotation pipeline and careful review process, each image within FashionFail is precisely annotated with a single label, bounding box, and segmentation mask. This process has resulted in a dataset comprising 2,49524952,4952 , 495 pairs of images and annotations.The dataset is divided into training, validation, and test sets, consisting of 1,34413441,3441 , 344, 150150150150, and 1,00110011,0011 , 001 images, respectively. The split preserves class frequencies in each subset, and maintains a similar distribution to Fashionpedia. Fig.\u00a06a details the class distribution in different splits. For example, the most common class in both datasets is \u2018shoe\u2019, constituting 30%percent3030\\%30 % in Fashionpedia and 33%percent3333\\%33 % in FashionFail, respectively.All FashionFail images maintain a consistent resolution of 2400240024002400 \u00d7 2400240024002400 pixels, representing a considerable improvement compared to Fashionpedia\u2019s resolution of 755755755755 \u00d7 986986986986.\nEach FashionFail image exclusively features a singular item, centered within the frame. Despite its likewise object-centered distribution, Fashionpedia consists of smaller objects in terms of relative size, as depicted in\u00a0Fig.\u00a06b. With the heightened resolution and larger relative object size, FashionFail offers significantly finer pixel-level masks for its objects.\nIn this section, we outline the experiments conducted, covering model architectures, training schedules, data augmentations, evaluation, and other relevant details, followed by a comprehensive discussion of the results.In our experimentation, we employed two top-performing models on the Fashionpedia dataset, alongside our Mask R-CNN based models. To facilitate a fair and unbiased comparison between models, we included variations of the two top-performing models that share the same backbone network as our Mask R-CNN.Attribute-Mask R-CNN\u00a0(A-MRCNN)\u00a0[13]\nis trained for instance segmentation with attribute localization on Fashionpedia.\nTheir best-performing model\nincorporates SpineNet-143\u00a0[54] as backbone and adheres to a training schedule of 6\u00d76\\times6 \u00d7 (189 epochs of training) with a linear scaling of the learning rate and a batch size of 256. Notably, during training, the model benefits from large-scale jittering in addition to standard random horizontal flipping and cropping. For a comprehensive comparison, we also present results for A-MRCNN using the ResNet-50\u00a0[55] backbone with a feature pyramid network\u00a0(R50-FPN)\u00a0[56].\nWe use the publicly available official implementation111https://bit.ly/GitHub-Fashionpedia.Fashionformer\u00a0[14]\nis the current state-of-the-art model on the Fashionpedia dataset. Unlike previous work that treats each task separately as a multi-head prediction problem, Fashionformer uses a single unified vision transformer [45] for multiple tasks. Their best-performing model uses Swin-Transformer\u00a0[57] as its backbone. They adopted 3\u00d73\\times3 \u00d7 schedule (95 epochs of training) with a batch size of 16 while using large-scale jittering. For a comprehensive comparison, we also present results for Fashionformer using R50-FPN as backbone.\nWe use the publicly available official implementation222https://github.com/xushilin1/FashionFormer.Facere333Facere\nis Latin for \u2018to make\u2019, from which the word fashion is derived.\nis based on Mask R-CNN\u00a0[58] with a R50-FPN backbone, which has been initially pretrained on the COCO dataset\u00a0[59]. We finetuned Facere using a modified 75-25 split of Fashionpedia-train, where 75% used for training and 25% for validation, for 125 epochs\u00a0(\u22483\u00d7\\approx 3\\times\u2248 3 \u00d7 schedule) with a batch size of 8 and Adam\u00a0[60] as optimizer with default parameters from PyTorch.\nFor faster training and reduced memory usage, we used half-precision (16-bit) training provided by PyTorch Lightning444https://lightning.ai/docs/pytorch/, converting it back to full precision (32-bit) for enhanced performance.\nOur training incorporated standard techniques such as horizontal flipping, photometric distortion\u00a0[61], and large-scale jittering\u00a0[62]. To deal with the varying scale of items while minimizing context information from the background, we additionally introduce a simple yet effective data augmentation approach: we crop the image based on the smallest box containing all bounding boxes (inspired by [63]) and apply large-scale jittering to these custom image crops. All data augmentation techniques are applied with a probability of 50%. Examples of the augmentations are illustrated in Fig.\u00a07.Facere+ follows the same setup as Facere but finetuned on FashionFail-train for 150 epochs.\nThe motivation behind Facere+ is to showcase the learnability of FashionFail\u2013 a form of data quality check\nfor the usability and cleanliness of FashionFail.\nA prediction is called a true positive (TP) if it matches the ground truth class and exceeds a specified intersection over union (IoU) [64] threshold; otherwise, it is called a false positive (FP). Similarly, a ground truth object is identified as a false negative (FN) if there are no predictions for that object that surpass the specified IoU detection threshold.\nFurthermore, object detectors assign a (normalized) score to each predicted bounding box\u00a0(referred to as confidence score), indicating the likelihood of the box containing an object of a specific class. Typically, boxes with low confidence scores are rejected. In our evaluation, we maintained the default of 0.050.050.050.05 as the minimum confidence threshold.\nThe quantities TP, FP, and FN are used to determine measures such as precision=TPTP+FPprecisionTPTPFP\\mathrm{precision}=\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}}roman_precision = divide start_ARG roman_TP end_ARG start_ARG roman_TP + roman_FP end_ARG and recall=TPTP+FNrecallTPTPFN\\mathrm{recall}=\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}roman_recall = divide start_ARG roman_TP end_ARG start_ARG roman_TP + roman_FN end_ARG. Note that they also depend on the specified IoU threshold \u03d5italic-\u03d5\\phiitalic_\u03d5 and confidence threshold \u03c4\ud835\udf0f\\tauitalic_\u03c4. In the following, we use the notation precision\u03d5\u2062(\u03c4)subscriptprecisionitalic-\u03d5\ud835\udf0f\\mathrm{precision}_{\\phi}(\\tau)roman_precision start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( italic_\u03c4 ) and recall\u03d5\u2062(\u03c4)subscriptrecallitalic-\u03d5\ud835\udf0f\\mathrm{recall}_{\\phi}(\\tau)roman_recall start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( italic_\u03c4 ) to explicitly denote the dependency of the measures on the thresholds.The interpolated average precision\u00a0(AP)\u00a0[65] is a commonly used metric to assess object detection performance, which is defined as the mean precision at a set of equally spaced recall levels, e.g.\u00a0for 101 points \u211b={0.00,0.01,0.02,\u2026,1.00}\u211b0.000.010.02\u20261.00\\mathcal{R}=\\{0.00,0.01,0.02,\\ldots,1.00\\}caligraphic_R = { 0.00 , 0.01 , 0.02 , \u2026 , 1.00 }:where \ud835\udcaf\u03d5:={t\u2208[0,1]:recall\u03d5\u2062(t)\u2208\u211b}assignsubscript\ud835\udcafitalic-\u03d5conditional-set\ud835\udc6101subscriptrecallitalic-\u03d5\ud835\udc61\u211b\\mathcal{T}_{\\phi}:=\\{t\\in[0,1]:\\mathrm{recall}_{\\phi}(t)\\in\\mathcal{R}\\}caligraphic_T start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT := { italic_t \u2208 [ 0 , 1 ] : roman_recall start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( italic_t ) \u2208 caligraphic_R }.555Note that AP metrics, as employed by Pascal VOC\u00a0[66] and MS COCO\u00a0[59], feature slightly different calculations.\n\nHowever, this metric might not be ideal for small datasets, particularly those that are unbalanced, akin to FashionFail.\nConsider the \u201cumbrella\u201d class in our FashionFail-test, with only one sample among 1,00110011,0011 , 001 examples. If a model accurately predicts this single sample, it could significantly inflate the mean of average precision (mAP) across classes.\nTherefore, in our evaluation, we use a weighted version of mAP [67]:\nwhere the weight wcsubscript\ud835\udc64\ud835\udc50w_{c}italic_w start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT is the relative frequency of class c\u2208{1,\u2026,C}\ud835\udc501\u2026\ud835\udc36c\\in\\{1,\\ldots,C\\}italic_c \u2208 { 1 , \u2026 , italic_C } in the test set\nand APc@\u2062IoU=\u03d5subscriptsuperscriptAP@IoUitalic-\u03d5\ud835\udc50\\mathrm{AP}^{@\\mathrm{IoU}=\\phi}_{c}roman_AP start_POSTSUPERSCRIPT @ roman_IoU = italic_\u03d5 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT is the interpolated AP according to (1) for class c\ud835\udc50citalic_c.\nTo have a threshold-free evaluation metric, the weighted mAP from (2) is additionally averaged over a range of IoU thresholds \u2110={0.50,0.55,\u2026\u20620.95}\u21100.500.55\u20260.95\\mathcal{I}=\\{0.50,0.55,\\ldots 0.95\\}caligraphic_I = { 0.50 , 0.55 , \u2026 0.95 }:which also serves as our primary evaluation metric.Analogously, we also report the recall for a fixed number of maximum detections k\u2208\u2115\ud835\udc58\u2115k\\in\\mathbb{N}italic_k \u2208 blackboard_N. Given the set of all confidences \ud835\udcaensubscript\ud835\udcae\ud835\udc5b\\mathcal{S}_{n}caligraphic_S start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT of the predictions for test example n\u2208{1,\u2026,N}\ud835\udc5b1\u2026\ud835\udc41n\\in\\{1,\\ldots,N\\}italic_n \u2208 { 1 , \u2026 , italic_N }, where \u03c41,\u2026,\u03c4|\ud835\udcaen|\u2208\ud835\udcaensubscript\ud835\udf0f1\u2026subscript\ud835\udf0fsubscript\ud835\udcae\ud835\udc5bsubscript\ud835\udcae\ud835\udc5b\\tau_{1},\\ldots,\\tau_{|\\mathcal{S}_{n}|}\\in\\mathcal{S}_{n}italic_\u03c4 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_\u03c4 start_POSTSUBSCRIPT | caligraphic_S start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT | end_POSTSUBSCRIPT \u2208 caligraphic_S start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT are assumed to be in decreasing order, the average recall for k\ud835\udc58kitalic_k detections is defined aswith \ud835\udcaenk={\u03c4j\u2208\ud835\udcaen:j=1,\u2026,min\u2061{k,|\ud835\udcaen|}}superscriptsubscript\ud835\udcae\ud835\udc5b\ud835\udc58conditional-setsubscript\ud835\udf0f\ud835\udc57subscript\ud835\udcae\ud835\udc5b\ud835\udc571\u2026\ud835\udc58subscript\ud835\udcae\ud835\udc5b\\mathcal{S}_{n}^{k}=\\{\\tau_{j}\\in\\mathcal{S}_{n}:j=1,\\ldots,\\min\\{k,|\\mathcal{%\nS}_{n}|\\}\\}caligraphic_S start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT = { italic_\u03c4 start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u2208 caligraphic_S start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT : italic_j = 1 , \u2026 , roman_min { italic_k , | caligraphic_S start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT | } } if |\ud835\udcaen|>0subscript\ud835\udcae\ud835\udc5b0|\\mathcal{S}_{n}|>0| caligraphic_S start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT | > 0, else \ud835\udcaenk=\ud835\udcaenk=\u2205superscriptsubscript\ud835\udcae\ud835\udc5b\ud835\udc58superscriptsubscript\ud835\udcae\ud835\udc5b\ud835\udc58\\mathcal{S}_{n}^{k}=\\mathcal{S}_{n}^{k}=\\emptysetcaligraphic_S start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT = caligraphic_S start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT = \u2205, the set of the top k\ud835\udc58kitalic_k confidences in test example n\ud835\udc5bnitalic_n and recall\u03d5\u2062(\u22c5;n)subscriptrecallitalic-\u03d5bold-\u22c5\ud835\udc5b\\mathrm{recall}_{\\phi}(\\boldsymbol{\\cdot};n)roman_recall start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( bold_\u22c5 ; italic_n ) the recall score only for test example n\ud835\udc5bnitalic_n. Thus, the weighted mean average recall [68] for the top k\ud835\udc58kitalic_k detections is computed aswhere the class weights wcsubscript\ud835\udc64\ud835\udc50w_{c}italic_w start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT for classes c\u2208{1,\u2026,C}\ud835\udc501\u2026\ud835\udc36c\\in\\{1,\\ldots,C\\}italic_c \u2208 { 1 , \u2026 , italic_C } are the same as in Equation\u00a02 and ARctop\u2062ksubscriptsuperscriptARtop\ud835\udc58\ud835\udc50\\mathrm{AR}^{\\mathrm{top}\\,k}_{c}roman_AR start_POSTSUPERSCRIPT roman_top italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT is the average recall as in Equation\u00a04 but restricted on class c\ud835\udc50citalic_c.\nIn\u00a0Table\u00a0I, we present the model performance on the FashionFail-test dataset, evaluated using several metrics covering both detection and segmentation. Facere significantly outperforms state-of-the-art models, even those with larger backbones, across all evaluation metrics. It exhibits improvement in box-mAPwsubscriptmAPw\\text{mAP}_{\\text{w}}mAP start_POSTSUBSCRIPT w end_POSTSUBSCRIPT over Fashionformer by +1.81.8+1.8+ 1.8 and mask-mAPwsubscriptmAPw\\text{mAP}_{\\text{w}}mAP start_POSTSUBSCRIPT w end_POSTSUBSCRIPT over A-MRCNN by +3.13.1+3.1+ 3.1. Furthermore, as the box-IoU increases, indicating a stricter model, the performance gap between Facere and the second best-performing model widens from +2.42.4+2.4+ 2.4 to +3.63.6+3.6+ 3.6. Remarkably, Facere outperforms all models on both box- and mask-ARtop1superscriptARtop1\\text{AR}^{\\text{top1}}AR start_POSTSUPERSCRIPT top1 end_POSTSUPERSCRIPT, surpassing the closest competitor by +11.711.7+11.7+ 11.7 and +13.413.4+13.4+ 13.4, respectively.\nThe performance gap in ARtop100superscriptARtop100\\text{AR}^{\\text{top100}}AR start_POSTSUPERSCRIPT top100 end_POSTSUPERSCRIPT diminishes due to the enhanced performance of Swin-base Fashionformer. This improvement stems from Fashionformer generating 100 predictions for each image without employing filtering techniques such as non-maximum suppression. However, the R50-FPN counterpart of Fashionformer does not experience as much benefit. Nevertheless, Facere maintains its superiority over all models.\nLastly, Facere+, which used FashionFail-train during training, shows exceptional performance across all metrics, indicating that the constructed dataset is adequate.In\u00a0Table\u00a0II, we present the model performance on the Fashionpedia-val dataset, evaluated using the same metrics. As expected, Swin-base Fashionformer outperforms other models. In addition, Facere achieves comparable results to its R50-FPN counterparts, with insignificant performance differences of \u22120.50.5-0.5- 0.5 for box-mAPwsubscriptmAPw\\text{mAP}_{\\text{w}}mAP start_POSTSUBSCRIPT w end_POSTSUBSCRIPT and \u22121.31.3-1.3- 1.3 for box-mARwsubscriptmARw\\text{mAR}_{\\text{w}}mAR start_POSTSUBSCRIPT w end_POSTSUBSCRIPT.\nOur objective is not to improve the performance of our models on Fashionpedia-val but to improve\nthe robustness to other datasets or domains, such as FashionFail-test, while maintaining comparable performance to the baselines on Fashionpedia-val.\nWith negligible performance differences between Facere and its R50-FPN counterparts, we observe that Facere effectively aligns with our goals by maintaining on-par performance on its training data while enhancing\nrobustness against scale and context.Moreover, we evaluate the model performance at the class level.\nTo get an intuition about the failure cases, we first analyzed whether there is a correlation between the size of objects in Fashionpedia and the model performance of A-MRCNN and FashionFormer on FashionFail.\nTo this end, we calculated the absolute difference in relative mask sizes of objects between Fashionpedia-val and FashionFail-test, and plotted it against the absolute difference in average precision (AP), see Fig.\u00a08.\nInterestingly, the plots reveal a clear positive correlation between the absolute difference in relative mask size and the absolute difference in AP.In Fig.\u00a09a and Fig.\u00a09b we report the model performances on Fashionpedia and FashionFail per class.\nThe models exhibit similar performance distribution across classes on Fashionpedia-val. In particular, Fashionformer performs slightly better on classes such as dress, and skirt, while Facere shows a slight advantage on the shoe class, which is the most dominant class.\nOn the other hand, all models perform poorly for most classes on our FashionFail-test data. However, for classes such as coat, jacket, pants, and dress the models demonstrate promising results. Notably, Facere performs consistently well in those classes whereas the other models generally show inconsistent performances across classes.\nMoreover, Facere accurately predicts classes such as shoe, glasses, and bag, wallet, where the others entirely fail.These results, combined with the scale analysis from Fig.\u00a08, explain why the models do not suffer from a severe drop in performance on certain classes such as coat, pants, jackets, as their mask sizes are similar in both datasets. On the contrary, for classes such as hat, shoe the relative object sizes differ significantly between Fashionpedia and FashionFail, leading to a dramatic performance drop on FashionFail-test.We provide a qualitative comparison of model predictions of A-MRCNN, FashionFormer, and Facere in Fig.\u00a09c.In this work, we introduce FashionFail, a dataset for precise fashion object detection and segmentation, serving as a robustness benchmark for fashion parsing models trained on \u201cin-the-wild\u201d images such as in Fashionpedia. Our dataset was efficiently created with a novel web crawling and annotation pipeline. Moreover, we address the limitations that state-of-the-art models face in generalizing to the domain of e-commerce images due to scale and context issues. Emphasizing the impact of carefully designed domain-specific data augmentations, we demonstrate their effectiveness in improving model generalizability while preserving original domain performance with our Facere model.\nThe data augmentations used in Facere provide a simple yet effective approach to improving the robustness of fashion parsing for use in real-world applications. In particular, our results show that even with constraints such as less training data, reduced model complexity, and shorter training times, our approach has the potential to outperform existing and well-established models.\nHowever, it is important to acknowledge that the presented results still require further refinement.Moving forward, possible improvements lie in enhancing the efficiency of the LLM within the annotation pipeline, considering the latest state-of-the-art LLMs, and exploring vision-language models.\nIn terms of methodology, testing scale-invariant approaches [69, 70] could alleviate the observed scale variation problems.\nFurthermore, there is potential to explore the reliability of these models in more detail with further evaluation schemes to test for robust performance.",
    "11": "Effective ontology transfer has been a major goal of recent work on event argument extraction (EAE). Two methods in particular\u2014question answering (QA) and template infilling (TI)\u2014have emerged as promising approaches to this problem. However, detailed explorations of these techniques\u2019 ability to actually enable this transfer are lacking. In this work, we provide such a study, exploring zero-shot transfer using both techniques on six major EAE datasets at both the sentence and document levels. Further, we challenge the growing reliance on LLMs for zero-shot extraction, showing that vastly smaller models trained on an appropriate source ontology can yield zero-shot performance superior to that of GPT-3.5 or GPT-4.111https://github.com/wgantt/eae-transferTraditional approaches to event argument extraction (EAE) involve training a model to identify and classify arguments against a fixed ontology, rendering zero-shot transfer to new ontologies impossible. However, recent works have proposed reformulations of the EAE task that in principle enable such transfer. Among these approaches, question answering (QA; Du and Cardie, 2020; Liu et\u00a0al., 2020; Li et\u00a0al., 2020, i.a.) and template infilling (TI; Chen et\u00a0al., 2020; Li et\u00a0al., 2021; Hsu et\u00a0al., 2022, i.a.) have emerged as especially promising. In the former approach, role labels are recast as participant-focused questions, and in the latter, the full role set for a given event type is expressed in a templatic prompt to be filled with extracted arguments (Figure\u00a01). Handling new roles thus becomes a matter of writing new questions or templates.Investigations of the effectiveness of zero-shot transfer using these methods have generally been limited either to only one or two ontology pairs (Zhang et\u00a0al., 2022), or to splits of event types within the same ontology (Li et\u00a0al., 2021; Du and Cardie, 2020). This work broadens these investigations with the following contributions:We study transfer with both TI and QA at a larger scale, benchmarking and analyzing transfer performance between six sentence- and document-level EAE datasets.We provide expert-written questions and templates for all roles in all six ontologies, which may be leveraged for transfer learning.We show that small QA and TI models based on Flan-T5 (Chung et\u00a0al., 2022) can yield zero-shot performance superior to that of GPT-3.5 and GPT-4, given a suitable source ontology.Many works have sought to recast EAE as a question answering (QA) task, in which questions correspond to specific roles (Figure\u00a01). Such questions range widely in form\u2014from almost brute verbalizations of role names (e.g.\u00a0What is the Org?; Du and Cardie, 2020), to semantically bleached queries with rule-generated syntax (What was someone expected to do?; He et\u00a0al., 2015), to detailed, expert-written questions (What did this person/network do that was corrupt?; Holzenberger et\u00a0al., 2022).We imagine a typical use case for a QA-based EAE system as one in which a human user is able to write queries about roles of interest in fairly plain language. Accordingly, we (the authors) write simple questions for all roles in the six ontologies we consider, avoiding use of the actual role names in the question text. Our questions draw stylistic inspiration from QA-SRL (He et\u00a0al., 2015), with the key difference that they are type-level, and not relativized to a predicate in context.222Having to write a new question for each predicate does not align with our envisioned use case.Other works have treated EAE as an infilling task, in which slots in fixed templates for each event type are populated with extracted arguments, either via masked language modeling (Chen et\u00a0al., 2020) or via autoregressive generation of the filled template (Ma et\u00a0al., 2022; Li et\u00a0al., 2021; Huang et\u00a0al., 2021; Du et\u00a0al., 2021). There are two notable advantages of this approach over QA: (1) for architectures with an encoder, only a single encoder forward pass is needed to extract all arguments, and (2) roles are considered jointly.A template typically consists of a short, semi-natural description of its event type, with slots occupying the syntactic position that arguments for the corresponding role are expected to fill. We manually construct templates for all ontologies considered in this work. We follow Ma et\u00a0al. (2022) in representing slots by their associated role name (leveraging label semantics), and Li et\u00a0al. (2021) in coordinating multiple arguments for the same role with \u201cand.\u201d Figure\u00a01 shows example templates.Our experiments consider both (1) Flan-T5-based (Chung et\u00a0al., 2022) QA and TI models fine-tuned and evaluated on all possible combinations of source and target dataset for our six datasets, and (2) GPT-3.5333https://openai.com/blog/chatgpt and GPT-4 (OpenAI, 2023) evaluated zero-shot on each dataset. We briefly describe the datasets and models below.We use six datasets, each with its own ontology. All but ACE are document-level, i.e., arguments may appear in sentences other than the one containing their trigger. Appendix\u00a0A has summary statistics.is the most popular benchmark for sentence-level EAE, featuring a two-level event ontology of 33 types, along with 26 role types. The corpus contains news articles and broadcast transcripts covering business, financial transactions, military conflict, and life events. Sentences in ACE may contain multiple events.were developed during the DARPA DEFT program (DARPA, 2012) as document-level successors to ACE and are based on its ontology, but differ in key ways. ERE-Light (ERE-L) and ACE each contain two event types the other does not, while ERE-L\u2019s 17 role types are a subset of the 26 in ACE. ERE-Rich (ERE-R) augments ERE-L with six further event types and nine further role types, and the resulting ontology overlaps with, but still differs from, ACE. ERE-L and ERE-R cover newswire and discussion forum documents, with only the latter annotated for both ontologies. Documents are annotated exhaustively for relevant events.is a dataset of short report passages excerpted from Wikipedia articles, each paired with a corresponding (non-Wikipedia) source article. A single event trigger is annotated in each report against a broad-coverage, 253-frame subset of the FrameNet ontology (with 318 role types; Baker et\u00a0al., 1998) that contains only situation-denoting frames. Event arguments are annotated in both the report and the source, and may appear anywhere within either document. We use only the FAMuS reports in our experiments.is a document-level dataset of news articles annotated against the three-level AIDA-I ontology with 139 event types and 65 role types. Like FAMuS, each document has one annotated event trigger, but arguments are limited to a five-sentence context window around it.covers web articles annotated against the three-level KAIROS ontology, with 49 event types and 57 role types. As in ERE, documents are exhaustively annotated.We adopt a standard extractive QA architecture (Du and Cardie, 2020) that uses a Flan-T5-base encoder to jointly embed a question concatenated with the document context, containing a highlighted event trigger. The embedding of the BOS token is then passed to a final linear layer to predict the start and end offsets of the answer span.444For roles with no arguments, we construct an example with target offsets (0,0)00(0,0)( 0 , 0 )\u2014i.e.\u00a0the BOS token. Since questions (roles) may have multiple answers (arguments), we construct a single example per argument during training. At inference time, we construct a single example per role, predicting up to k\ud835\udc58kitalic_k argument spans for which model confidence exceeds a dev-tuned threshold (we set k=5\ud835\udc585k=5italic_k = 5). We use the average of the cross-entropy losses w.r.t.\u00a0the gold start and end offsets as the training objective.We draw on the approach of Li et\u00a0al. (2021) for our TI models. We use Flan-T5-base to (1) jointly encode a document containing a highlighted trigger together with its (unfilled) template, then (2) autoregressively decode the filled template. We use the NLL of the gold filled templates as the objective and beam search for decoding (beam size=5beam size5\\text{beam size}=5beam size = 5).We evaluate GPT-3.5 and GPT-4 zero-shot, using the same questions as for our QA models. In the prompts, we provide instructions for the task and the same context passages with highlighted triggers as before. All questions are provided together in the same prompt, and we report averages across three prompt variants.555Further details on models and prompts in Apps.\u00a0B and C.\nWe report (typed) exact match argument F1subscriptF1\\text{F}_{1}F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT for all settings. To allow direct comparisons among our models, we consider an argument to be correct iff it is string-equivalent to the reference.Results for all models can be found in Table\u00a01. We present observations and further analysis below.We find that for each target ontology, some Flan-T5 model (TI or QA) obtains zero-shot performance superior to GPT-3.5\u2014often by wide margins. Remarkably, the same is also true w.r.t.\u00a0GPT-4, with the lone exception of FAMuS. This is notable given the massive differences in the amount of pretraining data and in the parameter counts between Flan-T5-base (250M) and the GPT models (believed to be at least 175B for GPT-3.5 and at least 1.0T for GPT-4).666https://en.wikipedia.org/wiki/GPT-4 Even setting aside transfer between the most similar ontologies (ACE, ERE-{R,L}) and focusing on more distant ontology pairs, we still observe stronger transfer results with some (indeed, for ERE-{L,R} and RAMS, even all) Flan-T5 model(s) than with GPT-3.5. That said, transfer between distant pairs (e.g.\u00a0WikiEvents \u2192\u2192\\rightarrow\u2192 FAMuS) remains challenging in absolute terms. Given the cost and runtime of even the OpenAI inference APIs, these results indicate that for many use cases, training a smaller model in-house on recasted existing resources can be a cheaper, more effective first-line approach to extraction in a new domain.We observe often sizable performance gaps between TI and QA models trained on the same source dataset, both in the in-domain evaluations (e.g.\u00a065.95 on ACE for TI vs.\u00a060.35 for QA) and and in the zero-shot evaluations (e.g.\u00a025.37 on WikiEvents for RAMS-trained TI vs.\u00a036.47 for RAMS-trained QA). Yet, neither method is consistently dominant across domains: TI obtains best in-domain performance on 4/6 datasets, and best zero-shot performance on only 2/6. This suggests both TI and QA should be considered when attempting extraction on novel datasets and/or ontologies.Figure\u00a02 presents correlations (\u03c1\ud835\udf0c\\rhoitalic_\u03c1) between in-domain and zero-shot argument F1subscriptF1\\text{F}_{1}F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT at the event type level, with higher correlations indicating that the same event types tend to be hard/easy in both in-domain and zero-shot evaluations. We find higher correlations where the source and target ontologies are more similar (notably, among ACE and ERE-{L,R}), reflecting the smaller domain shifts.777In principle, this need not be the case\u2014e.g.\u00a0if the distributions of examples across types differed markedly. WikiEvents stands out for its low (often negative) correlations, indicating little overlap in the types that are hard/easy in-domain vs.\u00a0when transferring.Lastly, we investigate the value of augmenting training data with paraphrases of the questions and templates, focusing on transfer from FAMuS as a case study. We use GPT-4 to generate five paraphrases for each FAMuS question and template, and retrain the Flan-T5 QA and TI models on the augmented datasets.888GPT-4 prompts and hyperparameters are in Appendix\u00a0C. Figure\u00a03 reports average results across three runs. We find some gain from paraphrases across all ontologies in the TI setting (from 0.1 F1subscriptF1\\text{F}_{1}F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT on ACE, up to 3.7 F1subscriptF1\\text{F}_{1}F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT on ERE-L), with more mixed results in the QA setting (modest gains for ERE-R, RAMS, and WikiEvents only). Paraphrases thus may (not) be worthwhile depending on the size of the gains and one\u2019s training budget.This work has investigated two prominent methods for EAE transfer learning\u2014question answering (QA) and template infilling (TI)\u2014across six datasets. While neither method consistently outperforms the other, we have shown that small models trained with these methods on an appropriate source ontology can yield zero-shot extraction performance superior to that of vastly larger models (GPT-3.5, GPT-4). At least for EAE, our results suggest that, far from being obsolete, small models trained on recasted existing resources remain an effective first choice for extraction in new domains.\n\nFirst, this work has focused on EAE, which uses gold event triggers. It is possible findings could differ in the full event extraction setting. Such an investigation would be an interesting direction for future work, though we note that it would be difficult to carry out with the same set of datasets considered here, as some (RAMS and FAMuS) are not exhaustively annotated for relevant events, creating challenges around censored data.Second, we use the term zero-shot to refer to settings in which a model is evaluated on a different dataset/ontology from that on which it was trained. However, there is some (variable) overlap in the event and role types in the ontologies we consider. We do not think this impugns the import of our findings, since most practical EAE transfer scenarios are ones in which at least some key features of the source domain are at least partially preserved in the target (e.g.\u00a0agent-like and patient-like roles may be expected to exist in both). However, it is possible we would find poorer transfer results for yet more divergent domain pairs than those considered here.Finally, transfer performance for most ontology pairs remains low in absolute terms, which may present an obstacle to deploying these methods in some real-world scenarios.As this work primarily evaluates existing models on existing resources, we do not believe it introduces any novel ethical concerns. However, many of the corpora studied in this work discuss historical situations involving military and political violence, and caution is therefore warranted in using them to train models for real world applications.\n\n\n\n\n\nDataset\nEvent Types\nRole Types\nEvents\nArguments\nDoc Level?\n\n\n\nACE\n33\n26\n5,223\n9,629\nNo\n\nERE-L\n33\n17\n4,066\n5,474\nYes\n\nERE-R\n38\n26\n5,763\n10,621\nYes\n\nFAMuS\n253\n318\n1,265\n3,953\nYes\n\nRAMS\n139\n65\n9,124\n21,237\nYes\n\nWikiEvents\n49\n57\n3,951\n5,536\nYes\n\n\nAll the data used in this work is English only. Table\u00a02 contains summary statistics for all of the datasets/ontologies considered in this work. Counts reflect totals over all splits.ACE, ERE-L, and ERE-R are available through the Linguistic Data Consortium (LDC; ACE Catalog No.: LDC2006T06; ERE Catalog No.: LDC2023T0). These are licensed under the LDC User Agreement for Non-Members, which allows for use of LDC artifacts \u201conly for non-commercial linguistic education, research and technology development.\u201d Our use of these datasets is for research purposes only and thus adheres to the license.\nWe use RAMS v1.0c and FAMuS (unversioned; commit=4c8cc1f), which are both released under a CC-BY-SA-4.0 License. This license allows one to \u201cremix, transform, and build upon the material for any purpose, even commercially,\u201d provided the resulting work contains proper attribution to the original and is released under the same license. We do not alter the RAMS or FAMuS datasets in any way, though our experiments use (and acknowledge) them, and our code will be released under a CC-BY-SA-4.0 license, which is consistent with the terms of use.We use the version of WikiEvents released at https://github.com/raspberryice/gen-arg/ (unversioned; commit=253e088). The only license provided is an MIT License, covering the repository as a whole, which grants permission to \u201cto use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software.\u201d We believe our use of WikiEvents is thus consistent with this license.All Flan-T5 models (both TI and QA) are initialized from the google/flan-t5-base pretrained model and are trained using the HuggingFace Transformers package (v4.38.2; Wolf et\u00a0al., 2019), which relies on PyTorch (v2.0.1; Paszke et\u00a0al., 2019). We use the HuggingFace Tokenizers package (v0.15.2; ibid.) and SpaCy (v3.7.4; Honnibal and Montani, 2017) for tokenization.999Preprocessing details are available in our GitHub repo: https://github.com/wgantt/eae-transfer.Evaluation is performed using the Metametric package (v0.1.0-alpha.4; Chen et\u00a0al., 2023). We train for a maximum of 50 epochs using the AdamW optimizer (Loshchilov and Hutter, 2018) with default hyperparameters and full precision on a combination of NVIDIA RTX 6000 and A100 GPUs (CUDA v11.7). We use argument F1subscriptF1\\text{F}_{1}F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT on the dev split as the stopping criterion, with a patience of 10 epochs. The only hyperparameter tuning we perform is manual tuning of the batch size. For the QA models, batch size was set to 32 (except for the FAMuS models, where it was set to 4). For the TI models, batch size was set to 8. Results in Table\u00a01 reflect averages across three runs using random seeds 1337, 1338, and 1339 set globally. GitHub Copilot was used to assist in coding.To choose the confidence threshold used for argument selection in the QA models, we sweep thresholds in increments of 5% from the 5thsuperscript5th5^{\\text{th}}5 start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT to the 95thsuperscript95th95^{\\text{th}}95 start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT percentiles of confidence scores over all k\u22c5|\ud835\udc9fdev|\u22c5\ud835\udc58subscript\ud835\udc9fdevk\\cdot\\lvert\\mathcal{D}_{\\text{dev}}\\rvertitalic_k \u22c5 | caligraphic_D start_POSTSUBSCRIPT dev end_POSTSUBSCRIPT | candidate arguments for the dev split, where k\ud835\udc58kitalic_k is a hyperparameter that determines the maximum number of arguments that may be predicted for a given role and where |\ud835\udc9fdev|subscript\ud835\udc9fdev\\lvert\\mathcal{D}_{\\text{dev}}\\rvert| caligraphic_D start_POSTSUBSCRIPT dev end_POSTSUBSCRIPT | is the total number of examples (=role instances) in the dev split. At the end of each epoch, we select the threshold that yields the highest dev argument F1subscriptF1\\text{F}_{1}F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. For test set evaluation, we use the dev threshold tdevsubscript\ud835\udc61devt_{\\text{dev}}italic_t start_POSTSUBSCRIPT dev end_POSTSUBSCRIPT of the checkpoint with highest dev argument F1subscriptF1\\text{F}_{1}F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. The same threshold is used for all datasets at test time time. Finally, we require that model confidence for all predicted arguments exceeds the confidence in the \u201cno answer\u201d response (i.e.\u00a0confidence in the BOS token). Thus if the model confidence for \u201cno answer\u201d is cnullsubscript\ud835\udc50nullc_{\\text{null}}italic_c start_POSTSUBSCRIPT null end_POSTSUBSCRIPT, the final threshold is given by max\u2062(tdev,cnull)maxsubscript\ud835\udc61devsubscript\ud835\udc50null\\text{max}(t_{\\text{dev}},c_{\\text{null}})max ( italic_t start_POSTSUBSCRIPT dev end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT null end_POSTSUBSCRIPT ).We use the gpt-3.5-turbo-0125 version of GPT-3.5 and the gpt-4-0125-preview version of GPT-4. For both, we set top_p=1.0top_p1.0\\texttt{top\\_p}=1.0top_p = 1.0, \ud835\ude9d\ud835\ude8e\ud835\ude96\ud835\ude99\ud835\ude8e\ud835\ude9b\ud835\ude8a\ud835\ude9d\ud835\ude9e\ud835\ude9b\ud835\ude8e=0.7\ud835\ude9d\ud835\ude8e\ud835\ude96\ud835\ude99\ud835\ude8e\ud835\ude9b\ud835\ude8a\ud835\ude9d\ud835\ude9e\ud835\ude9b\ud835\ude8e0.7\\texttt{temperature}=0.7temperature = 0.7, max_new_tokens=512max_new_tokens512\\texttt{max\\_new\\_tokens}=512max_new_tokens = 512, and we use no frequency or presence penalties. Our three prompts each use a different system prompt, listed below. The corresponding user prompts are described further down. Prompts and hyperparameters were chosen based on their promising results in manual prompt engineering efforts on the OpenAI playground that used several training set examples.101010https://platform.openai.com/playgroundYou are the world champion of extractive question answering.You are an expert at question answering from text.You are the best in the world at reading comprehension.Our three user prompts share the same basic format (see below), but vary in the wording of their instructions. Complete (system+user) prompts are constructed by pairing system prompt i\u2208{1,2,3}\ud835\udc56123i\\in\\{1,2,3\\}italic_i \u2208 { 1 , 2 , 3 } with a user prompt containing instruction set i\ud835\udc56iitalic_i. As indicated in the instructions, each \u27e8\ud835\ude73\ud835\ude98\ud835\ude8c\ud835\ude9e\ud835\ude96\ud835\ude8e\ud835\ude97\ud835\ude9d\u27e9delimited-\u27e8\u27e9\ud835\ude73\ud835\ude98\ud835\ude8c\ud835\ude9e\ud835\ude96\ud835\ude8e\ud835\ude97\ud835\ude9d\\langle\\texttt{Document}\\rangle\u27e8 Document \u27e9 contains a single highlighted event trigger.Instructions: \u27e8\ud835\ude78\ud835\ude97\ud835\ude9c\ud835\ude9d\ud835\ude9b\ud835\ude9e\ud835\ude8c\ud835\ude9d\ud835\ude92\ud835\ude98\ud835\ude97\ud835\ude9c\u27e9delimited-\u27e8\u27e9\ud835\ude78\ud835\ude97\ud835\ude9c\ud835\ude9d\ud835\ude9b\ud835\ude9e\ud835\ude8c\ud835\ude9d\ud835\ude92\ud835\ude98\ud835\ude97\ud835\ude9c\\langle\\texttt{Instructions}\\rangle\u27e8 Instructions \u27e9\nYou must give your answers as JSON in the following format:Input Passage: \u27e8\ud835\ude73\ud835\ude98\ud835\ude8c\ud835\ude9e\ud835\ude96\ud835\ude8e\ud835\ude97\ud835\ude9d\u27e9delimited-\u27e8\u27e9\ud835\ude73\ud835\ude98\ud835\ude8c\ud835\ude9e\ud835\ude96\ud835\ude8e\ud835\ude97\ud835\ude9d\\langle\\texttt{Document}\\rangle\u27e8 Document \u27e9\nAnswers:I will give you an input passage containing an event trigger demarcated with \u201c<trigger></trigger>\u201d HTML tags. I will also give you a set of questions about the event denoted by that trigger. Your task is to answer each question with a list of contiguous spans extracted from the input passage. Answers may contain zero, one, or multiple spans. The list should be empty if no answer can be found.I will show you a document that contains an event trigger that is highlighted with \u201c<trigger></trigger>\u201d HTML tags. After the document, I will list out a set of questions about the event referred to by the highlighted trigger. Please answer each question with a list of zero or more contiguous spans extracted from the input passage. Spans MUST appear in the document. Some questions may not have answers, in which case the answer should be an empty list.I will give you a passage of text featuring a phrase that refers to some event and that is highlighted with \u2019<trigger></trigger>\u2019 HTML tags. I will additionally provide you with a list of questions about the event referred to by the highlighted phrase. You must answer each question with a list of zero, one, or multiple contiguous spans that appear in the input passage. Some questions do not have any answer in the input passage. For these cases, your answer should be an empty list.To generate paraphrases for the experiments in \u00a74, we use gpt-4-0125-preview with top_p=1.0top_p1.0\\texttt{top\\_p}=1.0top_p = 1.0, \ud835\ude9d\ud835\ude8e\ud835\ude96\ud835\ude99\ud835\ude8e\ud835\ude9b\ud835\ude8a\ud835\ude9d\ud835\ude9e\ud835\ude9b\ud835\ude8e=0.7\ud835\ude9d\ud835\ude8e\ud835\ude96\ud835\ude99\ud835\ude8e\ud835\ude9b\ud835\ude8a\ud835\ude9d\ud835\ude9e\ud835\ude9b\ud835\ude8e0.7\\texttt{temperature}=0.7temperature = 0.7, and max_new_tokens=512max_new_tokens512\\texttt{max\\_new\\_tokens}=512max_new_tokens = 512 with no system prompt and with the user prompts shown below (different for questions and for templates).Instructions: Please generate five paraphrases of the following template, but you ABSOLUTELY CANNOT change any words that are in between brackets ([]). Your paraphrases MUST be formatted as a JSON list of strings.\n\u00a0\nTemplate: \u27e8\ud835\ude9d\ud835\ude8e\ud835\ude96\ud835\ude99\ud835\ude95\ud835\ude8a\ud835\ude9d\ud835\ude8e\u27e9delimited-\u27e8\u27e9\ud835\ude9d\ud835\ude8e\ud835\ude96\ud835\ude99\ud835\ude95\ud835\ude8a\ud835\ude9d\ud835\ude8e\\langle\\texttt{template}\\rangle\u27e8 template \u27e9\n\u00a0\nParaphrases:Instructions: Please generate five paraphrases of the following question. Your answer MUST be formatted as a JSON list of strings.\n\u00a0\nQuestion: \u27e8\ud835\ude9a\ud835\ude9e\ud835\ude8e\ud835\ude9c\ud835\ude9d\ud835\ude92\ud835\ude98\ud835\ude97\u27e9delimited-\u27e8\u27e9\ud835\ude9a\ud835\ude9e\ud835\ude8e\ud835\ude9c\ud835\ude9d\ud835\ude92\ud835\ude98\ud835\ude97\\langle\\texttt{question}\\rangle\u27e8 question \u27e9\n\u00a0\nParaphrases:",
    "12": "This paper introduces CRITICAL, a novel closed-loop framework for autonomous vehicle (AV) training and testing. CRITICAL stands out for its ability to generate diverse scenarios, focusing on critical driving situations that target specific learning and performance gaps identified in the Reinforcement Learning (RL) agent. The framework achieves this by integrating real-world traffic dynamics, driving behavior analysis, surrogate safety measures, and an optional Large Language Model (LLM) component. It is proven that the establishment of a closed feedback loop between the data generation pipeline and the training process can enhance the learning rate during training, elevate overall system performance, and augment safety resilience. Our evaluations, conducted using the Proximal Policy Optimization (PPO) and the HighwayEnv simulation environment, demonstrate noticeable performance improvements with the integration of critical case generation and LLM analysis, indicating CRITICAL\u2019s potential to improve the robustness of AV systems and streamline the generation of critical scenarios. This ultimately serves to hasten the development of AV agents, expand the general scope of RL training, and ameliorate validation efforts for AV safety.\nWe make our code publicly available at https://github.com/zachtian/CRITICALIn the past decade, autonomous vehicles (AV) have achieved remarkable progress. This swift advancement can be attributed to improvements in the effectiveness of the collection of models that characterize an AV, such as perception, planning, localization, etc. Additionally, the implementation of robust risk metrics and safety assurances for handling critical situations, combined with the integration of these elements into a comprehensive control system that facilitates communication between modules, has played a crucial role [1]. A central component within the AI framework of an AV is the path planning and trajectory forecast module. The abundance of high-fidelity simulation environments and real-world scenario data collected over the years have allowed this decision-making process to be advantageously outsourced to reinforcement learning (RL) algorithms. The adaptive learning capability of RL is particularly beneficial in creating algorithms that can adeptly navigate dynamic and unpredictable road traffic environments [2].However, the progress of open-source planning and prediction modules for RL in autonomous driving is impeded by the absence of an interpretable framework capable of generating diverse and pertinent safety-critical scenarios [3]. Traditional AV training methods and environments often involve controlled, repeatable scenarios, which, while beneficial for initial learning, may not adequately cover the spectrum of real-world complexities [4]. And consequently, because the effectiveness of training RL for autonomous driving depends heavily on the variety and complexity of training scenarios provided, RL models exclusively trained in such environments might find themselves inadequately prepared to handle the unpredictable dynamics of road traffic [5]. A major challenge lies in accurately replicating the diverse conditions of real-world driving and addressing specific failure modes that AVs might encounter [6].An intuitive method to bolster RL-based planning algorithms is to incorporate edge-case scenarios within an ego-vehicle\u2019s operational design domain (ODD) into the training process. A common approach is to use unsupervised clustering techniques, such as K-Means and Hierarchical Clustering Algorithms (HCA), to identify these ODD edge cases [7]. These techniques, if used in isolation, fall short since they lack ground truth for validation. Other methodologies opt to utilize graph-based approaches to extract ontological relations between objects to sieve and generate a subset of edge-case scenarios from a set of boundary scenarios. While these techniques are effective, they are limited by their current intractable interpretability [8].In this paper, we present a novel framework for training AVs that automatically generates critical scenarios, thereby augmenting conventional RL training. Additionally, we explore integrating Large Language Models (LLMs) to further refine these scenarios, leveraging real-world driving data to enrich training with diverse and challenging situations. By dissecting training episodes with LLMs to detect failure patterns, we significantly widen the scope of potential driving situations, enhancing the overall training process. This approach establishes a dynamic mechanism for the continuous, closed-loop refinement and validation of AV planning and prediction algorithms: we refer to this framework as \u201dCRITICAL\u201d, and is overviewed at a high-level in Fig. 1.The proposed framework is designed to isolate critical scenarios and leverage this identification to generate similar situations. This methodology combines data-driven and knowledge-driven approaches, leveraging the advantages of each while mitigating the drawbacks that arise when relying exclusively on either one [3].Our contributions are as follows:We introduce a novel RL-based framework, CRITICAL, for enhancing the training and evaluation of AV algorithms. CRITICAL augments RL agents\u2019 exposure to a variety of scenarios, with a specific focus on critical driving scenarios to bolster AV performance and resilience.Utilizing the highD dataset, we enrich our simulation environment with real-world traffic dynamics. Through clustering techniques, we analyze and replicate diverse driving behaviors, leveraging risk metrics to craft and incorporate high-fidelity critical scenarios into our training regimen.Our empirical findings validate that CRITICAL\u2019s closed-loop feedback mechanism between scenario generation and RL training significantly elevates the learning rate, overall AV performance, and adaptability to safety-critical situations.In AV development, it is crucial to create a variety of challenging driving scenarios for evaluation purposes [1]. Data-driven generation methods in autonomous vehicle testing mainly rely on real-world traffic data to create diverse driving scenarios. Techniques range from direct sampling and clustering using Traffic Primitives[9] to more complex strategies involving random perturbation for scenario augmentation [10]. Additionally, advanced methods like Bayesian Networks for probabilistic modeling[11] and Deep Generative Models [12] are utilized to generate realistic unseen traffic scenarios.Adversarial generation in autonomous vehicle testing actively creates high-risk scenarios by simulating challenging interactions. This approach involves a generator and a victim model [13]. For static scenarios, high-dimensional data generation is common, with techniques ranging from differentiable rendering for attacking detection algorithms [14] to optimization methods for manipulating point clouds and images [15]. Dynamic scenario generation is crucial for evaluating planning and control modules.Knowledge-based generation in AV testing combines domain expertise with predefined rules to create realistic scenarios adhering to traffic laws and physical principles. Techniques range from optimizing dynamic object behaviors and environmental conditions [16] to employing RL for adversarial policy development [17].LLMs have significantly evolved in recent years, exhibiting profound capabilities in text generation, comprehension, and other areas [20]. The recent unveiling of Llama 2 distinctly marks a new phase; epitomizing collaborative advancements within the AI community through its open-source community license [21].Furthermore, LLMs have showcased remarkable potential in reasoning-intensive question-answering scenarios [22]. The rising proclivity of prompt tuning over traditional fine-tuning techniques (in certain domains) accentuates the growing importance of the former for tailored tasks [23]. This focus on prompts receives validation from research, emphasizing its pivotal role in amplifying the reasoning competencies of LLMs [24].There has also been a surge of research using LLMs for autonomous driving tasks. DriveLikeAHuman [25] and DiLu [26] explore the potential of using an LLM to understand the driving environment in a human-like manner and analyze its ability to reason and interpret. DriveLM [27] is a dataset built upon nuScenes and CARLA, and proposes a VLM-based baseline approach for jointly performing GraphVQA and end-to-end driving.\nAmbiguous definitions of edge-case, boundary, and critical scenarios are commonplace in autonomous driving literature [28, 29]. Edge-case scenarios typically refer to an infrequent or unique scenario that lies at the extreme limits within a distribution of scenarios (or environment specifications, i.e. scenario configurations omitting the trajectory of the ego-vehicle) [30]. Evolutionary Algorithms (EAs) rank highly among the preferred methods for creating edge-case scenarios [31].On the other hand, boundary scenarios are often defined as those scenarios whose execution results are in the proximate area around the boundary between safe and unsafe, i.e. perturbing the scenario parameters might lead to significantly different execution results [32]. To generate these scenarios, the following methods are popular: Accelerated Evaluation [33], Genetic Algorithms (GA) [34], Particle Swarm Optimization [35], and so on [36].\nCritical scenarios are defined in the literature to be the union of edge-case scenarios and boundary scenarios [37], illustrated in Fig. 2. For clarity, and in hopes of easing the acquisition of unified safety standards by AV regulatory frameworks [29], we reiterate this distinction to partition scarcity and safety concepts that are too often bundled together for scenario categorization [38].Current research in AV training primarily focuses on predefined and dynamic scenarios. However, the use of LLMs for enhancing scenario generation, particularly for complex and critical situations, remains largely underexplored. There\u2019s a need for a comprehensive, open-source framework that generates critical scenarios to augment RL training data using LLMs and criticality metrics. This type of solution can adaptively create diverse and challenging scenarios, exponentiating AV safety and path planning development.An overview of our pipeline is shown in Fig. 3. We employed highway-env [19] as our base simulation environment, notable for its adjustable parameters like traffic dynamics and vehicle behaviors. Adjusting the parameters within the environmental configuration (such as vehicle density and behaviors) naturally changes the nature of scenarios exposed to the RL agent during training.Our AV path planning and prediction models utilize Proximal Policy Optimization (PPO), favored for striking an optimal balance between enhancing performance and maintaining training stability. This is primarily achieved through its unique objective function:In this equation, rt\u2062(\u03b8)subscript\ud835\udc5f\ud835\udc61\ud835\udf03r_{t}(\\theta)italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_\u03b8 ) represents the ratio of the new to the old policy\u2019s probability, with A^tsubscript^\ud835\udc34\ud835\udc61\\hat{A}_{t}over^ start_ARG italic_A end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT denoting the advantage estimate. The key aspect here is the \u03f5italic-\u03f5\\epsilonitalic_\u03f5-clipped objective function that regulates the update size, ensuring gradual and stable learning improvements. For a more comprehensive insight into PPO and its significance within the context of deep reinforcement learning, refer to [39]. The choice of PPO over other algorithms is guided by its proven efficacy in handling diverse and unpredictable scenarios, aligning with our goal of enhancing autonomous driving technologies.A critical component of our approach involves incorporating real-world traffic dynamics into our simulations. To achieve this, we incorporate the highD dataset [18], which offers a fairly comprehensive and realistic representation of vehicle behaviors observed at various times on highways. The highD dataset provides extensive recordings of natural vehicle trajectories on German highways (capturing details like vehicle type, size, maneuvers, precise positioning, etc.) and is collected through drone surveillance cameras to overcome common data collection limitations that may occur when gathering such data via conventional infrastructure sensors. Details of how we used the highD dataset, including our methodology for clustering driving behaviors and generating traffic scenarios, will be explained in Section IV-A.To diversify the training dataset, we need to edit the environment configuration after a certain number of training episodes (after a single epoch). To automate this process, we choose to leverage the highD dataset or via an LLM. This is largely a knowledge-driven approach because we either exploit the real-world knowledge base of the highD dataset or the in-context knowledge and meaningful priors that are embedded within the LLM. In particular, we exploit knowledge relating to traffic objects and human driving behavior. This methodology fundamentally preserves natural language interpretability and augments the potential for specific scenario querying [40].\nIn our framework, two primary surrogate safety measures are employed to evaluate the safety-criticality of maneuvers performed by the ego vehicle: the Time to Collision (T\u2062T\u2062C\ud835\udc47\ud835\udc47\ud835\udc36TTCitalic_T italic_T italic_C) and a Unified Risk Index (r\ud835\udc5fritalic_r).T\u2062T\u2062C\ud835\udc47\ud835\udc47\ud835\udc36TTCitalic_T italic_T italic_C is defined as:Here, xrelsubscript\ud835\udc65rel{x_{\\text{rel}}}italic_x start_POSTSUBSCRIPT rel end_POSTSUBSCRIPT denotes the relative distance between the two vehicles in question, and |v|relsubscript\ud835\udc63rel\\lvert{v}\\rvert_{\\text{rel}}| italic_v | start_POSTSUBSCRIPT rel end_POSTSUBSCRIPT implies the relative speed of the two vehicles. A lower TTC indicates a higher risk of collision. If T\u2062T\u2062C\ud835\udc47\ud835\udc47\ud835\udc36TTCitalic_T italic_T italic_C at any timestep in a scenario drops below a predefined threshold value, we increment a T\u2062T\u2062C\ud835\udc47\ud835\udc47\ud835\udc36TTCitalic_T italic_T italic_C near miss count.The unified risk index (r\ud835\udc5fritalic_r) is a function of the longitudinal and lateral risk index (rl\u2062o\u2062nsubscript\ud835\udc5f\ud835\udc59\ud835\udc5c\ud835\udc5br_{lon}italic_r start_POSTSUBSCRIPT italic_l italic_o italic_n end_POSTSUBSCRIPT and rl\u2062a\u2062tsubscript\ud835\udc5f\ud835\udc59\ud835\udc4e\ud835\udc61r_{lat}italic_r start_POSTSUBSCRIPT italic_l italic_a italic_t end_POSTSUBSCRIPT) [41, 42]. To calculate rl\u2062o\u2062nsubscript\ud835\udc5f\ud835\udc59\ud835\udc5c\ud835\udc5br_{lon}italic_r start_POSTSUBSCRIPT italic_l italic_o italic_n end_POSTSUBSCRIPT and rl\u2062a\u2062tsubscript\ud835\udc5f\ud835\udc59\ud835\udc4e\ud835\udc61r_{lat}italic_r start_POSTSUBSCRIPT italic_l italic_a italic_t end_POSTSUBSCRIPT, we must first ascertain the minimum safe longitudinal and lateral distances (dminl\u2062o\u2062nsuperscriptsubscript\ud835\udc51\ud835\udc59\ud835\udc5c\ud835\udc5bd_{\\min}^{lon}italic_d start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l italic_o italic_n end_POSTSUPERSCRIPT and dminl\u2062a\u2062tsuperscriptsubscript\ud835\udc51\ud835\udc59\ud835\udc4e\ud835\udc61d_{\\min}^{lat}italic_d start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l italic_a italic_t end_POSTSUPERSCRIPT). These are defined below:where [x]+:=max\u2062(x,0)assignsubscriptdelimited-[]\ud835\udc65max\ud835\udc650[x]_{+}:=\\text{max}(x,0)[ italic_x ] start_POSTSUBSCRIPT + end_POSTSUBSCRIPT := max ( italic_x , 0 ), a vehicle with velocity vrsubscript\ud835\udc63\ud835\udc5fv_{r}italic_v start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT drives behind another vehicle (in the same direction) with velocity vfsubscript\ud835\udc63\ud835\udc53v_{f}italic_v start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT. For any braking of at most bmaxsubscript\ud835\udc4fmaxb_{\\text{max}}italic_b start_POSTSUBSCRIPT max end_POSTSUBSCRIPT, the trailing vehicle has a response time of \u03c1\ud835\udf0c\\rhoitalic_\u03c1 during which it accelerates by at most amaxsubscript\ud835\udc4emaxa_{\\text{max}}italic_a start_POSTSUBSCRIPT max end_POSTSUBSCRIPT, and immediately starts braking (after the response kicks in) by at least bminsubscript\ud835\udc4fminb_{\\text{min}}italic_b start_POSTSUBSCRIPT min end_POSTSUBSCRIPT.The minimum safe lateral distance (dminl\u2062a\u2062tsuperscriptsubscript\ud835\udc51\ud835\udc59\ud835\udc4e\ud835\udc61d_{\\min}^{lat}italic_d start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l italic_a italic_t end_POSTSUPERSCRIPT) is given as the following:With ve\u2062g\u2062osuperscript\ud835\udc63\ud835\udc52\ud835\udc54\ud835\udc5cv^{ego}italic_v start_POSTSUPERSCRIPT italic_e italic_g italic_o end_POSTSUPERSCRIPT and vn\u2062l\u2062nsuperscript\ud835\udc63\ud835\udc5b\ud835\udc59\ud835\udc5bv^{nln}italic_v start_POSTSUPERSCRIPT italic_n italic_l italic_n end_POSTSUPERSCRIPT being the velocity of the ego vehicle and the nearest lane neighbor vehicle, vl\u2062a\u2062tsubscript\ud835\udc63\ud835\udc59\ud835\udc4e\ud835\udc61v_{lat}italic_v start_POSTSUBSCRIPT italic_l italic_a italic_t end_POSTSUBSCRIPT is the lateral speed of the specified vehicle, response time \u03c1\ud835\udf0c\\rhoitalic_\u03c1, and braking of at least bm\u2062i\u2062nsubscript\ud835\udc4f\ud835\udc5a\ud835\udc56\ud835\udc5bb_{min}italic_b start_POSTSUBSCRIPT italic_m italic_i italic_n end_POSTSUBSCRIPT.\nThe longitudinal and lateral risk index, rl\u2062o\u2062nsubscript\ud835\udc5f\ud835\udc59\ud835\udc5c\ud835\udc5br_{lon}italic_r start_POSTSUBSCRIPT italic_l italic_o italic_n end_POSTSUBSCRIPT and rl\u2062a\u2062tsubscript\ud835\udc5f\ud835\udc59\ud835\udc4e\ud835\udc61r_{lat}italic_r start_POSTSUBSCRIPT italic_l italic_a italic_t end_POSTSUBSCRIPT respectively, is given by the conditional equation(s) below:The current relative longitudinal and lateral distances between the ego vehicle and another background traffic vehicle (usually the nearest neighbor vehicle) are denoted as dl\u2062o\u2062nsuperscript\ud835\udc51\ud835\udc59\ud835\udc5c\ud835\udc5bd^{lon}italic_d start_POSTSUPERSCRIPT italic_l italic_o italic_n end_POSTSUPERSCRIPT and dl\u2062a\u2062tsuperscript\ud835\udc51\ud835\udc59\ud835\udc4e\ud835\udc61d^{lat}italic_d start_POSTSUPERSCRIPT italic_l italic_a italic_t end_POSTSUPERSCRIPT.Multiplying the lateral and longitudinal risk indices, and adding risk propensity parameters \u03b2>0\ud835\udefd0\\beta>0italic_\u03b2 > 0 and \u03b3>0\ud835\udefe0\\gamma>0italic_\u03b3 > 0, the unified risk index (r\ud835\udc5fritalic_r) is defined as:In our study, we set both \u03b2\ud835\udefd\\betaitalic_\u03b2 and \u03b3\ud835\udefe\\gammaitalic_\u03b3 to 1111 for simplicity. This approach allows us to quantify risk in a way that comprehensively accounts for both longitudinal and lateral dynamics, crucial for assessing the safety of AV maneuvers. When this exceeds a predefined threshold value, we increment a unified risk index r\ud835\udc5fritalic_r threshold count.In our training framework, we evaluate environment configurations based on their criticality, determined by metrics such as TTC and r\ud835\udc5fritalic_r. After a specified number of epochs, we analyze the distribution of these configurations. Configurations that repeatedly surpass predefined thresholds in TTC and r\ud835\udc5fritalic_r are marked for their propensity to create boundary scenarios, indicating their critical nature. Conversely, configurations with high criticality but low occurrence are categorized as edge-case scenarios. This dual categorization helps identify configurations that effectively represent critical scenarios. These identified configurations, both low in occurrence and high in criticality, are then used directly or fed into an LLM to inspire similar critical scenario generation. This process enriches the training by continuously presenting the RL agent with safety-critical challenges, effectively testing and enhancing its capabilities in handling real-world complexities.In our experiment, we categorized driving behaviors from the highD dataset into three distinct types: aggressive, defensive, and normal [43]. To simulate these behaviors realistically, we developed specialized vehicle models with unique parameters like politeness and preferred following distance.We employed the KPrototypes clustering method to analyze driving styles, focusing on variables such as speed, acceleration, and lane-changing behavior. This approach effectively grouped vehicles into behavior categories, providing essential insights for regenerating traffic scenarios in the HighwayEnv simulation environment. This process aims to replicate real-world traffic conditions for a thorough evaluation of autonomous driving algorithms under various realistic scenarios.Furthermore, we emphasized the recreation of high-risk scenarios from the highD dataset. For this, we computed the Risk Perception (RP) metric at each timestep for every vehicle pair. The RP, calculated as R\u2062P=AT\u2062H\u2062W+BT\u2062T\u2062C\ud835\udc45\ud835\udc43\ud835\udc34\ud835\udc47\ud835\udc3b\ud835\udc4a\ud835\udc35\ud835\udc47\ud835\udc47\ud835\udc36RP=\\frac{A}{THW}+\\frac{B}{TTC}italic_R italic_P = divide start_ARG italic_A end_ARG start_ARG italic_T italic_H italic_W end_ARG + divide start_ARG italic_B end_ARG start_ARG italic_T italic_T italic_C end_ARG, where A is set to 1 and B to 4 [44], quantifies the risk level based on Time Headway (THW) and Time To Collision (TTC). By identifying instances with the highest RP values, we could accurately replicate the most critical and dangerous scenarios for our simulations.Our analysis included 60 diverse traffic configurations from the highD dataset. We meticulously recorded details such as vehicle types, counts, and dynamic behaviors like speed and acceleration. We also included information on critical vehicle pairs. This comprehensive data was collated into a scenario database, summarized in Table I, to inform our simulation scenarios.Number of aggressive vehiclesNumber of defensive vehiclesNumber of regular vehiclesTotal number of trucksTotal number of carsVehicle densityUnique scenario identifierLocation, speed, acceleration, and lane of Vehicle ILocation, speed, acceleration, and lane of Vehicle JOur pipeline, CRITICAL, utilizes LangChain [45], a tool designed for leveraging language models, to enhance scenario analysis and generation in AV simulations. Integrating a diverse array of data and constraints, including real-world traffic patterns from the highD dataset, T\u2062T\u2062C\ud835\udc47\ud835\udc47\ud835\udc36TTCitalic_T italic_T italic_C near miss counts, and unified risk index r\ud835\udc5fritalic_r threshold counts, CRITICAL ensures that configuration modification suggestions are well-informed and realistic. This setup allows CRITICAL to use its embedded language models effectively for analyzing our data and configuration, thereby generating relevant and practical changes to our simulation scenarios.For each simulation run, we systematically recorded configurations, their outcomes, and vehicle failure types, creating a rich history that aids in understanding trends and patterns. Based on insights from this historical data and real-world traffic behaviors, the language model suggests modifications to key simulation parameters.Our approach involves creating a JSON dictionary for each scenario parameter, as detailed in Table I. The values within this dictionary are constrained within realistic ranges to reflect real-world driving behaviors and traffic conditions accurately. By employing this method, we ensure that our simulation scenarios remain authentic and grounded in reality. This structured approach, using a JSON format with predefined value limits for each parameter, facilitates efficient parsing of LLM outputs and allows for a flexible yet controlled adaptation of the simulation environment.Our experiments were conducted on a system powered by an Intel(R) Xeon(R) w5-2455X processor and an NVIDIA A6000 graphics card, supported by 256GB of RAM.We conducted evaluations comparing three distinct approaches to assess the effectiveness of our proposed CRITICAL framework. The first approach, the Baseline Model (PPO Only), utilizes the PPO algorithm in its original form. The second approach, Critical Case Generation, adopts our framework and characterizes it with an increased rate of critical scenario generation during training. The third and most comprehensive approach, LLM-Enhanced Critical Case Generation, combines the framework with LLM analysis, specifically employing the Mistral-7B-Instruct model [46]. This model was chosen for its exceptional capability in understanding and generating relevant, instructional content, vital for creating varied and intricate training scenarios.All models were trained using a set of 50 different configurations selected from our scenario database and were evaluated on a separate set of 10 configurations. Each model was run ten times on each test configuration, and the average performance was calculated to determine the final results. The training loss curves shown in Fig. 4 reveal the impact of the different training enhancements on model performance. The inclusion of critical scenarios indicates a more significant reduction in loss compared to the baseline model, suggesting an accelerated learning process. The LLM-Enhanced approach, which combines critical case generation and LLM analysis, exhibits the lowest and most stable loss, indicating superior learning efficacy and the potential for higher predictive accuracy in complex scenarios.The evaluation of our autonomous vehicle (AV) training models provided meaningful insights, as summarized in Table II. The baseline model, utilizing just the PPO algorithm, recorded a reward of 50.068, an average episode length of 55.760, and a total of 89 crashes. The introduction of critical case generation without LLM led to enhanced performance, with the reward increasing to 61.527, the average episode length extending to 63.319, and crash counts dropping to 81.\nThe most notable improvement was observed in LLM-enhanced critical case generation. This approach achieved a remarkable reward of 76.886, significantly increased the average episode length to 93.019, and reduced crash incidents to 61. These results not only highlight the enhanced navigational skills and safety capabilities of the model but also underscore the value of incorporating LLM analysis for crafting more complex and realistic training environments.Figure 5 provides insights into the RL agent\u2019s performance. Throughout the training process, criticality scores and risk metrics, including unified risk index r\ud835\udc5fritalic_r and TTC near-miss, were continuously collected and analyzed. The baseline performance is illustrated in the first row. Criticality scores serve as indirect indicators of the agent\u2019s effectiveness, with low scores suggesting either competent performance or non-critical nature of the scenario. The second row shows how the agent fared when the framework actively sought out critical case generation without LLM. A noticeable reduction is observed in the criticality score associated with the r\ud835\udc5fritalic_r threshold count. The third row highlights LLM-enhanced configuration distributions, where a marked decrease in the mean value of r\ud835\udc5fritalic_r threshold counts strongly indicates superior agent performance across scenarios, translating to very low criticality scores. This demonstrates the framework\u2019s success in enriching the training dataset with critical scenarios to facilitate RL training, thereby confirming its effectiveness.These findings underscore the efficacy of our approach, particularly the integration of LLM analysis in refining AV training. This method enriches the training process, preparing AVs for a wide range of real-world driving conditions and enhancing overall safety and performance.A novel framework (CRITICAL) is developed to support the training and testing of the RL-based AV control algorithm. This closed-loop framework incorporates real-world traffic dynamics from the highD dataset and surrogate safety measures to evaluate the criticality of scenarios encountered by a PPO agent during training. The closed-loop design of CRITICAL ensures that the RL agent is dynamically exposed to diverse challenging scenarios. This continual feedback creates a positive loop that enhances the agent\u2019s performance, improves the learning rate, and increases its robustness.The use of two surrogate safety measures (TTC and the unified risk index) in plotting configuration distributions provides insight into the evolving risk profile of scenarios encountered by the AV during training. Interestingly, we observe a decrease in the mean criticality of configurations as the AV\u2019s performance improves, reflecting its growing proficiency in handling challenging situations. This finding confirms the effectiveness of CRITICAL in improving the training of RL-based AV algorithms.Future work will focus on expanding the validation of our framework across additional datasets beyond highD to ensure generalizability across different traffic conditions and road layouts. We also aim to test the efficacy of the framework with a wider variety of RL algorithms to assess its versatility and to optimize its performance across diverse training paradigms. Through this, we seek to establish CRITICAL as an open-source tool in the pursuit of safe and reliable autonomous vehicle navigation systems.",
    "13": "In response to the rising interest in large multimodal models, we introduce Cross-Attention Token Pruning (CATP), a precision-focused token pruning method. Our approach leverages cross-attention layers in multimodal models, exemplified by BLIP-2, to extract valuable information for token importance determination. CATP employs a refined voting strategy across model heads and layers. In evaluations, CATP achieves up to 12.1X higher accuracy compared to existing token pruning methods, addressing the trade-off between computational efficiency and model precision.Harvard UniversityThe BLIP-2 is a state-of-the-art multi-modal model proposed by Li et al.[1]. It enables a Large Language Models (LLMs) to understand images, and could be applied to various vision-language tasks such as image captioning, visual question answering (VQA) and chat-like conversation with an input of images.As shown in Figure 1, BLIP-2 model consists of a frozen image encoder, a frozen LLM and a Querying Transformer (Q-Former) bridging the vision-language modality gap. Q-Former comprises two modules: an image transformer and a text transformer. These modules work to extract visual representation relevant to the corresponding text.LLM dominates BLIP-2 inference time. In total, the BLIP-2 model has 3.1 billion parameters: 0.3 billion for Image Encoder, 0.1 billion for Q-Former and 2.7 billion for LLM Decoder. In other words, LLM Decoder accounts for over 87% of total parameters, responsible for major computation cost of the BLIP-2 model during inference.State-of-the-art pruning strategies reduce model accuracy. Though state-of-the-art pruning strategies are effective in reducing computational costs for the LLM Decoder, they often result in a significant degradation of model accuracy when applied on BLIP-2. In our initial step, we evaluate two SOTA pruning methods\u2013magnitude-based pruning and self-attention probability pruning algorithm [2]. Both methods exhibit low accuracy in VQA task, which we will further discuss in Section VI.Accuracy Preserved Query-token Pruning. Since the time-complexity and space-complexity of the attention mechanism in transformers are quadratic in the length of the input, the computational cost of LLM Deocoder can be reduced if the seq_len dimension of the query token input is reduced.\nWith these challenges in mind, we aim to design an end-to-end post-training pruning pipeline that maintain decent accuracy for large-scale multi-modal models.We propose Cross-Attention Token Pruning, a newly designed token-wise pruning method for BLIP-2. CATP suggests that the importance of each input query token of the LLM Decoder depends on its cross-attention probabilities with the input image tokens in the Q-Former. This is because the cross-attention probabilities indicate the relevancy between each image token and each query token and keeping the query tokens that are closely related to all image tokens from being pruned is necessary to preserving model accuracy. However, we cannot compute the importance of each query token by simply accumulating its cross-attention probabilities across all image tokens because they are computed using the softmax function and would sum up to 1. To address this issue, CATP additionally proposes a scheme that translate these probabilities into importance scores: for each query token, every image token votes different amount of points to it based on the cross-attention probability between them. As shown in Algorithm 1, CATP takes the multi-head cross-attention layers inserted every other block in Q-Former (layer position marked in Figure 3 and extracts cross-attention probability map from each head. For each map, every image token votes L0\u2212nsubscript\ud835\udc3f0\ud835\udc5bL_{0}-nitalic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - italic_n points to the query token with the n\u2062t\u2062h\ud835\udc5b\ud835\udc61\u210enthitalic_n italic_t italic_h largest cross-attention probability where L0subscript\ud835\udc3f0L_{0}italic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is the total number of query tokens. The importance score of each query token are computed by accumulating the points it received across layers and heads. The voting procedure is illustrated in Figure 2. We prune the query tokens with the top L0\u00d7psubscript\ud835\udc3f0\ud835\udc5dL_{0}\\times pitalic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u00d7 italic_p lowest importance where p is the prune ratio.L0subscript\ud835\udc3f0L_{0}italic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT: number of query tokens \n\u00a0L1subscript\ud835\udc3f1L_{1}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT: number of image patches \n\u00a0h\u210ehitalic_h: number of heads\n\u00a0Previous cumulative token importance score: i\u2062m\u2062p\u2062_\u2062s\u2062c\u2062o\u2062r\u2062e\u2208L0\ud835\udc56\ud835\udc5a\ud835\udc5d_\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52subscript\ud835\udc3f0imp\\_score\\in L_{0}italic_i italic_m italic_p _ italic_s italic_c italic_o italic_r italic_e \u2208 italic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT\n\u00a0Current points assigned to token: s\u2208L0\ud835\udc60subscript\ud835\udc3f0s\\in L_{0}italic_s \u2208 italic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT\n\u00a0Pruning_ratio:p:Pruning_ratio\ud835\udc5d\\text{Pruning\\_ratio}:pPruning_ratio : italic_p\n\u00a0Cross-attention probability: p\u2062r\u2062o\u2062b\u2208\u211dh\u00d7L0\u00d7L1\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc4fsuperscript\u211d\u210esubscript\ud835\udc3f0subscript\ud835\udc3f1prob\\in\\mathbb{R}^{h\\times L_{0}\\times L_{1}}italic_p italic_r italic_o italic_b \u2208 blackboard_R start_POSTSUPERSCRIPT italic_h \u00d7 italic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u00d7 italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT\nfor\u00a0t\u2062o\u2062k\u2062e\u2062ni\u2062d\ud835\udc61\ud835\udc5c\ud835\udc58\ud835\udc52subscript\ud835\udc5b\ud835\udc56\ud835\udc51token_{id}italic_t italic_o italic_k italic_e italic_n start_POSTSUBSCRIPT italic_i italic_d end_POSTSUBSCRIPT = 0 to L0subscript\ud835\udc3f0L_{0}italic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT\u00a0do Although model compression has been a popular field for years, how to prune multimodal models is still under-explored. Many established pruning methods and rules that perform well on models with single modality cannot achieve acceptable results on multimodal models. Our project is able to provide a satisfactory solution that outperforms SOTA pruning strategies. By utilizing the cross-attention layers, CATP takes into account of all information from different modalities when computing the importance of query tokens and is able to make prune decisions that are more accurate than other pruning methods. In addition, CATP also proposes a new way of understanding the relationship between different modality inputs by applying voting strategies in analyzing cross-attention probabilities.In this section, we first describe the setups of our experiments including models, datasets and benchmarks. Then we show that CATP performs better than the SOTA methods.We evaluated CATP on the Blip-2 (blip2-opt-2.7b) model and applied it to the Visual Question Answering (VQA) dataset with a sampling rate of 10%. This subset was deemed adequate to assess the performance of our method. In our approach, we computed importance scores in two distinct ways to investigate their impact on model performance:For the first variant of CATP, importance scores were computed utilizing cross-attention probabilities across all attention layers of QFormer.For the second variant, importance scores were derived exclusively from the first attention layer\u2019s cross-attention probabilities.We compared our approach against two baselines for a comprehensive performance analysis:L2-norm Baseline: This method selects query tokens with the largest L2 norm under the assumption that tokens with higher magnitudes are more informative for the task.Self-attention Baseline: It selects query tokens with the highest self-attention scores as per the algorithm proposed in the SpAtten[2] paper.In the subsequent subsections, we present a comparative analysis of CATP against the established baselines.The results of applying CATP to the VQA dataset are presented in Table 1. These results illustrate the performance of CATP in comparison to the baselines and highlight the improvements achieved through the application of cross-attention based pruning.CATP demonstrates a performance improvement of up to 6.6X over the L2-norm baseline and up to 12.1X over the self-attention baseline. These improvements are particularly notable when pruning on the last layer\u2019s cross-attention probabilities. The difference between two variants of our methods is particularly interesting. In later section, we will study the factor of layer importance.Figure 4 shows an example of the model output for the visual question answering task before and after the model being pruned at different level.These results emphasize the potential of cross-attention mechanisms in enhancing model performance in query-token pruning tasks.In an effort to further refine the query-token pruning process of CATP, we have advanced our method to account for image-token importance. This enhancement is informed by the premise, as postulated by the SpAtten algorithm, that tokens with larger self-attention scores carry greater significance. Under this new scheme, image tokens are assigned varying weights derived from normalizing their self-attention scores, essentially allowing for a weighted voting mechanism in the pruning process.The implementation of this advanced version utilizes the last attention layer of the visual encoder exclusively. The weighted algorithm is applied in conjunction with the first variant of our method, which leverages self-attention scores from all attention layers of QFormer. This approach is designed to harmonize the depth of attention insights across the model with the individual contribution of each image token, thereby enhancing the overall pruning strategy.The results of incorporating image-token importance weighting are depicted in the table 2, which compares the performance of the standard cross-attention pruning (variant 1 in Table 1) against the performance when image patch importance weighting is applied.The data indicates that image-token importance weighting provides a notable improvement in model accuracy, particularly at more aggressive pruning ratios.Further experiments illustrate that the information from cross-attention layers is not uniformly valuable for CATP.We evaluate inference accuracy on 10% of the VQA dataset when using a single cross-attention layer as the input for CATP. Considering the BLIP-2 model comprises 6 cross-attention layers, we run CATP for each layer and measure the accuracy at various pruning levels (retaining 1212\\frac{1}{2}divide start_ARG 1 end_ARG start_ARG 2 end_ARG, 1414\\frac{1}{4}divide start_ARG 1 end_ARG start_ARG 4 end_ARG, and 1818\\frac{1}{8}divide start_ARG 1 end_ARG start_ARG 8 end_ARG of the tokens).Figure 5 displays the BLIP-2 inference accuracy following the execution of CATP on each individual cross-attention layer. Our observation reveals that the initial and final cross-attention layers offer more valuable information to CATP, resulting in up to 1.9X, 3.2X, and 2.9X higher inference accuracy compared to utilizing middle layers.Large Multimodal Models. There is a growing emphasis on large multimodal models capable of comprehending diverse types of data modalities, such as text, images, and videos. Examples of such models include Salesforce Research\u2019s BLIP-2 [1], OpenAI\u2019s GPT-4, and Google\u2019s recently introduced Gemini. Given that BLIP-2 is open-source on GitHub and has released its pretrained weights, CATP opts to leverage the BLIP-2 model for evaluating the cross-attention pruning idea.Self-attention based token pruning. SpAtten [2] prunes tokens in NLP models based on self-attention probabilities, calculating token importance scores by aggregating attention probabilities across heads and layers. In contrast, CATP focuses on query-token pruning for multimodal models (e.g., BLIP-2) and employs cross-attention to determine the importance of query-tokens.We introduce CATP, a cross-attention token pruning method aimed at maintaining the accuracy of multimodal model inference. Our approach begins by utilizing cross-attention probabilities to assess token importance. Additionally, we propose a novel voting strategy designed to aggregate cross-attention probabilities and estimate importance scores. In experiments, CATP demonstrates an increase of up to 12.1x times in accuracy on the VQA dataset compared to state-of-the-art solutions, showcasing its effectiveness. Furthermore, our results indicate that incorporating image-token importance and layer-importance into the voting strategy has the potential to further enhance inference accuracy.",
    "14": "Software vulnerabilities are a challenge in cybersecurity. Manual security patches are often difficult and slow to be deployed, while new vulnerabilities are created. Binary code vulnerability detection is less studied and more complex compared to source code, and this has important practical implications. Deep learning has become an efficient and powerful tool in the security domain, where it provides end-to-end and accurate prediction. Modern deep learning approaches learn the program semantics through sequence and graph neural networks, using various intermediate representation of programs, such as abstract syntax trees (AST) or control flow graphs (CFG). Due to the complex nature of program execution, the output of an execution depends on the many program states and inputs. Also, a CFG generated from static analysis can be an overestimation of the true program flow. Moreover, the size of programs often does not allow a graph neural network with fixed layers to aggregate global information. To address these issues, we propose DeepEXE, an agent-based implicit neural network that mimics the execution path of a program. We use reinforcement learning to enhance the branching decision at every program state transition and create a dynamic environment to learn the dependency between a vulnerability and certain program states. An implicitly defined neural network enables nearly infinite state transitions until convergence, which captures the structural information at a higher level. The experiments are conducted on two semi-synthetic and two real-world datasets. We show that DeepEXE is an accurate and efficient method and outperforms the state-of-the-art vulnerability detection methods.\nSoftware vulnerabilities have been an ongoing challenge in the cybersecurity domain. It is an inevitable problem, as the scale of software grows in complexity. Many malicious cyber attacks exploit vulnerabilities within systems and can cause tremendous economical and security damages. Often, the security analysts cannot even patch vulnerabilities fast enough, as new ones are created\u00a0(Alexopoulos et\u00a0al., 2020; Farris et\u00a0al., 2018). Common Vulnerability Exposures (CVE) show that the total number of vulnerabilities more than doubled from 2016 to 2017 and it continued to increase throughout the recent years111Statistics on Common Vulnerabilities and Exposures (CVE) Details. Many traditional static and dynamic analysis methods are manually expensive and inefficient. This motivates automated and end-to-end approaches, such as neural networks.Vulnerabilities can be detected at either the source code or binary code level. Source code provides much more meaningful semantics, syntax, and structures, which in turn help both analysts and machine learning models to track vulnerabilities. Existing methods at the source code level are accurate and capable of finding complex vulnerabilities\u00a0(Li et\u00a0al., 2018; Harer et\u00a0al., 2018). For binary code, as much information is lost during the compilation process, it is much harder to detect vulnerabilities. Moreover, the absence of the original source is a practical problem under many circumstances, such as third-party or off-the-shelf programs. Binary code is best analyzed as assembly code, a form of intermediate representation that provides analysts readable content. Assembly code contains instructions that provide some semantics and structures of the program. In this paper, we are only interested in binary code vulnerability detection, as it is still a prevalent challenge in the security domain.Deep learning methods aim to learn the latent representation of a piece of binary code for classification. Existing works for binary code learning can be categorized into two main streams. The first approach focuses on text-based representation learning to extract the token semantics. The instructions are broken down and embedded into vectors through some unsupervised learning such as Word2Vec\u00a0(Mikolov et\u00a0al., 2013), then these vectors are fed into a sequential deep learning model for classification. Instruction2Vec\u00a0(Lee et\u00a0al., 2019), HAN-BSVD\u00a0(Yan et\u00a0al., 2021), and BVDetector\u00a0(Tian et\u00a0al., 2020) all use this semantic-based approach for detection. The second method involves collecting and aggregating structural information at a higher level. Usually, CFGs are parsed from the assembly code basic blocks, which create dependencies between different blocks of code. This is crucial in vulnerability detection, since programs are complex and hierarchical, and vulnerabilities are often triggered in specific program states. Using only the semantics of instruction tokens are often insufficient. Gemini\u00a0(Xu et\u00a0al., 2017), Diff\u00a0(Liu et\u00a0al., 2018), Order\u00a0(Yu et\u00a0al., 2020), InnerEye\u00a0(Zuo et\u00a0al., 2018), and BinDeep\u00a0(Tian et\u00a0al., 2021) all use graph-based methods for binary code structure embedding.Unfortunately, there are major drawbacks to either approach that can hinder the performance or scalability of the model. The more obvious disadvantage is the scalability when large programs are present. Semantic-based approaches usually introduce a maximum input length, in order to prevent vanishing gradient, especially for large and deep sequence models. Structure-based approaches perform graph neural network (GNN) for aggregating node information. The number of layers dictates the receptive field of the model by performing k\ud835\udc58kitalic_k-hop message passing, thus limiting the amount of global information that can be learned. Both of them need to carefully manage the memory footprint during training.\nThe other drawback is the absence of modelling how programs naturally run. Unlike natural language, programs are executed dynamically. The state of a program can be different, depending on the input and its previous states. By using fixed graph learning techniques, the dynamic nature of the program structure is difficult to capture and thus lead to undesired performance.Given assembly code, one has to respectively find a program execution path that can potentially yield the same final program state. In general, a sound and complete static analysis method generates a representation of the code (i.e., CFG) with overestimation. This means paths created in a graph can potentially never execute. Therefore, learning the topological information solely from the default CFG can be inaccurate and result in a false execution path. Ideally, symbolic execution\u00a0(Baldoni et\u00a0al., 2018; King, 1976) is one of the formal methods that enable one to compare and verify all the possible paths through equivalence checking. However, its applicability is limited, as it requires storing all the possible program states associated with all possible execution paths. This will cause the path explosion problem\u00a0(Xie et\u00a0al., 2009), especially for large functions with loops. Existing works try to address the pathfinding problem statically from an incomplete view, focusing on partial or local structures. For example, DeepBinDiff\u00a0(Duan et\u00a0al., 2020) and InnerEye\u00a0(Zuo et\u00a0al., 2018) match the CFGs based on semi-exhaustive path comparison, which is not scalable, and also misses the iterative graph learning.\nGenius\u00a0(Xu et\u00a0al., 2017), BinGo\u00a0(Chandramohan et\u00a0al., 2016), and Tracelet\u00a0(David and Yahav, 2014) use partial path matching, which lacks robustness when programs are easily altered through artificial means.\nBinaryAI\u00a0(Yu et\u00a0al., 2020) uses graph convolution for message passing. However, this approach does not consider mutually exclusive dependencies among edges, covering invalid paths. The message passing mechanism also assumes a static adjacency matrix, which lacks high-level guidance from a global state.\nThe current research in this domain lacks a dedicated way to simulate the program state transitions along the guided valid execution path, with a focus on a higher order of node neighbourhood proximity.Inspired by symbolic execution for path-finding, we propose a neural network model, DeepEXE, which mimics a program state-guided execution process over the CFG to detect binary code vulnerabilities at the function or file level.\nDeepEXE relies on an execution agent that simulates and learns which direction to take, resulting in simulated paths across different epochs.\nThe combined node embedding represents the program state, and the branching actions guiding the program flow are based on the program state and code semantics of the current node.\nDeepEXE leverages the implicit neural network paradigm, where only the final program state is stored before back-propagation. This enables a large simulation step over the execution flow.\nCompared to the existing methods with only local or partial graph information, DeepEXE enables modelling on the highest global-level view over the execution path.\nOur contributions are as follows:We propose DeepEXE, a neural program execution model over a CFG for binary vulnerability detection. It simulates a semantic-guided decision process for stepping through a given function\u2019s CFG.To simulate the program execution steps over the graph, we propose a learning agent for making branching decisions with an implicit neural network structure for program state transitions. It enables modelling program semantics on a higher level views over the execution path.\nTo address the scalability and limited receptive field of graph neural networks, we use the implicit deep learning paradigm for nearly infinite message passing, which significantly enables global information aggregation in the graph and reduces the memory footprint.We conduct experiments on two semi-synthetic datasets and two real world vulnerability datasets. We compare our methods against several state-of-the-art approaches and show that DeepEXE can consistently outperform the baselines in all scenarios.Vulnerability Detection\nWhile vulnerability detection can be conducted at either the source code or binary code level, we will discuss them together, since most methods can be applied to both levels, with some modifications. Machine learning-based (non-deep learning) methods involve the manual extraction of metrics and the input of these metrics as features\u00a0(Gupta et\u00a0al., 2021; Sultana et\u00a0al., 2021). The metrics can be multi-level and leverage the complexity characteristics of a program, such as the number of nested loops within a function. Manual feature extraction is more expensive and requires expert knowledge. Also, the features need to be constantly updated to accommodate changes in the codebase. Text-based deep learning is very popular for source code vulnerability detection, where different granularity levels can be leveraged in order to obtain text features or embeddings. Li et al. group tokens based on semantics and syntax into slices or gadgets\u00a0(Li et\u00a0al., 2021, 2018; Zou et\u00a0al., 2019), and feed them into a LSTM model. For binary code, Instruction2Vec\u00a0(Lee et\u00a0al., 2017) and Bin2img\u00a0(Lee et\u00a0al., 2019) use instruction embedding as a preprocessing step. Similar to Word2Vec, the embedding contains contextual dependency and can be used to detect vulnerabilities at a later stage, which is a 1D CNN model. These models solely focus on the semantics of the tokens, where the structural information is omitted. There are several GNN models at the source code that use different graphs that can be parsed from source code, such as abstract syntax trees, data dependence graphs, and control flow graphs\u00a0(\u015eahin et\u00a0al., 2022; Zhou et\u00a0al., 2019; Cao et\u00a0al., 2021). For GNN message passing, there are multiple styles that we will discuss next.Graph Neural Networks and Implicit Models\nIn binary code, GNN methods aim at learning the structures by first parsing the assembly code into control flow graphs and performing message passing. There are multiple variants related to graph neural networks. The pioneer works of graph neural networks are mostly associated with recurrent graph neural networks \u00a0(Gori et\u00a0al., 2005; Scarselli et\u00a0al., 2008; Gallicchio and Micheli, 2010; Li et\u00a0al., 2015; Dai et\u00a0al., 2018), where the node representations are aggregated with a fixed set of parameters. Convolutional graph neural networks\u00a0(Kipf and Welling, 2016; Hamilton et\u00a0al., 2017; Veli\u010dkovi\u0107 et\u00a0al., 2017) expand the GNN by using multiple layers with different parameters. This approach addresses the cyclic mutual dependencies architecturally\u00a0(Wu et\u00a0al., 2020) and is more efficient and powerful. However, GNNs struggle to capture long-range dependencies in large graphs, due to the finite number of message passing iterations. One potential solution is the recently studied implicit neural networks. The implicit learning paradigm is different from traditional deep learning, as it solves the solution for a given equilibrium problem, which is formulated as an nearly infinite layer network. Implicit models have previously shown success in domains such as sequence learning\u00a0(Bai et\u00a0al., 2019), physics engine\u00a0(de\u00a0Avila Belbute-Peres et\u00a0al., 2018), and graph neural networks \u00a0(Gu et\u00a0al., 2020).In this section, we define some necessary notation involving our learning problem, including the input and output. We provide further discussion about the graph neural network and the reinforcement learning in Appendix\u00a0A.4.CFGs and Basic Blocks The input of the model is a binary file in assembly code. The assembly functions and their CFGs are both obtained from the IDA Pro disassembler\u00a0222IDA Pro. Each function is regarded as a graph \ud835\udca2\ud835\udca2{\\mathcal{G}}caligraphic_G that contains segmented code blocks called basic blocks, which are sequences of instructions without any jump or call to other blocks. As the input to the neural network, a graph \ud835\udca2=(\ud835\udc7d,\ud835\udc68)\ud835\udca2\ud835\udc7d\ud835\udc68{\\mathcal{G}}=({\\bm{V}},{\\bm{A}})caligraphic_G = ( bold_italic_V , bold_italic_A ) has the blocks \ud835\udc7d\u2208\u211dn\u00d7v\ud835\udc7dsuperscript\u211d\ud835\udc5b\ud835\udc63{\\bm{V}}\\in\\mathbb{R}^{n\\times v}bold_italic_V \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n \u00d7 italic_v end_POSTSUPERSCRIPT with n\ud835\udc5bnitalic_n nodes, v\ud835\udc63vitalic_v tokens, and the adjacency matrix \ud835\udc68\u2208\u211dn\u00d7n\ud835\udc68superscript\u211d\ud835\udc5b\ud835\udc5b{\\bm{A}}\\in\\mathbb{R}^{n\\times n}bold_italic_A \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n \u00d7 italic_n end_POSTSUPERSCRIPT. \ud835\udc68\ud835\udc68{\\bm{A}}bold_italic_A defines all directed edges within the graph and is obtained by extracting call statements between the blocks. Note that \ud835\udc68\ud835\udc68{\\bm{A}}bold_italic_A has 0 across the diagonal element and is non-symmetrical. Moreover, we apply the re-normalization trick to \ud835\udc68\ud835\udc68{\\bm{A}}bold_italic_A\u00a0(Kipf and Welling, 2016), in order to prevent numerical instabilities during deep network training. For file level classification, we merge the function graphs as a whole, based on the function call information. Moreover, additional information, such as comments and names, are removed. The basic block \ud835\udc7d\ud835\udc7d{\\bm{V}}bold_italic_V only contains operations and operands of instructions.Problem Statement We define several neural network modules within our architecture F=(FS,FI,FA)\ud835\udc39subscript\ud835\udc39\ud835\udc46subscript\ud835\udc39\ud835\udc3csubscript\ud835\udc39\ud835\udc34{F}=({F_{S}},F_{I},{F_{A}})italic_F = ( italic_F start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT , italic_F start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT , italic_F start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT ), where FSsubscript\ud835\udc39\ud835\udc46F_{S}italic_F start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT is the sequential model for semantics embedding, FIsubscript\ud835\udc39\ud835\udc3cF_{I}italic_F start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT is the implicit graph neural network model for structure and node embedding, and FAsubscript\ud835\udc39\ud835\udc34F_{A}italic_F start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT is the reinforcement learning agent for dynamic pathing optimizer, given certain program states. The goal is to predict whether each function contains a vulnerability. Given the input graph \ud835\udca2=(\ud835\udc7d,\ud835\udc68)\ud835\udca2\ud835\udc7d\ud835\udc68{\\mathcal{G}}=({\\bm{V}},{\\bm{A}})caligraphic_G = ( bold_italic_V , bold_italic_A ), our model learns several levels of information and aggregates them together for the final output of the model, which is a binary classification score F:\ud835\udca2\u2192y^\u2208\u211d:\ud835\udc39\u2192\ud835\udca2^\ud835\udc66\u211dF:{\\mathcal{G}}\\rightarrow\\hat{y}\\in\\mathbb{R}italic_F : caligraphic_G \u2192 over^ start_ARG italic_y end_ARG \u2208 blackboard_R. Formally, we define the following learning task parameterized by \u03b8\ud835\udf03\\thetaitalic_\u03b8:We design the DeepEXE architecture with semantic-driven and execution-guided principles.\nCFGs extracted from disassembly contain crucial information about the program logic and paths, which dictates the outputs and functionalities of assembly code. An important characteristic to differentiate CFGs from graphs in other domains, such as social networks or chemistry, is that node states should be dependent on the execution logic. Programs are executed following specific orders based on the dependencies among the edges conditioned by the program state, where the results and semantics can substantially differ when orders vary.\nWe borrow the idea of symbolic execution\u00a0(Baldoni et\u00a0al., 2018) and create a neural CFG executor.\nA training epoch contains a full iteration of the executive session, which corresponds to a concrete execution path.\nNote that each epoch can have completely different execution paths, as the model learns. The overall architecture is shown in Appendix\u00a0A.1.\nAn example of the learning process is shown in Figure\u00a01.\nThe training consists of many training epochs. In Figure\u00a01, the path for Epoch 1 goes into a loop, while Epoch 2 directly goes into the exit point.\nThe execution agent performs multiple steps within an epoch. It starts from the entry node, then transitions to other possible nodes in each step.\nThe decision on which branch to select depends on the program state X\ud835\udc4bXitalic_X, and\nXjisubscriptsuperscript\ud835\udc4b\ud835\udc56\ud835\udc57X^{i}_{j}italic_X start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT indicates the updated program state at step i\ud835\udc56iitalic_i for node j\ud835\udc57jitalic_j. After jumping to the next node, the agent updates the program state and repeats the decision process until it reaches an equilibrium state.In this section, we discuss the preprocessing, embedding, and sequential learning task. A basic block contains a stream of instructions, which can be further broken down into operations and operands, and be tokenized. We treat the entire block as one sentence and apply a subword and unigram\u00a0(Kudo, 2018; Kudo and Richardson, 2018) model for the token encoding, which mitigates the out-of-vocabulary problem.\nAssembly code is compiler dependent and can easily result in out-of-vocabulary (OOV) tokens. A way to address the OOV issue is to break down the tokens into characters for encoding. Even with a fixed vocabulary size, unseen tokens can be encoded by matching the subword to their closest known tokens. Moreover, it is not language dependent and can be trained from scratch very efficiently.\nWe increase the subspace representation power by simply applying an embedding layer E:\ud835\udc7d\u2192\u211dn\u00d7v\u00d7h:\ud835\udc38\u2192\ud835\udc7dsuperscript\u211d\ud835\udc5b\ud835\udc63\u210eE:{\\bm{V}}\\rightarrow\\mathbb{R}^{n\\times v\\times h}italic_E : bold_italic_V \u2192 blackboard_R start_POSTSUPERSCRIPT italic_n \u00d7 italic_v \u00d7 italic_h end_POSTSUPERSCRIPT next, where h\u210ehitalic_h is the hidden dimension. Note that we use h\u210ehitalic_h as the hidden dimension throughout the paper for simplicity, but different dimensions can be used for any layers in practice. The sequential model used in this task is a bi-directional GRU\u00a0(Chung et\u00a0al., 2014). The output of the GRU layer \ud835\udc7c\u2208\u211dn\u00d7v\u00d7h\ud835\udc7csuperscript\u211d\ud835\udc5b\ud835\udc63\u210e{\\bm{U}}\\in\\mathbb{R}^{n\\times v\\times h}bold_italic_U \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n \u00d7 italic_v \u00d7 italic_h end_POSTSUPERSCRIPT further embeds the token semantics by taking contextual information into account. In order to obtain a representation for the entire basic block, a maximum or average pooling along the time dimension is used to compute \ud835\udc7c\u2208\u211dn\u00d7h\ud835\udc7csuperscript\u211d\ud835\udc5b\u210e{\\bm{U}}\\in\\mathbb{R}^{n\\times h}bold_italic_U \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n \u00d7 italic_h end_POSTSUPERSCRIPT for block embedding.Program State Initial node representation U\ud835\udc48Uitalic_U establishes the semantics within basic blocks, but it is not sufficient to simply globally aggregate U\ud835\udc48Uitalic_U for a high-level representation of the graph. In this regard, a reinforcement agent at\u2062(st\u22121)superscript\ud835\udc4e\ud835\udc61superscript\ud835\udc60\ud835\udc611a^{t}(s^{t-1})italic_a start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_s start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT ) that decides the next execution path is defined, given the previous program state st\u22121superscript\ud835\udc60\ud835\udc611s^{t-1}italic_s start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT. Unlike traditional neural networks that perform forward and backward pass one at a time, our approach internally loops through multiple states t\ud835\udc61titalic_t within a training epoch. We define the program state as a linear transformation of the node state Xtsuperscript\ud835\udc4b\ud835\udc61X^{t}italic_X start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT, where X0=Usuperscript\ud835\udc4b0\ud835\udc48X^{0}=Uitalic_X start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT = italic_U, and some trainable parameter Ws\u2208\u211dh\u00d71subscript\ud835\udc4a\ud835\udc60superscript\u211d\u210e1W_{s}\\in\\mathbb{R}^{h\\times 1}italic_W start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_h \u00d7 1 end_POSTSUPERSCRIPT:Agent Reparameterization Due to the backpropagation algorithm, categorical variables are hard to train in this stochastic environment in the neural network. This layer effectively becomes non-differentiable when using normal sampling process such as argmaxargmax\\operatorname*{argmax}roman_argmax. A solution is to use the Gumbel softmax\u00a0(Jang et\u00a0al., 2016) to re-parameterize the state while maintaining the ability to backpropagate efficiently during training. Gumbel softmax is a continuous and differentiable distribution that can sample categorical distribution, it is given by:where zitsuperscriptsubscript\ud835\udc67\ud835\udc56\ud835\udc61z_{i}^{t}italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT is the sample drawn from the state, gi\u223cG\u2062u\u2062m\u2062b\u2062e\u2062l\u2062(0,1)similar-tosubscript\ud835\udc54\ud835\udc56\ud835\udc3a\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc5901g_{i}\\sim Gumbel(0,1)italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u223c italic_G italic_u italic_m italic_b italic_e italic_l ( 0 , 1 ) are samples drawn i.i.d from the Gumbel distribution, and \u03c4\ud835\udf0f\\tauitalic_\u03c4 is the temperature controlling the discreteness of the new samples. Gumbel softmax works better with a lower value for \u03c4\u2208[0,\u221e]\ud835\udf0f0\\tau\\in[0,\\infty]italic_\u03c4 \u2208 [ 0 , \u221e ] as it approaches to argmaxargmax\\operatorname*{argmax}roman_argmax smoothly, whereas setting a large value makes the samples become uniform.Adjacency Matrix Update In each state update, the agent walks through the graph with updated program state to capture the intermediate execution path that leads to certain results.\nWe have the flexibility to design the agent to be either hard or soft. A soft agent at=ztsuperscript\ud835\udc4e\ud835\udc61superscript\ud835\udc67\ud835\udc61a^{t}=z^{t}italic_a start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT = italic_z start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT preserves the probabilities drawn from Gumbel softmax, which implies that a program information can flow in different execution paths at the same time based on the probabilities \u2211izit=1subscript\ud835\udc56subscriptsuperscript\ud835\udc67\ud835\udc61\ud835\udc561\\sum_{i}z^{t}_{i}=1\u2211 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_z start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1. A hard agent mimics the execution path and is one-hot, leading to one strictly one execution at a time.\nThe agent at\u2208\u211dn\u00d71superscript\ud835\udc4e\ud835\udc61superscript\u211d\ud835\udc5b1a^{t}\\in\\mathbb{R}^{n\\times 1}italic_a start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n \u00d7 1 end_POSTSUPERSCRIPT is then used to select a path and generate the state-dependent adjacency matrix \ud835\udc68~tsuperscript~\ud835\udc68\ud835\udc61\\tilde{{\\bm{A}}}^{t}over~ start_ARG bold_italic_A end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT, which is updated as: \ud835\udc68~t=\ud835\udc68\u2062atsuperscript~\ud835\udc68\ud835\udc61\ud835\udc68superscript\ud835\udc4e\ud835\udc61\\tilde{{\\bm{A}}}^{t}={\\bm{A}}a^{t}over~ start_ARG bold_italic_A end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT = bold_italic_A italic_a start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT.\nImplicit GNN With the updated adjacency matrix from the agent, one can perform graph neural network on the CFG to aggregate neighbour information into the nodes. However, assembly code can be large for various reasons. For example, a GCC compiler can use an optimization level that minimizes the execution size and reduces the size of CFGs. While GNN is a suitable approach to learn the structural dependency of a function, it requires a pre-defined number of layers, where each layer usually performs 1-hop message passing. Intuitively, the vanilla GNNs do not scale well with large graphs and can fail to capture global information. The dependency between further nodes can be crucial to understand the overall semantics of a program. Such long range dependency is difficult to capture with longer edges.\nTo alleviate the above stated problem, we perform the program state transitions in an implicitly defined style. In general, the transition at state t\ud835\udc61titalic_t can be written as an implicit form of the GNN layer:\nSuch form of layer does not explicitly output a vector to be fed into the next layer. Instead, it uses a fixed point iteration in equation\u00a04 that aims to find the equilibrium vector state X*superscript\ud835\udc4bX^{*}italic_X start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT as t\u2192\u221e\u2192\ud835\udc61t\\rightarrow\\inftyitalic_t \u2192 \u221e. The equilibrium state is then used for the prediction task in equation\u00a05, where f\u03c8subscript\ud835\udc53\ud835\udf13f_{\\psi}italic_f start_POSTSUBSCRIPT italic_\u03c8 end_POSTSUBSCRIPT is an output function parameterized by \u03c8\ud835\udf13\\psiitalic_\u03c8 for the desired classification task.\nWith the reinforcement agent embedded in the updated adjacency matrix \ud835\udc68~*=\ud835\udc68~t:t\u2192\u221e:superscript~\ud835\udc68superscript~\ud835\udc68\ud835\udc61\u2192\ud835\udc61\\tilde{{\\bm{A}}}^{*}=\\tilde{{\\bm{A}}}^{t}:t\\rightarrow\\inftyover~ start_ARG bold_italic_A end_ARG start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT = over~ start_ARG bold_italic_A end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT : italic_t \u2192 \u221e, our equilibrium solution is formulated as follows:W\u2208\u211dh\u00d7h\ud835\udc4asuperscript\u211d\u210e\u210eW\\in\\mathbb{R}^{h\\times h}italic_W \u2208 blackboard_R start_POSTSUPERSCRIPT italic_h \u00d7 italic_h end_POSTSUPERSCRIPT and \u03a9\u2208\u211dh\u00d7h\u03a9superscript\u211d\u210e\u210e\\Omega\\in\\mathbb{R}^{h\\times h}roman_\u03a9 \u2208 blackboard_R start_POSTSUPERSCRIPT italic_h \u00d7 italic_h end_POSTSUPERSCRIPT are parameters, and U\ud835\udc48Uitalic_U is the initial node feature.\nNote that only a single layer is required to produce the updated node representation X\ud835\udc4bXitalic_X iteratively instead of multiple stacking layers. We also inject U\ud835\udc48Uitalic_U into the equation through some affine transformation b\u03a9subscript\ud835\udc4f\u03a9b_{\\Omega}italic_b start_POSTSUBSCRIPT roman_\u03a9 end_POSTSUBSCRIPT. This ensures that original node semantics is preserved throughout the iterations when solving for the fixed point\u00a0(Bai et\u00a0al., 2019).Fixed Point Acceleration Although the equilibrium point can be obtained from iterating equation\u00a06 infinitely, it is not the most efficient and stable method for convergence. More importantly, it does not guarantee convergence. Anderson acceleration\u00a0(Walker and Ni, 2011) is an accelerated algorithm for finding fixed points. Given a function f\ud835\udc53fitalic_f to solve, which is equation\u00a06 in our case, we define (1) mk=m\u2062i\u2062n\u2062{m,t}subscript\ud835\udc5a\ud835\udc58\ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc5a\ud835\udc61m_{k}=min\\{m,t\\}italic_m start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_m italic_i italic_n { italic_m , italic_t } as the parameter for controlling past iteration memory by setting m\ud835\udc5amitalic_m to any positive integer; (2) g\u2062(x)=f\u2062(x)\u2212x\ud835\udc54\ud835\udc65\ud835\udc53\ud835\udc65\ud835\udc65g(x)=f(x)-xitalic_g ( italic_x ) = italic_f ( italic_x ) - italic_x as the residual with the matrix Gt=[gt\u2212mt,\u2026,gt]subscript\ud835\udc3a\ud835\udc61subscript\ud835\udc54\ud835\udc61subscript\ud835\udc5a\ud835\udc61\u2026subscript\ud835\udc54\ud835\udc61G_{t}=[g_{t-m_{t}},...,g_{t}]italic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = [ italic_g start_POSTSUBSCRIPT italic_t - italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT , \u2026 , italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ]. The root solving process using Anderson acceleration is formulated as:Instead of computing for xt+1superscript\ud835\udc65\ud835\udc611x^{t+1}italic_x start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT directly from xtsuperscript\ud835\udc65\ud835\udc61x^{t}italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT, Anderson acceleration solves for a coefficient \u03b1\ud835\udefc\\alphaitalic_\u03b1 in an optimization problem that minimizes the norm of g\u2062(x)\ud835\udc54\ud835\udc65g(x)italic_g ( italic_x ).State Transition Termination The executor terminates in three different scenarios. (1) If the executor reaches the exit point on the CFG, there will not be any updates to Xt+1superscript\ud835\udc4b\ud835\udc611X^{t+1}italic_X start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT after Equation\u00a06, naturally leading to an equilibrium state. (2) If the executor reaches an equilibrium state, but not at the program exit point, it logically indicates that further execution will not result in changes in the program state. Therefore, it is natural to terminate. (3) If the executor reaches a configured maximum steps.\nOnce X*superscript\ud835\udc4bX^{*}italic_X start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT is at equilibrium, we apply layer normalization\u00a0(Ba et\u00a0al., 2016) and global average pooling layer to obtain the graph representation \ud835\udc6e\ud835\udc6e{\\bm{G}}bold_italic_G:The prediction task can be simply computed by a linear transformation to get the logits:We want to emphasize that through the use of an implicitly defined GNN layer, it is no longer required to have multiple stacking GNN layers to achieve higher order node aggregation. Instead, each state transition within the layer effectively performs a message passing, as a normal GNN layer would. This has the benefits of lowering the memory costs, while maintaining the same level of representational power, given similar parameter count. Moreover, the long range dependency issue can be effectively addressed by iterating a nearly infinite number of state transitions.While the forward pass in an implicit network possesses some nice properties for the network discussed earlier, it is not a trivial task to train the backward pass. Traditionally, a neural network contains exact operations with explicitly defined input and output, where the gradients can be computed via chain rule. We first define the loss term l\ud835\udc59litalic_l:F\u03c8subscript\ud835\udc39\ud835\udf13F_{\\psi}italic_F start_POSTSUBSCRIPT italic_\u03c8 end_POSTSUBSCRIPT is the prediction rule that takes the graph embedding \ud835\udc6e\ud835\udc6e{\\bm{G}}bold_italic_G. \u2112\u2062(\u22c5)\u2112\u22c5\\mathcal{L}(\\cdot)caligraphic_L ( \u22c5 ) computes the cross entropy loss and outputs the scalar l\ud835\udc59litalic_l. Using chain rule, the loss can be backpropagated as:The terms \u2202l\u2202\ud835\udc6e\ud835\udc59\ud835\udc6e\\frac{\\partial l}{\\partial{\\bm{G}}}divide start_ARG \u2202 italic_l end_ARG start_ARG \u2202 bold_italic_G end_ARG and \u2202\ud835\udc6e\u2202X*\ud835\udc6esuperscript\ud835\udc4b\\frac{\\partial{\\bm{G}}}{\\partial X^{*}}divide start_ARG \u2202 bold_italic_G end_ARG start_ARG \u2202 italic_X start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT end_ARG can be both computed using any autograd software. However, the term \u2202X*\u2202\u03b8superscript\ud835\udc4b\ud835\udf03\\frac{\\partial X^{*}}{\\partial\\theta}divide start_ARG \u2202 italic_X start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT end_ARG start_ARG \u2202 italic_\u03b8 end_ARG is difficult to compute, since the equilibrium point X*superscript\ud835\udc4bX^{*}italic_X start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT is obtained through iterative root finding. If we unroll this computation graph, the network needs to store all intermediate gradients for every state transition. Depending on the number of transitions, it is not a practical approach. Instead, we write X*superscript\ud835\udc4bX^{*}italic_X start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT in its implicitly defined form:where FIsubscript\ud835\udc39\ud835\udc3cF_{I}italic_F start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT denotes the implicit graph neural network. By taking the derivative with respect to \u03b8\ud835\udf03\\thetaitalic_\u03b8, we obtain:By applying the chain rule on the right hand side of equation\u00a014, we expand it into the following:At this point, both \u2202FI\u2062(X*,U)\u2202\u03b8subscript\ud835\udc39\ud835\udc3csuperscript\ud835\udc4b\ud835\udc48\ud835\udf03\\frac{\\partial F_{I}(X^{*},U)}{\\partial\\theta}divide start_ARG \u2202 italic_F start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT ( italic_X start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT , italic_U ) end_ARG start_ARG \u2202 italic_\u03b8 end_ARG and \u2202FI\u2062(X*,U)\u2202X*subscript\ud835\udc39\ud835\udc3csuperscript\ud835\udc4b\ud835\udc48superscript\ud835\udc4b\\frac{\\partial F_{I}(X^{*},U)}{\\partial X^{*}}divide start_ARG \u2202 italic_F start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT ( italic_X start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT , italic_U ) end_ARG start_ARG \u2202 italic_X start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT end_ARG can again be obtained using autograd software. The last unknown term \u2202X*\u2062(\u03b8)\u2202\u03b8superscript\ud835\udc4b\ud835\udf03\ud835\udf03\\frac{\\partial X^{*}(\\theta)}{\\partial\\theta}divide start_ARG \u2202 italic_X start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_\u03b8 ) end_ARG start_ARG \u2202 italic_\u03b8 end_ARG is computed by solving the linear system. In our approach, we use Anderson acceleration to iteratively solve this term.Through implicit differentiation, we directly evaluate the gradient at the equilibrium point. We avoid the computation of any intermediate state transition and can efficiently backpropagate through the network, even with a nearly infinite number of transitions. This also has a better memory footprint.\nIn this section, we demonstrate the ability of DeepEXE on predicting binary code vulnerability in a variety of scenarios. To properly evaluate DeepEXE, we conduct experiments using two semi-synthetic datasets and two real world datasets. The NDSS18333https://samate.nist.gov/SRD/index.php, Software Assurance Reference Dataset and Juliet Test Suites444https://samate.nist.gov/SARD/test-suites, NIST Test Suites are both semi-synthetic datasets commonly used as for vulnerability detection tasks. Though the practical implications for a method should not solely depend on the synthetic results, as they are less complex. For real world datasets that are larger and can contain less trivial vulnerabilities, we employ the FFmpeg555https://ffmpeg.org/, FFmpeg and Esh\u00a0(David et\u00a0al., 2016) datasets. The details of the datasets can be found in Appendix A.2. For the baseline methods, we inherit the results reported in previous works, due to the large amount of experiments and different setups. The evaluation metrics reported include accuracy, precision, recall, F1 score, and area under the ROC curve (AUC). We randomly split each dataset into 75% for training and 25% for evaluation. Some metrics are not shown in the baselines because of their absence in the original works.For each dataset, we compare DeepEXE with the benchmarks that are also evaluated on the same dataset due to limited space. The details of each baseline is described in Appendix A.3. Additionally, we build baseline models that work inherently well in this task including bi-directional LSTM (Bi-LSTM)\u00a0(Hochreiter and Schmidhuber, 1997) and graph convolution network (GCN)\u00a0(Kipf and Welling, 2016).Semi-Synthetic Results We first analyze the results for the NDSS18 dataset shown in Table.\u00a01. The two baselines we implemented (Bi-LSTM and GCN) have surprisingly comparable results with the benchmarks, including MDSAE-NR and TDNN-NR. All the MDSAE-based methods have imbalanced precision and recall, where the models tend to overestimate the vulnerable code. DeepEXE has the best overall performance, leading the accuracy and AUC by \u00a03%. Moreover, DeepEXE is a CFG-based method and we empirically show that by adding the execution-guided agent and expanding the receptive field of graph convolution, it is able to capture more topological information. Note that even in a scenario where the recall metric is highly important, the classification threshold can always be adjusted to accommodate the balance between precision and recall. We are also able to outperform VulDeePecker, which is a source code level method that only leverages the sequential information of the code gadget, potentially omitting the much useful topological knowledge of the source code.The Juliet dataset evaluation is shown in Table\u00a02. As a synthetic dataset, the test cases contain much shorter code. However, there are over 100 different CWEs among all test cases. In reality, a detection tool should be robust enough to detect unseen or zero-day vulnerabilities. It is useful for evaluating the robustness and generalizability of an approach. DeepEXE shows nearly perfect detection accuracy and AUC for this dataset. This shows that even with the single-layer design, DeepEXE is able to generalize well enough. As the graphs are usually small in these test cases, the execution paths generated by static analysis are likely more accurate. Therefore, we believe the implicit GNN contributes more to the performance increase than the agent in this case.Real CVE Results We evaluate the FFmpeg dataset shown in Table\u00a03, which specifies the code levels and input types. Since Devign detects vulnerabilities at the source code level, it is significantly easier with the rich semantics, syntax, and structures. DeepEXE is able to outperform most of the approaches, even at the binary code level. In particular, when only using the CFG as input, DeepEXE achieves better accuracy than both the Devign and GGRN models. Devign-composite utilizes multiple input graphs, such as AST, DFP and NCS. These additional graphs are usually only available for source code. DeepEXE shows its capability at detecting vulnerabilities for real-world and complex programs. Moreover, source code CFGs are less complicated to generate, whereas binary CFGs often can be an over-estimation of the true control flow. With our execution-guided approach, we limit the errors caused by such approximation, while maintaining a high level of global information. The receptive field of GNN in DeepEXE is practically unlimited, allowing us to accommodate for much larger graphs.Lastly, we show the evaluation results for the Esh dataset in Table\u00a04. Due to the extreme imbalance of labels distribution, which is the case in many real-life scenarios, the Bi-LSTM and GCN baselines have lower recalls. The recall metric is important when there are fewer vulnerable cases. DeepEXE, on the other hand, is able to distinguish vulnerable code from non-vulnerable code, given the small number of positive labels. Note that the class weight is not manually adjusted during training, as it is cumbersome and inefficient to tune it for every dataset in practice. With over 90% percision, DeepEXE is able to identify 95% of the vulnerable CVE cases. Similar to FFmpeg, although many cases in the Esh dataset contain a large number of nodes, DeepEXE is inherently designed to handle such large graphs and outperform other baselines.We have proposed DeepEXE, a control flow execution-guided deep learning framework for binary code vulnerability detection. Given the importance of binary code learning, we address two major gaps in the existing research works, which are the lack of modelling program state transition and scalability for large graphs. Instead of assuming the CFG is accurate, which is often not the case, due to the over-estimation from static analysis, we use a reinforcement agent to guide the execution of a program flow that mimics the behaviour of dynamic analysis. DeepEXE is able to capture certain program state transitions that lead to specific vulnerability results, creating a higher dependency between the output and internal node state and topological information.\nWe also show the benefits of training an implicitly defined network, which are directly obtaining the gradients for the equilibrium point and mitigating the heavy memory footprint in large networks. In the experiments, we demonstrate that DeepEXE outperforms all state-of-the-art vulnerability detection methods for the NDSS18 and Juliet datasets. DeepEXE is also very competitive in detecting real world CVEs, even when compared to source code level methods, which are less difficult, given the amount of available information. Overall, DeepEXE is a robust and accurate tool for binary vulnerability detection.In the future, there are several potential directions to grow for DeepEXE. First of all, the training time is slower than for the traditional neural network, due to the many iterations for obtaining equilibrium. This can be improved by using more sophisticated solvers to reduce the number of steps for equilibrium computation. Next, DeepEXE does not have to be restricted to vulnerability detection in the cybersecurity domain. For other security tasks, such as binary code similarity comparison or malware detection, matching the graph structures of malicious programs is often done using GNN. By modifying the training objective, DeepEXE can be used for a lot more of supervised and unsupervised tasks. Moreover, as long as the input data has some form of graphical structures, we can apply the same design to many other domains, such as social network and chemistry studies.We show the overall architecture including the input preprocessing, semantics learning, state transition, and predictions and training in Figure\u00a02.The hardware used for the experiments includes a RTX6000 GPU, Intel Xeon Gold 5218 CPU, and 64GB of memory. The main software used includes Python 3.9.10 and PyTorch 1.10.2 on Ubuntu 20.04.3 LTS.Semi-Synthetic Datasets include the NDSS18 dataset and Juliet Test Suite. The NDSS18 dataset is a derivation from the National Institute of Standards and Technology (NIST): NVD666https://nvd.nist.gov/, National Institute of Standards and Technology and the Software Assurance Reference Dataset (SARD) project777https://samate.nist.gov/SRD/index.php, Software Assurance Reference Dataset. NDSS18 was first published by\u00a0(Li et\u00a0al., 2018) as a source code vulnerability dataset and later compiled to binary code by\u00a0(Le et\u00a0al., 2018) for binary level detection. It includes a total of 32,281 binary functions that are compiled using Windows and Linux. There are two types of Common Weakness Enumerations (CWEs)888https://cwe.mitre.org/, Common Weakness Enumeration (CWE) in NDSS18: CWE119 and CWE399. Juliet Test Suite is a collection of 81,000 test cases in C/C++ and Java from NIST999https://samate.nist.gov/SARD/test-suites, NIST Test Suites that contain 112 different CWEs. Both datasets have nearly balanced distributions for the labels.Real CVE Datasets include the FFmpeg vulnerabilities and Esh datasets, which are both extracted from real world applications or open-source libraries. The codebase is significantly larger than the ones in semi-synthetic datasets. Vulnerabilities are often harder to detect in these programs, due to the much increased complexity. FFmpeg101010https://ffmpeg.org/, FFmpeg is an open-source suite of libraries written in C for handling media files, such as video and audio. It was first used in source code vulnerability detection\u00a0(Zhou et\u00a0al., 2019), where the authors manually collected and labelled the data for various vulnerability commits on Github. We compile the FFmpeg source code provided by the authors into binary code and obtain 16,494 binary functions, where 7,257 are vulnerable and 9,237 are non-vulnerable. The Esh dataset contains CVE cases collected by David et al.\u00a0(David et\u00a0al., 2016), which include 8 different CVEs: cve-2014-0160, cve-2014-6271, cve-2015-3456, cve-2014-9295, cve-2014-7169, cve-2011-0444, cve-2014-4877, and cve-2015-6826. In total, there are 3,379 cases and only 60 are vulnerable. The distribution of vulnerability in the Esh dataset is highly imbalanced, which represents a more realistic scenario.NDSS18 baselines Maximal Divergence sequential Autoencoder (MDSAE) was proposed by\u00a0(Le et\u00a0al., 2018) and uses a deep representation learning approach. The model aims to discriminate the vulnerable and non-vulnerable code by forcing it to be maximally divergent. The input to MDSAE is a sequence of binary code instructions. We report the three best variants in this paper. MMDSAE\u00a0(Albahar, 2020) is a modified version inspired by MDSAE and uses a similar approach that adds a regularization technique. The authors propose two variants, namely MDSAE-NR and TDNN-NR, that have similar performance. The last baseline we include for this dataset is VulDeePecker\u00a0(Li et\u00a0al., 2018), which is a source code vulnerability detection. Instead of using binary code instructions, VulDeePecker takes source code gadgets as input, which are blocks of code with tightly associated semantics. Although this is a source code evaluation, the underlying dataset used is the same. We therefore include VulDeePecker as one of the baselines. It also only uses sequences of code as input and does not consider any topological information from the code graphs.Juliet baselines Bin2Vec\u00a0(Arakelyan et\u00a0al., 2020) is a graph-based binary vulnerability detection approach that utilizes graph convolution network by taking the CFG as input. Instruction2Vec\u00a0(Lee et\u00a0al., 2017) is a representation learning approach to embed the assembly instructions into vectors and apply the downstream vulnerability detection task. The instruction embedding is similar to Word2Vec, it utilizes different parts of an instruction and combines them as a single vector. The downstream vulnerability detection is achieved by training a CNN or Text-CNN using the vectors. The same authors later proposed an updated version of Instruction2Vec\u00a0(Lee et\u00a0al., 2019) and includes a few variants, including Word2Vec and Binary2Img. All Instruction2Vec related methods use the assembly instructions as input and do not consider the structural information.FFmpeg baselines For the FFmpeg dataset, we compared DeepEXE to Devign\u00a0(Zhou et\u00a0al., 2019), which provides the source code for FFmpeg. Devign uses a gated graph recurrent network (GGRN)\u00a0(Ruiz et\u00a0al., 2020) as a graph learning technique. Unlike binary code, where only CFG can be extracted, Devign detects vulnearbilities at the source code level. It takes several intermediate graph representations of source code, such as AST, CFG, DFG, and NCS. We include several variants of Devign, such as Bi-LSTM, GGRN with CFG or all graphs, and Devign with CFG or all graphs. Note that we directly compare our results on the binary code with the original results of Devign, which are based on source code.Esh baselines To the best of our knowledge, Esh is not used in any other papers for vulnerability detection evaluation. The original paper that provided this dataset evaluates it at the basic block level and focuses on code matching. Therefore, we compare DeepEXE to the Bi-LSTM and GCN baselines we implemented ourselves.For the baseline methods, we directly inherit the results reported in previous works, due to the large amount of experiments and different setups. The evaluation metrics reported include accuracy, precision, recall, F1 score, and area under the ROC curve (AUC). Cross-validation is used for tuning hyperparameters in order to obtain optimal accuracy and reasonable memory usage. We use a universal hidden dimension of 64, learning rate of 0.01 with the Adam optimizer\u00a0(kingma2014adam), dropout rate of 0.5, batch size of 192, and a maximum iteration of 50 for the Anderson acceleration solver. We randomly split each dataset into 75% for training and 25% for evaluation. Some metrics are not shown in the baselines because of their absence in the original works. The hardware used for the experiments includes a RTX6000 GPU, Intel Xeon Gold 5218 CPU, and 64GB of memory. The main software used includes Python 3.9.10 and PyTorch 1.10.2 on Ubuntu 20.04.3 LTS.\nNDSS18 baselines Maximal Divergence sequential Autoencoder (MDSAE) was proposed by\u00a0(Le et\u00a0al., 2018) and uses a deep representation learning approach. The model aims to discriminate the vulnerable and non-vulnerable code by forcing it to be maximally divergent. The input to MDSAE is a sequence of binary code instructions. We report the three best variants in this paper. MMDSAE\u00a0(Albahar, 2020) is a modified version inspired by MDSAE and uses a similar approach that adds a regularization technique. The authors propose two variants, namely MDSAE-NR and TDNN-NR, that have similar performance. The last baseline we include for this dataset is VulDeePecker\u00a0(Li et\u00a0al., 2018), which is a source code vulnerability detection. Instead of using binary code instructions, VulDeePecker takes source code gadgets as input, which are blocks of code with tightly associated semantics. Although this is a source code evaluation, the underlying dataset used is the same. We therefore include VulDeePecker as one of the baselines. It also only uses sequences of code as input and does not consider any topological information from the code graphs.Juliet baselines Bin2Vec\u00a0(Arakelyan et\u00a0al., 2020) is a graph-based binary vulnerability detection approach that utilizes graph convolution network by taking the CFG as input. Instruction2Vec\u00a0(Lee et\u00a0al., 2017) is a representation learning approach to embed the assembly instructions into vectors and apply the downstream vulnerability detection task. The instruction embedding is similar to Word2Vec, it utilizes different parts of an instruction and combines them as a single vector. The downstream vulnerability detection is achieved by training a CNN or Text-CNN using the vectors. The same authors later proposed an updated version of Instruction2Vec\u00a0(Lee et\u00a0al., 2019) and includes a few variants, including Word2Vec and Binary2Img. All Instruction2Vec related methods use the assembly instructions as input and do not consider the structural information.FFmpeg baselines For the FFmpeg dataset, we compared DeepEXE to Devign\u00a0(Zhou et\u00a0al., 2019), which provides the source code for FFmpeg. Devign uses a gated graph recurrent network (GGRN)\u00a0(Ruiz et\u00a0al., 2020) as a graph learning technique. Unlike binary code, where only CFG can be extracted, Devign detects vulnearbilities at the source code level. It takes several intermediate graph representations of source code, such as AST, CFG, DFG, and NCS. We include several variants of Devign, such as Bi-LSTM, GGRN with CFG or all graphs, and Devign with CFG or all graphs. Note that we directly compare our results on the binary code with the original results of Devign, which are based on source code.Esh baselines To the best of our knowledge, Esh is not used in any other papers for vulnerability detection evaluation. The original paper that provided this dataset evaluates it at the basic block level and focuses on code matching. Therefore, we compare DeepEXE to the Bi-LSTM and GCN baselines we implemented ourselves.Graph Neural Network GNN is a topological learning technique for input data with graph structures. A graph is represented as \ud835\udca2=(V,E)\ud835\udca2\ud835\udc49\ud835\udc38{\\mathcal{G}}=(V,E)caligraphic_G = ( italic_V , italic_E ) that contains n:=|V|assign\ud835\udc5b\ud835\udc49n:=|V|italic_n := | italic_V | nodes and e:=|E|assign\ud835\udc52\ud835\udc38e:=|E|italic_e := | italic_E | edges. An edge Ei\u2062j:=(Vi,Vj)assignsubscript\ud835\udc38\ud835\udc56\ud835\udc57subscript\ud835\udc49\ud835\udc56subscript\ud835\udc49\ud835\udc57E_{ij}:=(V_{i},V_{j})italic_E start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT := ( italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) represents the directed or un-directed connection between node (i,j)\ud835\udc56\ud835\udc57(i,j)( italic_i , italic_j ). In practice, the edge information is represented in the form of an adjacency matrix \ud835\udc68\u2208\u211dn\u00d7n\ud835\udc68superscript\u211d\ud835\udc5b\ud835\udc5b{\\bm{A}}\\in\\mathbb{R}^{n\\times n}bold_italic_A \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n \u00d7 italic_n end_POSTSUPERSCRIPT. Generally, one can obtain some initial node embedding U\u2208\u211dn\u00d7h\ud835\udc48superscript\u211d\ud835\udc5b\u210eU\\in\\mathbb{R}^{n\\times h}italic_U \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n \u00d7 italic_h end_POSTSUPERSCRIPT before feeding into the network. The message passing (i.e. node aggregation) is performed at each GNN layer as follows:where Wt\u2208\u211dh\u00d7hsuperscript\ud835\udc4a\ud835\udc61superscript\u211d\u210e\u210eW^{t}\\in\\mathbb{R}^{h\\times h}italic_W start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_h \u00d7 italic_h end_POSTSUPERSCRIPT is a trainable parameter at layer t\ud835\udc61titalic_t. Each message passing step aggregates 1-hop neighbour information into the current node given that an edge exists in \ud835\udc68\ud835\udc68{\\bm{A}}bold_italic_A. The final node vector XTsuperscript\ud835\udc4b\ud835\udc47X^{T}italic_X start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT then learns the topological information from all T\ud835\udc47Titalic_T-hop away neighbours. In case of graph classification, a pooling layer such as add pooling can be used to obtain the graph embedding \ud835\udc6e\ud835\udc6e{\\bm{G}}bold_italic_G:REINFORCE Algorithm Reinforcement learning is a class of algorithms that specify the actions within an environment that optimizes the reward r\ud835\udc5fritalic_r. In particular, the REINFORCE algorithm\u00a0(Williams, 1992) is a form of policy gradient algorithm that computes the stochastic gradient with respect to the reward. It involves a state s\ud835\udc60sitalic_s that can be obtained from a neural network, an agent a\ud835\udc4eaitalic_a that specifies the action space \ud835\udc9c\ud835\udc9c\\mathcal{A}caligraphic_A, and a policy \u03c0\u2062(a|s)\ud835\udf0bconditional\ud835\udc4e\ud835\udc60\\pi(a|s)italic_\u03c0 ( italic_a | italic_s ) that takes the action a\ud835\udc4eaitalic_a given a state s\ud835\udc60sitalic_s with probabilities. Usually, the policy is randomly initialized and the algorithm iterates through epochs, where backpropagation is performed at each epoch to update the policy in the context of a neural network setup.Equation\u00a0(6) needs to have a unique solution X*superscript\ud835\udc4bX^{*}italic_X start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT when iterated infinitely. Such property is called the well-posedness. According to Gu et al.\u00a0(Gu et\u00a0al., 2020), W\ud835\udc4aWitalic_W and \ud835\udc68~~\ud835\udc68\\tilde{{\\bm{A}}}over~ start_ARG bold_italic_A end_ARG are well-posed for \u03d5italic-\u03d5\\phiitalic_\u03d5 when there is a unique solution. First of all, the choice of \u03d5italic-\u03d5\\phiitalic_\u03d5 needs to satisfy the component-wise non-expansive (CONE) property, where most activation functions such as ReLU, Sigmoid, and Tanh, possess such property\u00a0(El\u00a0Ghaoui et\u00a0al., 2021). Then, we need to construct sufficient conditions on W\ud835\udc4aWitalic_W and \ud835\udc68~~\ud835\udc68\\tilde{{\\bm{A}}}over~ start_ARG bold_italic_A end_ARG with a CONE activation function for well-posedness. It is stated that \u2016W\u2016\u221e<\u03ba/\u03bbp\u2062f\u2062(\ud835\udc68~)subscriptnorm\ud835\udc4a\ud835\udf05subscript\ud835\udf06\ud835\udc5d\ud835\udc53~\ud835\udc68||W||_{\\infty}<\\kappa/\\lambda_{pf}(\\tilde{{\\bm{A}}})| | italic_W | | start_POSTSUBSCRIPT \u221e end_POSTSUBSCRIPT < italic_\u03ba / italic_\u03bb start_POSTSUBSCRIPT italic_p italic_f end_POSTSUBSCRIPT ( over~ start_ARG bold_italic_A end_ARG ) needs to be true, where \u2016W\u2016\u221esubscriptnorm\ud835\udc4a||W||_{\\infty}| | italic_W | | start_POSTSUBSCRIPT \u221e end_POSTSUBSCRIPT is the infinity norm, \u03bbp\u2062f\u2062(\ud835\udc68~)subscript\ud835\udf06\ud835\udc5d\ud835\udc53~\ud835\udc68\\lambda_{pf}(\\tilde{{\\bm{A}}})italic_\u03bb start_POSTSUBSCRIPT italic_p italic_f end_POSTSUBSCRIPT ( over~ start_ARG bold_italic_A end_ARG ) is the Perron-Frobenius (PF) eigenvalue\u00a0(Berman and Plemmons, 1994), \u03ba\u2208[0,1)\ud835\udf0501\\kappa\\in[0,1)italic_\u03ba \u2208 [ 0 , 1 ) is the scaling constant. Equation\u00a0(6) then has a unique solution. This is ensured by projecting W\ud835\udc4aWitalic_W in each update to satisfy this condition:\nwhere ||\u22c5||F||\\cdot||_{F}| | \u22c5 | | start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT is the Frobenius norm. Note that even with a gated convolution which results in an updated \ud835\udc68~~\ud835\udc68\\tilde{{\\bm{A}}}over~ start_ARG bold_italic_A end_ARG for every iteration, we still maintain a well-posed \ud835\udc68~~\ud835\udc68\\tilde{{\\bm{A}}}over~ start_ARG bold_italic_A end_ARG as it contains a strictly smaller or equal PF eigenvalue than the original A\ud835\udc34Aitalic_A, given the agent a\ud835\udc4eaitalic_a is non-expansive, resulting in \u03ba/\u03bbp\u2062f\u2062(\ud835\udc68~)\u2265\u03ba/\u03bbp\u2062f\u2062(A)\ud835\udf05subscript\ud835\udf06\ud835\udc5d\ud835\udc53~\ud835\udc68\ud835\udf05subscript\ud835\udf06\ud835\udc5d\ud835\udc53\ud835\udc34\\kappa/\\lambda_{pf}(\\tilde{{\\bm{A}}})\\geq\\kappa/\\lambda_{pf}(A)italic_\u03ba / italic_\u03bb start_POSTSUBSCRIPT italic_p italic_f end_POSTSUBSCRIPT ( over~ start_ARG bold_italic_A end_ARG ) \u2265 italic_\u03ba / italic_\u03bb start_POSTSUBSCRIPT italic_p italic_f end_POSTSUBSCRIPT ( italic_A ).",
    "15": "Intelligent vehicle systems require a deep understanding of the interplay between road conditions, surrounding entities, and the ego vehicle\u2019s driving behavior for safe and efficient navigation. This is particularly critical in developing countries where traffic situations are often dense and unstructured with heterogeneous road occupants. Existing datasets, predominantly geared towards structured and sparse traffic scenarios, fall short of capturing the complexity of driving in such environments. To fill this gap, we present IDD-X, a large-scale dual-view driving video dataset. With 697K bounding boxes, 9K important object tracks, and 1-12 objects per video, IDD-X offers comprehensive ego-relative annotations for multiple important road objects covering 10 categories and 19 explanation label categories. The dataset also incorporates rearview information to provide a more complete representation of the driving environment. We also introduce custom-designed deep networks aimed at multiple important object localization and per-object explanation prediction. Overall, our dataset and introduced prediction models form the foundation for studying how road conditions and surrounding entities affect driving behavior in complex traffic situations.Understanding the influence of road and traffic conditions on ego vehicle\u2019s driving behavior is crucial for enabling explainability in automated driving decision-making. This capability helps in developing reliable and efficient intelligent vehicle systems. Unlike Western countries, developing nations contain dense and unstructured traffic situations, with heterogeneous road occupants (two-wheelers, animals, three-wheelers, etc), and static road objects (speed breakers, potholes, and traffic lights, etc.). Given the complex variety of road occupants and objects, it is important to know which of the multiple road entities influence the ego vehicle\u2019s driving behavior and more importantly, how they do so.Existing datasets capture annotations for identifying important road entities [1], [2], [3], with accompanying explanations in some cases [4], [5], [6], [3]. In general, these datasets contain structured and sparsely populated traffic situations. In most cases, only a single road occupant is responsible for influencing the ego vehicle\u2019s driving behavior. On the contrary, in dense traffic scenarios found in developing countries, it is common to find multiple important road entities simultaneously affecting the ego vehicle\u2019s driving behavior. In structured driving scenarios, most vehicles adhere to traffic laws due to which there is limited variability in the ways they interact with the ego vehicle. However, road occupants usually do not obey the traffic rules in unstructured traffic scenarios. As a result, much more diverse variety of atypical and unexpected interaction patterns tend to exist.To ensure the representation of such scenarios among the datasets, we contribute a large-scale dual-view driving video dataset, IDD-X, captured in dense, heterogeneous, and unstructured traffic environments. The proposed dataset provides ego-relative annotations for multiple important road objects, their corresponding explanation labels, and the ego vehicle\u2019s driving behavior for the duration of the video clip. Drivers routinely utilize rearview information in their assessment of important objects. To account for this important aspect, IDD-X additionally captures road objects perceived in the rearview. By encompassing both front and rear views, IDD-X enables a more comprehensive analysis of driving behavior, providing a panoramic view of objects, their interactions, and the intricate cues that influence the driver\u2019s choices.Harnessing the potential of our IDD-X dataset, we also introduce novel deep network architectures to address two key tasks - (1) multiple important object localization and (2) per object explanation prediction. These tasks serve as foundational components for unraveling the nuanced relationships between road conditions and ego vehicle\u2019s driving behavior in diverse and challenging traffic contexts.\nOur contributions are summarised below.We contribute IDD-X, the first ever driving video dataset with ego-relative annotations for both multiple important object localization and corresponding explanations in dense and unstructured traffic. IDD-X contains 697K bounding boxes, 9K important object tracks, and 1-12 objects per video and comprehensive ego-relative annotations for multiple important road objects (10 categories) and explanations (19 categories).In IDD-X, we provide the first dataset with multi-view important object annotations for ego vehicle\u2019s driving behavior understanding.We introduce custom-designed deep networks for a) multiple important object localization and b) per object explanation prediction with respect to the ego vehicle.\u2714\u2714\u2714 (9K)\u2714\u2714\u2714Categorical-\u2713----17K1-3Textual-\u2713----8K1-3-4-----\u2713---8111For a fair comparison with our dataset, explanations referring to at least one important or action-inducing object are considered and similar explanations for left and right turns are merged into one category.6Categorical-\u2713----4K1-2-2--------154Categorical----------Textual----\u2713\u2713-----Visit our project page https://idd-x.github.io for details.Important Object Identification: Identifying important road entities that influence the ego vehicle\u2019s driving behavior forms an important component of the generated behavior explanations. Approaches for important object identification use the ego vehicle\u2019s trajectory, the object\u2019s trajectory, and the video context information [2] [1]. However, these methods do not consider the category of road objects. Given the heterogeneous road object categories in our dataset, we explicitly incorporate object class data along with trajectory information for predicting its importance.\nImportant Object Explanation: Explaining the ego vehicle\u2019s driving behavior in the light of identified important objects is crucial for reliable usage of vision-based driving models [8] [9] [10] [11]. However, existing approaches [5] [12] [13] [14] do not identify important objects when generating explanations. Specifically, they do not localize road entities that form the basis for the explanation. This introduces ambiguity in the evaluation of generated explanations. The model proposed by Kim et al. [5] and Jing et al. [13] provide pixel-level saliency maps along with explanations. However, these maps do not overlap reliably with causal road entities. Recently proposed approaches [3] [4] provide object-level explanations using an RGB image or its optical flow image as input. However, these image-based methods do not utilize temporal information or trajectory of important objects for explanation prediction. In our proposed approach, we extract spatiotemporal features of important object tracks and utilize them for generating ego vehicle explanations.Datasets: Multiple datasets exist for identifying risk objects\u00a0[3], anomalous objects\u00a0[15], causal objects\u00a0[6], and important objects\u00a0[2, 1] that affect the ego vehicle\u2019s driving behavior. However, the traffic scenarios in these datasets are typically sparse and mostly contain a single important object\u2019s annotation in a driving sequence. Such datasets cannot be used to explain the dense traffic conditions where multiple road objects simultaneously influence the ego\u2019s driving decisions. Although some datasets specifically capture dense traffic scenes [4] [7] [16], they do not provide category and location information for ego-relative important objects. In contrast, our IDD-X dataset has spatial and temporal location annotations for multiple important road objects that explain the ego\u2019s driving situation in dense traffic.\nOur dataset offers explanations for challenging unstructured traffic situations containing complex and unpredictable interaction patterns which are often rare in structured driving scenarios found in existing datasets. Additionally, IDD-X is the first dataset to consider rearview information for important object annotations \u2013 see Table I for a detailed comparison.\nOur dataset IDD-X is specifically designed to explain ego vehicle\u2019s driving behavior due to multiple important road objects. These objects affect the ego vehicle\u2019s driving decision and it is important to know the identity and role of the respective objects. To this end, we utilize front and rearview driving videos captured from vehicle-mounted cameras to assess the ego\u2019s surrounding traffic situation. Temporal modeling of road objects is crucial for describing complex and unpredictable driving maneuvers commonly observed in unstructured traffic. Therefore, individual object-level tracks are included. Our dataset also provides annotations for 10 different road object categories: car, motorcycle, autorickshaw, truck, bus, bicycle, person/animal, speed breaker and damaged road (pothole). If an object does not fall into any of the listed categories, it is assigned to \u2018others\u2019 class.Our dataset considers the following predominant driving situations: a) slowing down on straight roads, b) deviating on straight roads, and c) slowing down on left/right/U-turns. Every important object is associated with an ego-relative explanation during the driving situation.\nThe explanations in IDD-X comprise a set of pre-defined categories that encompass the diversity of complex ego-relative interaction patterns of the heterogeneous road objects in unstructured traffic conditions. We define 19 different explanation categories for the important objects: congestion, obstruction, on-road living being, stopped vehicle, avoid congestion, avoid obstruction, avoid on-road living being, avoid stopped vehicle, merging, cut-in, overtake, confrontation, crossing, slow down, deviate, left turn, right turn, u-turn, red light.A bird\u2019s eye view illustration for each of these explanations in different traffic situations is shown in Figure\u00a01. Each row in the figure corresponds to a group of explanations with similar interaction patterns between the important road objects and the ego vehicle.Row A: Important objects appearing in the front of ego vehicle are in close interaction such that the ego vehicle\u2019s driving path is blocked.Row B: The objects are relatively far such that sufficient driving space is available for the ego vehicle to avoid or move ahead of the object by deviating from its path.Row C: Lateral interactions between the ego vehicle and the important road objects.Row D: Important objects doing a turning maneuver in front of the ego vehicle.Row E: Important objects approaching the ego vehicle such that the ego\u2019s future trajectory is hindered.Row F: Primitive important object explanations whose consideration is crucial for the ego vehicle to make safe and careful driving decisions.Broadly, Rows A, B, and G describe the passive influence of different road objects on ego vehicle\u2019s driving decision. Rows C, D, E, and F define the ego-relative maneuvering styles of the important road objects. Note that the situations in Figure\u00a01 are representative and variants may exist in data. For instance, in Row B, the ego vehicle may avoid the important objects from its left depending on the available driving space.Samples of annotated driving scenarios from our dataset are shown in Figure\u00a02. In scenario S1subscriptS1\\text{S}_{1}S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, the ego vehicle slows down because multiple important road objects (motorcycle, truck, and autorickshaw appearing in front view S1FsuperscriptsubscriptS1F\\text{S}_{1}^{\\text{F}}S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT F end_POSTSUPERSCRIPT) block ego vehicle\u2019s driving path. This shows that ego vehicle\u2019s driving behavior is influenced by the interactions of multiple road objects. Such scenarios are common in dense and unpredictable traffic conditions captured in our dataset. The road objects appearing in the rearview may also impact the ego vehicle\u2019s driving decision. In scenario S1subscriptS1\\text{S}_{1}S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, the ego vehicle may collide with important objects (autorickshaw and motorcycle in rear view S1RsuperscriptsubscriptS1R\\text{S}_{1}^{\\text{R}}S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT R end_POSTSUPERSCRIPT) if ego chooses to deviate from its driving path. The ego vehicle may collide with important object (car in rear view) if it chooses to abruptly slow down. Therefore, both the front and rear views are crucial for comprehensive analysis of the ego vehicle\u2019s driving decisions.We recorded 85 hours of dual-view driving videos from urban, rural, and highway roads in and around Hyderabad city, India. The dataset was captured in day, night, rainy, cloudy, and sunny climates and weather conditions. The front-view and the rear-view videos were captured with camera resolutions of 2560 \u00d7 1440, and 1920 x 1080, recording at 25 fps.We used the open-source CVAT222\nhttps://github.com/openvinotoolkit/cvat tool for annotating the IDD-X dataset. The annotations are designed to capture all the important interactions of the surrounding road occupants when the ego vehicle takes a driving decision.Annotaion procedure: The annotators watch the driving videos and filter out video intervals or scenarios where the ego driver either slows down or deviates from the influence of any road entity. These driving behaviors are called stimulus-driven actions which were initially defined in the HDD dataset [6]. A total of 3635 driving scenarios were filtered out with their durations ranging from 0.3 to 11 seconds. These video intervals with annotated driving behaviors were analyzed by a different group of annotators (experienced drivers) to identify all the important road objects while imagining themselves driving the ego vehicle. The annotators create bounding boxes around such objects and track them by adjusting the bounding boxes across all frames at the frequency of 25Hz for the entire duration of the driving scenario. Next, they re-watch the entire video sequence, to specifically observe the interaction patterns of the important objects and assign suitable explanation labels from the pre-defined set of explanations described before. The annotators refer to the bird\u2019s eye view illustration similar to Figure\u00a01 for identifying the best-fitting explanation category for the road object. The explanation label and the object\u2019s category information are provided as the object\u2019s track attributes.\nThe dataset includes 3635 driving action scenarios containing 8689 important object annotations out of which 6427 appear in the front view and 2262 objects appear in the rear view. Figure\u00a03 shows the distribution of the number of important objects observed per driving scenario. The larger number of observed important objects indicates denser traffic scenarios where 54% of the total scenarios in the dataset constitute more than one important object per scene. The distribution of explanations for heterogeneous road objects is shown in Figure\u00a04. A heavy-tailed distribution of the labels can be observed in the figure.\nGiven the inter-related nature of Important Object Localization and Important Object Explanation Prediction, we introduce a custom-designed multi-task deep network which obtains the related predictions for a given driving video. Figure\u00a05 illustrates the deep network along with essential components and procedures for the aforementioned tasks.In this task, we localize the important objects in a driving situation using their relative motion information. The approach for the task is divided into two steps 1) Multi-Object Tracking for localizing all the road objects observed in the driving scenario, and 2) Important Object Track Identification for predicting the importance of each object track. Multi-Object Tracking: For this sub-task, we first detect all the objects in a driving scene observed from the front view camera and then use a tracking algorithm to track all the detected objects in subsequent video frames. A YOLOv4 detection model\u00a0[17] was applied over all the frames in the front-view driving scenarios to get bounding boxes of all the road occupants. These bounding boxes were provided as input to the SORT tracker\u00a0[18] algorithm for their association across frames to get their unique track-ids. This gives us the tracks of all the objects in the driving scene with their class labels.Important Object Track Identification: This sub-task is formulated as a binary classification problem to predict the importance of an object track obtained from the tracking algorithm. We first prepare the training data with the binary importance labels for all the road objects in the driving scenes. Our dataset contains the ground-truth track information for the important road objects. We use this to label all the predicted object tracks, obtained from the previous sub-task, with the importance class. This is done by matching the predicted tracks with the ground truth tracks using spatial Intersection Over Union (IOU) between their bounding boxes averaged across all frames. The predicted object track is labeled as important if the IOU value is above a threshold otherwise it is marked as unimportant. This labeled training data is now used for importance prediction as described below.The predicted object track\u2019s bounding boxes and its object category are provided as input to an object class conditioned Bi-directional Gated Recurrent Unit (BiGRU) model\u00a0[19] to output the importance class. A BiGRU model helps in effective temporal modeling of the object\u2019s relative trajectory with respect to the ego vehicle. Additionally, it also considers the object category for differentiating between the bounding box size variations of different road objects. This is achieved by providing the encoded information of the object class along with its bounding box track coordinates at every time step inside the BiGRU model. Overall, our custom-designed approach helps in maintaining the relevant spatiotemporal context information throughout the track length. As shown in Figure\u00a05, this model is separately applied over all the object tracks for their importance prediction.37.517.47.30.036.354.871.047.248.6Driving Behavior Recognition: We adopt the TSN model [20] with Resnet-101 backbone for recognizing the driving behaviors in our dataset as it has demonstrated substantial success in video recognition tasks. The model was first pre-trained on the large-scale AVA action recognition dataset\u00a0[21]. The pre-trained model was then used for transfer learning on our dataset with the trimmed video clips of the driving action scenarios.\nThis is illustrated as the Driving Behavior Recognizer module in Figure\u00a05, where the video backbone corresponds to the TSN model\u2019s Resnet backbone with optical flow frames as input and the recognition head corresponds to the TSN\u2019s average consensus head module for action prediction. The optical flow frames are extracted using the TVL1 algorithm [22] implemented in OpenCV.Important Object Track Explanation: We use the ground truth important object tracks along with their explanation labels for training this task. The track bounding boxes were resampled at the frame numbers corresponding to the extracted video features from the flow-based TSN model. The resampled bounding boxes were then used to extract the per-object track video features by applying the Track-Of-Interest Align (TOI-Align) operator\u00a0[23] to the video features.\nApart from this, the global driving action context information was also extracted by the spatial and temporal averaging of the video features from the flow-based TSN model. Finally, the global context information and the averaged per-track video features were concatenated and fed as input to a multi-layer perception (MLP) for explanation prediction. This is demonstrated as the Important Object Explanation Generator module in Figure\u00a05, where it can be seen that multiple tracks video features are simultaneously extracted and fed to the classifier for explanation prediction.We split the IDD-X dataset on the basis of driving action scenarios into train 70%, validation 15%, and test 15% sets. The number of ground truth important object annotations present in these driving scenarios comprised 6697 in training, 1728 in validation, and 264 in test. We consider the videos from the front view for the training and evaluation of all the tasks.Important Object Track Identification:\nThe dataset for this task was prepared using the YOLOv4 detection model\u00a0[17] and the SORT tracking algorithm\u00a0[18]. The YOLOv4 model was pre-trained on the COCO dataset [24] and\nfine-tuned on the IDD dataset [25] for detecting road objects specific to Indian driving scenarios. The tracks for each driving scenario were obtained from the SORT tracker algorithm. A standard threshold of 0.5 was used for the average spatial IOU value between the predicted and the ground-truth object tracks. A total of 10882 predicted important object tracks and 108042 unimportant object tracks were obtained.\nThis data was used for training the BiGRU model with a hidden size of 5, and a single hidden layer. Given the highly skewed nature between the important and the unimportant object classes, we adopted mini-batch class-balanced sampling strategy while training this model with standard cross-entropy loss. A batch size of 32 and a learning rate of 0.001 with Adam optimizer [26] were used for training the BiGRU model until 100 epochs. Utilizing the object class information at all timesteps enabled the best performance of 86.8% Recall and 50.7% F1-Score. Our experiments show that consideration of heterogeneous road object categories throughout the temporal domain enriches the model\u2019s identification capability.Driving Behavior Recognition:\nThe input to the TSN model was randomly cropped video frames resized to 224x224, with random temporal sampling at the frame interval of 1, clip length 5, and number of clips 8.\nThe TSN model was trained with the standard cross-entropy loss, learning rate 0.000125, batch size 6, and for 40 epochs. The per-class recall and the weighted average accuracy of the predicted driving behaviors were used as evaluation metrics for this task.\nTable\u00a0II shows the performances of the TSN driving action recognition models using different modalities: RGB and optical flow. The model trained with optical flow modality shows significantly better performance with a 7.3% improvement in overall accuracy. For specific driving actions, the per-class recalls also show superior performances with 1.9%, 11.4%, and 24% improvements in \u2019Slowdown\u2019, \u2019Deviate\u2019, and \u2019Turn and Slowdown\u2019 classes respectively.\nThis finding is crucial and substantiates our choice to employ the optical flow-based TSN model for feature extraction in the explanation prediction task.Important Object Track Explanation:\nThe per-track video features were extracted using the trained TSN model with 256x224-sized image frames as input instead of 224x224. The frame width is modified so as to maintain the same aspect ratio with respect to the original image after resizing. This ensured unaltered aspect ratios for the track\u2019s bounding boxes as well. The video features extracted from the last layer of the Resnet backbone in the TSN model with spatial dimension 7x13, temporal size 8, and channel size 2048. The track bounding boxes were downsampled to the video feature spatial dimensions before the application of the TOI-Align operator. The output from TOI-Align was 1x1x2048x8 (width x height x channel x time) per-object video features. The global context information obtained from spatial and temporal averaging of the video features was of dimension 1x1x2048x1. The concatenated output, using the global context information and the per-object track features, was of dimension 4096. This was fed as input to an MLP with a hidden layer of size 128. The explanation labels in the training dataset with a total count of less than 100 were not considered for the training and evaluation of this task. Standard cross-entropy loss with learning rate 0.001, and batch size 16, was used for training the MLP classifier until epochs 50. Per-class F1-score, average F1-score, and weighted average F1-score are reported as evaluation metrics for this task.We compare two variants: TOI-Aligned per-track features, and TOI-Aligned per-track features combined with global context information. As observed from the Table\u00a0III, the TOI-Aligned model excels in the \u2019Congestion\u2019, \u2019Avoid Congestion\u2019, and \u2019Overtaking\u2019 categories, with better per-class F1 scores. When the global context information is added, the performance of the tail classes (\u2019Crossing\u2019, \u2019Interfering Being\u2019, \u2019Cut-in\u2019, and \u2019Avoid On-road Being\u2019) improves by a large margin (7.1% for average F1-score, 1% for weighted average F1-score). Overall, the results suggest that inclusion of global context information significantly improves the model\u2019s ability to correctly identify the underprivileged explanation categories. However, for more common driving situations (e.g. \u2018Congestion\u2019 and \u2018Overtaking\u2019) per-track-features-only appear to suffice. The performance in categories such as \u2018Cut-in\u2019 and \u2018Avoid On-road Animal\u2019 is low across both variants, pointing out areas for improvement.Acknowledgement The project is funded by the iHubData and Mobility at IIIT Hyderabad. We thank the data collection and annotation team for their effort.",
    "16": "State-of-the-art large language models (LLMs) have become indispensable tools for various tasks.\nHowever, training LLMs to serve as effective assistants for humans requires careful consideration.\nA promising approach is reinforcement learning from human feedback (RLHF), which leverages human feedback to update the model in accordance with human preferences and\nmitigate issues like toxicity and hallucinations.\nYet, an understanding of RLHF for LLMs is largely entangled with initial design choices that popularized the method and current research focuses on augmenting those choices rather than fundamentally improving the framework.\nIn this paper, we analyze RLHF through the lens of reinforcement learning principles to develop an understanding of its fundamentals,\ndedicating substantial focus to the core component of RLHF\u2014the reward model.\nOur study investigates modeling choices, caveats of function approximation, and their implications on RLHF training algorithms, highlighting the underlying assumptions made about the expressivity of reward.\nOur analysis improves the understanding of the role of reward models and methods for their training, concurrently revealing limitations of the current methodology.\nWe characterize these limitations, including incorrect generalization, model misspecification, and the sparsity of feedback, along with their impact on the performance of a language model.\nThe discussion and analysis are substantiated by a categorical review of current literature, serving as a reference for researchers and practitioners to understand the challenges of RLHF and build upon existing efforts.Large Language Models (LLMs) demonstrate remarkable capabilities that extend beyond basic language tasks, leading to their widespread adoption across various industries.\nThe remarkable utility of these models holds the potential to transform established workflows in critical sectors such as technology, healthcare, finance, and education\u00a0Singhal et\u00a0al. [2022]; Wu et\u00a0al. [2023a]; Yan et\u00a0al. [2023].\nAs they become integral to these domains, it\u2019s crucial to ensure that the behavior of LLMs is predictable, safe, and trustworthy\u2014\u2013meeting the expectations set for a human performing the same tasks.\nThis challenge of making LLMs exhibit human-like qualities, known as alignment with human objectives, is central to making these models suitable for diverse tasks.\nAn effective method for addressing this challenge is reinforcement learning from human feedback (RLHF).RLHF first gained popularity due to its ability to solve reinforcement learning (RL) problems like simulated robotic locomotion and playing Atari games Christiano et\u00a0al. [2017] without access to a reward function, by simply leveraging human feedback about preferences on demonstrated behaviors.\nIt has since been adopted for fine-tuning LLMs using human feedback.\nThis leads to a natural inquiry: How can a method designed to master games be effectively used to align LLMs with human objectives?\nThe method has proven to be immensely successful OpenAI [2022], but not without well-documented limitations Casper et\u00a0al. [2023].\nA comprehensive understanding of why it achieves its success remains largely elusive.\nConsequently, research efforts on the topic are stuck in a local minima, with variants focused on augmenting the components of the method\u2014including the training algorithm Ramamurthy et\u00a0al. [2022], reward model Wu et\u00a0al. [2023c], and even RL-free approaches Rafailov et\u00a0al. [2023].\nHowever, some fundamental limitations of the approach remain obscured due to the overarching goal of recent work to refine the initial design choices.In this work, we develop a comprehensive understanding of RLHF by analyzing the core components of the method.\nWe begin the study by motivating the necessity for RLHF by highlighting the problem of objective mismatch in pre-trained LMs (Section 2).\nTo formulate foundational questions about the framework, we adopt a Bayesian perspective of RLHF.\nIt serves to highlight the significance of the reward function in particular (Section 4).\nThe reward function forms the central cog of the RLHF procedure, and the design choices used to model it form a major focus of our study.The current formulation of RLHF relies on a set of assumptions to model the reward function (Section 4.1, 4.2).\nFollowing the delineation of these assumptions, an analysis of the reward model independent of specific modeling choices follows.\nThe analysis, in a principled manner, provides an understanding of issues such as:\nThe impractical requirement for extensive amounts of feedback data for training accurate reward models.The combination of very limited feedback data and the use of function approximation results in misgeneralization, wherein inaccurate reward values are assigned to inputs not seen during training.These imperfections of the reward model, along with challenges such as reward sparsity and reward model misspecification, are highlighted in the paper (Section 5.1).\nTheir impact on the performance of a language model is explored in detail (Section 6.2).\nThe course of the analysis leads to the formalization of concepts such as an oracular reward that serve as the theoretical golden standard for future efforts (Section 4.1).\nAn overview of the RLHF procedure along with the various challenges studied in this work is provided in Figure 1.The discussion is followed by an extensive survey of an expanding body of literature related to the topic.\nThe survey is organized into sections that outline the framework of RLHF.\nStarting with a high-level overview of Large Language Models (LLMs), the survey systematically covers various aspects:Different types of human (and non-human) feedback (Section 7.3),The training methods in RLHF (Section 7.6),Alternative approaches that do not rely on RL or reward models (Section 7.9).This structure aims to provide a comprehensive overview of the extensive landscape of works that have contributed to the remarkable success of RLHF.Large pre-trained language models (PLMs) are massive neural networks that are trained on a huge corpus of texts using a self-supervised learning objective.\nOriginally utilized for representation learning\u00a0Devlin et\u00a0al. [2019]; Liu et\u00a0al. [2019] with encoder-only models, recent research, particularly influenced by Brown et\u00a0al. [2020], has shifted its focus towards training PLMs to directly generate answers for textual problems.\nState-of-the-art PLMs typically employ an auto-regressive transformer architecture Vaswani et\u00a0al. [2017] and are trained with a causal language modeling objective.\nThese models implicitly capture a conditional probability distribution \u03c0\u03b8subscript\ud835\udf0b\ud835\udf03\\pi_{\\theta}italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT, reflecting the likelihood of sampling the next token after observing a sequence of previous tokens.\nThe probability of a text sequence x:=(x1,\u2026,xT)assign\ud835\udc65subscript\ud835\udc651\u2026subscript\ud835\udc65\ud835\udc47x:=(x_{1},\\ldots,x_{T})italic_x := ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ), under this model is denoted as Pr\u2061(x;\u03c0\u03b8)=\u220ft=1T\u22121\u03c0\u03b8\u2062(xt+1\u2223xt,\u2026,x1)Pr\ud835\udc65subscript\ud835\udf0b\ud835\udf03superscriptsubscriptproduct\ud835\udc611\ud835\udc471subscript\ud835\udf0b\ud835\udf03conditionalsubscript\ud835\udc65\ud835\udc611subscript\ud835\udc65\ud835\udc61\u2026subscript\ud835\udc651\\Pr(x;\\pi_{\\theta})=\\prod_{t=1}^{T-1}\\pi_{\\theta}(x_{t+1}\\mid x_{t},\\ldots,x_{%\n1})roman_Pr ( italic_x ; italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ) = \u220f start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T - 1 end_POSTSUPERSCRIPT italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT \u2223 italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ).\nThe model is trained to estimate the pre-training data generating probability distribution over text sequences by minimizing the (forward) KL divergence between the model\u2019s data-generating distribution and the pre-training data distribution, denoted by Ppre-train\u2062(\u22c5)subscript\ud835\udc43pre-train\u22c5P_{\\text{pre-train}}(\\cdot)italic_P start_POSTSUBSCRIPT pre-train end_POSTSUBSCRIPT ( \u22c5 ).The first term, representing the entropy of Ppre-trainsubscript\ud835\udc43pre-trainP_{\\text{pre-train}}italic_P start_POSTSUBSCRIPT pre-train end_POSTSUBSCRIPT, is independent of \u03b8\ud835\udf03\\thetaitalic_\u03b8 and can be disregarded during optimization.\nConsequently, the objective simplifies to the following cross-entropy minimization form:\nThe expectation is approximated using samples from an unsupervised pretraining text corpus \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic_D, which comprises text sequences sampled from Ppre-trainsubscript\ud835\udc43pre-trainP_{\\text{pre-train}}italic_P start_POSTSUBSCRIPT pre-train end_POSTSUBSCRIPT. This leads us to the following objective:The remarkable property about PLMs lies in the contrast between the simplicity of the training recipe and the remarkable results that they deliver Brown et\u00a0al. [2020].\nSimply capturing language statistics along with scaling up the number of trainable parameters, endows PLMs with robust semantic representations, vast commonsense knowledge, and strong pattern-following capabilities.\nHowever, for adopting PLMs to assist humans with tasks that require an understanding of human intentions and the ability to follow instructions, the simple training recipe of PLMs is insufficient.\nThese models demonstrate a shallow understanding of human intentions, often generating undesirable outputs, including incorrect facts or conveying biased and toxic opinions.Fundamentally, PLMs suffer from an objective mismatch problem: the training-time objective of capturing language statistics does not necessarily align with the deployment-time objective of fulfilling a human user\u2019s specific goals.\nEliminating this mismatch at first glance seems feasible: just train PLMs to optimize for the user objective.\nUnfortunately, for many tasks, it is impossible to express the user objective as an optimization target.\nFor example, when a user\u2019s objective pertains to eliciting humorous responses, establishing specific criteria for objectively evaluating the humor in a generated response becomes an inherently challenging task.There are currently two primary ways to deal with the problem: the behaviorist approach and the cognition-driven approach.\nThe behaviorist approach, implemented by supervised fine-tuning (SFT), aims to replicate observable behaviors that humans perceive as desirable without explicit consideration of the underlying user objective. For instance, if a user desires good summaries of articles, this approach trains a model to imitate examples of good summaries without explicitly defining the criteria for a good summary.\nIn contrast, the cognition-driven approach, implemented by reinforcement learning from human feedback (RLHF), aims to uncover the underlying user objective that governs the observed behaviors. It then updates the model by optimizing the uncovered objective.\nThis approach relies on certain assumptions\u2014which in the case of RLHF are: (i) the user objective can bear the form of a reward function, which can assign a numerical score to behaviors of the model, and (ii) this function can be approximated by a machine learning model (e.g., a neural network).\nRLHF estimates this reward function and updates the PLM via reinforcement learning to optimize for rewards.\nRegardless of the approach, the process of addressing the objective mismatch problem is commonly referred to as the fine-tuning or alignment process.\nPresently, state-of-the-art language models typically initiate this process with the behaviorist approach, followed by the cognition-driven approach.\nRLHF relies on observing human feedback to deduce the (latent) user reward function.\nHuman feedback is provided on the outputs from a language model.\nRLHF assumes that there exists an underlying human reward function that governs the feedback they provide in a particular manner,\ni.e., there exists some mapping from reward to actions of a human.\nSuppose the reward function is being inferred by a model R\u03d5subscript\ud835\udc45italic-\u03d5R_{\\phi}italic_R start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT parameterized by \u03d5italic-\u03d5\\phiitalic_\u03d5.\nAdopting a Bayesian inference perspective Korbak et\u00a0al. [2022], the parameters \u03d5italic-\u03d5\\phiitalic_\u03d5 can be viewed as a hypothesis with the dataset of human feedback \ud835\udc9fHFsubscript\ud835\udc9fHF\\mathcal{D}_{\\text{HF}}caligraphic_D start_POSTSUBSCRIPT HF end_POSTSUBSCRIPT as the evidence for this hypothesis.\nGiven a prior distribution over the hypothesis Pr\u2061(\u03d5)Pritalic-\u03d5\\Pr(\\phi)roman_Pr ( italic_\u03d5 ), we can apply Bayes\u2019 rule to derive the posterior distribution over the hypotheses after observing the evidence as:Reward modeling in RLHF can be seen as computing the maximum a posteriori (MAP) estimate of the parameters of a reward model,The first term (a) is the log-likelihood of the feedback dataset, specifying how a human\u2019s internal objective (reward function) governs their feedback.\nThe second term (b) represents constraints on the hypothesis space, which is enforced through explicit and implicit regularization techniques in neural-network training.The presented framework raises two major questions:What is the form of the likelihood function Pr\u2061(\ud835\udc9fHF\u2223R\u03d5)Prconditionalsubscript\ud835\udc9fHFsubscript\ud835\udc45italic-\u03d5\\Pr(\\mathcal{D}_{\\text{HF}}\\mid R_{\\phi})roman_Pr ( caligraphic_D start_POSTSUBSCRIPT HF end_POSTSUBSCRIPT \u2223 italic_R start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT )? In other words, how do we mathematically model the influence of a human\u2019s latent objective on their observable feedback?\nWhat is the reinforcement learning algorithm used for optimizing the reward model? In other words, how do we ensure the model acts consistently with its objective?A set of answers to these questions forms the basis for an RLHF algorithm.\nThe RLHF methodology, popularized by Christiano et\u00a0al. [2017], employs pairwise ranking feedback and uses the Bradley-Terry model\u00a0Bradley and Terry [1952] as the likelihood function.\nProximal Policy Optimization (PPO)\u00a0Schulman et\u00a0al. [2017] is elected as the reinforcement learning algorithm.Before we move into the analysis of this method, we urge the readers to take a moment to reflect on the choices and assumptions we have made so far to derive the general recipe of RLHF.\nAre there alternative choices?\nCan the assumptions be relaxed or improved?\nThinking critically about these foundational decisions is the key to understanding the strengths and weaknesses of RLHF algorithms and innovating them.\nFor example, the recently proposed direct preference optimization (DPO) approach\u00a0Rafailov et\u00a0al. [2023] replaces reinforcement learning with a reformulation of the objective.\nNext, we formalize the problem setup of text generation as an agent interacting with a sequential decision process, laying the foundation for the analysis of RLHF.\nWe refer the reader to Section 7.6 for a detailed outline of the RLHF procedure, and Figure 5 for a summarized overview.In this section, we formulate the text generation procedure from a language model as a sequential decision-making process.\nThis formulation is essential for constructing reinforcement learning algorithms.A common framework for modeling sequential decision-making processes is Markov Decision Process (MDP) Markov [1954].\nAn MDP is defined as a tuple (\ud835\udcae,\ud835\udc9c,p,R,\u03c1)\ud835\udcae\ud835\udc9c\ud835\udc5d\ud835\udc45\ud835\udf0c(\\mathcal{S},\\mathcal{A},p,R,\\rho)( caligraphic_S , caligraphic_A , italic_p , italic_R , italic_\u03c1 ) where \ud835\udcae\ud835\udcae\\mathcal{S}caligraphic_S is the set of states, \ud835\udc9c\ud835\udc9c\\mathcal{A}caligraphic_A is the set of actions, p:\ud835\udcae\u00d7\ud835\udc9c\u2192\u0394\u2062(\ud835\udcae):\ud835\udc5d\u2192\ud835\udcae\ud835\udc9c\u0394\ud835\udcaep:\\mathcal{S}\\times\\mathcal{A}\\to\\Delta(\\mathcal{S})italic_p : caligraphic_S \u00d7 caligraphic_A \u2192 roman_\u0394 ( caligraphic_S ) is the transition function, R:\ud835\udcae\u00d7\ud835\udc9c\u2192\u211d:\ud835\udc45\u2192\ud835\udcae\ud835\udc9c\u211dR:\\mathcal{S}\\times\\mathcal{A}\\to\\mathbb{R}italic_R : caligraphic_S \u00d7 caligraphic_A \u2192 blackboard_R is the reward function, and \u03c1:\ud835\udcae\u2192\u0394\u2062(\ud835\udcae):\ud835\udf0c\u2192\ud835\udcae\u0394\ud835\udcae\\rho:\\mathcal{S}\\to\\Delta(\\mathcal{S})italic_\u03c1 : caligraphic_S \u2192 roman_\u0394 ( caligraphic_S ) is the initial state distribution.\nEach sequential time step of the process is denoted by t\ud835\udc61titalic_t, and st,at,rtsubscript\ud835\udc60\ud835\udc61subscript\ud835\udc4e\ud835\udc61subscript\ud835\udc5f\ud835\udc61s_{t},a_{t},r_{t}italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT denote the values of the state, action, and reward at time step t\ud835\udc61titalic_t.\nA discounting factor \u03b3\u2208(0,1]\ud835\udefe01\\gamma\\in(0,1]italic_\u03b3 \u2208 ( 0 , 1 ] is defined for discounting rewards over time, particularly useful for modeling an MDP with an infinite number of time steps (i.e., an infinite-horizon MDP).\nHowever, the outputs of language models are truncated after a finite number of steps.\nWe use T\ud835\udc47Titalic_T to denote the maximum time step.An agent acts in an MDP using a policy \u03c0:\ud835\udcae\u2192\u0394\u2062(\ud835\udc9c):\ud835\udf0b\u2192\ud835\udcae\u0394\ud835\udc9c\\pi:\\mathcal{S}\\rightarrow\\Delta(\\mathcal{A})italic_\u03c0 : caligraphic_S \u2192 roman_\u0394 ( caligraphic_A ).\nThe agent starts in state s1\u223c\u03c1\u2062(\u22c5)similar-tosubscript\ud835\udc601\ud835\udf0c\u22c5s_{1}\\sim\\rho(\\cdot)italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u223c italic_\u03c1 ( \u22c5 ).\nAt time step t\ud835\udc61titalic_t, it chooses an action at\u223c\u03c0(\u22c5\u2223st)a_{t}\\sim\\pi(~{}\\cdot\\mid s_{t})italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u223c italic_\u03c0 ( \u22c5 \u2223 italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ), executes the action, transitions to a new state st+1\u223cp(\u22c5\u2223st,at)s_{t+1}\\sim p(~{}\\cdot\\mid s_{t},a_{t})italic_s start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT \u223c italic_p ( \u22c5 \u2223 italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ), and receives a reward rt=R\u2062(st,at)subscript\ud835\udc5f\ud835\udc61\ud835\udc45subscript\ud835\udc60\ud835\udc61subscript\ud835\udc4e\ud835\udc61r_{t}=R(s_{t},a_{t})italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_R ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ).\nThe term \u201cMarkov\u201d in MDP refers to the Markov property, in that the distribution over the next state st+1subscript\ud835\udc60\ud835\udc611s_{t+1}italic_s start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT depends on only the current state stsubscript\ud835\udc60\ud835\udc61s_{t}italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and action atsubscript\ud835\udc4e\ud835\udc61a_{t}italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT.For simplicity, we consider text generation tasks that include only one turn of interaction between the user and the model.\nWe make a distinction between the text that a user inputs into the model, denoted by c\ud835\udc50citalic_c and referred to as the context or the prompt, and the text that the model generates by itself to the context, denoted by o\ud835\udc5coitalic_o and referred to as the output or simply the generated text.Let V\ud835\udc49Vitalic_V be the set of all tokens that the model can generate (the vocabulary), \ud835\udc9e\ud835\udc9e\\mathcal{C}caligraphic_C the set of all possible contexts, and \ud835\udcaa\ud835\udcaa\\mathcal{O}caligraphic_O the set of all possible outputs.\nGiven a context c\u2208\ud835\udc9e\ud835\udc50\ud835\udc9ec\\in\\mathcal{C}italic_c \u2208 caligraphic_C as input, the model generates an output o\u2208\ud835\udcaa\ud835\udc5c\ud835\udcaao\\in\\mathcal{O}italic_o \u2208 caligraphic_O token by token.\nSpecifically, let otsubscript\ud835\udc5c\ud835\udc61o_{t}italic_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT be the t\ud835\udc61titalic_t-th token in generated output o\ud835\udc5coitalic_o, then the model parameterized by \u03b8\ud835\udf03\\thetaitalic_\u03b8 first outputs token o1\u223c\u03c0\u03b8(\u22c5\u2223c)o_{1}\\sim\\pi_{\\theta}(~{}\\cdot\\mid c)italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u223c italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( \u22c5 \u2223 italic_c ), and then conditioned on the concatenation of o1subscript\ud835\udc5c1o_{1}italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and c\ud835\udc50citalic_c it generates o2\u223c\u03c0\u03b8(\u22c5\u2223[c,o1])o_{2}\\sim\\pi_{\\theta}(~{}\\cdot\\mid[c,o_{1}])italic_o start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u223c italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( \u22c5 \u2223 [ italic_c , italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ] ), and so on.We can see that this generation process resembles an agent traversing in an MDP (Figure 2).\nThe model acts according to a policy \u03c0\u03b8subscript\ud835\udf0b\ud835\udf03\\pi_{\\theta}italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT.\nThe start-state distribution \u03c1\ud835\udf0c\\rhoitalic_\u03c1 is the distribution over user-provided contexts.\nThe action space is the vocabulary V\ud835\udc49Vitalic_V.\nThe action atsubscript\ud835\udc4e\ud835\udc61a_{t}italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the generated token otsubscript\ud835\udc5c\ud835\udc61o_{t}italic_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT.\nThe state stsubscript\ud835\udc60\ud835\udc61s_{t}italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the concatenation of the context c\ud835\udc50citalic_c and all the tokens the model has generated up to time step t\u22121\ud835\udc611t-1italic_t - 1.\nThe transition function p(\u22c5\u2223st,at)=\u03b4([st,at])p(~{}\\cdot\\mid s_{t},a_{t})=\\delta([s_{t},a_{t}])italic_p ( \u22c5 \u2223 italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = italic_\u03b4 ( [ italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] ) is a delta distribution, i.e., the next state is deterministic given the current state and action.\nReward rtsubscript\ud835\udc5f\ud835\udc61r_{t}italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT given at time step t\ud835\udc61titalic_t is computed by the reward model as R\u03d5\u2062(st,at)subscript\ud835\udc45italic-\u03d5subscript\ud835\udc60\ud835\udc61subscript\ud835\udc4e\ud835\udc61R_{\\phi}(s_{t},a_{t})italic_R start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) which is either a human or a function learned from human feedback.\nThe text generation MDP has several special properties:\nThe action space is extremely large. For example, the LLaMa model\u00a0Touvron et\u00a0al. [2023a, b] employs a vocabulary of size 32K. Having a gigantic action space blows up the search space for reinforcement learning algorithms.The structure of the state space is complex, as a state is essentially a text sequence. Pre-training on large amounts of texts is necessary to learn an initially good representation of this space.The initial state distribution has an enormous support. All conceivable contexts lie in the support, thus strongly testing the ability of the policy to generalize to out-of-distribution states.The reward function used for training can differ from the evaluation reward function.\nThis is because the humans providing rewards during evaluation may be different from the humans involved in trainnig the reward model. Analogous to transfer learning in RL, the agent must then adapt to the new reward function.The transition function is deterministic. Algorithmic and analysis tools tailored for deterministic MDPs can be applied.Thus, solving a text generation MDP requires specialized treatment that takes advantage of its properties and overcomes its inherent challenges.\nReinforcement learning Sutton and Barto [2018]; Bertsekas and Tsitsiklis [1996] provides solutions for optimally solving an MDP, i.e., learning a policy that maximizes the accumulated reward. Consequently, RLHF updates the language model to generate more rewarding outputs. Naturally, the reward function plays a critical role in the process of fine-tuning model outputs, determining practical and fundamental limits Casper et\u00a0al. [2023] of the efficacy of RLHF.The goal of reward learning in RLHF is to convert human feedback into an optimizable reward function.\nThe reward serves a dual purpose:\nit encodes the task information (for example, identical input-output pairs would receive distinct rewards depending on whether the task involved summarization or\ntext expansion)\n111Unless the task is specified in the input prompt itself, in which case the inputs differ.\nas well as preferences over those outputs (a condescending summary is rewarded less than a neutral summary).\nThe reward thus encodes relevant information for measuring (Section 4.3) as well as inducing alignment with human objectives.\nBy setting the reward function of the sequential decision process to the one estimated from human feedback R\u03d5subscript\ud835\udc45italic-\u03d5R_{\\phi}italic_R start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT, reinforcement learning algorithms can be used to learn a language model policy that maximizes the cumulative reward, resulting in an aligned language model.An implicit assumption made in RLHF is that a human\u2019s feedback behavior is governed by and can be represented as an oracular reward function R\u22c6:\ud835\udc9e\u00d7\ud835\udcaa\u2192\u211d:superscript\ud835\udc45\u22c6\u2192\ud835\udc9e\ud835\udcaa\u211dR^{\\star}:\\mathcal{C}\\times\\mathcal{O}\\rightarrow\\mathbb{R}italic_R start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT : caligraphic_C \u00d7 caligraphic_O \u2192 blackboard_R.\nWe assume that this function is deterministic in line with the current methodology.\nThe function takes as input a context c\ud835\udc50citalic_c and an output o\ud835\udc5coitalic_o, and outputs a scalar number reflecting the preference on o\ud835\udc5coitalic_o as a continuation of c\ud835\udc50citalic_c.\nBecause the [c,o]\ud835\udc50\ud835\udc5c[c,o][ italic_c , italic_o ] is essentially a state in the MDP formulation, the reward function is essentially defined over states of the MDP.\nThe language model that maximizes the oracular reward accurately reflects the goals and preferences inherent in the human feedback, and maximization of this reward consequently aligns the model with the human preferences.\nThe oracular reward may not be accessible or learnable, but under the reward hypothesis Sutton [2004]; Silver et\u00a0al. [2021], the mere existence of such a reward may be assumed\u2014though this may be challenged Knox et\u00a0al. [2022]. The oracular reward forms the golden standard for training as well as evaluating any language model.In general, humans can give a variety of feedback.\nRLHF operates with feedback that discloses information about the oracular reward function.\nMost methods focus on two types of feedback: point-wise numerical feedback (or rating), and pairwise ranking feedback (or preferences).\nProviding ratings is the most straightforward way to communicate the reward function.\nGiven a pair (c,o)\ud835\udc50\ud835\udc5c(c,o)( italic_c , italic_o ), the rating is a scalar r=R\u22c6\u2062(c,o)\ud835\udc5fsuperscript\ud835\udc45\u22c6\ud835\udc50\ud835\udc5cr=R^{\\star}(c,o)italic_r = italic_R start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT ( italic_c , italic_o ).\nWhile ratings can be fed directly into a reinforcement learning algorithm, learning a reward model takes advantage of the generalizability of the reward model on unseen outputs and contexts.Preference feedback compares two outputs generated for the same context.\nGiven two outputs o\ud835\udc5coitalic_o and o\u2032superscript\ud835\udc5c\u2032o^{\\prime}italic_o start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT generated for context c\ud835\udc50citalic_c, a human denoted a preference o\u227bo\u2032succeeds\ud835\udc5csuperscript\ud835\udc5c\u2032o\\succ o^{\\prime}italic_o \u227b italic_o start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT if the first input is preferred and o\u2032\u227bosucceedssuperscript\ud835\udc5c\u2032\ud835\udc5co^{\\prime}\\succ oitalic_o start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u227b italic_o otherwise.\nPreferences in their raw form are not compatible learning signals for reinforcement learning algorithms.\nHence, a reward model must be learned for this type of feedback.\nTo do so, an assumption must be made about the relationship between preferences and R\u22c6superscript\ud835\udc45\u22c6R^{\\star}italic_R start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT.\nWe will discuss this in more detail in the next section.\nA discussion about the various methodologies used for encoding preferences can be found in Section 7.5.\nAn alternative approach for ranking outputs on the basis of preferences is provided by the learning-to-rank paradigm Liu et\u00a0al. [2009].Using preference feedback offers several advantages compared to using ratings.\nFirstly, we get more training data for the reward model.\nIn practice, people collect a ranking of N\ud835\udc41Nitalic_N outputs and create preference pairs\u00a0Ouyang et\u00a0al. [2022].\nCollecting N\ud835\udc41Nitalic_N ratings for N\ud835\udc41Nitalic_N outputs provides we get N\ud835\udc41Nitalic_N training points. Ranking N\ud835\udc41Nitalic_N outputs provided N\u2062(N\u22121)/2\ud835\udc41\ud835\udc4112N(N-1)/2italic_N ( italic_N - 1 ) / 2 pairwise comparisons.\nSecond, preferences require assigning a only relative order rather than an absolute precise score to an output; the latter task could take significantly more cognitive effort and is more prone to inconsistency.\nFinally, a preference is presumably easier to provide because it offers a \u201cbaseline\u201d for comparison (the worse output).\nIn contrast, when giving a rating, a human can rely on only the evaluation guidelines.A note on stochastic rewards:\nThe reward function is considered to be a deterministic mapping from text to a scalar value. This amounts to averaging the preferences of all humans that provided human feedback. Moreover, it assumes that a human must always rate an input-output pair with the same score, discounting the inherent variability of human preferences. There are numerous scenarios\u2014like personalization, in-context adaptation to ongoing dialogue, and diverse output generation\u2014where a deterministic mapping is limiting. The rewards are more appropriately modeled as being stochastic, wherein each input-output pair is scored by a distribution over scalar rewards, say r\u223cRhuman(\u22c5\u2223c,o)r\\sim R_{\\text{human}}(\\cdot\\mid c,o)italic_r \u223c italic_R start_POSTSUBSCRIPT human end_POSTSUBSCRIPT ( \u22c5 \u2223 italic_c , italic_o ).\nThis modeling accounts for the two sources of uncertainty: (i) uncertainty over the specific human from a group of humans who provide feedback, and (ii) variability in a human\u2019s preferences due to changes in unobserved factors Nguyen et\u00a0al. [2017].\nSome work in reinforcement learning aims to address this by learning Bayesian preferences, primarily for uncertainty quantification and safety analysis Ramachandran and Amir [2007]; Brown and Niekum [2019], and can be adapted to model a distribution of preferences over text.\nSome recent efforts along these lines Barnett et\u00a0al. [2023] have proven to be effective.\nWe focus on deterministic rewards for the analysis that follows.Learning a reward model serves two purposes: (i) to convert RLHF into a canonical reinforcement learning problem, and (ii) to reduce the cost of online feedback-collection.\nReinforcement learning algorithms define their objective in terms of a reward function.\nTo apply these algorithms, we need to infer a reward function from a feedback dataset, collecting which is notoriously expensive.\nCurrently, large language models require thousands to millions of feedback data points.\nTo gather that amount, many human evaluators need to be recruited to work in parallel.\nTo ensure the assumptions regarding the oracular reward function hold, the evaluators must be trained to agree with one another on the evaluation criteria.\nThis process is continual: multiple rounds of feedback collections need to be conducted to iteratively improve the model.\nThe premise of approaches that learn a reward model is that the generalization error of the reward model is expected to decrease faster than that of the policy as a function of the number of labeled data points, arising from the notion that supervised learning is often considered a simpler problem than generative modeling.\nFollowing the previous section, we denote the reward model by R\u03d5\u2062(c,o)subscript\ud835\udc45italic-\u03d5\ud835\udc50\ud835\udc5cR_{\\phi}(c,o)italic_R start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT ( italic_c , italic_o ) and the feedback dataset by \ud835\udc9fHFsubscript\ud835\udc9fHF\\mathcal{D}_{\\text{HF}}caligraphic_D start_POSTSUBSCRIPT HF end_POSTSUBSCRIPT.\nOur goal is to decide a likelihood function Pr\u2061(\ud835\udc9fHF\u2223\u03d5)Prconditionalsubscript\ud835\udc9fHFitalic-\u03d5\\Pr(\\mathcal{D}_{\\text{HF}}\\mid\\phi)roman_Pr ( caligraphic_D start_POSTSUBSCRIPT HF end_POSTSUBSCRIPT \u2223 italic_\u03d5 )\nand find \u03d5italic-\u03d5\\phiitalic_\u03d5 that maximizes this function:With rating feedback, the reward-modeling problem can be formulated as a prediction problem with continuous output.\nA common objective for this type of problem is the minimization of the mean squared error (MSE):To incorporate preference feedback, we need to choose the form of the likelihood function denoting each preference, i.e., Pr\u2061((o\u227bo\u2032,c)\u2223\u03d5)Prconditionalsucceeds\ud835\udc5csuperscript\ud835\udc5c\u2032\ud835\udc50italic-\u03d5\\Pr((o\\succ o^{\\prime},c)\\mid\\phi)roman_Pr ( ( italic_o \u227b italic_o start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT , italic_c ) \u2223 italic_\u03d5 ).\nThe RLHF method of Ouyang et\u00a0al. [2022] employs the Bradley-Terry model to represent the likelihood of a data point:where \u03c3\u2062(x)=11+e\u2212x\ud835\udf0e\ud835\udc6511superscript\ud835\udc52\ud835\udc65\\sigma(x)=\\frac{1}{1+e^{-x}}italic_\u03c3 ( italic_x ) = divide start_ARG 1 end_ARG start_ARG 1 + italic_e start_POSTSUPERSCRIPT - italic_x end_POSTSUPERSCRIPT end_ARG is the sigmoid function.\nThe learning objective for maximizing the log-likelihood of the dataset \ud835\udc9fHFsubscript\ud835\udc9fHF\\mathcal{D}_{\\text{HF}}caligraphic_D start_POSTSUBSCRIPT HF end_POSTSUBSCRIPT is,In Section 5, we further generalize the form of feedback and the likelihood function to conduct an analysis independent of the specifics of particular design choices.Evaluation of natural language tasks is a difficult problem, and the study of evaluation metrics is an active area of research.\nOf particular importance, and difficulty, is to measure the alignment of a language model to a human\u2019s objectives, which in practice is evaluated along the axes of helpfulness, harmlessness, and honesty.\nThe oracular reward that governs a human\u2019s preferences serves as a yardstick for measuring the degree of alignment.\nThe task of alignment is then reformulated as encoding the preferences demonstrated by a human into a reward function, and updating the parameters of the language model to produce output that maximizes this reward.A reward provides an analytical metric to measure the overall performance of a language model \u03c0\ud835\udf0b\\piitalic_\u03c0, where the performance captures the degree of alignment with human preferences along with the degree of satisfaction of the task itself Ngo et\u00a0al. [2022].\nThe performance of a model \u03c0\ud835\udf0b\\piitalic_\u03c0, for distribution over contexts dC\u2062(\u22c5)subscript\ud835\udc51\ud835\udc36\u22c5d_{C}(\\cdot)italic_d start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT ( \u22c5 ), can be measured by averaging the rewards for the outputs generated by \u03c0\ud835\udf0b\\piitalic_\u03c0 given the contexts. Let the performance be denoted by J\u2062(\u03c0)\ud835\udc3d\ud835\udf0bJ(\\pi)italic_J ( italic_\u03c0 ):The context distribution dC\u2062(\u22c5)subscript\ud835\udc51\ud835\udc36\u22c5d_{C}(\\cdot)italic_d start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT ( \u22c5 ) can be the distribution of contexts in the training data, test data, or a held-out validation dataset, depending on the data on which the performance of the model is being evaluated.\nThe sequential nature of the output generation equivalently allows us to express J\u2062(\u03c0)\ud835\udc3d\ud835\udf0bJ(\\pi)italic_J ( italic_\u03c0 ) as:In practice, most current reward models only provide a reward after the complete output has been generated and Equation 13 reduces to Equation 12. The definition of J\u2062(\u03c0)\ud835\udc3d\ud835\udf0bJ(\\pi)italic_J ( italic_\u03c0 ) uses the oracular reward that is not accessible in practice. An estimate of the performance can obtained from the estimated reward R\u03d5subscript\ud835\udc45italic-\u03d5R_{\\phi}italic_R start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT, by plugging it into Equation 12:The pre-trained language model is denoted by \u03c0pre:\ud835\udc9e\u2192\u0394\u2062(\ud835\udcaa):subscript\ud835\udf0bpre\u2192\ud835\udc9e\u0394\ud835\udcaa\\pi_{\\text{pre}}:\\mathcal{C}\\to\\Delta(\\mathcal{O})italic_\u03c0 start_POSTSUBSCRIPT pre end_POSTSUBSCRIPT : caligraphic_C \u2192 roman_\u0394 ( caligraphic_O ) and the model updated using RLHF by \u03c0rlhf:\ud835\udc9e\u2192\u0394\u2062(\ud835\udcaa):subscript\ud835\udf0brlhf\u2192\ud835\udc9e\u0394\ud835\udcaa\\pi_{\\text{rlhf}}:\\mathcal{C}\\to\\Delta(\\mathcal{O})italic_\u03c0 start_POSTSUBSCRIPT rlhf end_POSTSUBSCRIPT : caligraphic_C \u2192 roman_\u0394 ( caligraphic_O ).\nThe goal of RLHF is to update the parameters of \u03c0rlhfsubscript\ud835\udf0brlhf\\pi_{\\text{rlhf}}italic_\u03c0 start_POSTSUBSCRIPT rlhf end_POSTSUBSCRIPT such that J\u2062(\u03c0rlhf)\u2265J\u2062(\u03c0pre)\ud835\udc3dsubscript\ud835\udf0brlhf\ud835\udc3dsubscript\ud835\udf0bpreJ(\\pi_{\\text{rlhf}})\\geq J(\\pi_{\\text{pre}})italic_J ( italic_\u03c0 start_POSTSUBSCRIPT rlhf end_POSTSUBSCRIPT ) \u2265 italic_J ( italic_\u03c0 start_POSTSUBSCRIPT pre end_POSTSUBSCRIPT ), i.e., as evaluated using the oracular reward. In practice, it is only possible to verify that J^\u2062(\u03c0rlhf)\u2265J^\u2062(\u03c0pre)^\ud835\udc3dsubscript\ud835\udf0brlhf^\ud835\udc3dsubscript\ud835\udf0bpre\\widehat{J}(\\pi_{\\text{rlhf}})\\geq\\widehat{J}(\\pi_{\\text{pre}})over^ start_ARG italic_J end_ARG ( italic_\u03c0 start_POSTSUBSCRIPT rlhf end_POSTSUBSCRIPT ) \u2265 over^ start_ARG italic_J end_ARG ( italic_\u03c0 start_POSTSUBSCRIPT pre end_POSTSUBSCRIPT ), which may be non-informative when the estimated reward model R\u03d5subscript\ud835\udc45italic-\u03d5R_{\\phi}italic_R start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT has inaccuracies for the context-output pairs being evaluated (Section 6.2).In the following sections, we study the properties of the reward estimated from human feedback.\nAs reviewed in Section 7.5, various procedures exist for encoding human feedback into a reward model.\nBoth the form of human feedback and the encoding mechanism continue to be studied further, with the procedures continually evolving and improving.\nCurrently, the most common form of human feedback is pair-wise preference feedback that is encoded into a reward according to the Bradley-Terry model (Section 4.2).\nTo perform an analysis agnostic to specifics of a particular reward learning method,Let feedback denote a general form of sufficiently informative feedback.Let \u03a9\u03a9\\Omegaroman_\u03a9 denote the model of human behavior, or the encoding mechanism, that maps the feedback and the text to a reward value.The generality of this formulation allows the following analysis to cover all existing RLHF-style approaches (for example, RLAIF Bai et\u00a0al. [2022b]) as well as future methods for fine-tuning LLMs, that employ a reward model.Let \ud835\udc9f:={(c,o):c\u2208\ud835\udc9e,o\u2208\ud835\udcaa}assign\ud835\udc9fconditional-set\ud835\udc50\ud835\udc5cformulae-sequence\ud835\udc50\ud835\udc9e\ud835\udc5c\ud835\udcaa\\mathcal{D}:=\\{(c,o):c\\in\\mathcal{C},o\\in\\mathcal{O}\\}caligraphic_D := { ( italic_c , italic_o ) : italic_c \u2208 caligraphic_C , italic_o \u2208 caligraphic_O } denote a hypothetical dataset of all possible contexts and outputs that a language model can encounter, i.e., a humongous dataset of size |\ud835\udc9e|\u00d7|\ud835\udcaa|=|V|T\ud835\udc9e\ud835\udcaasuperscript\ud835\udc49\ud835\udc47|\\mathcal{C}|\\times|\\mathcal{O}|=|V|^{T}| caligraphic_C | \u00d7 | caligraphic_O | = | italic_V | start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT. This dataset cannot be realized in practice and is invoked to shed light on the practical limitations of the existing methodology.\nDenote the dataset of collected human feedback by \ud835\udc9fHF:={(c,o,\ud835\ude8f\ud835\ude8e\ud835\ude8e\ud835\ude8d\ud835\ude8b\ud835\ude8a\ud835\ude8c\ud835\ude94):c\u2208\ud835\udc9eHF,o\u2208\ud835\udcaaHF}assignsubscript\ud835\udc9fHFconditional-set\ud835\udc50\ud835\udc5c\ud835\ude8f\ud835\ude8e\ud835\ude8e\ud835\ude8d\ud835\ude8b\ud835\ude8a\ud835\ude8c\ud835\ude94formulae-sequence\ud835\udc50subscript\ud835\udc9eHF\ud835\udc5csubscript\ud835\udcaaHF\\mathcal{D}_{\\text{HF}}:=\\{(c,o,\\text{{feedback}}):c\\in\\mathcal{C_{\\text{HF}}}%\n,o\\in\\mathcal{O_{\\text{HF}}}\\}caligraphic_D start_POSTSUBSCRIPT HF end_POSTSUBSCRIPT := { ( italic_c , italic_o , feedback ) : italic_c \u2208 caligraphic_C start_POSTSUBSCRIPT HF end_POSTSUBSCRIPT , italic_o \u2208 caligraphic_O start_POSTSUBSCRIPT HF end_POSTSUBSCRIPT } where \ud835\udc9eHF\u2282\ud835\udc9e,\ud835\udcaaHF\u2282\ud835\udcaaformulae-sequencesubscript\ud835\udc9eHF\ud835\udc9esubscript\ud835\udcaaHF\ud835\udcaa\\mathcal{C_{\\text{HF}}}\\subset\\mathcal{C},\\mathcal{O_{\\text{HF}}}\\subset%\n\\mathcal{O}caligraphic_C start_POSTSUBSCRIPT HF end_POSTSUBSCRIPT \u2282 caligraphic_C , caligraphic_O start_POSTSUBSCRIPT HF end_POSTSUBSCRIPT \u2282 caligraphic_O are the subsets of context-output pairs (human-)annotated with feedback.222The subsets are significantly smaller than \ud835\udc9e\ud835\udc9e\\mathcal{C}caligraphic_C and \ud835\udcaa\ud835\udcaa\\mathcal{O}caligraphic_O. Additionally, the feedback can be of any form: ratings, pair-wise feedback, or language feedback (Section 7.3).\nThe reward encoding mechanism that maps context-output pairs along human feedback to rewards (for instance, the Bradley-Terry model) is denoted by \u03a9:(c,o,\ud835\ude8f\ud835\ude8e\ud835\ude8e\ud835\ude8d\ud835\ude8b\ud835\ude8a\ud835\ude8c\ud835\ude94)\u2192\u211d:\u03a9\u2192\ud835\udc50\ud835\udc5c\ud835\ude8f\ud835\ude8e\ud835\ude8e\ud835\ude8d\ud835\ude8b\ud835\ude8a\ud835\ude8c\ud835\ude94\u211d\\Omega:(c,o,\\text{{feedback}})\\to\\mathbb{R}roman_\u03a9 : ( italic_c , italic_o , feedback ) \u2192 blackboard_R.333feedback is overloaded to capture additional mechanism-specific meta-data.\nFor instance, for pair-wise preference, feedback can store the preference relation and the (c,o)\ud835\udc50\ud835\udc5c(c,o)( italic_c , italic_o ) pair compared against.To uncover R\u22c6superscript\ud835\udc45\u22c6R^{\\star}italic_R start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT, it is assumed that \u03a9\u03a9\\Omegaroman_\u03a9 accurately maps back human feedback to the oracular reward, i.e., for sufficiently informative feedback, we haveUnder that assumption, \u03a9\u03a9\\Omegaroman_\u03a9 can operate on \ud835\udc9fHFsubscript\ud835\udc9fHF\\mathcal{D}_{\\text{HF}}caligraphic_D start_POSTSUBSCRIPT HF end_POSTSUBSCRIPT to create a dataset of context-output-reward tuples,\n\ud835\udc9frew={(c,o,r):c\u2208\ud835\udc9eHF,o\u2208\ud835\udcaaHF}subscript\ud835\udc9frewconditional-set\ud835\udc50\ud835\udc5c\ud835\udc5fformulae-sequence\ud835\udc50subscript\ud835\udc9eHF\ud835\udc5csubscript\ud835\udcaaHF\\mathcal{D}_{\\text{rew}}=\\{(c,o,r):c\\in\\mathcal{C_{\\text{HF}}},o\\in\\mathcal{O_%\n{\\text{HF}}}\\}caligraphic_D start_POSTSUBSCRIPT rew end_POSTSUBSCRIPT = { ( italic_c , italic_o , italic_r ) : italic_c \u2208 caligraphic_C start_POSTSUBSCRIPT HF end_POSTSUBSCRIPT , italic_o \u2208 caligraphic_O start_POSTSUBSCRIPT HF end_POSTSUBSCRIPT } where r=R\u22c6\u2062(c,o)\ud835\udc5fsuperscript\ud835\udc45\u22c6\ud835\udc50\ud835\udc5cr=R^{\\star}(c,o)italic_r = italic_R start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT ( italic_c , italic_o ).\nWith \ud835\udc9frewsubscript\ud835\udc9frew\\mathcal{D}_{\\text{rew}}caligraphic_D start_POSTSUBSCRIPT rew end_POSTSUBSCRIPT, learning the reward model R\u03d5subscript\ud835\udc45italic-\u03d5R_{\\phi}italic_R start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT reduces to a regression problem employing a function approximator.\nThe regression problem is however underdetermined Bishop [2006], and consequently multiple R\u03d5subscript\ud835\udc45italic-\u03d5R_{\\phi}italic_R start_POSTSUBSCRIPT italic_\u03d5 end_POSTSUBSCRIPT functions can perfectly fit the training data \ud835\udc9frewsubscript\ud835\udc9frew\\mathcal{D}_{\\text{rew}}caligraphic_D start_POSTSUBSCRIPT rew end_POSTSUBSCRIPT. However, almost all of these functions fail to accurately represent the oracular reward (Figure 3).\n",
    "17": "This study explores object detection in historical aerial photographs of Namibia to identify long-term environmental changes.\nSpecifically, we aim to identify key objects \u2013 Waterholes, Omuti homesteads, and Big trees \u2013 around Oshikango in Namibia using sub-meter gray-scale aerial imagery from 1943 and 1972.\nIn this work, we propose a workflow for analyzing historical aerial imagery using a deep semantic segmentation model on sparse hand-labels. To this end, we employ a number of strategies including class-weighting, pseudo-labeling and empirical p-value-based filtering to balance skewed and sparse representations of objects in the ground truth data.\nResults demonstrate the benefits of these different training strategies resulting in an average F1=0.661subscript\ud835\udc3910.661F_{1}=0.661italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.661 and F1=0.755subscript\ud835\udc3910.755F_{1}=0.755italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.755 over the three objects of interest for the 1943 and 1972 imagery, respectively.\nWe also identified that the average size of Waterhole and Big trees increased while the average size of Omutis decreased between 1943 and 1972 reflecting some of the local effects of the massive post-Second World War economic, agricultural, demographic, and environmental changes. This work also highlights the untapped potential of historical aerial photographs in understanding long-term environmental changes beyond Namibia (and Africa). With the lack of adequate satellite technology in the past, archival aerial photography offers a great alternative to uncover decades-long environmental changes.Index Terms\u2014\u2009\nAerial photos, Geo-spatial machine learning, Climate impact, Sustainability, AfricaSatellite imagery is a valuable source of data that can shed light on the long-term impacts of climate change\u00a0[1].\nHowever, until the launch of IKONOS in 1999, commercial satellite imagery with a spatial resolution of <1\u2062m/pixelabsent1mpixel<1\\text{m}/\\text{pixel}< 1 m / pixel was not available. The spatial resolution of older satellite images is insufficient to uncover detailed and long-term changes for specific areas of interest. Moreover, the archive of satellite imagery does not start early enough to analyze changes such as the massive post-Second World War global transformation \u2013 the Landsat-1 satellite was the first that collected continuous imagery over the Earth starting in 1972\u00a0[2]. In contrast, archival aerial photographs \u2013 widely available since the early 20th century (for military observation, mapping and planning) provide a longer temporal coverage bringing the post-Second World War \u201cSecond Industrial Revolution\u201d or \u201cGreat Acceleration\u201d into focus at sub-meter resolution to monitor subtle changes on the ground in local areas. Massive stocks of historical aerial photos remain underutilized in archives across the globe.\nFor example, the US National Archives preserves 35 million historical aerial photos; tens of millions more are found in private and state archives, store rooms and offices in other countries\u00a0[3].In this work, we aim to utilize archival aerial photos from north-central Namibia, taken in 1943 and 1972, to uncover the decades long changes on the ground predating the introduction of high-resolution satellite imagery. The 1943 aerial photos are assumed to be the first instance where aerial photography technology was systematically\nused in capturing the landscape of northern Namibia\u00a0[4].\nThis region is home to a significant portion of Namibia\u2019s population, but it is highly vulnerable to climate changes due to its semi-arid environment. Individual aerial photos were first digitized, geo-referenced and joined into a large orthomosaic for further machine learning (ML) driven analysis as described in\u00a0[4]. We particularly focused on identifying Waterholes, Omuti homesteads and Big trees.\nWaterholes used to be the main source of water for the population in the dry season, which resulted in a dispersed settlement pattern of Omuti homesteads in the past. Big trees, e.g., marula and palm trees, were main sources of nutrition\u00a0[5].With the encouraging potential of machine learning (ML) algorithms to decipher large collection of data and identify patterns, we employ a deep learning framework that aims to take the digitized aerial photos as input and detected these objects of interest. Specifically, the framework contains a U-Net-based segmentation model\u00a0[6] with a backbone of a pre-trained ResNet\u00a0[7] network. To validate the framework, we utilized a sparsely annotated portion of a 45454545 k\u2062m2\ud835\udc58superscript\ud835\udc5a2km^{2}italic_k italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT area as our train and test region. Once the model was trained, we scaled up the detection stage to identify Waterholes, Omuti homesteads and Big trees in an area of \u22485000absent5000\\approx 5000\u2248 5000 k\u2062m2\ud835\udc58superscript\ud835\udc5a2km^{2}italic_k italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT.\nIn summary, this work offers the following contributions: i.) utilizing aerial photos to identify long-term environmental changes, ii.) a class weighting strategy that jointly optimizes both the sparsity of annotated objects (classes) and the inter-class imbalance, iii.) empirical p-value based post-processing to plausibly select pseudo-labels from the previous prediction stage for a semi-supervised learning strategy.The remainder of the paper is organized as follows. Section\u00a02 presents the methodology, with details on the main contributions. Section\u00a03 describes the experimental setup including the specifics of the datasets used, segmentation model employed and its setting, and evaluation metrics. We present the notable results and follow up discussions in Section\u00a04. Finally, Section\u00a05 concludes the paper with next steps as a future work.The overview of our approach is shown in Fig.\u00a01. Given aerial photos from the Oshikango region in Namibia from 1943 and 1972, we aim to detect specific objects of interest: Big Trees, Omuti homesteads and Waterholes at each of the time stamps to uncover long-term environmental changes. To this end, we employed a pre-processing step that aims to digitize and geo-reference each of the photos and merge them into a large orthomosaic input following the steps in \u00a0[4]. The domain experts annotated a sparse examples of these objects in the subset of the input data (\u224845absent45\\approx 45\u2248 45 k\u2062m2\ud835\udc58superscript\ud835\udc5a2km^{2}italic_k italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) as a ground truth data for our semantic segmentation framework (shown in Fig.\u00a02). Next, we describe the details of the main steps in the framework.Let \ud835\udc03\ud835\udc2dsubscript\ud835\udc03\ud835\udc2d\\bf D_{t}bold_D start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT represents an orthomosaic of multiple aerial photos taken in a year, t\ud835\udc61titalic_t, after each photo is digitized and geo-referenced. The problem is related to evaluating the potential of these aerial photos to quantify the long-term environment changes by detecting a set of objects of interest - b\ud835\udc4fbitalic_b: Big tree, o\ud835\udc5coitalic_o: Omuti and w\ud835\udc64witalic_w: Waterhole at t=1943\ud835\udc611943t=1943italic_t = 1943 and t=1972\ud835\udc611972t=1972italic_t = 1972. To this end, we employ a deep learning framework to detect these objects at each \ud835\udc03\ud835\udc2dsubscript\ud835\udc03\ud835\udc2d\\bf D_{t}bold_D start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT with a dedicated model, \ud835\udeaf\ud835\udc2dsubscript\ud835\udeaf\ud835\udc2d\\bf\\Theta_{t}bold_\u0398 start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT. We assume a few examples of these objects are available as polygons or mask data, \ud835\udc0c\ud835\udc2dsubscript\ud835\udc0c\ud835\udc2d\\bf M_{t}bold_M start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT, annotated by an experienced expert in the region. Each pixel, mi\u2208\ud835\udc0c\ud835\udc2dsubscript\ud835\udc5a\ud835\udc56subscript\ud835\udc0c\ud835\udc2dm_{i}\\in\\bf M_{t}italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 bold_M start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT, assumed to be one of the classes: \ud835\udc9e={b,o,w,u}\ud835\udc9e\ud835\udc4f\ud835\udc5c\ud835\udc64\ud835\udc62\\mathcal{C}=\\{b,o,w,u\\}caligraphic_C = { italic_b , italic_o , italic_w , italic_u }, where u\ud835\udc62uitalic_u represents unknown or background pixels. Due to the sparse nature of annotation performed in a smaller region, i.e., |\ud835\udc0c\ud835\udc2d|)<<|\ud835\udc03\ud835\udc2d|)|\\bf M_{t}|)<<|\\bf D_{t}|)| bold_M start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT | ) < < | bold_D start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT | ), where |\u22c5||\\cdot|| \u22c5 | represents dimension, a key aspect of the framework involves effective usage of \ud835\udc0c\ud835\udc2dsubscript\ud835\udc0c\ud835\udc2d\\bf M_{t}bold_M start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT where the number of labeled pixels, Nksubscript\ud835\udc41\ud835\udc58N_{k}italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, is quite small compared to the unknown pixels, Nusubscript\ud835\udc41\ud835\udc62N_{u}italic_N start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT. Furthermore, there is a large degree of imbalance in annotated pixels among classes b,o\ud835\udc4f\ud835\udc5cb,oitalic_b , italic_o and w\ud835\udc64witalic_w. This also poses a critical question on how to utilize the larger number of u\ud835\udc62uitalic_u pixels in \ud835\udc0c\ud835\udc2dsubscript\ud835\udc0c\ud835\udc2d\\bf M_{t}bold_M start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT thereby assisting the training process and enhancing detection performance. \ud835\udc0c\ud835\udc2dsubscript\ud835\udc0c\ud835\udc2d\\bf M_{t}bold_M start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT is, first, split into train (\ud835\udc0c\ud835\udc2d\ud835\udc2bsuperscriptsubscript\ud835\udc0c\ud835\udc2d\ud835\udc2b\\bf M_{t}^{r}bold_M start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_r end_POSTSUPERSCRIPT) and test (\ud835\udc0c\ud835\udc2d\ud835\udc1esuperscriptsubscript\ud835\udc0c\ud835\udc2d\ud835\udc1e\\bf M_{t}^{e}bold_M start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_e end_POSTSUPERSCRIPT) splits with no overlapping between the two splits. The model, \u0398tsubscript\u0398\ud835\udc61\\Theta_{t}roman_\u0398 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, is trained using \ud835\udc0c\ud835\udc2d\ud835\udc2bsuperscriptsubscript\ud835\udc0c\ud835\udc2d\ud835\udc2b\\bf M_{t}^{r}bold_M start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_r end_POSTSUPERSCRIPT and evaluated for both \ud835\udc0c\ud835\udc2d\ud835\udc2bsuperscriptsubscript\ud835\udc0c\ud835\udc2d\ud835\udc2b\\bf M_{t}^{r}bold_M start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_r end_POSTSUPERSCRIPT and \ud835\udc0c\ud835\udc2d\ud835\udc1esuperscriptsubscript\ud835\udc0c\ud835\udc2d\ud835\udc1e\\bf M_{t}^{e}bold_M start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_e end_POSTSUPERSCRIPT. We further extend \ud835\udc0c\ud835\udc2d\ud835\udc2bsuperscriptsubscript\ud835\udc0c\ud835\udc2d\ud835\udc2b\\bf M_{t}^{r}bold_M start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_r end_POSTSUPERSCRIPT by incorporating new masks derived from predicted polygons from previously unannotated regions in \ud835\udc03\ud835\udc2dsubscript\ud835\udc03\ud835\udc2d\\bf D_{t}bold_D start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT as pseudo-labels.Class imbalance is a common challenge in geospatial machine learning as it is often resource-demanding to do manual annotations, resulting in a sparse set of annotated regions. This is also partly due to the different observation frequencies of objects of interest. For example, in the related aerial photo Oshikango region in this work, we observed a higher occurrence of Big trees compared to Omuti homesteads. In addition, the coverage area of each class may vary (see Table\u00a01) resulting imbalanced number of pixels across classes.\nVarieties of solutions have been employed to address class imbalance challenges over the years that could be clustered under re-sampling\u00a0[8, 9] and re-weighting\u00a0[8, 10]. Re-sampling includes sampling minority classes\u00a0[8], which can lead to overfitting, or under-sampling majority classes\u00a0[9], potentially losing valuable data in cases of extreme imbalance. Data-augmentation also helps to synthetically generate additional samples for minority classes\u00a0[11]. On the other hand, re-weighting assigns adaptive weights to classes often inversely to the frequency of the class\u00a0[8, 10]. Sample-based re-weighting, such as Focal loss, adjusts weights based on individual sample characteristics, targeting well-classified examples and outliers\u00a0[12, 13].\nIn this work, we propose a simple class weighting strategy that considers both the sparsity of annotated regions) (compared to unannotated regions) and the imbalance of pixels annotated across the classes of interest. Our weighting strategy follows a re-weighting approach that aims to provide a class weight that is inversely proportional to the ratio of pixels annotated per each class (compared to the remaining classes). Let Nwsubscript\ud835\udc41\ud835\udc64N_{w}italic_N start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT, Nosubscript\ud835\udc41\ud835\udc5cN_{o}italic_N start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT and Nbsubscript\ud835\udc41\ud835\udc4fN_{b}italic_N start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT be the number of pixels annotated with Waterhole, Omuti and Big tree classes in the training set, \ud835\udc0c\ud835\udc2d\ud835\udc2bsuperscriptsubscript\ud835\udc0c\ud835\udc2d\ud835\udc2b\\bf M_{t}^{r}bold_M start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_r end_POSTSUPERSCRIPT, respectively. The number of unlabeled pixels is denoted by Nusubscript\ud835\udc41\ud835\udc62N_{u}italic_N start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT. The total number of pixels in Mtrsuperscriptsubscript\ud835\udc40\ud835\udc61\ud835\udc5fM_{t}^{r}italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT is Nk+Nusubscript\ud835\udc41\ud835\udc58subscript\ud835\udc41\ud835\udc62N_{k}+N_{u}italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_N start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT, where Nk=Nw+No+Nbsubscript\ud835\udc41\ud835\udc58subscript\ud835\udc41\ud835\udc64subscript\ud835\udc41\ud835\udc5csubscript\ud835\udc41\ud835\udc4fN_{k}=N_{w}+N_{o}+N_{b}italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_N start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT + italic_N start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT + italic_N start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT. Thus, the weight of each class is formulated as follows:\n\u03bbu=Nk/(Nk+Nu)subscript\ud835\udf06\ud835\udc62subscript\ud835\udc41\ud835\udc58subscript\ud835\udc41\ud835\udc58subscript\ud835\udc41\ud835\udc62\\lambda_{u}=N_{k}/(N_{k}+N_{u})italic_\u03bb start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT = italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT / ( italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_N start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ),\n\u03bbw=Nk/Nwsubscript\ud835\udf06\ud835\udc64subscript\ud835\udc41\ud835\udc58subscript\ud835\udc41\ud835\udc64\\lambda_{w}=N_{k}/N_{w}italic_\u03bb start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT = italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT / italic_N start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT,\n\u03bbo=Nk/Nosubscript\ud835\udf06\ud835\udc5csubscript\ud835\udc41\ud835\udc58subscript\ud835\udc41\ud835\udc5c\\lambda_{o}=N_{k}/N_{o}italic_\u03bb start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT = italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT / italic_N start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT, and\n\u03bbb=Nk/Nbsubscript\ud835\udf06\ud835\udc4fsubscript\ud835\udc41\ud835\udc58subscript\ud835\udc41\ud835\udc4f\\lambda_{b}=N_{k}/N_{b}italic_\u03bb start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT = italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT / italic_N start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT,\nTo further improve the efficiency of our training steps, we propose to incorporate the weak labels generated from the inference of the model on the previously un-annotated pixels in training \u2013 i.e. a pseudo-labeling based approach\u00a0[14, 15]. We assume that we have large amounts of unlabeled imagery, however we only have sparse labels (Section\u00a02.1 ). Once the deep semantic segmentation model,\u0398tsubscript\u0398\ud835\udc61\\Theta_{t}roman_\u0398 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, is trained and model parameters W\u03b8tsuperscriptsubscript\ud835\udc4a\ud835\udf03\ud835\udc61W_{\\theta}^{t}italic_W start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT are obtained, the inference is applied on the training set \ud835\udc0c\ud835\udc2d\ud835\udc2bsuperscriptsubscript\ud835\udc0c\ud835\udc2d\ud835\udc2b\\bf M_{t}^{r}bold_M start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_r end_POSTSUPERSCRIPT, resulting a class prediction probability for each pixel, mi\u2208\ud835\udc0c\ud835\udc2d\ud835\udc2bsubscript\ud835\udc5a\ud835\udc56superscriptsubscript\ud835\udc0c\ud835\udc2d\ud835\udc2bm_{i}\\in\\bf M_{t}^{r}italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 bold_M start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_r end_POSTSUPERSCRIPT. New instances for each class are then recruited from the pseudo-labels to further train the model semi-supervised.However, deep learning models are known to provide over-confident predictions even in cases where the predicted classes are not correct\u00a0[16],\nwhich may result in re-training our model with noisy labels. To this end, we propose a post-processing approach that is based on an empirical p-value derived from the features of the predicted polygons, such as area and perimeter. This approach is motivated by recent studies on the robustness of deep learning frameworks, where similar empirical evaluations were conducted to identify out-of-distribution samples coming from synthesized content\u00a0[17] or adversarial attacks\u00a0[18]. This approach also aligns with a growing interest in data-centric research\u00a0[19] that aims to improve model performance by focusing on the data than the model, e.g., by improving the quality of data\u00a0[20].In this work, we aim to utilize area feature and discard predicted polygons with area values that are out-of-distributions from areas of training polygons per each class. Typical threshold-based filtering could be applied directly on the histogram of the area values. However, it has been found that the distribution is skewed heavily (see Fig.\u00a03 (a)) and hence a threshold based filtering will be very sensitive to the threshold value. On the other hand, the distributions of empirical p-values (see Fig.\u00a03 (b)) is relatively less skewed and hence more stable for threshold-based filtering. The pseudo-code of the proposed empirical p-value based post-processing is shown in Algorithm\u00a01. Let\u2019s assume that we are given the set of annotated training polygons, Pajsuperscriptsubscript\ud835\udc43\ud835\udc4e\ud835\udc57P_{a}^{j}italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT, and predicted polygons, Ppjsuperscriptsubscript\ud835\udc43\ud835\udc5d\ud835\udc57P_{p}^{j}italic_P start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT, for each jt\u2062hsuperscript\ud835\udc57\ud835\udc61\u210ej^{th}italic_j start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT class of interest in \ud835\udc9e^={b,o,w}^\ud835\udc9e\ud835\udc4f\ud835\udc5c\ud835\udc64\\hat{\\mathcal{C}}=\\{b,o,w\\}over^ start_ARG caligraphic_C end_ARG = { italic_b , italic_o , italic_w }. Then we compute the area of each annotated polygon in Pajsuperscriptsubscript\ud835\udc43\ud835\udc4e\ud835\udc57P_{a}^{j}italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT, e.g., Aa\u2062njsuperscriptsubscript\ud835\udc34\ud835\udc4e\ud835\udc5b\ud835\udc57A_{an}^{j}italic_A start_POSTSUBSCRIPT italic_a italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT. Similarly, we compute the area of each polygon in Ppjsuperscriptsubscript\ud835\udc43\ud835\udc5d\ud835\udc57P_{p}^{j}italic_P start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT, e.g., Ap\u2062kjsuperscriptsubscript\ud835\udc34\ud835\udc5d\ud835\udc58\ud835\udc57A_{pk}^{j}italic_A start_POSTSUBSCRIPT italic_p italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT. The empirical p-value of each predicted polygon, Ep\u2062kjsuperscriptsubscript\ud835\udc38\ud835\udc5d\ud835\udc58\ud835\udc57E_{pk}^{j}italic_E start_POSTSUBSCRIPT italic_p italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT, is calculated by counting the number of polygons in Pajsuperscriptsubscript\ud835\udc43\ud835\udc4e\ud835\udc57P_{a}^{j}italic_P start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT that are greater than or equal with Ap\u2062kjsuperscriptsubscript\ud835\udc34\ud835\udc5d\ud835\udc58\ud835\udc57A_{pk}^{j}italic_A start_POSTSUBSCRIPT italic_p italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT. Thus, a predicted polygon with out-of-distribution area will have extreme empirical p-value, i.e., Ep\u2062kj\u22480superscriptsubscript\ud835\udc38\ud835\udc5d\ud835\udc58\ud835\udc570E_{pk}^{j}\\approx 0italic_E start_POSTSUBSCRIPT italic_p italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT \u2248 0 and Ep\u2062kj\u22481superscriptsubscript\ud835\udc38\ud835\udc5d\ud835\udc58\ud835\udc571E_{pk}^{j}\\approx 1italic_E start_POSTSUBSCRIPT italic_p italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT \u2248 1 for predicted polygons with very small area and very large area, compared to the training set, respectively. Finally, the predicted polygons that satisfy the empirical-value-based threshold, et\u2062hjsuperscriptsubscript\ud835\udc52\ud835\udc61\u210e\ud835\udc57e_{th}^{j}italic_e start_POSTSUBSCRIPT italic_t italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT, are considered as pseudo labels to be used in the follow up recursive training steps.In this section, we describe the data sources used in this work, along with the distribution of annotations across classes, the deep learning model architecture and hyper-parameters set for our experiments, and performance evaluation metrics.We have used aerial photos taken in Northern Namibia in the years 1943 and 1972. See Fig.\u00a04 for an example of these photos and the types of classes annotated in these photos. Note the annotations were done manually by a domain expert. Table\u00a01 shows the distributions of annotations in pixels and polygons. The aggregated percentage of annotated pixels is <1%absentpercent1<1\\%< 1 % in 1943 imagery and <4%absentpercent4<4\\%< 4 % in 1972 imagery demonstrating the sparsity of annotated pixels (regions) compared to the unannotated regions - a typical challenge in geospatial imagery. Furthermore, Table\u00a01 demonstrates the nature of the imbalanced number of annotated pixels or polygons across classes, e.g., \u224890%absentpercent90\\approx 90\\%\u2248 90 % or more of these annotated polygons belong to Tree class whereas Waterhole constitute only <4%absentpercent4<4\\%< 4 % of the polygons in both 1943 and 1972 images..We have employed a U-Net-based\u00a0[6] semantic segmentation deep learning framework, using a pre-trained ResNet-50\u00a0[7] architecture as a backbone architecture for each of the 1943 and 1972 aerial photos. We have also employed a 70%\u221230%percent70percent3070\\%-30\\%70 % - 30 % train-test split of the annotated regions. We also employed a cross-entropy loss and a learning rate of 0.0010.0010.0010.001. The batch size and maximum epochs were set to 64646464 and 50505050, respectively. Note that the pixels with no annotation are treated as background class, and weighted accordingly so as not to affect the optimization significantly.Inference is performed at each pixel level in the test set, which can also be aggregated to polygon-level inference. Thus, the evaluation metrics are also computed corresponding to the level of inference. Generally, we employed Accuracy, Precision, Recall, and F1subscript\ud835\udc391F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT score to evaluate how well each class\u2019s annotated pixels (regions) were detected during inference.\nAll the four metrics represent detection performance based on true positive (tp), true negative (tn), false positive (fp) and false negative (fn) values. For pixel-level performance metrics,\ntrue positive is when the class pixel is correctly identified; true negative is when the pixels associated with the remaining classes are correctly identified as negative; false positive is the case when pixels corresponding to the remaining classes are incorrectly detected as the class pixel, and false negative refers to the case when class pixels are incorrectly detected as the remaining class pixels. For polygon-level performance metrics, tp, tn, fp, fn are computed from a threshold-based overlapping of regions, e.g., 5%percent55\\%5 %, between the predicted and ground truth polygons. Note that we have not computed the evaluation metrics for the background u\ud835\udc62uitalic_u class as it could still be a real background class or any of the classes but left unlabeled during annotation.Table\u00a02\nshows the results derived from different training strategies employed in our semantic segmentation framework across two imagery timestamps: 1943 and 192. Compared to F1=0.549subscript\ud835\udc3910.549F_{1}=0.549italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.549 in 1943 imagery, we achieved a higher F1=0.706subscript\ud835\udc3910.706F_{1}=0.706italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.706 on the 1972 imagery. This is partly due to the higher number of examples from 1972 imagery to train its model (see Table\u00a01). Furthermore, our different training strategies, i.e., class weighting, pseudo labeling and post-processed pseudo-labeling, outperformed the Baseline that does not include any of these strategies. Particularly, the class weighting strategy alone improved the Recall values from 0.2610.2610.2610.261 to 0.6560.6560.6560.656 in 1943 imagery and from 0.6630.6630.6630.663 to 0.8090.8090.8090.809 in 1972 imagery by effectively weighting the cross-entropy loss by the inverse of the observation of each class. Pseudo-labeling that aims to utilize high-confident predictions in a semi-supervised learning fashion is also shown to further improve the Precision (by reducing the false positives) and then the F1subscript\ud835\udc391F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT score in both imagery sources. Additional difference between 1943 and 1972 images involve the impact of using pseudo-labels after empirical p-value based post-processing (i.e., Post-processed Pseudo Labeling). Since the ground truth data of 1943 imagery suffers from very few number of training samples per class (i.e., <1%absentpercent1<1\\%< 1 % of the imagery is annotated), discarding the predicted polygons based on a threshold did not result in an improved performance. On the other hand, filtering the pseudo-labels before recursive training improved all the metrics in 1972 imagery, resulting in the highest F1=0.706subscript\ud835\udc3910.706F_{1}=0.706italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.706.Tables\u00a03 demonstrates the need of filtering predicted polygons as a part of our empirical area p-value based post-processing even for evaluating the performance metrics. The highest average F1subscript\ud835\udc391F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT score across the three classes is achieved in 1943 (F1=0.661subscript\ud835\udc3910.661F_{1}=0.661italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.661) imagery using a p-value threshold of et\u2062h=0.5subscript\ud835\udc52\ud835\udc61\u210e0.5e_{th}=0.5italic_e start_POSTSUBSCRIPT italic_t italic_h end_POSTSUBSCRIPT = 0.5 STD. Similarly, the post-processing has improved the average F1subscript\ud835\udc391F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT score from 0.7060.7060.7060.706 to 0.7550.7550.7550.755 using an p-value threshold of et\u2062h=1.0subscript\ud835\udc52\ud835\udc61\u210e1.0e_{th}=1.0italic_e start_POSTSUBSCRIPT italic_t italic_h end_POSTSUBSCRIPT = 1.0 STD, which is partly due to a larger and more balanced set of ground truth data and hence does not require a strong threshold that would discard more polygons.Among the metrics employed to evaluate our detection performance, Recall values are found to be consistently higher than Precision values for both imagery sources (see Table\u00a02). This is partly due to the higher observation of false positives compared to false negatives. Further analysis demonstrates that all false positives are not actually falsely identified objects of interest but previously unlabeled objects. Figure\u00a05 shows such an instance, where two objects were shown unlabeled in the ground truth polygons in Figure\u00a05 (a). But these two objects are detected as Waterholes during inference time, thereby suggesting the framework could also help to discover objects of interest that were not labeled during annotation though they might still be evaluated as false positives. This further motivates the need of pseudo-labeling in our framework that aimed to utilize such objects that were left unlabeled during annotation but later found out to be objects of interest with high confidence.Moreover, our analysis facilitates understanding of past events with less/limited data, and provides added quantitative and qualitative details on historic reports (see Fig.\u00a06). For example, the number and location of Waterholes and homesteads confirm the sharing of Waterholes by neighbors, low-yielding reports of these water holes, and increased population density. Furthermore, class-based changes (e.g., Waterholes) in number, area, coverage, locations and/or proximity to other objects of interest reveal further insights on the changes that took place between 1943 and 1972.\nFurthermore, we have utilized the trained semantic segmentation model in a larger area covering \u22485000absent5000\\approx 5000\u2248 5000 k\u2062m2\ud835\udc58superscript\ud835\udc5a2km^{2}italic_k italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. See Fig.\u00a01(B) to visualize the scale of a region compared to the relatively smaller annotated region (\u224845absent45\\approx 45\u2248 45 k\u2062m2\ud835\udc58superscript\ud835\udc5a2km^{2}italic_k italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT) used as a ground truth. Such large scale implementation of the framework helps to benefit domain experts by reducing the resource necessary to make exhaustive annotation, and generate large scale insights.Understanding long-term environmental changes requires the use of old and remotely sensed images such as satellite imagery. However, the resolutions of satellite images were not at sub-meter level a few decades back.\nOld aerial photos, on the other hand, satisfy these requirements though they are often stored unused in archives and museums across the world. In this work, we aim to demonstrate the capabilities for understanding long-term environmental changes in Namibia using aerial photos taken during 1943 and 1972 using deep learning to detect the following objects: Waterholes, Omuti homesteads and Big trees.\nTo this end, we employed a deep semantic segmentation framework that includes U-Net\u00a0[6] model with a pre-trained ResNet-50\u00a0[7] architecture as its backbone. To address the challenges associated with the sparseness of annotated regions and imbalance among classes, we proposed a class weighting strategy followed with a pseudo labeling step that aims to utilize predicted polygons. The pseudo-labels were further filtered using an empirical p-value based post-processing step.\nThe results demonstrate the capabilities of aerial photos to understand long-term environmental changes by detecting those classes with encouraging performance.\nThus, efforts need to be accelerated to digitize and analyze them to better understand long-term environmental and socio-demographic changes. This work highlighted that aerial photos provide a promising alternative to study environmental changes prior to 1990s as there was no adequate satellite technology to capture images with <1\u2062m/p\u2062i\u2062x\u2062e\u2062labsent1\ud835\udc5a\ud835\udc5d\ud835\udc56\ud835\udc65\ud835\udc52\ud835\udc59<1m/pixel< 1 italic_m / italic_p italic_i italic_x italic_e italic_l resolution.Future work aims to investigate further use cases where lower detection performance metrics, both in precision and recall, were observed. In addition, we aim to scale up the validation of the proposed approach beyond the use case in Namibia. Deploying under-utilized archival aerial photographs is, potentially, a promising alternative to validate current understanding of the past and uncover new insights, which is critical to ensure sustainability in Africa, where climate change poses a significant and disproportionate risk compared to its emission.",
    "18": "supplementstuff\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[]\n\\credit[]\n\\credit[orcid=0000-0003-4123-4228]\n\\cormark[1]\n\\credit[]\n\\credit[]\n\\credit[]\n\\credit[]\n\\credit[]\n\\credit[cor1]Corresponding authorOver the past decades, the increase in both frequency and intensity of large-scale wildfires due to climate change has emerged as a significant natural threat. The pressing need to design resilient landscapes capable of withstanding such disasters has become paramount, requiring the development of advanced decision-support tools. Existing methodologies, including Mixed Integer Programming, Stochastic Optimization, and Network Theory, have proven effective but are hindered by computational demands, limiting their applicability.In response to this challenge, we propose using artificial intelligence techniques, specifically Deep Reinforcement Learning, to address the complex problem of firebreak placement in the landscape. We employ value-function based approaches like Deep Q-Learning, Double Deep Q-Learning, and Dueling Double Deep Q-Learning. Utilizing the Cell2Fire fire spread simulator combined with Convolutional Neural Networks, we have successfully implemented a computational agent capable of learning firebreak locations within a forest environment, achieving good results.Furthermore, we incorporate a pre-training loop, initially teaching our agent to mimic a heuristic-based algorithm and observe that it consistently exceeds the performance of these solutions. Our findings underscore the immense potential of Deep Reinforcement Learning for operational research challenges, especially in fire prevention. Our approach demonstrates convergence with highly favorable results in problem instances as large as 40\u00d740404040\\times 4040 \u00d7 40 cells, marking a significant milestone in applying Reinforcement Learning to this critical issue.\nTo the best of our knowledge, this study represents a pioneering effort in using Reinforcement Learning to address the aforementioned problem, offering promising perspectives in fire prevention and landscape management.The connection between climate change and the rising risk of wildfires underscores the need to rethink our approach to living with fire and our environment. Worldwide, fires are triggered by a variety of causes such as lightning, volcanic activities, accidental sparks from rock falls, or human carelessness, as detailed in Scott et\u00a0al. (2013). In Canada, from 1990 to 2016, a combination of lightning, human activities, and other unknown factors was responsible for 47%, 49%, and 4% of forest fires, respectively, as reported by Tymstra et\u00a0al. (2020). In light of recent global events, including the devastating Australian bushfires of 2019 and 2020, California\u2019s massive Dixie Fire in 2021, the Fort McMurray wildfire in 2016, and Canada\u2019s ongoing 2023 fire season \u2014 which is on track to be one of the most destructive in the nation\u2019s history \u2014 it has become evident that merely reactive measures are not enough. These incidents underscore the urgent need for more proactive and comprehensive strategies in fire management and environmental conservation. Furthermore, they highlight the importance of exploring novel technologies to address this serious and escalating crisis.Among the diverse preventive strategies employed in forest landscapes, such as cutting, clearing, controlled burning, and thinning, the establishment of firebreaks is a key method (North et\u00a0al., 2015; Carrasco et\u00a0al., 2023). This involves identifying strategic areas and replacing the existing vegetation with non-flammable materials. Consequently, these firebreaks serve as barriers, impeding the spread of fires should they occur. By halting the fire\u2019s progress, firebreaks play a crucial role in prevention and management. The implementation of preventive strategies in wildfire management has been increasingly informed by Operations Research (OR), integrating the fire risk and uncertainty into its mathematical models. Specifically, stochastic Optimization (SO) has emerged as a pivotal tool in this context, effectively accommodating uncertainty through a range of probabilistic scenarios, as evidenced in studies like Hoganson and Rose (1987); Boychuk and Martell (1996); Eriksson (2006); Garcia-Gonzalo et\u00a0al. (2016). The latter study, in particular, underscores SO\u2019s potential in addressing climate change-related issues. However, the effectiveness of these methods can be limited when dealing with numerous scenarios. Combinatorial techniques have also been employed to tackle forest fire challenges, as demonstrated by Gonz\u00e1lez-Olabarria and Pukkala (2011). Despite these advances, current research still faces significant challenges in optimally integrating fire risk into spatially explicit plans. These challenges include overly simplistic landscape models, reliance on a limited set of fire simulations, and the use of basic fire spread simulators, as indicated in the works of Bettinger (2009); Kim et\u00a0al. (2009); Konoshima et\u00a0al. (2008); Gonz\u00e1lez-Olabarria and Pukkala (2011). Broadly, a notable shortcoming of these techniques is their lack of adaptability in learning from past experiences. Consequently, when managing a new landscape, models must be developed from scratch, failing to capitalize on the insights and knowledge gained from previously solved scenarios.The advent of machine learning (ML) techniques in wildfire analysis and management opens up new opportunities to tackle this complex issue. ML enables computers to autonomously learn from data and encompasses three key types: i) supervised learning, which uses pre-labeled data to develop predictive models through classification and regression; ii) unsupervised learning, employing unlabeled data, often using clustering for exploratory data analysis; iii) reinforcement learning (RL), which aims to create models that maximize cumulative rewards through a series of actions, diverging from traditional input-output based learning. Since the 1990s, ML has been instrumental in various wildfire science and management areas, including fire occurrence and risk assessment (Amatulli et\u00a0al., 2006; Costafreda-Aumedes et\u00a0al., 2018), detection, climate change impact studies, effect analysis (Sousa et\u00a0al., 2020), behavior prediction (Hodges and Lattimer, 2019), wildland-urban interface mapping (Carrasco\u00a0Barra, 2019; Miranda et\u00a0al., 2020), and fuel management (Lauer et\u00a0al., 2017). However, most studies have primarily utilized supervised and unsupervised Learning, with a few notable exceptions such as (Lauer et\u00a0al., 2017; Altamimi et\u00a0al., 2022), which employed RL.In RL, an agent learns optimal actions by interacting with its environment, aiming to maximize rewards in a Markov Decision Process (MDP), where transition probabilities are learned rather than predefined (Sutton et\u00a0al., 1998). This learning style is ideal for automated decision-making tasks in areas like robotics or policy optimization. MDPs, however, struggle with the challenges of modeling in intricate systems (modeling curse) and managing extensive state spaces and transition probability matrices (dimensionality curse). This has spurred research into methods that operate within simulators, bypassing the need to generate transition probabilities, crucial in MDPs. Notable algorithms in this domain include Q-Learning (Watkins and Dayan, 1992), the SARSA algorithm (Sutton and Barto, 2018), deep Q-network (Mnih et\u00a0al., 2013), and the Policy Gradient (PG) algorithm (Williams, 1992), collectively advancing the field of RL.Traditionally, OR in wildfire management has focused on developing exact mathematical models. These models often require simplifying assumptions to be manageable, such as adopting specific distributions for random variables in the system or using representative scenarios in Stochastic Optimization. While these assumptions lead to mathematically elegant solutions, they can sometimes overlook the complexities of real-life situations. A notable challenge has been optimizing without closed-form expressions for objective functions, which provide direct mathematical solutions (Gosavi et\u00a0al., 2015). Recognizing these limitations, our study adopts a reinforcement learning (RL) approach, renowned for its technical capabilities and effectiveness in handling complex, dynamic problems. Specifically, we develop a framework based on an RL algorithm that interacts with a wildfire simulator, Cell2Fire Pais et\u00a0al. (2021a). This approach allows for a more nuanced and adaptable model, better suited to the unpredictable nature of wildfire behavior.\nA common flaw of OR models is that they necessitate a fresh computational effort for each new problem encountered, as if no prior problem had been solved. This contrasts sharply with the capabilities of ML models, especially Deep Learning, which excel in leveraging accumulated knowledge. This knowledge comes in the form of parameters, which can be used either as a starting point for the new learning process or directly as a feature extractor after the input has been transformed to fit the dimensions of the input layer (Goodfellow et\u00a0al., 2016). Notably, Deep Learning models possess the unique advantage of combining specific insights from distinct problem instances, thereby enhancing solution efficiency and promoting the recognition of underlying similarities (Bengio et\u00a0al., 2013). This dual capability of accelerating the problem-solving process and integrating diverse instance-specific knowledge marks a significant departure from the conventional approach of OR models.This study makes a dual contribution to the field. Firstly, it pioneers the application of RL techniques for addressing the Firebreak Placement Problem (FPP) marking a significant departure from conventional OR methods and positioning RL as a viable and promising alternative. Secondly, it conducts an exhaustive comparison of various algorithms, demonstrating their effectiveness and appropriateness for use in this specific context. To the best of our knowledge, this is the first instance of such an innovative approach being applied to the firebreak placement challenge.In this subsection, we define the FPP, building upon the groundwork laid by Palacios et\u00a0al. (2023). We consider a landscape segmented into a set of spatially georeferenced cells in the landscape, represented by N\ud835\udc41Nitalic_N. Each cell is linked to a decision variable xi,i\u2208Nsubscript\ud835\udc65\ud835\udc56\ud835\udc56\ud835\udc41x_{i},i\\in Nitalic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_i \u2208 italic_N, which is assigned a value of 1 if a firebreak is placed in cell i\ud835\udc56iitalic_i and 0 otherwise. This assignment results in a solution vector x\u2208{0,1}|N|\ud835\udc65superscript01\ud835\udc41x\\in\\{0,1\\}^{|N|}italic_x \u2208 { 0 , 1 } start_POSTSUPERSCRIPT | italic_N | end_POSTSUPERSCRIPT. Implementing a firebreak in a cell completely removes its vegetative fuel, rendering the cell non-combustible. Given the financial implications of firebreak placement, it is practical to limit the number of cells managed in this manner. In our approach, this limit is precisely 5%percent55\\%5 % of the total cell count in a forest. Formally, this constraint is expressed as:where \u03b1\u2208(0,1)\ud835\udefc01\\alpha\\in(0,1)italic_\u03b1 \u2208 ( 0 , 1 ) caps the overall quantity of firebreaks that can be deployed. The primary aim of placing firebreaks is to minimize the expected tally of cells consumed by fire under any arbitrary wildfire event f\ud835\udc53fitalic_f:In this expression, L\u2062(x,f)\ud835\udc3f\ud835\udc65\ud835\udc53L(x,f)italic_L ( italic_x , italic_f ) denotes a stochastic variable representing the number of cells incinerated by a random fire event f\ud835\udc53fitalic_f after firebreaks x\ud835\udc65xitalic_x have been placed. In this work, \ud835\udd3cf\u2062[L\u2062(x,f)]subscript\ud835\udd3c\ud835\udc53delimited-[]\ud835\udc3f\ud835\udc65\ud835\udc53\\mathbb{E}_{f}[L(x,f)]blackboard_E start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT [ italic_L ( italic_x , italic_f ) ] is unknown, relying instead on a series of simulated realizations of the variable L\u2062(x,f)\ud835\udc3f\ud835\udc65\ud835\udc53L(x,f)italic_L ( italic_x , italic_f ) to model fire spread dynamics.RL is a ML paradigm in the intersection between supervised and unsupervised learning. In this, an agent interacts with an environment in a series of discrete time steps. In each time step t\u2208{1,2,\u2026,T}\ud835\udc6112\u2026\ud835\udc47t\\in\\{1,2,...,T\\}italic_t \u2208 { 1 , 2 , \u2026 , italic_T }, the environment presents an observation to the agent, generally known as state st\u2208Ssubscript\ud835\udc60\ud835\udc61\ud835\udc46s_{t}\\in Sitalic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u2208 italic_S until the episode ends at time T\ud835\udc47Titalic_T. In turn, the agent takes an action at\u2208Astsubscript\ud835\udc4e\ud835\udc61subscript\ud835\udc34subscript\ud835\udc60\ud835\udc61a_{t}\\in A_{s_{t}}italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u2208 italic_A start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT, where Astsubscript\ud835\udc34subscript\ud835\udc60\ud835\udc61A_{s_{t}}italic_A start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT is the set of available actions at state stsubscript\ud835\udc60\ud835\udc61s_{t}italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, causing the environment to transition to a new state st+1subscript\ud835\udc60\ud835\udc611s_{t+1}italic_s start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT and emit a reward signal rtsubscript\ud835\udc5f\ud835\udc61r_{t}italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. The agent takes actions in order to maximize the sum of discounted rewards:where \u03b3\ud835\udefe\\gammaitalic_\u03b3 is a discount factor that represents the notion that present rewards are more valuable than future rewards. The decision-making behavior of an agent is defined through the concept of a policy \u03c0\ud835\udf0b\\piitalic_\u03c0, which is a mapping from states to actions. The full set of transitions an episode determines is called a trajectory: {(st,at,rt,st+1)}t=0T\u22121superscriptsubscriptsubscript\ud835\udc60\ud835\udc61subscript\ud835\udc4e\ud835\udc61subscript\ud835\udc5f\ud835\udc61subscript\ud835\udc60\ud835\udc611\ud835\udc610\ud835\udc471\\{(s_{t},a_{t},r_{t},s_{t+1})\\}_{t=0}^{T-1}{ ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_t = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T - 1 end_POSTSUPERSCRIPT. Therefore, a successful resolution of the RL problem implies determining the optimal policy \u03c0*superscript\ud835\udf0b\\pi^{*}italic_\u03c0 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT. A particular policy, determines what is known as the state-action function:The former set of equations has a special formulation when it corresponds to the optimal policy called the Optimality Bellman Equations:where p\u2062(s,a)\ud835\udc5d\ud835\udc60\ud835\udc4ep(s,a)italic_p ( italic_s , italic_a ) are known as the transition probabilities, which enclose the whole environment dynamics and in general are not available. If not available, one has to recur to RL algorithms and in particular when the number of states is too large, to deep reinforcement learning (DRL) where it must be approximated: q\u2062(s,a,\u03b8)\u2248q\u2062(s,a)\ud835\udc5e\ud835\udc60\ud835\udc4e\ud835\udf03\ud835\udc5e\ud835\udc60\ud835\udc4eq(s,a,\\theta)\\approx q(s,a)italic_q ( italic_s , italic_a , italic_\u03b8 ) \u2248 italic_q ( italic_s , italic_a ). Once q*\u2062(s,a)subscript\ud835\udc5e\ud835\udc60\ud835\udc4eq_{*}(s,a)italic_q start_POSTSUBSCRIPT * end_POSTSUBSCRIPT ( italic_s , italic_a ) is found or approximated, the optimal policy will be to choose the action atsubscript\ud835\udc4e\ud835\udc61a_{t}italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT that maximizes q\u2062(st,at,\u03b8)\ud835\udc5esubscript\ud835\udc60\ud835\udc61subscript\ud835\udc4e\ud835\udc61\ud835\udf03q(s_{t},a_{t},\\theta)italic_q ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_\u03b8 ) in each step. In order to ensure that the agent visits a variety of states, regardless of them being profitable, it takes a random action with probability \u03b5\ud835\udf00\\varepsilonitalic_\u03b5. This type of policies are known as \u03b5\ud835\udf00\\varepsilonitalic_\u03b5-greedy.As mentioned in Section 2.2.1, the optimal policy in this work was learned through the approximation of q*\u2062(s,a)subscript\ud835\udc5e\ud835\udc60\ud835\udc4eq_{*}(s,a)italic_q start_POSTSUBSCRIPT * end_POSTSUBSCRIPT ( italic_s , italic_a ), which in turn was performed through convolutional neural networks.The general idea of the algorithms implemented is the following: the agent interacts with the environment for a number of episodes, collecting experiences (st,at,rt,st+1)subscript\ud835\udc60\ud835\udc61subscript\ud835\udc4e\ud835\udc61subscript\ud835\udc5f\ud835\udc61subscript\ud835\udc60\ud835\udc611(s_{t},a_{t},r_{t},s_{t+1})( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ) which are saved in a experience replay buffer D\ud835\udc37Ditalic_D (Lin (1992)). Two copies of the Q-Network will be stored, one will be used to generate this experiences, denoted by q\u2062(s,a,\u03b8)\ud835\udc5e\ud835\udc60\ud835\udc4e\ud835\udf03q(s,a,\\theta)italic_q ( italic_s , italic_a , italic_\u03b8 ). The other, q\u2062(s,a,\u03b8\u2212)\ud835\udc5e\ud835\udc60\ud835\udc4esuperscript\ud835\udf03q(s,a,\\theta^{-})italic_q ( italic_s , italic_a , italic_\u03b8 start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT ), will be used for computing the target value used in every step to update q\u2062(s,a,\u03b8)\ud835\udc5e\ud835\udc60\ud835\udc4e\ud835\udf03q(s,a,\\theta)italic_q ( italic_s , italic_a , italic_\u03b8 ). After a C\ud835\udc36Citalic_C number of episodes, the parameters \u03b8\ud835\udf03\\thetaitalic_\u03b8 will be copied into \u03b8\u2212superscript\ud835\udf03\\theta^{-}italic_\u03b8 start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT. The rule used to update this parameters will be what is going to change between algorithms.The use of an experience replay buffer is a common practice in RL. For once, it provides greater sample efficiency than collecting and then discarding every time the network is updated. Secondly, neural networks assume i.i.d samples in each batch, something that is easier achieved if a large number of transitions are stored and then sampled from. Finally, it prevents the neural network from getting stuck in local optima by regularly providing it with transitions that were not generated by the current implicit policy. In a similar way, the use of a target network q\u2062(s,a,\u03b8\u2212)\ud835\udc5e\ud835\udc60\ud835\udc4esuperscript\ud835\udf03q(s,a,\\theta^{-})italic_q ( italic_s , italic_a , italic_\u03b8 start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT ) allows for learning processes considerably more stable than using a single network to both, collect experiences and generate target values.To update the parameters the following loss function was used:with:where the \u201ctarget\u201d is an approximation of q\u2062(s,a,\u03b8)\ud835\udc5e\ud835\udc60\ud835\udc4e\ud835\udf03q(s,a,\\theta)italic_q ( italic_s , italic_a , italic_\u03b8 ) based on Eq.\u2005(7) and therefore Eq.\u2005(4) should indicate how close is the estimate to the actual value. Eq.\u2005(5) gives place to the first algorithm, Deep Q-Learning introduced in Mnih et\u00a0al. (2013). Although Deep Q-Learning in general works well, it suffers from a considerable flaw: it usually overestimates the values of q*\u2062(s,a)subscript\ud835\udc5e\ud835\udc60\ud835\udc4eq_{*}(s,a)italic_q start_POSTSUBSCRIPT * end_POSTSUBSCRIPT ( italic_s , italic_a ), this is because if q\u2062(s,a,\u03b8)\ud835\udc5e\ud835\udc60\ud835\udc4e\ud835\udf03q(s,a,\\theta)italic_q ( italic_s , italic_a , italic_\u03b8 ) contains errors, then the target as constructed in Eq.\u2005(7) is overestimated (Thrun and Schwartz, 2014). One way to address this complication is to decouple action selection and evaluation:This way of constructing the target defines the second algorithm: Double Deep Q-Learning (Hasselt et\u00a0al. (2015)).A final improvement to the presented algorithms introduces the third one: Dueling Double Deep Q-Learning (Wang et\u00a0al. (2016)). In this, the network instead of outputting only q\u2062(s,a,\u03b8)\ud835\udc5e\ud835\udc60\ud835\udc4e\ud835\udf03q(s,a,\\theta)italic_q ( italic_s , italic_a , italic_\u03b8 ), outputs v\u2062(s,\u03b8)\ud835\udc63\ud835\udc60\ud835\udf03v(s,\\theta)italic_v ( italic_s , italic_\u03b8 ) as well as A\u2062(s,a,\u03b8)\ud835\udc34\ud835\udc60\ud835\udc4e\ud835\udf03A(s,a,\\theta)italic_A ( italic_s , italic_a , italic_\u03b8 ). The first is known as the value-function and corresponds to the expected discounted rewards the agent obtains when faced with s\ud835\udc60sitalic_s and then following a particular policy. The second is known as the advantage-function and similarly, it is the difference between the discounted rewards obtained by executing action a\ud835\udc4eaitalic_a in state s\ud835\udc60sitalic_s in comparison to the expectation for all actions in that state. The benefits of estimating v*\u2062(s)subscript\ud835\udc63\ud835\udc60v_{*}(s)italic_v start_POSTSUBSCRIPT * end_POSTSUBSCRIPT ( italic_s ) and A*\u2062(s,a)subscript\ud835\udc34\ud835\udc60\ud835\udc4eA_{*}(s,a)italic_A start_POSTSUBSCRIPT * end_POSTSUBSCRIPT ( italic_s , italic_a ) instead of q*\u2062(s,a)subscript\ud835\udc5e\ud835\udc60\ud835\udc4eq_{*}(s,a)italic_q start_POSTSUBSCRIPT * end_POSTSUBSCRIPT ( italic_s , italic_a ) directly is that in some states, it is not necessary to estimate each of the possible actions values, since they could have little effect in the environment. In such states, it is sufficient to use the value function, this leads usually to better performance. With these two functions q\u2062(s,a,\u03b8)\ud835\udc5e\ud835\udc60\ud835\udc4e\ud835\udf03q(s,a,\\theta)italic_q ( italic_s , italic_a , italic_\u03b8 ) can be reconstructed and used as before:Having this new formulation for q\u2062(s,a,\u03b8)\ud835\udc5e\ud835\udc60\ud835\udc4e\ud835\udf03q(s,a,\\theta)italic_q ( italic_s , italic_a , italic_\u03b8 ), the target is constructed as in Eq.\u2005(8).A particular advantage of these methods with respect to usual PG approaches is that there is no assumption about the distribution from which the experiences are sampled. In particular, one can generate experiences according to a completely different distribution with respect to the current policy, such as a demonstrator. This idea is known as learning from demonstrations Hester et\u00a0al. (2018). These demonstrations are stored in the replay buffer and will not be overwritten by experiences gathered by the agent. To incorporate these new experiences, new terms must be added to the loss function. Firstly, to ensure that the agent regularly chooses to follow the demonstrator behavior one forces those actions to have marginally larger values than the rest:where, aesubscript\ud835\udc4e\ud835\udc52a_{e}italic_a start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT is the action taken by the demonstrator when faced with state s\ud835\udc60sitalic_s and l\u2062(ae,a)\ud835\udc59subscript\ud835\udc4e\ud835\udc52\ud835\udc4el(a_{e},a)italic_l ( italic_a start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT , italic_a ) is a margin function that is 0 when a=ae\ud835\udc4esubscript\ud835\udc4e\ud835\udc52a=a_{e}italic_a = italic_a start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT and a positive value otherwise. In addition, to ensure that q\u2062(s,a,\u03b8)\ud835\udc5e\ud835\udc60\ud835\udc4e\ud835\udf03q(s,a,\\theta)italic_q ( italic_s , italic_a , italic_\u03b8 ) is well adjusted for early steps of an episode, the n-step loss is included:And finally, to prevent parameters from getting too large, an L2 penalization is added:Then, the global loss function is defined by summing up Eq.\u2005(6), Eq.\u2005(10), Eq.\u2005(11) and Eq.\u2005(12):where in general, \u03bb1=\u03bb2=\u03bb3=1subscript\ud835\udf061subscript\ud835\udf062subscript\ud835\udf0631\\lambda_{1}=\\lambda_{2}=\\lambda_{3}=1italic_\u03bb start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = italic_\u03bb start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = italic_\u03bb start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT = 1. JD\u2062Q\u2062(\u03b8)subscript\ud835\udc3d\ud835\udc37\ud835\udc44\ud835\udf03J_{DQ}(\\theta)italic_J start_POSTSUBSCRIPT italic_D italic_Q end_POSTSUBSCRIPT ( italic_\u03b8 ) corresponds to the loss function defined in Eq.\u2005(6), where the target is calculated according to which of the three algorithms is to be used. Previous to the general training loop presented at the beginning of this section, a pre-training phase is added. In this phase, experiences are sampled from the experience replay buffer, which at this point is composed solely of demonstrations, and the network\u2019s parameters are updated according to Eq.\u2005(13). This procedure allows the algorithm to learn how to mimic the decisions taken by the demonstrator, then this behavior is improved through the agent-environment interaction.In addition to the aforementioned methods, others belonging to the PG family were initially considered, but then discarded because of the complexity of including demonstrations on their learning loops. Such is the case of Proximal Policy Optimization (Schulman et\u00a0al., 2017) and Trust Region Policy Optimization (Schulman et\u00a0al., 2015).A crucial element in model-free algorithms is the ability to generate samples from the system within which the problem is framed. In the case of this work, that translates to being able to model the behavior of fire spreading and its interactions with the presence of firebreaks. In this study, we use a simulator known as Cell2Fire (Pais et\u00a0al., 2021a), which models fire spreading using a cellular automata approach, representing the landscape as a regular grid composed of cells characterized by a set of environmental variables, including fuel type and topographic features. Cell2Fire effectively integrates decision-making (e.g., firebreaks) with spatial simulation, an advantage that distinguishes it from other simulators such as Prometheus or Burn-P3 (Tymstra et\u00a0al., 2010; Parisien et\u00a0al., 2005).A crucial element for a RL system to work successfully is to feed the agent a suitable representation of the environment. This representation must contain enough information so that the agent can infer a considerable portion of the state of the system, in particular the immediate effects that its actions have on the environment. The representation designed in this work is based on Shantia et\u00a0al. (2011) idea of \u201cvision grids\", which can be understood as a \u201csnapshots of the environment from the agent\u2019s point of view\".The general interaction loop consists of the agent choosing a single firebreak on each step of the episode. This will continue until the number of chosen firebreaks equals 5%percent55\\%5 % (\u03b1=0.05\ud835\udefc0.05\\alpha=0.05italic_\u03b1 = 0.05 in Eq.\u2005(1)) of the total number of cells in the forest, then a series of fires are simulated on Cell2Fire over the forest with the chosen firebreaks allocated. In order to conduct the simulation, a particular weather scenario and ignition point are sampled at random, determining the spreading direction and starting point. This is represented in Fig.\u20051.Following the idea of vision grids, the agent is provided with a matrix representation of the environment of size N\u00d7N\ud835\udc41\ud835\udc41N\\times Nitalic_N \u00d7 italic_N. On each cell, there is a value that identifies uniquely the type of fuel present in it.On every time step, there is a set of available and forbidden cells. This last set allows for the inclusion of restrictions of which cells can be chosen to be treated as firebreaks. Then, the agent chooses an action that corresponds to one of the cells in the available set. When a cell is chosen, the corresponding fuel value is changed for that of a non-fuel material and is added to the forbidden set (Fig.\u20052).Another fundamental element in the construction of an RL system is to give the agent appropriate rewards for its actions. Specifically, maximizing this reward must be equivalent to solving the problem dealt with. A simple reward structure was used during this work:where n\u2062b\u2062(st)\ud835\udc5b\ud835\udc4fsubscript\ud835\udc60\ud835\udc61nb(s_{t})italic_n italic_b ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) is equal to the average number of cells that where burned on the simulator and k\ud835\udc58kitalic_k is a penalization factor.As was briefly discussed in previous sections, due to the number of states being too large to store in a table, function approximators must be used. \"In particular, Convolutional Neural Networks were selected because of their well-studied success in visual recognition tasks.Since q\u2062(s,a)\ud835\udc5e\ud835\udc60\ud835\udc4eq(s,a)italic_q ( italic_s , italic_a ) is going to be approximated, the output of the neural network is different than in most applications. The general intuition is the following: a particular state s\ud835\udc60sitalic_s is fed to the neural network in the form of a (N,N) matrix. A series of convolution + max pool + dropout blocks are applied to the input, which act as feature extractors. In the case of the first two algorithms presented in 2.2.2, these features are passed through one set of feedforward-fully-connected layers which act as a multi-output regression, outputting all the entries of q\u2062(s,a)\ud835\udc5e\ud835\udc60\ud835\udc4eq(s,a)italic_q ( italic_s , italic_a ). In the case of the third algorithm, another independent set of forward-fully-connected layers is added that perform a simple regression over the features, resulting in the state\u2019s value v\u2062(s)\ud835\udc63\ud835\udc60v(s)italic_v ( italic_s ). The other set results in all the entries of the advantage function A\u2062(s,a)\ud835\udc34\ud835\udc60\ud835\udc4eA(s,a)italic_A ( italic_s , italic_a ). In all three algorithms, entries corresponding to forbidden actions are masked.Two network architectures were trained, varying in their depth, in order to test how much complexity was needed to approximate q\u2062(s,a)\ud835\udc5e\ud835\udc60\ud835\udc4eq(s,a)italic_q ( italic_s , italic_a ). The first one, called small-net (Fig.\u20053), is composed of two convolution + max pool + dropout blocks plus two separate output flows each consisting in two feedforward-fully-connected layers of 512 and 128 neurons correspondingly.The second architecture, named big-net (Fig.\u20053) has three convolution + max pool + dropout blocks and three feedforward-fully-connected layers per head of 2048, 48 and 32 neurons.Besides the proposed architectures, two larger pre-trained models were used, through a technique called Transfer Learning.Transfer learning is a ML technique that was introduced by Bozinovski and Fulgosi (1976), yet its fruits only blossomed in the second decade of this century.Through this method, pre-trained, usually large models, are reused in order to construct a different model thought for a different yet similar task. The intuition behind it rests on the assumption that if both tasks are sufficiently similar, then some of the features extracted by the larger model should be similar to the ones the second task requires. Whether this premise holds is usually empirically checked. Then, the pre-trained model is used as a feature extractor for the second model, which generally adds a series of layers before and after the extractor. The point of this technique is to lighten the computational load for the second model, which might be critical if the data or resources are scarce. For the procedure to fulfill this, an arbitrary number of parameters in the pre-trained model are not adjusted or are \u201cfrozen\".\nIn this work, two pre-trained models were incorporated as feature extractors:MobileNet: presented in Howard et\u00a0al. (2019), it is a model targeted for low resource applications, which translates into a reasonably small amount of parameters while not compromising performance. Two versions were used, MobileNetV3-Large and MobileNetV3-Small, this second having even fewer parameters than the first.EfficientNet: presented in Tan and Le (2019), it follows the same principle as MobileNet, yet focuses on balancing network depth, width, and resolution.Both models are considered state-of-the-art under resource constraints. To provide more details, in this application, a convolutional layer was added before the pre-trained models and a regressor after, varying slightly on their dimensions to match the size of the feature vector provided by each architecture. Regarding the parameters, the pre-trained weights were used as starting point and those after the 8th layer were \u201cfrozen\".Most complex AI models are often referred to as \u201cblack boxes\" in the sense that, for most applications,\nthe results are not really understood or interpreted to determine which features of the input allow the\ntask to be successfully completed. To use these models lightly without any study of their properties is\nno minor gamble. It is of particular importance in the context of this work, where decisions taken by the model translate into changes that affect individuals and communities and hence must be explained to some degree.In this work, following the work in Pais et\u00a0al. (2021c), we implemented a very common methodology to achieve certain level of explainability regarding the outputs generated by the model: gradient-weighted class activation or GradCam (Selvaraju et\u00a0al., 2016). This method is widely used in computer vision and its extension to RL methods is straight-forward according to Alharin et\u00a0al. (2020): given an action a*superscript\ud835\udc4ea^{*}italic_a start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT the gradient of the corresponding output entry q\u2062(s,a*,\u03b8)\ud835\udc5e\ud835\udc60superscript\ud835\udc4e\ud835\udf03q(s,a^{*},\\theta)italic_q ( italic_s , italic_a start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT , italic_\u03b8 ) with respect to the last convolutional layer of the network is computed. This gradient is a tensor which is combined and scaled in order to generate an image of the same dimensions as the original input, where larger values indicate that in order to compute q\u2062(s,a*,\u03b8)\ud835\udc5e\ud835\udc60superscript\ud835\udc4e\ud835\udf03q(s,a^{*},\\theta)italic_q ( italic_s , italic_a start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT , italic_\u03b8 ), the network puts higher relative importance to that pixel. A schema of this procedure is presented in Fig.\u20054.\nWe employed two real landscapes in our experiments: Sub20 and Sub40, both situated in the Alberta region of Canada. The original instance is shown in Fig.\u20056. The Sub20 landscape, a 400-hectare forest patch depicted in Fig.\u20056, was chosen for assessing computational performance and observing how the objective function evolves with changes in model parameters. To evaluate the model\u2019s computational efficacy over larger forest areas, we extended the study to include Sub40 (Fig.\u20058) under specific parameter configurations. Each landscape is divided into 100\u00d7100100100100\\times 100100 \u00d7 100 m22{}^{2}start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT cells, each containing a specific fuel (Fig.\u20058), detailed information about these landscapes can be found at https://github.com/fire2a/C2FFBP. Simulations of multiple wildfires were conducted using Cell2Fire, requiring fire-weather scenarios specific to the study area (Parisien et\u00a0al., 2005; Pais et\u00a0al., 2021a). These scenarios include crucial factors such as temperature, relative humidity, wind speed, wind direction, and fire weather indices-essential inputs for the Canadian Fire Behavior Prediction (FBP) System (Hirsch et\u00a0al., 1996).Going into more detail, in Sub20 ignition points were delimited to a circumference of a radio of 4 cells at the center of the forest. Regarding weather scenarios, these were constructed using real data comprising the zone of interest. Without any fire treatment, on average around 18% of the total number of cells are burned.Similarly for Sub40, ignitions were delimited to a circumference of radio 9 cells at the center of the forest. Without any intervention, close to 31% of the total number of cells are burned. The general spreading pattern of both instances is reflected in Fig.\u20059.This larger forests were then shrank to generate smaller forest that exhibit similar behavior of size 10\u00d710101010\\times 1010 \u00d7 10, in order to search efficiently in the hyperparameter space. An interpolation method designed for image shrinking was used, called Nearest-neighbor Interpolation, where the value of each pixel in the resulting image is equal to the one the algorithm determines to be the closest.As was mentioned in section 2.2.2, learning from demonstration was used as a way of guiding the agent\u2019s behavior towards better sections of the action space faster. In order to generate demonstrations, a baseline algorithm was constructed that makes use of a widely used landscape connectivity metric called Downstream Protection Value (DPV), presented in Pais et\u00a0al. (2021b). This measure considers a forest as an undirected graph G=(V,E)\ud835\udc3a\ud835\udc49\ud835\udc38G=(V,E)italic_G = ( italic_V , italic_E ), in which each node corresponds to a cell and edges to contiguous cells. Then a number of fire spreading simulations are run, each generating a directed sub graph Gd=(Vd,Ed)subscript\ud835\udc3a\ud835\udc51subscript\ud835\udc49\ud835\udc51subscript\ud835\udc38\ud835\udc51G_{d}=(V_{d},E_{d})italic_G start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = ( italic_V start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) over G\ud835\udc3aGitalic_G, where Vdsubscript\ud835\udc49\ud835\udc51V_{d}italic_V start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT contains all nodes that were burned and Edsubscript\ud835\udc38\ud835\udc51E_{d}italic_E start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT all edges that transmitted fire. With this sub graph, the minimum spanning tree is calculated for each node as root Td\u2062(j)=(Vd\u2062(j),Ed\u2062(j))subscript\ud835\udc47\ud835\udc51\ud835\udc57subscript\ud835\udc49\ud835\udc51\ud835\udc57subscript\ud835\udc38\ud835\udc51\ud835\udc57T_{d}(j)=(V_{d}(j),E_{d}(j))italic_T start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_j ) = ( italic_V start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_j ) , italic_E start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_j ) ) and the DPV for cell i\ud835\udc56iitalic_i is defined as:where v\u2062(j)\ud835\udc63\ud835\udc57v(j)italic_v ( italic_j ) is some value at risk for a node j\ud835\udc57jitalic_j and D\ud835\udc37Ditalic_D is a set of simulations. To this date, the DPV is among the top metrics to solve the FBP problem. Finally, the algorithm used as demonstrator, called baseline, was constructed and is presented in Algorithm.\u20051.Hyperparameter tuning was done performing a grid search using Deep Q-Learning with 1000 demonstrations over a shrinked version of the first forest of size 10\u00d710101010\\times 1010 \u00d7 10, which resulted in the values shown in Table 3. This process was done for small-net and big-net. For the pre-trained models those parameters found for big-net were used since it is the most similar architecture in terms of number of parameters and replicating the experiments was highly time consuming.In the analysis of the Sub20 landscape, the outcomes are highly satisfactory. This study systematically evaluates various algorithms across different network architectures, revealing that all tested algorithms not only converge but also outperform the baseline model detailed in 2.6. Significantly, these algorithms yield rewards that greatly exceed those of a random strategy, as depicted by the \u201crandom\" curve in our comparative analysis.A detailed qualitative examination of the reward curves in Fig.\u200510\nsuggests that the three primary algorithms under consideration perform comparably, with no single algorithm consistently outshining the others across all scenarios. This trend holds true across diverse architectures, except for efficient-net. Efficient-net uniquely demonstrates a marginal superiority in reward acquisition.Focusing on the aspect of fire spread mitigation, a qualitative comparison between the burn probability maps of the untreated landscape (Fig.\u20059) and the landscape post-treatment (Fig.\u200511) reveals a substantial reduction in fire spread. This is further quantified by examining the percentage of burned cells: under a random solution, 16.1% of the forest succumbs to fire, while the baseline algorithm limits this to 12.9%. In contrast, the solutions derived from DRL algorithms exhibit a narrower range of 11.31% to 12.86% in terms of burned forest area. These percentages correspond to the outcomes of the Deep Q-Learning (DQN) algorithm using the small-net architecture and the Dueling Double Deep Q-Learning (DDQN) algorithm implemented with efficient-net, respectively.In the context of the Sub40 forest simulation, the results are notably positive. A comparative analysis across five distinct network architectures reveals that all three employed algorithms not only achieve convergence but also significantly outperform the baseline algorithm. This superiority is evident in the reward metrics in Fig.\u200512, where the DRL algorithms attain reward levels close to -3.400, markedly better than the -3.700 achieved by the demonstrator algorithm. Moreover, these algorithms also surpass a random algorithm, which records a lower reward level of approximately -4.500. Interestingly, the performance of the three DRL algorithms is relatively comparable, with no single algorithm demonstrating a substantial edge over the others.Regarding the aspect of fire spread control, the DRL algorithms demonstrate a substantial reduction in the number of burned cells within the simulation. This efficacy is visually represented in Fig.\u200513\u2019s burn probability map, which exhibits lower values compared to the untreated landscape depicted in Fig.\u20059. Notably, Table 2. presents quantitative data where the percentage of burned cells is significantly lower for landscapes managed by the DRL algorithms compared to those managed by the baseline and a random algorithm. Specifically, while the baseline and random algorithms result in 23.25% and 28.36% of the total area being burned, respectively, the DRL algorithms, particularly the Deep Q-Learning (DQN) with efficient-net, limit this to a range of 21.55% to 21.78%. These findings underscore the effectiveness of DRL algorithms in mitigating fire spread in forest simulations.Another point to emphasize is that the results obtained in these applications do not show any consistent improvement through the introduction of network complexity. Specifically, small-net performs just as well as big-net, suggesting that the task at hand may not require increased model complexity. This notion is further supported by the observation that using transfer learning yields similar results to not using it. This could indicate that 1) the proposed architectures (small-net and big-net) are sufficiently deep and capable of extracting the necessary features for the task, or 2) it might suggest that too many layers of the pre-trained networks were \u2019frozen\u2019, preventing them from improving the results or 3) that the tasks for which the pre-trained models were trained for are not similar enough to the actual task at hand and therefore their usage is of no real convenience.Upon initial examination, the results suggest a greater improvement for Sub40 compared to Sub20. However, a closer analysis reveals a nuanced picture: the reduction in burned cells is 30% for Sub40 and slightly higher at 37% for Sub20. This observation indicates that the performance across both cases is similarly effective, despite the problem\u2019s complexity increasing exponentially with size. A plausible explanation for this counter intuitive finding could be that the more complex spreading patterns provide richer information inputs. These inputs potentially enable the agent to derive more sophisticated insights, offsetting the challenges posed by increased complexity.As previously discussed in 2.5.2, the models employed in this study inherently lack self-explanatory capabilities, this requiring the integration of explainability techniques to elucidate their predictive outcomes. This is particularly crucial in our application where understanding whether the model accounts for the impact of existing firebreaks is vital, a factor often overlooked in conventional methods. To address this, we generated attention maps using the methodology outlined in 2.5.2.These maps, specifically from Double Deep Q-Learning models using efficient-net (Fig.\u200514), were chosen to demonstrate the model\u2019s consideration of existing firebreaks, though they should not be viewed as indicative of overall results. The attention maps overlay the current forest state, with the red dot marking the proposed firebreak location. These maps highlight the model\u2019s focus areas when estimating the state-action function q\u2062(s,a,\u03b8)\ud835\udc5e\ud835\udc60\ud835\udc4e\ud835\udf03q(s,a,\\theta)italic_q ( italic_s , italic_a , italic_\u03b8 ), with higher values indicating greater attention.Figures \u200514, \u200514 and \u200514 illustrate consecutive decisions by the agent, reflecting a strategy to enclose a specific forest area. The model appears to prioritize cells safeguarded by the cumulative effect of the new and existing firebreaks, particularly in the lower-central region, suggesting an awareness of the synergistic effect with existing firebreaks. This observation tentatively supports the hypothesis that the model comprehends the complex interactions inherent in firebreak allocation.However, caution is advised in interpreting these results due to their subjective nature. Fig.\u200514 exemplifies this, showing the model\u2019s focus on seemingly unrelated areas for firebreak placement. This discrepancy underscores the need for cautious interpretation and further investigation into the model\u2019s decision-making process.\nThe primary motivation of this work was to assess the feasibility of using DRL to solve a challenging problem in forest engineering. This goal was achieved, under certain conditions. Firstly, considering the algorithms used, the incorporation of demonstrations is crucial for obtaining good results as shown in Fig.\u200515. Without them, neither convergence nor good performance is achieved. This could be due to various reasons, including: an environment with such high stochasticity that it does not allow the algorithm to consistently find good solutions, an action space where the majority of actions do not yield good results, forcing the algorithms to need guidance to move towards sections of the space with better returns, the possibility that the algorithms used may not be sophisticated enough to solve such complex problems, and the absence of individual feedback on each firebreak, which translates into a sparse reward structure inherent to the problem. Secondly, the performance of the algorithm is anchored to that of the demonstrator. This is a double-edged sword: on one hand, it allows the agent to quickly reach a reasonable level of performance, but on the other, it causes the agent not to deviate much from the demonstrator, and if the demonstrator is far from optimal behavior, so will the algorithm. Thus, learning by demonstration as used in this work is recommended if 1) a heuristic is known that solves the problem quickly, and 2) the performance of this heuristic is not too far from the optimal behavior (or the one that is sought to be achieved).A second motivation for this work was to see if these techniques could be used to improve the performance of a sub optimal algorithm. In this case, the algorithm chosen to be improved was the DPV, currently the best to solve the FPP. This was fully accomplished, surpassing this algorithm in all instances, with some showing a greater difference than others, but in all cases nonetheless. It is worth highlighting that even though our results surpass the DPV based heuristic, this last is considerably faster and able to solve much larger instances.A notable element of using RL techniques in scenarios like the one above is the ease with which problem-specific constraints can be incorporated. For this, it is sufficient to restrict the agent\u2019s behavior at the level of the neural network, in contrast to what it means to do so in a mixed-integer programming model. Setting constraints in these latter models is generally complex and can create complications in the associated resolution algorithms.In contrasting our research with those of Lauer et\u00a0al. (2017) and Altamimi et\u00a0al. (2022), we discern a range of approaches and objectives within forest management paradigms. Lauer\u2019s research is anchored in the economic incentives derived from timber production, utilizing stochastic models and heuristic optimization to maximize the net present value of harvested timber. Altamimi, conversely, aims to maximize the total volume of stands, employing reinforcement learning algorithms such as DQN and A2C to determine treatments for each stand, thereby influencing fire probability. Our research, in contrast, is geared towards minimizing the environmental impact associated with biomass burning. Also, while Altamimi\u2019s employs Feedforward Neural Networks (FFNNs) as approximating functions within their RL algorithms, we use Convolutional Neural Networks (CNNs) within our RL algorithms to view the forest landscape as a multi-dimensional image. This approach leverages the spatial context and multivariate attributes of the landscape, thereby enhancing the precision and effectiveness of our fire management strategies.While Lauer et\u00a0al. (2017) focuses on economic optimization and Altamimi et\u00a0al. (2022) on volume maximization, our study emphasizes the importance of sustainability and ecological conservation. This reflects a methodological advancement by integrating artificial intelligence technologies to more effectively address current ecological challenges. Our approach signifies a shift from financial or volume-centric traditional objectives towards a model that prioritizes environmental preservation, demonstrating the evolving landscape of forest management strategies in response to the pressing demands of ecological stewardship.In conclusion, this research significantly advances the field by offering a dual contribution. Firstly, it introduces the innovative use of RL techniques as a novel solution to the FPP, thereby distinguishing itself from traditional OR methods and establishing RL as a promising alternative. Secondly, through a thorough comparison of different algorithms, this study validates their effectiveness and suitability for the specific challenge of firebreak placement. To the best of our knowledge, this study marks the first application of this approach to the problem at hand, signifying an important step in exploring alternative methods for tackling complex operational challenges in wildfire management.We identify several avenues to further advance the proposed methodology in future work. The first point considers the problem\u2019s structure. The algorithms developed herein are each tailored to solve a specific instance, and as such, their ability to generalize across various instances falls beyond the scope of this work. Furthermore, it is a widely known fact that the before-mentioned sparse reward structure is detrimental to learning. One possible avenue for exploration could be to consider the historical performance of a specific firebreak each time a fire simulation occurs.Secondly, we identify the potential of more advanced algorithms. Since the focus of our study is on effective firebreak placement, we did not use a category of more sophisticated algorithms known as Policy Gradient (PG). Within this group, certain algorithms are regarded as state-of-the-art, including Trust Policy Optimization (Schulman et\u00a0al., 2015) and Proximal Policy Optimization (Schulman et\u00a0al., 2017). The rationale for their exclusion lies in the complexities associated with incorporating demonstrations into their learning processes.Finally, an alternative modeling strategy merits consideration: the adoption of pre-defined connected patterns of firebreaks. Some research has been conducted on the optimal shapes for grouping firebreaks, such as the ones used in (Palacios et\u00a0al., 2023). Implementing these u-shaped groups could significantly reduce the action space, focusing primarily on determining the central placement and orientation of each figure. This approach, through simplifying the decision-making process, could potentially broaden the scope and applicability of this research.The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.JC acknowledges the support of the Agencia Nacional de Investigaci\u00f3n y Desarrollo (ANID), Chile, through funding Postdoctoral Fondecyt project No 3210311. This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 101037419.",
    "19": "Adversarial imitation learning (AIL) has stood out as a dominant framework across various imitation learning (IL) applications, with Discriminator Actor Critic (DAC) (Kostrikov et\u00a0al., 2019) demonstrating the effectiveness of off-policy learning algorithms in improving sample efficiency and scalability to higher-dimensional observations. Despite DAC\u2019s empirical success, the original AIL objective is on-policy and DAC\u2019s ad-hoc application of off-policy training does not guarantee successful imitation (Kostrikov et\u00a0al., 2019; 2020). Follow-up work such as ValueDICE (Kostrikov et\u00a0al., 2020) tackles this issue by deriving a fully off-policy AIL objective.\nInstead in this work, we develop a novel and principled AIL algorithm via the framework of boosting. Like boosting, our new algorithm, AILBoost, maintains an ensemble of properly weighted weak learners (i.e., policies) and trains a discriminator that witnesses the maximum discrepancy between the distributions of the ensemble and the expert policy. We maintain a weighted replay buffer to represent the state-action distribution induced by the ensemble, allowing us to train discriminators using the entire data collected so far. In the weighted replay buffer, the contribution of the data from older policies are properly discounted with the weight computed based on the boosting framework.\nEmpirically, we evaluate our algorithm on both controller state-based and pixel-based environments from the DeepMind Control Suite. AILBoost outperforms DAC on both types of environments, demonstrating the benefit of properly weighting replay buffer data for off-policy training. On state-based environments, AILBoost outperforms ValueDICE and IQ-Learn(Garg et\u00a0al., 2021), achieving competitive performance with as little as one expert trajectory.Imitation learning (IL) is a promising paradigm for learning general policies without rewards from demonstration data, achieving remarkable success in autonomous driving (Bronstein et\u00a0al., 2022; Pomerleau, 1988), video games (Baker et\u00a0al., 2022; Shah et\u00a0al., 2022) and graphics (Peng et\u00a0al., 2021). Adversarial Imitation Learning (AIL) is an incredibly successful approach for imitation learning (Ho & Ermon, 2016; Fu et\u00a0al., 2018; Kostrikov et\u00a0al., 2019; Ke et\u00a0al., 2020). These methods cast IL as a distribution matching problem whereby the learning agent minimizes the divergence between the expert demonstrator\u2019s distribution and the state-action distribution induced by the agent. First introduced by (Ho & Ermon, 2016), this divergence minimization can be achieved in an iterative procedure reminiscent of GAN algorithms (Goodfellow et\u00a0al., 2014) with our learned reward function and policy being the discriminator and generator respectively.Originally, a limitation of many AIL methods was that they were on-policy. That is, for on-policy AIL methods like GAIL (Ho & Ermon, 2016) and AIRL (Fu et\u00a0al., 2018), the algorithm would draw fresh samples from the current policy in every iteration for the distribution matching process while discarding all old samples,\nrendering the sample complexity of these algorithms to be prohibitively large in many applications. Follow-up works (Kostrikov et\u00a0al., 2019; Sasaki et\u00a0al., 2019) attempt to relax the on-policy requirement by creating off-policy methods that utilize the entire history of observed data during the learning process. This history is often represented by a replay buffer and methods such as Discriminator Actor Critic (DAC) show large improvements in scalability and sample complexity over their on-policy counterparts. However, these methods modify the distribution matching objective as a divergence minimization between the replay buffer\u2019s and the expert\u2019s distribution, losing the guarantee of matching the expert\u2019s behavior.Algorithms like ValueDICE (Kostrikov et\u00a0al., 2020) address this problem by deriving a new formulation of the AIL divergence minimization objective to be entirely off-policy. ValueDICE, however, in principle relies on the environments to have deterministic dynamics.111One cannot derive an unbiased estimate of the objective function proposed in ValueDICE unless it has infinite expert samples and the transition is deterministic (Kostrikov et\u00a0al., 2020). See section\u00a03.3 for more detailed discussion. In this work, we consider a new perspective towards making AIL off-policy. We present a new principled off-policy AIL algorithm, AILBoost, via the gradient boosting framework (Mason et\u00a0al., 1999). AILBoost maintains an ensemble of properly weighted weak learners or policies as well as a weighted replay buffer to represent the state-action distribution induced by our ensemble. Our distribution matching objective is then to minimize the divergence between the weighted replay buffer\u2019s distribution (i.e., the state-action distribution induced by the ensemble) and the expert demonstrator\u2019s distribution, making the divergence minimization problem an off-policy learning problem. Similar to boosting and gradient boosting, at every iteration, we aim to find a weak learner, such that when added to the ensemble, the divergence between the updated ensemble\u2019s distribution and the expert\u2019s distribution decreases. In other words, our approach can be understood as performing gradient boosting in the state-action occupancy space, where black-box RL optimizer is used a weak learning procedure to train weak learners, i.e., policies.We evaluate AILBoost on the DeepMind Control Suite (Tassa et\u00a0al., 2018) and compare against a range of off-policy AIL algorithms (Behavior cloning, ValueDICE, DAC) as well as a state-of-the-art IL algorithm, IQ-Learn. We show that our algorithm is comparable to or more sample efficient than state-of-the-art IL algorithms in various continuous control tasks, achieving strong imitation performance with as little as one expert demonstration. We also show that our approach scales to vision-based, partially observable domains, where we again outperform DAC.There has also been a wide variety of research conducted on off-policy and offline IL, where the goal is to be either more sample efficient or safer by utilizing a replay buffer or not collecting any environmental transitions during training, respectively. The most prominent of said methods, and the closest to our work, is Discriminator-Actor-Critic (DAC) (Kostrikov et\u00a0al., 2019), which essentially replaces the on-policy RL algorithm in the adversarial IL setup with an off-policy one such as DDPG (Lillicrap et\u00a0al., 2019) or SAC (Haarnoja et\u00a0al., 2018). However, as mentioned previously, DAC doesn\u2019t necessarily guarantee a distribution match between the expert and the learned policy, prompting further work to be done. Further work has primarily focused on weighting on-policy and off-policy data differently in both the policy update and the discriminator update. ValueDICE (Kostrikov et\u00a0al., 2020) mitigates this problem by deriving an objective from the original distribution matching problem that only requires off-policy samples to compute. More recently, methods such as IQ-Learn (Garg et\u00a0al., 2021) have been developed to learn soft Q functions over the environment space, which encodes both a reward and a policy for inverse reinforcement learning, and model-based methods such as V-MAIL (Rafailov et\u00a0al., 2021) have shown that using expressive world models (Hafner et\u00a0al., 2020) leads to strong imitation results in domains with high-dimensional observations. Other off-policy IL works include SoftDICE (Sun et\u00a0al., 2021), SparseDICE (Camacho et\u00a0al., 2021), and AdVIL/AdRIL/DAeQuIL (Swamy et\u00a0al., 2021). \nOrthogonally, on the offline side, where environment interaction is prohibited, works both on the model-based side (Chang et\u00a0al., 2021) and the model-free side (Kim et\u00a0al., 2022; Yu et\u00a0al., 2023) has shown that distribution matching is still possible in these settings. These approaches generally operate either by learning a transition model of the environment, with which to roll out in to do policy optimization (Chang et\u00a0al., 2021), or optimizing a modified version of the objective introduced in (Kostrikov et\u00a0al., 2020) by using samples from the suboptimal offline dataset as opposed to on-policy samples for computation.The idea of using boosting for policy learning is not new in the deep learning or reinforcement learning literature. On the deep learning side, AdaGAN (Tolstikhin et\u00a0al., 2017) apply standard adaptive boosting to GANs (Goodfellow et\u00a0al., 2014) to address and fix issues such as mode collapse, while concurrent work (Grover & Ermon, 2017) showed benefits of boosting in general Bayesian mixture models. In RL, the conservative policy iteration (CPI) (Kakade & Langford, 2002) can be understood as performing gradient boosting in the policy space (Scherrer & Geist, 2014). The authors in (Hazan et\u00a0al., 2019) use a gradient boosting style approach to learn maximum entropy policies. In this work, we perform gradient boosting in the space of state-action occupancy measures, which leads to a principled off-policy IL approach.We consider a discounted infinite horizon MDP \u2133=\u27e8\ud835\udcae,P,\ud835\udc9c,r,\u03b3,\u03bc0\u27e9\u2133\ud835\udcae\ud835\udc43\ud835\udc9c\ud835\udc5f\ud835\udefesubscript\ud835\udf070\\mathcal{M}=\\langle{\\mathcal{S}},P,\\mathcal{A},r,\\gamma,\\mu_{0}\\ranglecaligraphic_M = \u27e8 caligraphic_S , italic_P , caligraphic_A , italic_r , italic_\u03b3 , italic_\u03bc start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u27e9 where \ud835\udcae\ud835\udcae{\\mathcal{S}}caligraphic_S is the state of states, \ud835\udc9c\ud835\udc9c\\mathcal{A}caligraphic_A is the set of actions, r:\ud835\udcae\u00d7\ud835\udc9c\u21a6\u211d:\ud835\udc5fmaps-to\ud835\udcae\ud835\udc9c\u211dr:{\\mathcal{S}}\\times\\mathcal{A}\\mapsto\\mathbb{R}italic_r : caligraphic_S \u00d7 caligraphic_A \u21a6 blackboard_R is the reward function and r\u2062(s,a)\ud835\udc5f\ud835\udc60\ud835\udc4er(s,a)italic_r ( italic_s , italic_a ) is the reward for the given state-action pair, \u03b3\u2208(0,1)\ud835\udefe01\\gamma\\in(0,1)italic_\u03b3 \u2208 ( 0 , 1 ) is the discount factor, \u03bc0\u2208\u0394\u2062(\ud835\udcae)subscript\ud835\udf070\u0394\ud835\udcae\\mu_{0}\\in\\Delta({\\mathcal{S}})italic_\u03bc start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u2208 roman_\u0394 ( caligraphic_S ) is the initial state distribution, and P:\ud835\udcae\u00d7\ud835\udc9c\u21a6\u0394\u2062(\ud835\udcae):\ud835\udc43maps-to\ud835\udcae\ud835\udc9c\u0394\ud835\udcaeP:{\\mathcal{S}}\\times\\mathcal{A}\\mapsto\\Delta({\\mathcal{S}})italic_P : caligraphic_S \u00d7 caligraphic_A \u21a6 roman_\u0394 ( caligraphic_S ) is the transition function. A policy \u03c0:\ud835\udcae\u2192\u0394\u2062(\ud835\udc9c):\ud835\udf0b\u2192\ud835\udcae\u0394\ud835\udc9c\\pi:{\\mathcal{S}}\\rightarrow\\Delta(\\mathcal{A})italic_\u03c0 : caligraphic_S \u2192 roman_\u0394 ( caligraphic_A ) interacts in said MDP, creating trajectories \u03c4\ud835\udf0f\\tauitalic_\u03c4 composed of state-action pairs {(st,at)}t=1Tsuperscriptsubscriptsubscript\ud835\udc60\ud835\udc61subscript\ud835\udc4e\ud835\udc61\ud835\udc611\ud835\udc47\\{(s_{t},a_{t})\\}_{t=1}^{T}{ ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT. We denote dt\u03c0subscriptsuperscript\ud835\udc51\ud835\udf0b\ud835\udc61d^{\\pi}_{t}italic_d start_POSTSUPERSCRIPT italic_\u03c0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT to represent the state-action visitation distribution induced by \u03c0\ud835\udf0b\\piitalic_\u03c0 at timestep t\ud835\udc61titalic_t and d\u03c0=(1\u2212\u03b3)\u2062\u2211t=0\u221e\u03b3t\u2062dt\u03c0superscript\ud835\udc51\ud835\udf0b1\ud835\udefesuperscriptsubscript\ud835\udc610superscript\ud835\udefe\ud835\udc61subscriptsuperscript\ud835\udc51\ud835\udf0b\ud835\udc61d^{\\pi}=(1-\\gamma)\\sum_{t=0}^{\\infty}\\gamma^{t}d^{\\pi}_{t}italic_d start_POSTSUPERSCRIPT italic_\u03c0 end_POSTSUPERSCRIPT = ( 1 - italic_\u03b3 ) \u2211 start_POSTSUBSCRIPT italic_t = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u221e end_POSTSUPERSCRIPT italic_\u03b3 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT italic_\u03c0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT as the average state-action visitation distribution induced by policy \u03c0\ud835\udf0b\\piitalic_\u03c0. We define the value function and Q\ud835\udc44Qitalic_Q-function of our policy as V\u03c0\u2062(s)=\ud835\udd3c\u03c0\u2062[\u2211t=0\u221e\u03b3t\u2062r\u2062(st)|s0=s]superscript\ud835\udc49\ud835\udf0b\ud835\udc60subscript\ud835\udd3c\ud835\udf0bdelimited-[]conditionalsuperscriptsubscript\ud835\udc610superscript\ud835\udefe\ud835\udc61\ud835\udc5fsubscript\ud835\udc60\ud835\udc61subscript\ud835\udc600\ud835\udc60V^{\\pi}(s)=\\mathbb{E}_{\\pi}[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t})|s_{0}=s]italic_V start_POSTSUPERSCRIPT italic_\u03c0 end_POSTSUPERSCRIPT ( italic_s ) = blackboard_E start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT [ \u2211 start_POSTSUBSCRIPT italic_t = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u221e end_POSTSUPERSCRIPT italic_\u03b3 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT italic_r ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) | italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = italic_s ] and Q\u03c0\u2062(s,a)=r\u2062(s,a)+\ud835\udd3cs\u2032\u223cP(\u22c5|s,a)\u2062[V\u03c0\u2062(s\u2032)]Q^{\\pi}(s,a)=r(s,a)+\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot|s,a)}[V^{\\pi}(s^{\\prime%\n})]italic_Q start_POSTSUPERSCRIPT italic_\u03c0 end_POSTSUPERSCRIPT ( italic_s , italic_a ) = italic_r ( italic_s , italic_a ) + blackboard_E start_POSTSUBSCRIPT italic_s start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u223c italic_P ( \u22c5 | italic_s , italic_a ) end_POSTSUBSCRIPT [ italic_V start_POSTSUPERSCRIPT italic_\u03c0 end_POSTSUPERSCRIPT ( italic_s start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) ]. The goal of RL is to find a policy that maximizes the expected cumulative reward.In imitation learning, instead of having access to the reward function, we assume access to demonstrations \ud835\udc9fe={(si,ai)}i=1Nsuperscript\ud835\udc9f\ud835\udc52superscriptsubscriptsubscript\ud835\udc60\ud835\udc56subscript\ud835\udc4e\ud835\udc56\ud835\udc561\ud835\udc41\\mathcal{D}^{e}=\\{(s_{i},a_{i})\\}_{i=1}^{N}caligraphic_D start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT = { ( italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT from an expert policy \u03c0esuperscript\ud835\udf0b\ud835\udc52\\pi^{e}italic_\u03c0 start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT that our policy can take advantage of while training. Note that \u03c0esuperscript\ud835\udf0b\ud835\udc52\\pi^{e}italic_\u03c0 start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT might not necessarily be a Markovian policy. It is possible that \u03c0esuperscript\ud835\udf0b\ud835\udc52\\pi^{e}italic_\u03c0 start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT is an ensemble of weighted Markovian policies, i.e., \u03c0e={\u03b1i,\u03c0i}i=1nsuperscript\ud835\udf0b\ud835\udc52superscriptsubscriptsubscript\ud835\udefc\ud835\udc56subscript\ud835\udf0b\ud835\udc56\ud835\udc561\ud835\udc5b\\pi^{e}=\\{\\alpha_{i},\\pi_{i}\\}_{i=1}^{n}italic_\u03c0 start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT = { italic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_\u03c0 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT with \u03b1i\u22650,\u2211i\u03b1i=1formulae-sequencesubscript\ud835\udefc\ud835\udc560subscript\ud835\udc56subscript\ud835\udefc\ud835\udc561\\alpha_{i}\\geq 0,\\sum_{i}\\alpha_{i}=1italic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2265 0 , \u2211 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1, which means that for each episode, \u03c0esuperscript\ud835\udf0b\ud835\udc52\\pi^{e}italic_\u03c0 start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT will first randomly sample a policy \u03c0isubscript\ud835\udf0b\ud835\udc56\\pi_{i}italic_\u03c0 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT with probability \u03b1isubscript\ud835\udefc\ud835\udc56\\alpha_{i}italic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT at t=0\ud835\udc610t=0italic_t = 0, and then execute \u03c0isubscript\ud835\udf0b\ud835\udc56\\pi_{i}italic_\u03c0 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT for the entire episode (i.e., no switch to other policies during the execution for an episode). It is well known that the space of state action distributions induced by such ensembles is larger than the space of state-action distributions induced by Markovian policies (Hazan et\u00a0al., 2019). The goal in IL is then to learn a policy that robustly mimics the expert. The simplest imitation learning algorithm to address this issue is behavior cloning (BC):\nargmin\u03c0\u2208\u03a0\ud835\udd3c(s,a)\u223c\ud835\udc9fe\u2062[\u2113\u2062(\u03c0\u2062(s),a)]subscriptargmin\ud835\udf0b\u03a0subscript\ud835\udd3csimilar-to\ud835\udc60\ud835\udc4esuperscript\ud835\udc9f\ud835\udc52delimited-[]\u2113\ud835\udf0b\ud835\udc60\ud835\udc4e\\mathop{\\mathrm{argmin}}_{\\pi\\in\\Pi}\\mathbb{E}_{(s,a)\\sim\\mathcal{D}^{e}}[\\ell%\n(\\pi(s),a)]roman_argmin start_POSTSUBSCRIPT italic_\u03c0 \u2208 roman_\u03a0 end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT ( italic_s , italic_a ) \u223c caligraphic_D start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT end_POSTSUBSCRIPT [ roman_\u2113 ( italic_\u03c0 ( italic_s ) , italic_a ) ]\nwhere \u2113\u2113\\ellroman_\u2113 is a classification loss and \u03a0\u03a0\\Piroman_\u03a0 is our policy class. Though this objective is simple, it is known to suffer from covariate shift at test time (Pomerleau, 1988; Ross et\u00a0al., 2011). Instead of minimizing action distribution divergence conditioned on expert states, algorithms such as inverse RL (Ziebart et\u00a0al., 2008) and adversarial IL (Ho & Ermon, 2016; Finn et\u00a0al., 2016; Ke et\u00a0al., 2020; Sun et\u00a0al., 2019) directly minimize some divergence metrics between state-action distributions, which help address the covariate shift issue (Agarwal et\u00a0al., 2019).The goal of AIL is to directly minimize some divergence between some behavior policy state-action visitation d\u03c0superscript\ud835\udc51\ud835\udf0bd^{\\pi}italic_d start_POSTSUPERSCRIPT italic_\u03c0 end_POSTSUPERSCRIPT and an expert policy state-action visitation d\u03c0esuperscript\ud835\udc51superscript\ud835\udf0b\ud835\udc52d^{\\pi^{e}}italic_d start_POSTSUPERSCRIPT italic_\u03c0 start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT. The choice of divergence results in variously different AIL algorithms.The most popular AIL algorithm is Generative Adversarial Imitation Learning (GAIL) (Ho & Ermon, 2016) which minimizes the JS-divergence. This algorithm is a on-policy adversarial imitation learning algorithm that connects Generative Adversarial Networks (GANs) (Goodfellow et\u00a0al., 2014) and maximum entropy IRL (Ziebart et\u00a0al., 2008). GAIL\u00a0 trains a binary classifier called the discriminator D\u2062(s,a)\ud835\udc37\ud835\udc60\ud835\udc4eD(s,a)italic_D ( italic_s , italic_a ) to distinguish between samples from the expert distribution and the policy generated distribution. Using the discriminator to define a reward function, GAIL\u00a0 then executes an on-policy RL algorithm such as Trust Region Policy Optimization (TRPO) (Schulman et\u00a0al., 2017a) or Proximal Policy Optimization (PPO) (Schulman et\u00a0al., 2017b) to maximize the reward. That gives us the following adversarial objective:where H\u2062(\u03c0)\ud835\udc3b\ud835\udf0bH(\\pi)italic_H ( italic_\u03c0 ) is an entropy regularization term. The first term in eq.\u00a01 can be viewed as a pseudo reward that can be optimized with respect to the the policy \u03c0\ud835\udf0b\\color[rgb]{0,0,1}\\piitalic_\u03c0 on-policy samples. Note that GAIL typically optimizes both policies and discriminators using on-policy samples, making it quite sample inefficient.\nUsing different divergences, there are various reward functions that can be optimized with this framework (Orsini et\u00a0al., 2021). In this work, while our proposed approach in general is capable of optimizing many common divergences, we mainly focus on reverse KL divergence in our experiments. Reverse KL divergence has been studied in prior works including Fu et\u00a0al. (2018); Ke et\u00a0al. (2020). But different from prior works, we propose an off-policy method for optimizing reverse KL by leveraging the framework of boosting.One reason GAIL\u00a0 need a lot of interactions with the environment to learn properly is because of the dependency on using on-policy approaches to optimize discriminators and policies. In particular, GAIL does not reuse any old samples.\nDiscriminator Actor Critic (DAC) (Kostrikov et\u00a0al., 2019) extends GAIL\u00a0 algorithms to take advantage of off-policy learning to optimize the discriminators and policies.DAC introduces a replay buffer \u211b\u211b\\mathcal{R}caligraphic_R to represent the history of transitions observed throughout training in the context of IRL. This replay buffer allows DAC to perform off-policy training of the policy and the discriminator (similar to (Sasaki et\u00a0al., 2019)). Formally, DAC optimizes its discriminator with the objective:where this objective minimize the divergence between the expert distribution and the replay buffer \u211b\u211b\\color[rgb]{1,.5,0}\\mathcal{R}caligraphic_R distribution. Intuitively, this divergence does not strictly capture the divergence of our policy distribution and the expert distribution, but a mixture of evenly weighted policies learned up until the current policy. To rigorously recover a divergence between our policy distribution and the expert distribution we need to apply importance weights:\nmin\u03c0\u2061maxD\u2061\ud835\udd3cs,a\u223c\u211b\u2062[p\u03c0\u2062(s,a)p\u211b\u2062(s,a)\u2062log\u2061D\u2062(s,a)]+\ud835\udd3cs,a\u223c\u03c0e\u2062[log\u2061(1\u2212D\u2062(s,a))]\u2212\u03bb\u2062H\u2062(\u03c0)subscript\ud835\udf0bsubscript\ud835\udc37subscript\ud835\udd3csimilar-to\ud835\udc60\ud835\udc4e\u211bdelimited-[]subscript\ud835\udc5d\ud835\udf0b\ud835\udc60\ud835\udc4esubscript\ud835\udc5d\u211b\ud835\udc60\ud835\udc4e\ud835\udc37\ud835\udc60\ud835\udc4esubscript\ud835\udd3csimilar-to\ud835\udc60\ud835\udc4esuperscript\ud835\udf0b\ud835\udc52delimited-[]1\ud835\udc37\ud835\udc60\ud835\udc4e\ud835\udf06\ud835\udc3b\ud835\udf0b\\min\\limits_{\\color[rgb]{0,0,1}\\pi}\\max\\limits_{D}\\mathbb{E}_{s,a\\sim\\color[%\nrgb]{1,.5,0}\\mathcal{R}}\\left[\\frac{p_{{\\color[rgb]{0,0,1}\\pi}}(s,a)}{p_{{%\n\\color[rgb]{1,.5,0}\\mathcal{R}}}(s,a)}\\log D(s,a)\\right]+\\mathbb{E}_{s,a\\sim%\n\\pi^{e}}\\left[\\log(1-D(s,a))\\right]-\\lambda H(\\pi)roman_min start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_s , italic_a \u223c caligraphic_R end_POSTSUBSCRIPT [ divide start_ARG italic_p start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT ( italic_s , italic_a ) end_ARG start_ARG italic_p start_POSTSUBSCRIPT caligraphic_R end_POSTSUBSCRIPT ( italic_s , italic_a ) end_ARG roman_log italic_D ( italic_s , italic_a ) ] + blackboard_E start_POSTSUBSCRIPT italic_s , italic_a \u223c italic_\u03c0 start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT end_POSTSUBSCRIPT [ roman_log ( 1 - italic_D ( italic_s , italic_a ) ) ] - italic_\u03bb italic_H ( italic_\u03c0 ).\nWhile this objective recovers the on-policy objective of GAIL\u00a0(Equation\u00a01), the authors note that estimating the density ratio is difficult and has high variance in practice. Furthermore, they note that the not using importance weights (Equation\u00a02) works well in practice, but does not guarantee successful imitation, especially when the distribution induced by the replay buffer, \u211b\u211b\\color[rgb]{1,.5,0}\\mathcal{R}caligraphic_R, is far from our current policy\u2019s state-action distribution. This is a fundamental problem of DAC.ValueDICE\u00a0 (Kostrikov et\u00a0al., 2020) was proposed to address the density estimation issue of off-policy AIL algorithms formalized in DAC\u00a0(see section\u00a03.2). ValueDICE\u00a0aims to minimize the reverse KL divergence written in its Donsker-Varadhan (Donsker & Varadhan, 1983) dual form:Motivated from DualDICE (Nachum et\u00a0al., 2019), ValueDICE\u00a0performs a change of variable using the Bellman operator \u212c\u03c0superscript\u212c\ud835\udf0b\\mathcal{B}^{\\pi}caligraphic_B start_POSTSUPERSCRIPT italic_\u03c0 end_POSTSUPERSCRIPT222A bellman operator \u212c\u03c0superscript\u212c\ud835\udf0b\\mathcal{B}^{\\pi}caligraphic_B start_POSTSUPERSCRIPT italic_\u03c0 end_POSTSUPERSCRIPT is defined as follows: given any function f\u2062(s,a)\ud835\udc53\ud835\udc60\ud835\udc4ef(s,a)italic_f ( italic_s , italic_a ), we have \u212c\u03c0f(s,a):=r(s,a)+\ud835\udd3cs\u2032\u223cP\u2062(s,a)f(s\u2032,\u03c0(s\u2032),\u2200s,a\\mathcal{B}^{\\pi}f(s,a):=r(s,a)+\\mathbb{E}_{s^{\\prime}\\sim P(s,a)}f(s^{\\prime}%\n,\\pi(s^{\\prime}),\\forall s,acaligraphic_B start_POSTSUPERSCRIPT italic_\u03c0 end_POSTSUPERSCRIPT italic_f ( italic_s , italic_a ) := italic_r ( italic_s , italic_a ) + blackboard_E start_POSTSUBSCRIPT italic_s start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u223c italic_P ( italic_s , italic_a ) end_POSTSUBSCRIPT italic_f ( italic_s start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT , italic_\u03c0 ( italic_s start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) , \u2200 italic_s , italic_a. with respect to the policy \u03c0\ud835\udf0b\\piitalic_\u03c0; x\u2062(s,a)=\u03bd\u2062(s,a)\u2212\u212c\u03c0\u2062(s,a)\ud835\udc65\ud835\udc60\ud835\udc4e\ud835\udf08\ud835\udc60\ud835\udc4esuperscript\u212c\ud835\udf0b\ud835\udc60\ud835\udc4ex(s,a)=\\nu(s,a)-\\mathcal{B}^{\\pi}(s,a)italic_x ( italic_s , italic_a ) = italic_\u03bd ( italic_s , italic_a ) - caligraphic_B start_POSTSUPERSCRIPT italic_\u03c0 end_POSTSUPERSCRIPT ( italic_s , italic_a ); resulting the following objective:Now the objective function does not contain on-policy distribution d\u03c0superscript\ud835\udc51\ud835\udf0bd^{\\pi}italic_d start_POSTSUPERSCRIPT italic_\u03c0 end_POSTSUPERSCRIPT (in fact only the initial state distribution \u03bc0subscript\ud835\udf070\\mu_{0}italic_\u03bc start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and the expert distribution). Despite being able to only using d\u03c0esuperscript\ud835\udc51superscript\ud835\udf0b\ud835\udc52d^{\\pi^{e}}italic_d start_POSTSUPERSCRIPT italic_\u03c0 start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT and \u03bc0subscript\ud835\udf070\\mu_{0}italic_\u03bc start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, the authors have identified two aspects of the objective that will yield biased estimates. First, the first expectation has a logarithm outside of it which would make mini-batch estimates of this expectation biased. Moreover, inside the first expectation term, we have \u03bd\u2062(s,a)\u2212\u212c\u03c0\u2062\u03bd\u2062(s,a)\ud835\udf08\ud835\udc60\ud835\udc4esuperscript\u212c\ud835\udf0b\ud835\udf08\ud835\udc60\ud835\udc4e\\nu(s,a)-\\mathcal{B}^{\\pi}\\nu(s,a)italic_\u03bd ( italic_s , italic_a ) - caligraphic_B start_POSTSUPERSCRIPT italic_\u03c0 end_POSTSUPERSCRIPT italic_\u03bd ( italic_s , italic_a ) with \u212c\u03c0superscript\u212c\ud835\udf0b\\mathcal{B}^{\\pi}caligraphic_B start_POSTSUPERSCRIPT italic_\u03c0 end_POSTSUPERSCRIPT being the Bellman operator. This limits ValueDICE\u2019s objective to only be unbiased for environments with deterministic transitions. This is related to the famous double sampling issue in TD learning. Although many popular RL benchmarks have deterministic transitions (Bellemare et\u00a0al., 2013; Tassa et\u00a0al., 2018; Todorov et\u00a0al., 2012), this was a limitation not present in the GAIL.In this work, we take a different perspective than ValueDICE to derive an off-policy AIL algorithm. Different from ValueDICE, our approach is both off-policy and is amenable to mini-batch updates even with stochastic environment transition dynamics.Our algorithm, Adversarial Imitation Learning via Boosting (AILBoost) \u2013 motivated by classic gradient boosting algorithms (Friedman, 2001; Mason et\u00a0al., 1999) \u2013 attempts to mitigate a fundamental issue related to off-policy imitation learning formalized in DAC\u00a0(see section\u00a03.2). The key idea is to treat learned policies as weak learners, form an ensemble of them (with a proper weighting scheme derived from a gradient boosting perspective), and update the ensemble via gradient boosting.Weighted policy ensemble. Our algorithm will learn a weighted ensemble of policies, denoted as \ud835\udf45:={\u03b1i,\u03c0i}i=1nassign\ud835\udf45superscriptsubscriptsubscript\ud835\udefc\ud835\udc56subscript\ud835\udf0b\ud835\udc56\ud835\udc561\ud835\udc5b\\bm{\\pi}:=\\{\\alpha_{i},\\pi_{i}\\}_{i=1}^{n}bold_italic_\u03c0 := { italic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_\u03c0 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT with \u03b1i\u22650,\u2211i\u03b1i=1formulae-sequencesubscript\ud835\udefc\ud835\udc560subscript\ud835\udc56subscript\ud835\udefc\ud835\udc561\\alpha_{i}\\geq 0,\\sum_{i}\\alpha_{i}=1italic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2265 0 , \u2211 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1 and \u03c0isubscript\ud835\udf0b\ud835\udc56\\pi_{i}italic_\u03c0 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT being some Markovian policy. The way the mixture works is that when executing \ud835\udf45\ud835\udf45\\bm{\\pi}bold_italic_\u03c0, at the beginning of an episode, a Markovian policy \u03c0isubscript\ud835\udf0b\ud835\udc56\\pi_{i}italic_\u03c0 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is sampled with probability \u03b1isubscript\ud835\udefc\ud835\udc56\\alpha_{i}italic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, and then \u03c0isubscript\ud835\udf0b\ud835\udc56\\pi_{i}italic_\u03c0 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is executed for the entire episode (i.e., no policy switch in an episode). Note that \ud835\udf45\ud835\udf45\\bm{\\pi}bold_italic_\u03c0 itself is not a Markovian policy anymore due to the sampling process at the beginning of the episode, and in fact, such mixture policy\u2019s induced state-action distribution can be richer than that from Markovian policies (Hazan et\u00a0al., 2019). This is consistent with the idea of boosting: by combining weak learners, i.e., Markovian policies, we form a more powerful policy. Given the above definition of \ud835\udf45\ud835\udf45\\bm{\\pi}bold_italic_\u03c0, we immediately have d\ud835\udf45:=\u2211i\u03b1i\u2062d\u03c0iassignsuperscript\ud835\udc51\ud835\udf45subscript\ud835\udc56subscript\ud835\udefc\ud835\udc56superscript\ud835\udc51subscript\ud835\udf0b\ud835\udc56d^{\\bm{\\pi}}:=\\sum_{i}\\alpha_{i}d^{\\pi_{i}}italic_d start_POSTSUPERSCRIPT bold_italic_\u03c0 end_POSTSUPERSCRIPT := \u2211 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_d start_POSTSUPERSCRIPT italic_\u03c0 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, i.e., the weighted mixture of the state-action distributions induced by Markovian policies \u03c0isubscript\ud835\udf0b\ud835\udc56\\pi_{i}italic_\u03c0 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.Notation wise, given a dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic_D, we denote \ud835\udd3c^\ud835\udc9f\u2062[f\u2062(x)]subscript^\ud835\udd3c\ud835\udc9fdelimited-[]\ud835\udc53\ud835\udc65\\widehat{\\mathbb{E}}_{\\mathcal{D}}[f(x)]over^ start_ARG blackboard_E end_ARG start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT [ italic_f ( italic_x ) ] as the empirical function average across the dataset, i.e., \ud835\udd3c^\ud835\udc9f\u2062[f\u2062(x)]=\u2211x\u2208\ud835\udc9ff\u2062(x)/|\ud835\udc9f|subscript^\ud835\udd3c\ud835\udc9fdelimited-[]\ud835\udc53\ud835\udc65subscript\ud835\udc65\ud835\udc9f\ud835\udc53\ud835\udc65\ud835\udc9f\\widehat{\\mathbb{E}}_{\\mathcal{D}}[f(x)]=\\sum_{x\\in\\mathcal{D}}f(x)/\\left%\n\\lvert\\mathcal{D}\\right\\rvertover^ start_ARG blackboard_E end_ARG start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT [ italic_f ( italic_x ) ] = \u2211 start_POSTSUBSCRIPT italic_x \u2208 caligraphic_D end_POSTSUBSCRIPT italic_f ( italic_x ) / | caligraphic_D |.We would like to minimize the reverse KL divergence between our policy state-action distribution d\ud835\udf45superscript\ud835\udc51\ud835\udf45d^{\\bm{\\pi}}italic_d start_POSTSUPERSCRIPT bold_italic_\u03c0 end_POSTSUPERSCRIPT and the expert distribution d\u03c0esuperscript\ud835\udc51superscript\ud835\udf0b\ud835\udc52d^{\\pi^{e}}italic_d start_POSTSUPERSCRIPT italic_\u03c0 start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT \u2013 denoted by \u2113(d\ud835\udf45,d\u03c0e)=KL(d\ud835\udf45||d\u03c0e):=\u2211s,ad\ud835\udf45(s,a)ln(d\ud835\udf45(s,a)/d\u03c0e(s,a))\\ell(d^{\\bm{\\pi}},d^{\\pi^{e}})=\\text{KL}(d^{\\bm{\\pi}}||d^{\\pi^{e}}):=\\sum_{s,a%\n}d^{\\bm{\\pi}}(s,a)\\ln(d^{\\bm{\\pi}}(s,a)/d^{\\pi^{e}}(s,a))roman_\u2113 ( italic_d start_POSTSUPERSCRIPT bold_italic_\u03c0 end_POSTSUPERSCRIPT , italic_d start_POSTSUPERSCRIPT italic_\u03c0 start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT ) = KL ( italic_d start_POSTSUPERSCRIPT bold_italic_\u03c0 end_POSTSUPERSCRIPT | | italic_d start_POSTSUPERSCRIPT italic_\u03c0 start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT ) := \u2211 start_POSTSUBSCRIPT italic_s , italic_a end_POSTSUBSCRIPT italic_d start_POSTSUPERSCRIPT bold_italic_\u03c0 end_POSTSUPERSCRIPT ( italic_s , italic_a ) roman_ln ( italic_d start_POSTSUPERSCRIPT bold_italic_\u03c0 end_POSTSUPERSCRIPT ( italic_s , italic_a ) / italic_d start_POSTSUPERSCRIPT italic_\u03c0 start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT ( italic_s , italic_a ) ). The reasons that we focus on reverse KL is that (1) it has been argued that the mode seeking property of reverse KL is more suitable for imitation learning (Ke et\u00a0al., 2020), (2) reverse KL is on-policy in nature, i.e., it focuses on minimizing the divergence of our policy\u2019s action distribution and the expert\u2019s at the states from our policy, which help address the covariate shift issue, and (3) the baselines we consider in experiments, DAC and ValueDICE, all minimize the reverse KL divergence such as AIRL\u00a0in practice 333See the official repository. At a high level, our approach directly optimizes \u2113\u2062(d\ud835\udf45,d\u03c0e)\u2113superscript\ud835\udc51\ud835\udf45superscript\ud835\udc51superscript\ud835\udf0b\ud835\udc52\\ell(d^{\\bm{\\pi}},d^{\\pi^{e}})roman_\u2113 ( italic_d start_POSTSUPERSCRIPT bold_italic_\u03c0 end_POSTSUPERSCRIPT , italic_d start_POSTSUPERSCRIPT italic_\u03c0 start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT ) via gradient boosting (Mason et\u00a0al., 1999) in the state-action occupancy space. Our ensemble \ud835\udf45\ud835\udf45\\bm{\\pi}bold_italic_\u03c0 induces the following mixture state-action occupancy measure:To compute a new weak learner \u03c0t+1subscript\ud835\udf0b\ud835\udc611\\pi_{t+1}italic_\u03c0 start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT, we will first compute the functional gradient of loss \u2113\u2113\\ellroman_\u2113 with respect to d\ud835\udf45superscript\ud835\udc51\ud835\udf45d^{\\bm{\\pi}}italic_d start_POSTSUPERSCRIPT bold_italic_\u03c0 end_POSTSUPERSCRIPT, i.e., \u2207\u2113\u2062(d,d\u03c0e)|d=d\ud835\udf45evaluated-at\u2207\u2113\ud835\udc51superscript\ud835\udc51superscript\ud835\udf0b\ud835\udc52\ud835\udc51superscript\ud835\udc51\ud835\udf45\\nabla\\ell(d,d^{\\pi^{e}})|_{d=d^{\\bm{\\pi}}}\u2207 roman_\u2113 ( italic_d , italic_d start_POSTSUPERSCRIPT italic_\u03c0 start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT ) | start_POSTSUBSCRIPT italic_d = italic_d start_POSTSUPERSCRIPT bold_italic_\u03c0 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT. The new weak learner \u03c0t+1subscript\ud835\udf0b\ud835\udc611\\pi_{t+1}italic_\u03c0 start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT is learned via the following optimization procedure: \u03c0t+1=argmax\u03c0~\u2208\u03a0\u27e8d\u03c0~,\u2212\u2207\u2113\u2062(d,d\u03c0e)|d=d\ud835\udf45\u27e9subscript\ud835\udf0b\ud835\udc611subscriptargmax~\ud835\udf0b\u03a0superscript\ud835\udc51~\ud835\udf0bevaluated-at\u2207\u2113\ud835\udc51superscript\ud835\udc51superscript\ud835\udf0b\ud835\udc52\ud835\udc51superscript\ud835\udc51\ud835\udf45\\pi_{t+1}=\\mathop{\\mathrm{argmax}}_{\\tilde{\\pi}\\in\\Pi}\\langle d^{\\tilde{\\pi}},%\n-\\nabla\\ell(d,d^{\\pi^{e}})|_{d=d^{\\bm{\\pi}}}\\rangleitalic_\u03c0 start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = roman_argmax start_POSTSUBSCRIPT over~ start_ARG italic_\u03c0 end_ARG \u2208 roman_\u03a0 end_POSTSUBSCRIPT \u27e8 italic_d start_POSTSUPERSCRIPT over~ start_ARG italic_\u03c0 end_ARG end_POSTSUPERSCRIPT , - \u2207 roman_\u2113 ( italic_d , italic_d start_POSTSUPERSCRIPT italic_\u03c0 start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT ) | start_POSTSUBSCRIPT italic_d = italic_d start_POSTSUPERSCRIPT bold_italic_\u03c0 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT \u27e9. Namely, we aim to search for a new policy \u03c0t+1subscript\ud835\udf0b\ud835\udc611\\pi_{t+1}italic_\u03c0 start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT such that its state-action occupancy measure d\u03c0t+1superscript\ud835\udc51subscript\ud835\udf0b\ud835\udc611d^{\\pi_{t+1}}italic_d start_POSTSUPERSCRIPT italic_\u03c0 start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT is aligned with the negative gradient \u2212\u2207\u2113\u2207\u2113-\\nabla\\ell- \u2207 roman_\u2113 as much as possible. Note that the above optimization problem can be understood as an RL procedure where the reward function is defined as \u2212\u2207\u2113\u2062(d,d\u03c0e)|d=d\ud835\udf45\u2208\u211dS\u2062Aevaluated-at\u2207\u2113\ud835\udc51superscript\ud835\udc51subscript\ud835\udf0b\ud835\udc52\ud835\udc51superscript\ud835\udc51\ud835\udf45superscript\u211d\ud835\udc46\ud835\udc34-\\nabla\\ell(d,d^{\\pi_{e}})|_{d=d^{\\bm{\\pi}}}\\in\\mathbb{R}^{SA}- \u2207 roman_\u2113 ( italic_d , italic_d start_POSTSUPERSCRIPT italic_\u03c0 start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) | start_POSTSUBSCRIPT italic_d = italic_d start_POSTSUPERSCRIPT bold_italic_\u03c0 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_S italic_A end_POSTSUPERSCRIPT. Once we compute the weak learner \u03c0t+1subscript\ud835\udf0b\ud835\udc611\\pi_{t+1}italic_\u03c0 start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT, we mix it into the policy ensemble with a fixed learning rate \u03b1\u2208(0,1)\ud835\udefc01\\alpha\\in(0,1)italic_\u03b1 \u2208 ( 0 , 1 ) \u2013 denoted as d\ud835\udf45\u2032=(1\u2212\u03b1)\u2062d\ud835\udf45+\u03b1\u2062d\u03c0t+1superscript\ud835\udc51superscript\ud835\udf45\u20321\ud835\udefcsuperscript\ud835\udc51\ud835\udf45\ud835\udefcsuperscript\ud835\udc51subscript\ud835\udf0b\ud835\udc611d^{\\bm{\\pi}^{\\prime}}=(1-\\alpha)d^{\\bm{\\pi}}+\\alpha d^{\\pi_{t+1}}italic_d start_POSTSUPERSCRIPT bold_italic_\u03c0 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT = ( 1 - italic_\u03b1 ) italic_d start_POSTSUPERSCRIPT bold_italic_\u03c0 end_POSTSUPERSCRIPT + italic_\u03b1 italic_d start_POSTSUPERSCRIPT italic_\u03c0 start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT. Note that the above mixing step can be interpreted as gradient boosting in the state-action occupancy space directly: we re-write the update procedure as d\ud835\udf45\u2032=d\ud835\udf45+\u03b1\u2062(d\u03c0t+1\u2212d\ud835\udf45)superscript\ud835\udc51superscript\ud835\udf45\u2032superscript\ud835\udc51\ud835\udf45\ud835\udefcsuperscript\ud835\udc51subscript\ud835\udf0b\ud835\udc611superscript\ud835\udc51\ud835\udf45d^{\\bm{\\pi}^{\\prime}}=d^{\\bm{\\pi}}+\\alpha(d^{\\pi_{t+1}}-d^{\\bm{\\pi}})italic_d start_POSTSUPERSCRIPT bold_italic_\u03c0 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT = italic_d start_POSTSUPERSCRIPT bold_italic_\u03c0 end_POSTSUPERSCRIPT + italic_\u03b1 ( italic_d start_POSTSUPERSCRIPT italic_\u03c0 start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT - italic_d start_POSTSUPERSCRIPT bold_italic_\u03c0 end_POSTSUPERSCRIPT ), where the ascent direction d\u03c0t+1\u2212d\ud835\udf45superscript\ud835\udc51subscript\ud835\udf0b\ud835\udc611superscript\ud835\udc51\ud835\udf45d^{\\pi_{t+1}}-d^{\\bm{\\pi}}italic_d start_POSTSUPERSCRIPT italic_\u03c0 start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT - italic_d start_POSTSUPERSCRIPT bold_italic_\u03c0 end_POSTSUPERSCRIPT is approximating the (negative) functional gradient \u2212\u2207\u2113\u2207\u2113-\\nabla\\ell- \u2207 roman_\u2113, since argmax\u03c0\u27e8d\u03c0\u2212d\ud835\udf45,\u2212\u2207\u2113\u27e9=\u03c0t+1subscriptargmax\ud835\udf0bsuperscript\ud835\udc51\ud835\udf0bsuperscript\ud835\udc51\ud835\udf45\u2207\u2113subscript\ud835\udf0b\ud835\udc611\\mathop{\\mathrm{argmax}}_{\\pi}\\langle d^{\\pi}-d^{\\bm{\\pi}},-\\nabla\\ell\\rangle=%\n\\pi_{t+1}roman_argmax start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT \u27e8 italic_d start_POSTSUPERSCRIPT italic_\u03c0 end_POSTSUPERSCRIPT - italic_d start_POSTSUPERSCRIPT bold_italic_\u03c0 end_POSTSUPERSCRIPT , - \u2207 roman_\u2113 \u27e9 = italic_\u03c0 start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT by the definition of \u03c0t+1subscript\ud835\udf0b\ud835\udc611\\pi_{t+1}italic_\u03c0 start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT. It has been shown that such procedure is guaranteed to minimize the objective function (i.e., reverse KL in this case) as long as the objective is smooth (our loss \u2113\u2113\\ellroman_\u2113 will be smooth as long as d\ud835\udf45superscript\ud835\udc51\ud835\udf45d^{\\bm{\\pi}}italic_d start_POSTSUPERSCRIPT bold_italic_\u03c0 end_POSTSUPERSCRIPT is non-zero everywhere) (e.g., see (Hazan et\u00a0al., 2019) for the claim).444Note that similar to AdaBoost, each weaker is not directly optimizing the original objective, but the weighted combination of the weaker learners optimizes the original objective function \u2013 the reverse KL in our case. Algorithmically, we first express the reverse KL divergence in its variational form (Nowozin et\u00a0al., 2016; Ke et\u00a0al., 2020):where g:\ud835\udcae\u00d7\ud835\udc9c\u21a6\u211d:\ud835\udc54maps-to\ud835\udcae\ud835\udc9c\u211dg:{\\mathcal{S}}\\times\\mathcal{A}\\mapsto\\mathbb{R}italic_g : caligraphic_S \u00d7 caligraphic_A \u21a6 blackboard_R is a discriminator. The benefit of using this variational form is that computing the functional (sub)-gradient of the reverse KL with respect to d\ud835\udf45superscript\ud835\udc51\ud835\udf45d^{\\bm{\\pi}}italic_d start_POSTSUPERSCRIPT bold_italic_\u03c0 end_POSTSUPERSCRIPT is easy, which is g^=argmaxg[\ud835\udd3cs,a\u223cd\u03c0e\u2062[\u2212exp\u2061(g\u2062(s,a))]+\ud835\udd3cs,a\u223cd\ud835\udf45\u2062g\u2062(s,a)]^\ud835\udc54subscriptargmax\ud835\udc54delimited-[]subscript\ud835\udd3csimilar-to\ud835\udc60\ud835\udc4esuperscript\ud835\udc51superscript\ud835\udf0b\ud835\udc52delimited-[]\ud835\udc54\ud835\udc60\ud835\udc4esubscript\ud835\udd3csimilar-to\ud835\udc60\ud835\udc4esuperscript\ud835\udc51\ud835\udf45\ud835\udc54\ud835\udc60\ud835\udc4e\\hat{g}=\\mathop{\\mathrm{argmax}}_{g}\\left[\\mathbb{E}_{s,a\\sim d^{\\pi^{e}}}%\n\\left[-\\exp(g(s,a))\\right]+\\mathbb{E}_{s,a\\sim d^{\\bm{\\pi}}}g(s,a)\\right]over^ start_ARG italic_g end_ARG = roman_argmax start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT [ blackboard_E start_POSTSUBSCRIPT italic_s , italic_a \u223c italic_d start_POSTSUPERSCRIPT italic_\u03c0 start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT end_POSTSUBSCRIPT [ - roman_exp ( italic_g ( italic_s , italic_a ) ) ] + blackboard_E start_POSTSUBSCRIPT italic_s , italic_a \u223c italic_d start_POSTSUPERSCRIPT bold_italic_\u03c0 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_g ( italic_s , italic_a ) ], i.e., we have g^^\ud835\udc54\\hat{g}over^ start_ARG italic_g end_ARG being a functional sub-gradient of the loss KL(d\ud835\udf45||d\u03c0e)\\text{KL}(d^{\\bm{\\pi}}||d^{\\pi^{e}})KL ( italic_d start_POSTSUPERSCRIPT bold_italic_\u03c0 end_POSTSUPERSCRIPT | | italic_d start_POSTSUPERSCRIPT italic_\u03c0 start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT ) with respect to d\ud835\udf45superscript\ud835\udc51\ud835\udf45d^{\\bm{\\pi}}italic_d start_POSTSUPERSCRIPT bold_italic_\u03c0 end_POSTSUPERSCRIPT.\nThe maximum discriminator g^^\ud835\udc54\\hat{g}over^ start_ARG italic_g end_ARG will serve as a reward function for learning the next weak learner \u03c0t+1subscript\ud835\udf0b\ud835\udc611\\pi_{t+1}italic_\u03c0 start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT, that isTo compute g^^\ud835\udc54\\hat{g}over^ start_ARG italic_g end_ARG in practice, we need unbiased estimates of the expectations via sample averaging which can be done easily in our case. The expectation \ud835\udd3cs,a\u223cd\u03c0esubscript\ud835\udd3csimilar-to\ud835\udc60\ud835\udc4esuperscript\ud835\udc51superscript\ud835\udf0b\ud835\udc52\\mathbb{E}_{s,a\\sim d^{\\pi^{e}}}blackboard_E start_POSTSUBSCRIPT italic_s , italic_a \u223c italic_d start_POSTSUPERSCRIPT italic_\u03c0 start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT end_POSTSUBSCRIPT can be easily approximated by the expert dataset \ud835\udc9fesuperscript\ud835\udc9f\ud835\udc52\\mathcal{D}^{e}caligraphic_D start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT. To approximate \ud835\udd3cs,a\u223cd\ud835\udf45subscript\ud835\udd3csimilar-to\ud835\udc60\ud835\udc4esuperscript\ud835\udc51\ud835\udf45\\mathbb{E}_{s,a\\sim d^{\\bm{\\pi}}}blackboard_E start_POSTSUBSCRIPT italic_s , italic_a \u223c italic_d start_POSTSUPERSCRIPT bold_italic_\u03c0 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT where d\ud835\udf45superscript\ud835\udc51\ud835\udf45d^{\\bm{\\pi}}italic_d start_POSTSUPERSCRIPT bold_italic_\u03c0 end_POSTSUPERSCRIPT is a mixture distribution, we maintain a replay buffer \ud835\udc9fisubscript\ud835\udc9f\ud835\udc56\\mathcal{D}_{i}caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT for each weak learner \u03c0isubscript\ud835\udf0b\ud835\udc56\\pi_{i}italic_\u03c0 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT which contains samples s,a\u223cd\u03c0isimilar-to\ud835\udc60\ud835\udc4esuperscript\ud835\udc51subscript\ud835\udf0b\ud835\udc56s,a\\sim d^{\\pi_{i}}italic_s , italic_a \u223c italic_d start_POSTSUPERSCRIPT italic_\u03c0 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, and then weight \ud835\udc9fisubscript\ud835\udc9f\ud835\udc56\\mathcal{D}_{i}caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT via the weight \u03b1isubscript\ud835\udefc\ud835\udc56\\alpha_{i}italic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT associated with \u03c0isubscript\ud835\udf0b\ud835\udc56\\pi_{i}italic_\u03c0 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. In summary, we optimize g\ud835\udc54gitalic_g as shown in Eq.\u00a05 in Alg\u00a01 (the highlighted red part denotes the empirical expectation induced by weighted replay buffer).\nThe optimization problem in Eq\u00a05 can be solved via stochastic gradient ascent on g\ud835\udc54gitalic_g.555Note that unlike ValueDICE, here we can easily use a finite number of samples to obtain an unbiased estimate of the loss by replacing expectations by their corresponding sample averages.\nWith g^^\ud835\udc54\\hat{g}over^ start_ARG italic_g end_ARG, we can optimize for \u03c0t+1subscript\ud835\udf0b\ud835\udc611\\pi_{t+1}italic_\u03c0 start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT using any off-shelf RL algorithm, making the entire algorithm off-policy. In our experiments, we use SAC as the RL oracle for argmax\u03c0\ud835\udd3cs,a\u223cd\u03c0\u2062[\u2212g^\u2062(s,a)]subscriptargmax\ud835\udf0bsubscript\ud835\udd3csimilar-to\ud835\udc60\ud835\udc4esuperscript\ud835\udc51\ud835\udf0bdelimited-[]^\ud835\udc54\ud835\udc60\ud835\udc4e\\mathop{\\mathrm{argmax}}_{\\pi}\\mathbb{E}_{s,a\\sim d^{\\pi}}[-\\hat{g}(s,a)]roman_argmax start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_s , italic_a \u223c italic_d start_POSTSUPERSCRIPT italic_\u03c0 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT [ - over^ start_ARG italic_g end_ARG ( italic_s , italic_a ) ]. Once \u03c0t+1subscript\ud835\udf0b\ud835\udc611\\pi_{t+1}italic_\u03c0 start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT is computed, we mix \u03c0t+1subscript\ud835\udf0b\ud835\udc611\\pi_{t+1}italic_\u03c0 start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT into the mixture, and adjust the weights of older policies accordingly, i.e., \u03b1t+1=\u03b1subscript\ud835\udefc\ud835\udc611\ud835\udefc\\alpha_{t+1}=\\alphaitalic_\u03b1 start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = italic_\u03b1, and \u03b1i\u2190\u03b1i\u2062(1\u2212\u03b1),\u2200i\u2264tformulae-sequence\u2190subscript\ud835\udefc\ud835\udc56subscript\ud835\udefc\ud835\udc561\ud835\udefcfor-all\ud835\udc56\ud835\udc61\\alpha_{i}\\leftarrow\\alpha_{i}(1-\\alpha),\\forall i\\leq titalic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2190 italic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( 1 - italic_\u03b1 ) , \u2200 italic_i \u2264 italic_t. Note that this weighting scheme ensures that older policies get less weighted in the ensemble.The use of SAC as the weak learning algorithm and the new way of computing discriminator from Eq.\u00a05 make the whole training process completely off-policy. Particularly, unlike most adversarial IL approaches, which compute discriminators by comparing on-policy samples from the latest policy and the expert samples, we train the discriminator using all the data collected so far (with proper weighting derived based on the boosting framework). The connection to boosting and the proper weighting provides a principled way of leveraging off-policy samples for updating discriminators. As we will show, compared to DAC which also uses off-policy samples for training policies and discriminators, our principled approach leads to better performance.Alg\u00a01 AILBoost, summarizes the above procedure. In Line\u00a010, we use SAC as the RL oracle for computing the weak learner. In practice, we do not run SAC from scratch every time in Line\u00a010. Instead, SAC maintains its own replay buffer which contains all interactions it has with the environment so far. When computing \u03c0t+1subscript\ud835\udf0b\ud835\udc611\\pi_{t+1}italic_\u03c0 start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT, we first update the reward in the replay buffer using the latest learned reward function \u2212g^^\ud835\udc54-\\hat{g}- over^ start_ARG italic_g end_ARG, and we always warm start from \u03c0tsubscript\ud835\udf0b\ud835\udc61\\pi_{t}italic_\u03c0 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. We include the detailed algorithmic description in Appendix\u00a0A.Memory cost. Note that at the end, our algorithm returns a weighted ensemble of Markovian policies. Comparing to prior works such as DAC, the maintenance of weak learners may increase additional memory cost. However, the benefit of the weighted ensemble is that it induces richer state-action distributions than that of Markovian policies. In practice, if memory cost really becomes a burden (not in our experiments even with image-based control policies), we may just keep the latest few policies (note that very old policy has exponentially small weight anyway).In this section we aim to empirically investigate the following questions: (1) How does AILBoost perform relative to other off-policy and state-of-the-art IL methods? (2) Does AILBoost enjoy the sample complexity and scalability benefits of modern off-policy IL methods? (3) How robust is AILBoost across various different adversarial training schedules?We evaluate AILBoost on 5 environments on the DeepMind Control Suite benchmark(Tassa et\u00a0al., 2018): Walker Walk, Cheetah Run, Ball in Cup Catch, Quadruped Walk, and Humanoid Stand. For each game, we train an expert RL agent using the environment\u2019s reward and collect 10101010 demonstrations which we use as the expert dataset throughout our experiments. We compare AILBoost against the following baselines: DAC, an empirically succesful off-policy IL algorithm; IQ-Learn, a state-of-the-art IL algorithm; ValueDICE, another off-policy IL method; and BC on the expert data used across all algorithms. We emphasize our comparison to IQ-Learn, as it has been shown to outperform many other imitation learning baselines (e.g., SQIL (Reddy et\u00a0al., 2019)) across a variety of control tasks (Garg et\u00a0al., 2021).The base RL algorithm we used for training the expert, as well as for AILBoost and DAC, was SAC for controller state-based experiments and DrQ-v2 (Yarats et\u00a0al., 2022) for image-based experiments. For IQ-Learn and ValueDICE, we used their respective codebases and hyperparameters provided by the authors and both methods use SAC as their base RL algorithm. Please refer to Appendix B for experimental details, training hyperparameters, and expert dataset specifications.Figure\u00a01 shows our aggregate results across the five DeepMind Control Suite (DMC) tasks that we tested on. We chose these five tasks by difficulty as shown in Table\u00a01. For evaluation, we follow the recommendations of (Agarwal et\u00a0al., 2021) and report the aggregate inter-quartile mean, mean, and optimiality gap of AILBoost and all the baselines on the DMC suite with 95% confidence intervals. We find that AILBoost not only outperforms all baselines but also consistently matches the expert with only 1 expert trajectory.When we inspect the 1 trajectory case closer, Figure\u00a02 shows the learning curves on three representative (1 easy, 1 medium, 1 hard task) environments where we see AILBoost maintain high sample efficiency and strong imitation while state-of-the-art baselines like IQ-Learn completely fail on Humanoid Stand. Finally, we note that AILBoost greatly outperforms ValueDICE which aimed to make AIL off-policy from a different perspective. We refer readers to Figure\u00a06 in the appendix for the learning curves on all five environments with different numbers of expert demonstrations.Figure\u00a03 demonstrates the scalability of AILBoost on a subset of environments with 10 expert trajectories. For these experiments, we use DrQ-v2 (Yarats et\u00a0al., 2022) as the underlying off-policy RL algorithm for both DAC and AILBoost. On Walker Walk and Cheetah Run, we see comparable to better performance than DAC demonstrating that our boosting strategy successfully maintains the empirical, scaling properties of DAC. Furthermore, our use of different off-policy RL algorithms show the versatility of AILBoost for IL.Our algorithm relies on solving optimization problems in Eq.\u00a06 and Eq.\u00a05 for weak learners and discriminators, where weak learner is optimized by SAC and discriminators are optimized by SGD. While it is hard to guarantee in general that we can exactly solve the optimization problem due to our policies and discriminators are both being non-convex neural networks, we in general found that approximately solving Eq.\u00a06 and Eq.\u00a05 via gradient based update is enough to ensure good performance. In this section, we test AILBoost across a variety of optimization schedules. Overall, we find that AILBoost to be robust to optimization schedules \u2014 approximately optimizing Eq.\u00a06 and Eq.\u00a05 with sufficient amount of gradient updates ensures successful imitation; however, there exists a sample complexity cost when over-optimizing either the discriminator or the policy.Figure\u00a04 shows our investigation of how sensitive AILBoost is to different optimization schedules for both the policy and discriminator on two representative DMC environments. In particular, we test with 5 expert demonstrations, where we vary the number of discriminator and policy updates. We test the following update schemes:1000 policy updates per 100 discriminator updates1000 policy updates per 10 discriminator updates1000 policy updates per 1 discriminator update100 policy updates per 100 discriminator updates\nThese ranges, test various optimization schemes around the schedule that we chose for the main results. We find that the more policy updates we do per discriminator update, the algorithm becomes significantly less sample efficient despite asymptotically reaching expert performance. We also found that an insufficient amount of updates on the discriminator general hurts the performance. This is also expected since insufficient update on the discriminators may result a g^^\ud835\udc54\\hat{g}over^ start_ARG italic_g end_ARG which does not optimize Eq.\u00a05 well enough.In this work, we present a fully off-policy adversarial imitation learning algorithm, AILBoost. Different from previous attempts at making AIL off-policy, via the gradient boosting framework, AILBoost provides a principled way of re-using old data for learning discriminators and policies.\nWe show that our algorithm achieves state-of-the-art performance on state-based results on the DeepMind Control Suite while being able to scale to high-dimensional, pixel observations. We are excited to extend this framework to discrete control as well as investigate imitation learning from observations alone under this boosting framework.We would like to acknowledge the support of NSF under grant IIS-2154711, NSF CAREER 2339395, and Cornell Infosys Collaboration. Jonathan Chang is supported by LinkedIn under the LinkedIn-Cornell Grant. Kiante Brantley is supported by NSF under grant No. 2127309 to the Computing Research Association for the CIFellows Project.\nAlgorithm\u00a02 presents a more detailed pseudocode of AILBoost. The main detail here is the 2-step process of learning our discriminator using a weighted replay buffer of weak learner samples and then learning a weak learner for a certain number of RL steps.After learning our ensemble, we evaluate it by randomly sampling a policy, \u03c0isubscript\ud835\udf0b\ud835\udc56\\pi_{i}italic_\u03c0 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, from our ensemble with probability \u03b1isubscript\ud835\udefc\ud835\udc56\\alpha_{i}italic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. With this weighted sampling, we then collect a trajectory. Algorithm\u00a03 details this process.Here we detail all environment specifications and hyperparameters used in the main text.Following the standards used by DrQ-v2 (Yarats et\u00a0al., 2022), all environments have a maximum horizon length of 500 timesteps. This is achieved by setting each environment\u2019s action repeat to be 2 frames. For image based tasks, each state is 3 stacked frames that are each 84 \u00d7\\times\u00d7 84 dimensional RGB images (thus 9\u00d784\u00d784984849\\times 84\\times 849 \u00d7 84 \u00d7 84).Using the publicly released implementation for SAC and DrQ-v2, we trained high quality expert policies for state-based and image-based environments respectively. We refer the readers to (Yarats et\u00a0al., 2022) and (Haarnoja et\u00a0al., 2018; Yarats & Kostrikov, 2020) for exact hyperparameters.\nFor ValueDICE and IQ-Learn, we used the base hyperparameters they reported for the MuJoCo benchmark suite. In order to ensure good performance, we tried different configurations for every environments (i.e. the configuration for Cheetah Run for Walker Walk) since despite using the same physics engine and models, there are minor differences for DeepMind Control Suite. For DAC and AILBoost, we used our own implementations. Table\u00a04 details the hyperparameters used. Note that all hyperparameters are shared between DAC and AILBoost except for the update frequency of the disciminrator vs the policy. Note that this is one of the core differences between DAC and AILBoost.For AILBoost we predominanty tested 4 hyperparameters: # of discriminator updates, steps to learn weak learners, weighting parameter \u03b1\ud835\udefc\\alphaitalic_\u03b1, and the TD n-step. For the # of discriminator updates we tested 10, 100, 500, 1000, and 5000. For the the steps to learn weak learners, we tested 1000, 5000, 10000, 20000, and 100000. For \u03b1\ud835\udefc\\alphaitalic_\u03b1, we swept 0.95, 0.7, 0.4, 0.2, and 0.05. Finally, we tested either TD n-step 1 or 3.Following the recommendations of (Agarwal et\u00a0al., 2021), we do an additional diagnostic of measuring the probability of improvement between two algorithms. This metric measures how likely it is for X\ud835\udc4bXitalic_X to outperform Y\ud835\udc4cYitalic_Y on a randomly selected task from the benchmark suite. Specifically, P\u2062(X>Y)=1m\u2062\u2211mP\u2062(Xm>Ym)\ud835\udc43\ud835\udc4b\ud835\udc4c1\ud835\udc5asubscript\ud835\udc5a\ud835\udc43subscript\ud835\udc4b\ud835\udc5asubscript\ud835\udc4c\ud835\udc5aP(X>Y)=\\frac{1}{m}\\sum_{m}P(X_{m}>Y_{m})italic_P ( italic_X > italic_Y ) = divide start_ARG 1 end_ARG start_ARG italic_m end_ARG \u2211 start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT italic_P ( italic_X start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT > italic_Y start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) where P\u2062(Xm>Ym)\ud835\udc43subscript\ud835\udc4b\ud835\udc5asubscript\ud835\udc4c\ud835\udc5aP(X_{m}>Y_{m})italic_P ( italic_X start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT > italic_Y start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) is the probability of X\ud835\udc4bXitalic_X outperforming Y\ud835\udc4cYitalic_Y on task m\ud835\udc5amitalic_m. Note that this measurement does not account for the size of improvement. Figure\u00a05 shows the comparison. AILBoost shows significant improvement over all other algorithms other than DAC. In conjuction with Figure\u00a01, we see that although the chance of AILBoost doing better than DAC is \u224850%absentpercent50\\approx 50\\%\u2248 50 %, the size of improvement AILBoost has over DAC denoted by the IQM and Mean are significantly larger.Here we present the complete suite of learning curves for all 5 environments.Here we present the full suite of learning curves where we vary how often the policy and the discriminator update relative to each other. We keep every other hyperparameter constant in this ablation.",
    "20": "When addressing the challenge of complex multi-objective optimization problems, particularly those with non-convex and non-uniform Pareto fronts, Decomposition-based Multi-Objective Evolutionary Algorithms (MOEADs) often converge to local optima, thereby limiting solution diversity. Despite its significance, this issue has received limited theoretical exploration. Through a comprehensive geometric analysis, we identify that the traditional method of Reference Point (RP) selection fundamentally contributes to this challenge. In response, we introduce an innovative RP selection strategy, the Weight Vector-Guided and Gaussian-Hybrid method, designed to overcome the local optima issue. This approach employs a novel RP type that aligns with weight vector directions and integrates a Gaussian distribution to combine three distinct RP categories. Our research comprises two main experimental components: an ablation study involving 14 algorithms within the MOEADs framework, spanning from 2014 to 2022, to validate our theoretical framework, and a series of empirical tests to evaluate the effectiveness of our proposed method against both traditional and cutting-edge alternatives. Results demonstrate that our method achieves remarkable improvements in both population diversity and convergence.Multi-objective optimization problems (MOPs) are referred to those required to optimize more than two conflicting objectives at the same time. Pareto front (PF) of a MOP is a set of mapping of non-dominated solutions, representing the best trade-offs in the objective function space. Solutions on the PF are considered optimal because improving one objective would lead to the deterioration of at least one other objective. The decomposition-based multi-objective evolutionary algorithm (MOEA/D)\u00a0[1] has become one popular framework to solve MOPs since it was presented. Within this framework, many variants of MOEA/D (MOEA/D-variants) have been developed\u00a0[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] over the past decade, focusing on enhancing different components of the algorithm. A common phenomenon is that when dealing with complex MOPs, especially those characterized by non-convex and non-uniform PFs, almost all MOEA/D-variants rapidly lose their effectiveness in maintaining diversity\u00a0[20, 21, 22, 23, 24]. This means that the population has fallen into local optima during the iteration process.As Hisao et al. proposed\u00a0[25], the performance of MOEAD-variants algorithms strongly depends on the shapes of the PF. Specifically, a slight change in the problem formulations of DTLZ and WFG can have a detrimental impact on the performance of MOEA/D-based Algorithms. When the PF is convex, MOEA/D algorithms still achieve satisfactory results, including both population diversity and convergence. However, when the PF is non-convex, especially with highly concave shapes such as in the IMOP and DTLZ2 problems, the performance of suffers a noticeable decline in both convergence and diversity.Although this phenomenon is widespread, there is no rigorous theoretical exploration to answer why almost all MOEA/D-variants exhibit this phenomenon. At present, most studies are based on experimental simulations followed by qualitative analysis to explain the reason for the existence of question\u00a0[20, 21, 22, 23, 24, 25].In this paper, we provide our rigorous geometric analysis for this critical issue and introduce a novel theoretically supported method to address this problem. specifically, we find under the conditions where weight vectors, genetic operators, and decomposition methods all adopt their default values, the traditional method of selecting the reference point, the default min method, stands out as the primary reason for the decline in algorithmic performance. This work intends to make the following contributions.It proposes a geometric proof demonstrating that the traditional method of selecting the RP is the primary reason why MOEA/D-variants tend to fall into local optima in complex MOPs. Furthermore, it empirically validates the accuracy of this theory, through the ablation study\u00a0[26] on selected fourteen MOEA/D-variants published between 2014 and 2022.\nIt proposes a efficient and novel strategy to select the RP, the Weight Vector-Guided and Gaussian-Hybrid method. The method generates a new type of RP along the direction of the weight vectors to ensure diversity and employs Gaussian distribution to ensure convergence.\nThe proposed strategy outperforms not only the traditional but also the state-of-the-art method in terms of diversity, while reaching the same level of them in convergence.\nThe remainder of this article is organized as follows. Section II introduces relevant definitions and existing methods. Section III provides a geometric analysis of why the MOEA/D framework tends to converge to local optima in complex MOPs. Following this, Section IV details the methodology of the proposed method. Section V presents an ablation study along with experiments on the proposed method. Finally, Section VI concludes the article.Multi-objective optimization problems (MOPs) are formulated as follows:where \u03a9\u2282\u211dD\u03a9superscript\u211d\ud835\udc37\\Omega\\subset\\mathbb{R}^{D}roman_\u03a9 \u2282 blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT represents the decision space in D\ud835\udc37Ditalic_D dimensions. The decision vector \ud835\udc31=(x1,\u2026,xD)\u22ba\ud835\udc31superscriptsubscript\ud835\udc651\u2026subscript\ud835\udc65\ud835\udc37\u22ba\\mathbf{x}=(x_{1},\\ldots,x_{D})^{\\intercal}bold_x = ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT \u22ba end_POSTSUPERSCRIPT and its associated objective vector \ud835\udc05\u2062(\ud835\udc31)\ud835\udc05\ud835\udc31\\mathbf{F}(\\mathbf{x})bold_F ( bold_x ) are central to the optimization. The functions f1\u2062(\ud835\udc31),\u2026,fM\u2062(\ud835\udc31)subscript\ud835\udc531\ud835\udc31\u2026subscript\ud835\udc53\ud835\udc40\ud835\udc31f_{1}(\\mathbf{x}),\\ldots,f_{M}(\\mathbf{x})italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_x ) , \u2026 , italic_f start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ( bold_x ) serve to define the mapping from the decision space to the objective space.Given two solutions \ud835\udc311subscript\ud835\udc311\\mathbf{x}_{1}bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and \ud835\udc312subscript\ud835\udc312\\mathbf{x}_{2}bold_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, we say \ud835\udc311subscript\ud835\udc311\\mathbf{x}_{1}bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT Pareto-dominates \ud835\udc312subscript\ud835\udc312\\mathbf{x}_{2}bold_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT (expressed as \ud835\udc311\u227c\ud835\udc312precedes-or-equalssubscript\ud835\udc311subscript\ud835\udc312\\mathbf{x}_{1}\\preccurlyeq\\mathbf{x}_{2}bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u227c bold_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT) if fi\u2062(\ud835\udc311)\u2264fi\u2062(\ud835\udc312)subscript\ud835\udc53\ud835\udc56subscript\ud835\udc311subscript\ud835\udc53\ud835\udc56subscript\ud835\udc312f_{i}(\\mathbf{x}_{1})\\leq f_{i}(\\mathbf{x}_{2})italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) \u2264 italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) across all objectives i\u2208{1,\u2026,M}\ud835\udc561\u2026\ud835\udc40i\\in\\{1,\\ldots,M\\}italic_i \u2208 { 1 , \u2026 , italic_M }, and there exists at least one objective j\u2208{1,\u2026,M}\ud835\udc571\u2026\ud835\udc40j\\in\\{1,\\ldots,M\\}italic_j \u2208 { 1 , \u2026 , italic_M } where fj\u2062(\ud835\udc311)<fj\u2062(\ud835\udc312)subscript\ud835\udc53\ud835\udc57subscript\ud835\udc311subscript\ud835\udc53\ud835\udc57subscript\ud835\udc312f_{j}(\\mathbf{x}_{1})<f_{j}(\\mathbf{x}_{2})italic_f start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) < italic_f start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ).A collection of optimal compromise solutions, termed the Pareto set (\ud835\udc0f\ud835\udc12\ud835\udc0f\ud835\udc12\\mathbf{PS}bold_PS), defined as \ud835\udc0f\ud835\udc12={\ud835\udc31*\u2208\u03a9|\u2204\u2062\ud835\udc31\u2208\u03a9,\ud835\udc31\u227c\ud835\udc31*}\ud835\udc0f\ud835\udc12conditional-setsuperscript\ud835\udc31\u03a9formulae-sequencenot-exists\ud835\udc31\u03a9precedes-or-equals\ud835\udc31superscript\ud835\udc31\\mathbf{PS}=\\{\\mathbf{x}^{*}\\in\\Omega\\ |\\ \\nexists\\mathbf{x}\\in\\Omega,\\mathbf{%\nx}\\preccurlyeq\\mathbf{x}^{*}\\}bold_PS = { bold_x start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT \u2208 roman_\u03a9 | \u2204 bold_x \u2208 roman_\u03a9 , bold_x \u227c bold_x start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT }. The mapping of \ud835\udc0f\ud835\udc12\ud835\udc0f\ud835\udc12\\mathbf{PS}bold_PS within the objective space is known as the Pareto front (\ud835\udc0f\ud835\udc05\ud835\udc0f\ud835\udc05\\mathbf{PF}bold_PF), represented by \ud835\udc0f\ud835\udc05={\ud835\udc1f\u2062(\ud835\udc31*)|\ud835\udc31*\u2208\ud835\udc0f\ud835\udc12}\ud835\udc0f\ud835\udc05conditional-set\ud835\udc1fsuperscript\ud835\udc31superscript\ud835\udc31\ud835\udc0f\ud835\udc12\\mathbf{PF}=\\{\\mathbf{f}(\\mathbf{x}^{*})\\ |\\ \\mathbf{x}^{*}\\in\\mathbf{PS}\\}bold_PF = { bold_f ( bold_x start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ) | bold_x start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT \u2208 bold_PS }.\nThe aim of multi-objective optimization is to obtain a group of non-dominated solutions as a result of the trade-off between different objectives. Therefore, an algorithm must guarantee both diversity and convergence. Under ideal conditions, the mapping of candidate solutions can be evenly distributed on the real PF.By utilizing weight vectors, MOPs are decomposed into several subproblems, with the goal of each subproblem being to minimize a specific weighted objective function. An initial population P\ud835\udc43Pitalic_P and its corresponding neighborhood T\ud835\udc47Titalic_T are established. Subsequently, offspring y\ud835\udc66yitalic_y are produced via crossover and mutation techniques, with their performance evaluated using a fitness function. If y\ud835\udc66yitalic_y demonstrate superior performance compared to the neighboring parents, it then takes their place. Below is the pseudocode 1 for MOEA/D outlined.Population size N\ud835\udc41Nitalic_Nthe number of function evaluations m\u2062a\u2062x\u2062F\u2062E\ud835\udc5a\ud835\udc4e\ud835\udc65\ud835\udc39\ud835\udc38maxFEitalic_m italic_a italic_x italic_F italic_ENumber of objectives M\ud835\udc40Mitalic_MCrossover and mutation ratesNeighborhood size T\ud835\udc47Titalic_T\nIn MOEA/D-based algorithms, the reference point\u00a0[1] is utilized to guide the algorithm\u2019s search direction and aid in a more accurate approximation to the PF. It gets updated throughout the algorithm\u2019s execution.For a multi-objective minimization problem, each component of the true ideal point represents the minimum value of its corresponding objective function across the entire feasible solution space. However, in practical optimization processes, the true ideal point is unknown. The aforementioned reference point is an obtained ideal point maintained and computed by the algorithm using certain methods.A fitness function [27] is an evaluation function used to assess and compare the quality of candidate solutions in an optimization problem. In MOPs, One common strategy is to use the decomposition methods as fitness functions. Another common strategy is to use Pareto ranking(PR)\u00a0[28].In some literature, decomposition methods are also referred to as scalar methods. They are a category of fitness functions in MOPs used to assess and compare the quality of candidate solutions. This includes Weighted Sum Approach (WS)\u00a0[29], Tchebycheff Approach (TCH)\u00a0[29], the penalty-based boundary intersection approach (PBI)\u00a0[1], among others. In the Geometric Proof section of this paper, we employed PBI, while in the Experiments phase, we utilized Modified Tchebycheff Approach (M-TCH)\u00a0[30]. The formulas for these methods are presented below. Let Equation2 correspond to PBI and Equation3 correspond to M-TCH.where X\ud835\udc4bXitalic_X represents the decision vector, F\u2062(X)\ud835\udc39\ud835\udc4bF(X)italic_F ( italic_X ) denotes the objective vector, W\ud835\udc4aWitalic_W is the weight vector, and Z\ud835\udc4dZitalic_Z is the reference point. fi\u2062(x)subscript\ud835\udc53\ud835\udc56\ud835\udc65f_{i}(x)italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x ) signifies the objective vector of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT subproblem, zisubscript\ud835\udc67\ud835\udc56z_{i}italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT represents the reference point corresponding to the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT subproblem, and Wisubscript\ud835\udc4a\ud835\udc56W_{i}italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the weight vector associated with the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT subproblem.We will begin by discussing the limitations of existing methods, introduce two prevalent methods, and then delve into the method we propose.\nIn existing methods, as evidenced from the proof section, the traditional min method tends to lead the algorithm straight into local solutions for concave problems. An issue with the more recent DRP method is that, since the reference point in DRP has a linear relationship with FE, the HV metric that measures the diversity of the population and the IGD metric that gauges convergence still exhibit considerable fluctuations, even after the algorithm has converged. The metrics even display a declining trend. Essentially, the DRP method[16] (as seen in Equation 5) is an improvement upon the min method, but it doesn\u2019t fully address the challenge of the algorithm getting stuck in local solutions. These conclusions can be verified in the experimental section. Below are the formulas for both methods.With the introduction of MOEA/D[1], the selection method of the reference point, commonly known as the min method, has been extensively utilized in various enhanced algorithms. Its formula is as follows.In this paper[16], the dynamic reference point (DRP) method is introduced . By incorporating the variable \u03f5isubscriptitalic-\u03f5\ud835\udc56\\epsilon_{i}italic_\u03f5 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT into zim\u2062i\u2062nsuperscriptsubscript\ud835\udc67\ud835\udc56\ud835\udc5a\ud835\udc56\ud835\udc5bz_{i}^{min}italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m italic_i italic_n end_POSTSUPERSCRIPT, it derives the following formula.This chapter aims to perform a theoretical analysis on how MOEA/D escapes LO in IMOP2. A significant challenge currently faced by decomposition-based multi-objective evolutionary algorithms is their propensity to fall into LO when MOEA/D-variants address MOPs characterized by non-convexity and non-uniformity of the PF [1]. To derive universally applicable conclusions, this chapter selects the classic MOEA/D and the classical problem IMOP2, which exhibits both characteristics, for theoretical analysis. It proves that the fundamental reason for MOEA/D\u2019s frequent entrapment in LO on complex MOPs is the traditional RF selection method: min. The first Section\u00a0III-A introduces the properties of the IMOP2 problem, followed by sections\u00a0III-B and\u00a0III-C, which analyze why MOEA/D cannot escape LO under the traditional min method.The representative IMOP2 problem has decision variables of dimension K+L\ud835\udc3e\ud835\udc3fK+Litalic_K + italic_L and two objective functions as follows:whereNotice that when xK+1=\u2026=xK+L=12subscript\ud835\udc65\ud835\udc3e1\u2026subscript\ud835\udc65\ud835\udc3e\ud835\udc3f12x_{K+1}=...=x_{K+L}=\\frac{1}{2}italic_x start_POSTSUBSCRIPT italic_K + 1 end_POSTSUBSCRIPT = \u2026 = italic_x start_POSTSUBSCRIPT italic_K + italic_L end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG 2 end_ARG, g=0\ud835\udc540g=0italic_g = 0, and for all x1,\u2026,xK\u2208[0,1],y\u2208[0,1]formulae-sequencesubscript\ud835\udc651\u2026subscript\ud835\udc65\ud835\udc3e01\ud835\udc6601x_{1},...,x_{K}\\in[0,1],y\\in[0,1]italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT \u2208 [ 0 , 1 ] , italic_y \u2208 [ 0 , 1 ], thus by eliminating the parameter y\ud835\udc66yitalic_y, the Pareto Front of IMOP2 can be expressed as:Nonconvexity is simply determined by calculating d2\u2062f2d\u2062f12=\u22123\u2062f12\u2062(1\u2212f14)\u221274<0superscriptd2subscript\ud835\udc532dsuperscriptsubscript\ud835\udc53123superscriptsubscript\ud835\udc5312superscript1superscriptsubscript\ud835\udc5314740\\frac{\\text{d}^{2}f_{2}}{\\text{d}f_{1}^{2}}=-3f_{1}^{2}(1-f_{1}^{4})^{-\\frac{7%\n}{4}}<0divide start_ARG d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG d italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG = - 3 italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 - italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - divide start_ARG 7 end_ARG start_ARG 4 end_ARG end_POSTSUPERSCRIPT < 0, hence the PF is a concave function. Furthermore, we calculate the area of the curvilinear triangle enclosed by the PF and the f1,f2subscript\ud835\udc531subscript\ud835\udc532f_{1},f_{2}italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT axes to be S=\u222b01(1\u2212f14)14\u2062\ud835\udc51f1=2\u2062\u03c0\u221212\u2062\u03932\u2062(54)=0.93\u22481\ud835\udc46superscriptsubscript01superscript1superscriptsubscript\ud835\udc531414differential-dsubscript\ud835\udc5312superscript\ud835\udf0b12superscript\u03932540.931S=\\int_{0}^{1}(1-f_{1}^{4})^{\\frac{1}{4}}df_{1}=2\\pi^{-\\frac{1}{2}}\\Gamma^{2}(%\n\\frac{5}{4})=0.93\\approx 1italic_S = \u222b start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( 1 - italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG 4 end_ARG end_POSTSUPERSCRIPT italic_d italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 2 italic_\u03c0 start_POSTSUPERSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG end_POSTSUPERSCRIPT roman_\u0393 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( divide start_ARG 5 end_ARG start_ARG 4 end_ARG ) = 0.93 \u2248 1, indicating that the PF is highly concave. The image of the PF is shown in Figure\u00a02a.Nonuniformity results from g=\u2211i=K+1K+L(xi\u221212)2\ud835\udc54superscriptsubscript\ud835\udc56\ud835\udc3e1\ud835\udc3e\ud835\udc3fsuperscriptsubscript\ud835\udc65\ud835\udc56122g=\\sum_{i=K+1}^{K+L}(x_{i}-\\frac{1}{2})^{2}italic_g = \u2211 start_POSTSUBSCRIPT italic_i = italic_K + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K + italic_L end_POSTSUPERSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT being a quadratic function in terms of variables xK+1,\u2026,xK+Lsubscript\ud835\udc65\ud835\udc3e1\u2026subscript\ud835\udc65\ud835\udc3e\ud835\udc3fx_{K+1},...,x_{K+L}italic_x start_POSTSUBSCRIPT italic_K + 1 end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_K + italic_L end_POSTSUBSCRIPT, which can be easily optimized by algorithms, thus in most cases, it is easy for algorithms to find points on the PF. Based on this, since the value of a\ud835\udc4eaitalic_a is very small (default value 0.05) [31], y\ud835\udc66yitalic_y is close to 1 in most cases, cos\u2062\u03c02\u2062ycos\ud835\udf0b2\ud835\udc66\\sqrt{\\text{cos}\\frac{\\pi}{2}y}square-root start_ARG cos divide start_ARG italic_\u03c0 end_ARG start_ARG 2 end_ARG italic_y end_ARG is close to 0, and sin\u2062\u03c02\u2062ysin\ud835\udf0b2\ud835\udc66\\sqrt{\\text{sin}\\frac{\\pi}{2}y}square-root start_ARG sin divide start_ARG italic_\u03c0 end_ARG start_ARG 2 end_ARG italic_y end_ARG is close to 1, meaning the PF solutions are nonuniformly distributed along the global PF, and the PF is biased towards solutions for which f1\u2062(\ud835\udc99)=0subscript\ud835\udc531\ud835\udc990f_{1}(\\bm{x})=0italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_italic_x ) = 0.Due to the nonuniformity of IMOP2, it is likely that the population in the algorithm will not be uniformly distributed on the PF but will stagnate near (f1,f2)=(0,1)subscript\ud835\udc531subscript\ud835\udc53201(f_{1},f_{2})=(0,1)( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) = ( 0 , 1 ). To illustrate this, we solve IMOP2 using the MOEA/D algorithm and observe the distribution of the population at F\u2062E=10,1000,3000,20000\ud835\udc39\ud835\udc38101000300020000FE=10,1000,3000,20000italic_F italic_E = 10 , 1000 , 3000 , 20000 as shown in Figure 1. As seen in Figure 1, with the increase in the number of iterations, MOEA/D gradually stagnates near (f1,f2)=(1,0)subscript\ud835\udc531subscript\ud835\udc53210(f_{1},f_{2})=(1,0)( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) = ( 1 , 0 ). The points with different colors represent the parent populations P\ud835\udc43Pitalic_P obtained at different evolutionary stages. The first graph uses a linear axis, while the second employs a logarithmic axis, providing a clearer representation of the obtained population set P\ud835\udc43Pitalic_P.Combining the above discussions, in the following analysis, we assume that MOEA/D has stagnated, that is, in the objective space, all individual positions are located near (f1,f2)=(0,1)subscript\ud835\udc531subscript\ud835\udc53201(f_{1},f_{2})=(0,1)( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) = ( 0 , 1 ). We assume the position of a certain individual as \ud835\udc99=[x1,\u2026,xK+L]\ud835\udc99subscript\ud835\udc651\u2026subscript\ud835\udc65\ud835\udc3e\ud835\udc3f\\bm{x}=[x_{1},...,x_{K+L}]bold_italic_x = [ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_K + italic_L end_POSTSUBSCRIPT ], whose coordinates in the objective function coordinate plane are F=(F1,F2)\ud835\udc39subscript\ud835\udc391subscript\ud835\udc392F=(F_{1},F_{2})italic_F = ( italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ), considering that the g\ud835\udc54gitalic_g part can be easily optimized, so we can set xK+1=\u2026=xK+L=0.5subscript\ud835\udc65\ud835\udc3e1\u2026subscript\ud835\udc65\ud835\udc3e\ud835\udc3f0.5x_{K+1}=...=x_{K+L}=0.5italic_x start_POSTSUBSCRIPT italic_K + 1 end_POSTSUBSCRIPT = \u2026 = italic_x start_POSTSUBSCRIPT italic_K + italic_L end_POSTSUBSCRIPT = 0.5, i.e., F\ud835\udc39Fitalic_F is on the Pareto Front, and close to (1,0)10(1,0)( 1 , 0 ). Clearly, according to the definition of the min method, at this time, the algorithm\u2019s Reference Point (RP) Z=[Z1,Z2]\ud835\udc4dsubscript\ud835\udc4d1subscript\ud835\udc4d2Z=[Z_{1},Z_{2}]italic_Z = [ italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_Z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ] is located below and to the left of \ud835\udc6d\ud835\udc6d\\bm{F}bold_italic_F, i.e., Z1\u2264F1,Z2\u2264F2formulae-sequencesubscript\ud835\udc4d1subscript\ud835\udc391subscript\ud835\udc4d2subscript\ud835\udc392Z_{1}\\leq F_{1},Z_{2}\\leq F_{2}italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2264 italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_Z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2264 italic_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT.Based on this, we assume the position of a newly generated individual as \ud835\udc9a=[y1,\u2026,yK+L]\ud835\udc9asubscript\ud835\udc661\u2026subscript\ud835\udc66\ud835\udc3e\ud835\udc3f\\bm{y}=[y_{1},...,y_{K+L}]bold_italic_y = [ italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_y start_POSTSUBSCRIPT italic_K + italic_L end_POSTSUBSCRIPT ], with its coordinates in the objective function space being G=(G1,G2)\ud835\udc3asubscript\ud835\udc3a1subscript\ud835\udc3a2G=(G_{1},G_{2})italic_G = ( italic_G start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_G start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ). Since all progenitor positions from the K+1\ud835\udc3e1K+1italic_K + 1th to K+L\ud835\udc3e\ud835\udc3fK+Litalic_K + italic_Lth components are approximately 0.5, we can set yK+1=\u2026=yK+L=0.5subscript\ud835\udc66\ud835\udc3e1\u2026subscript\ud835\udc66\ud835\udc3e\ud835\udc3f0.5y_{K+1}=...=y_{K+L}=0.5italic_y start_POSTSUBSCRIPT italic_K + 1 end_POSTSUBSCRIPT = \u2026 = italic_y start_POSTSUBSCRIPT italic_K + italic_L end_POSTSUBSCRIPT = 0.5, meaning G\ud835\udc3aGitalic_G is on the Pareto Front. Further, we assume that G\ud835\udc3aGitalic_G is to the right of F\ud835\udc39Fitalic_F, i.e., G1>F1,G2<F2formulae-sequencesubscript\ud835\udc3a1subscript\ud835\udc391subscript\ud835\udc3a2subscript\ud835\udc392G_{1}>F_{1},G_{2}<F_{2}italic_G start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_G start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT < italic_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. This assumption has clear significance: if the population generates the individual G\ud835\udc3aGitalic_G in some iteration, and if the algorithm can accept this individual with a relatively high probability, then the population can gradually move to the right, thereby ensuring the algorithm\u2019s ability to escape LO; if the algorithm can hardly accept this individual, it is considered difficult for the algorithm to escape LO. Further, after generating individual G\ud835\udc3aGitalic_G, the position of RP updates to Z=[Z1,G2]\ud835\udc4dsubscript\ud835\udc4d1subscript\ud835\udc3a2Z=[Z_{1},G_{2}]italic_Z = [ italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_G start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ]. Assuming the distance between G\ud835\udc3aGitalic_G and F\ud835\udc39Fitalic_F is not too large (offspring generated from parents naturally have a higher probability of being near the parents), and the value of G1subscript\ud835\udc3a1G_{1}italic_G start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT is closer to 0, we assume the slope of the PF at G\ud835\udc3aGitalic_G as kGsubscript\ud835\udc58\ud835\udc3ak_{G}italic_k start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT, then at this timeis evidently very small.Finally, we assume the positions of F,Z\ud835\udc39\ud835\udc4dF,Zitalic_F , italic_Z, and G\ud835\udc3aGitalic_G under the objective function coordinate plane as shown in Figure\u00a02b, here G\u2062Z\ud835\udc3a\ud835\udc4dGZitalic_G italic_Z is parallel to the f1subscript\ud835\udc531f_{1}italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT axis.For MOEA/D employing the PBI method, this section will analyze the possibility of the algorithm accepting a new individual G\ud835\udc3aGitalic_G based on Figure\u00a02b. Specifically, given the point W=[W1,W2]\ud835\udc4asubscript\ud835\udc4a1subscript\ud835\udc4a2W=[W_{1},W_{2}]italic_W = [ italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ], we only need to compare the magnitude of gp\u2062b\u2062i\u2062(F|W,Z)superscript\ud835\udc54\ud835\udc5d\ud835\udc4f\ud835\udc56conditional\ud835\udc39\ud835\udc4a\ud835\udc4dg^{pbi}(F|W,Z)italic_g start_POSTSUPERSCRIPT italic_p italic_b italic_i end_POSTSUPERSCRIPT ( italic_F | italic_W , italic_Z ) and gp\u2062b\u2062i\u2062(G|W,Z)superscript\ud835\udc54\ud835\udc5d\ud835\udc4f\ud835\udc56conditional\ud835\udc3a\ud835\udc4a\ud835\udc4dg^{pbi}(G|W,Z)italic_g start_POSTSUPERSCRIPT italic_p italic_b italic_i end_POSTSUPERSCRIPT ( italic_G | italic_W , italic_Z ), where gp\u2062b\u2062isuperscript\ud835\udc54\ud835\udc5d\ud835\udc4f\ud835\udc56g^{pbi}italic_g start_POSTSUPERSCRIPT italic_p italic_b italic_i end_POSTSUPERSCRIPT is the fitness function in the PBI method, with its expression seen in a certain formula\u00a02. The analysis unfolds under two scenarios: (1) kO\u2062W>kZ\u2062Fsubscript\ud835\udc58\ud835\udc42\ud835\udc4asubscript\ud835\udc58\ud835\udc4d\ud835\udc39k_{OW}>k_{ZF}italic_k start_POSTSUBSCRIPT italic_O italic_W end_POSTSUBSCRIPT > italic_k start_POSTSUBSCRIPT italic_Z italic_F end_POSTSUBSCRIPT, and (2) kO\u2062W\u2264kZ\u2062Fsubscript\ud835\udc58\ud835\udc42\ud835\udc4asubscript\ud835\udc58\ud835\udc4d\ud835\udc39k_{OW}\\leq k_{ZF}italic_k start_POSTSUBSCRIPT italic_O italic_W end_POSTSUBSCRIPT \u2264 italic_k start_POSTSUBSCRIPT italic_Z italic_F end_POSTSUBSCRIPT, where kX\u2062Ysubscript\ud835\udc58\ud835\udc4b\ud835\udc4ck_{XY}italic_k start_POSTSUBSCRIPT italic_X italic_Y end_POSTSUBSCRIPT represents the slope of the line X\u2062Y\ud835\udc4b\ud835\udc4cXYitalic_X italic_Y, defined as +\u221e+\\infty+ \u221e when X\u2062Y\ud835\udc4b\ud835\udc4cXYitalic_X italic_Y is parallel to the f2subscript\ud835\udc532f_{2}italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT axis, and similarly below. Moreover, we define the angle between O\u2062W\u2192\u2192\ud835\udc42\ud835\udc4a\\overrightarrow{OW}over\u2192 start_ARG italic_O italic_W end_ARG and O\u2062f1\ud835\udc42subscript\ud835\udc531Of_{1}italic_O italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT as \u03b1\ud835\udefc\\alphaitalic_\u03b1.First, we discuss the scenario where kO\u2062W>kZ\u2062Fsubscript\ud835\udc58\ud835\udc42\ud835\udc4asubscript\ud835\udc58\ud835\udc4d\ud835\udc39k_{OW}>k_{ZF}italic_k start_POSTSUBSCRIPT italic_O italic_W end_POSTSUBSCRIPT > italic_k start_POSTSUBSCRIPT italic_Z italic_F end_POSTSUBSCRIPT. We present and prove the following theorem:Given kO\u2062W>kZ\u2062Fsubscript\ud835\udc58\ud835\udc42\ud835\udc4asubscript\ud835\udc58\ud835\udc4d\ud835\udc39k_{OW}>k_{ZF}italic_k start_POSTSUBSCRIPT italic_O italic_W end_POSTSUBSCRIPT > italic_k start_POSTSUBSCRIPT italic_Z italic_F end_POSTSUBSCRIPT, ifthen\nThe objective function coordinate plane for this scenario is depicted in Figure\u00a03. Here, A\u2062D\u27c2O\u2062Wperpendicular-to\ud835\udc34\ud835\udc37\ud835\udc42\ud835\udc4aAD\\perp OWitalic_A italic_D \u27c2 italic_O italic_W, A\u2062G\u27c2O\u2062f1perpendicular-to\ud835\udc34\ud835\udc3a\ud835\udc42subscript\ud835\udc531AG\\perp Of_{1}italic_A italic_G \u27c2 italic_O italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, A\u2062F\u2062\u2016Z\u2062G\u2016\u2062O\u2062f1\ud835\udc34\ud835\udc39norm\ud835\udc4d\ud835\udc3a\ud835\udc42subscript\ud835\udc531AF\\parallel ZG\\parallel Of_{1}italic_A italic_F \u2225 italic_Z italic_G \u2225 italic_O italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, B\u2062F\u2062\u2016C\u2062G\u2016\u2062A\u2062D\ud835\udc35\ud835\udc39norm\ud835\udc36\ud835\udc3a\ud835\udc34\ud835\udc37BF\\parallel CG\\parallel ADitalic_B italic_F \u2225 italic_C italic_G \u2225 italic_A italic_D, A\u2062H\u2062\u2016F\u2062E\u2016\u2062O\u2062W\ud835\udc34\ud835\udc3bnorm\ud835\udc39\ud835\udc38\ud835\udc42\ud835\udc4aAH\\parallel FE\\parallel OWitalic_A italic_H \u2225 italic_F italic_E \u2225 italic_O italic_W are auxiliary lines. By the geometric meaning of PBI,Thus,\u220eNext, we discuss the scenario where kO\u2062W\u2264kZ\u2062Fsubscript\ud835\udc58\ud835\udc42\ud835\udc4asubscript\ud835\udc58\ud835\udc4d\ud835\udc39k_{OW}\\leq k_{ZF}italic_k start_POSTSUBSCRIPT italic_O italic_W end_POSTSUBSCRIPT \u2264 italic_k start_POSTSUBSCRIPT italic_Z italic_F end_POSTSUBSCRIPT. We propose and prove the following theorem:Given kO\u2062W\u2264kZ\u2062Fsubscript\ud835\udc58\ud835\udc42\ud835\udc4asubscript\ud835\udc58\ud835\udc4d\ud835\udc39k_{OW}\\leq k_{ZF}italic_k start_POSTSUBSCRIPT italic_O italic_W end_POSTSUBSCRIPT \u2264 italic_k start_POSTSUBSCRIPT italic_Z italic_F end_POSTSUBSCRIPT, ifthenThe objective function\u2019s coordinate plane under this condition is illustrated in Figure\u00a04. Here, A\u2062C\ud835\udc34\ud835\udc36ACitalic_A italic_C is tangent to the Pareto Front at point A\ud835\udc34Aitalic_A, B\u2062G\u27c2O\u2062Wperpendicular-to\ud835\udc35\ud835\udc3a\ud835\udc42\ud835\udc4aBG\\perp OWitalic_B italic_G \u27c2 italic_O italic_W, and D\u2062E\u27c2O\u2062Wperpendicular-to\ud835\udc37\ud835\udc38\ud835\udc42\ud835\udc4aDE\\perp OWitalic_D italic_E \u27c2 italic_O italic_W.First, we estimate the upper bound of gp\u2062b\u2062i\u2062(F|W,Z)superscript\ud835\udc54\ud835\udc5d\ud835\udc4f\ud835\udc56conditional\ud835\udc39\ud835\udc4a\ud835\udc4dg^{pbi}(F|W,Z)italic_g start_POSTSUPERSCRIPT italic_p italic_b italic_i end_POSTSUPERSCRIPT ( italic_F | italic_W , italic_Z ) as follows:Next, we estimate the lower bound of gp\u2062b\u2062i\u2062(G|W,Z)superscript\ud835\udc54\ud835\udc5d\ud835\udc4f\ud835\udc56conditional\ud835\udc3a\ud835\udc4a\ud835\udc4dg^{pbi}(G|W,Z)italic_g start_POSTSUPERSCRIPT italic_p italic_b italic_i end_POSTSUPERSCRIPT ( italic_G | italic_W , italic_Z ). Obviously,Let \u2220\u2062A\u2062G\u2062Z=\u03b2\u2220\ud835\udc34\ud835\udc3a\ud835\udc4d\ud835\udefd\\angle AGZ=\\beta\u2220 italic_A italic_G italic_Z = italic_\u03b2. Using the sine theorem in \u25b3\u2062A\u2062Z\u2062G\u25b3\ud835\udc34\ud835\udc4d\ud835\udc3a\\triangle AZG\u25b3 italic_A italic_Z italic_G:Combining the above equations, we getTherefore,\u220eIn summary, revisiting Theorems\u00a01 and\u00a02, as long as the respective assumptions are satisfied, gp\u2062b\u2062i\u2062(F|W,Z)<gp\u2062b\u2062i\u2062(G|W,Z)superscript\ud835\udc54\ud835\udc5d\ud835\udc4f\ud835\udc56conditional\ud835\udc39\ud835\udc4a\ud835\udc4dsuperscript\ud835\udc54\ud835\udc5d\ud835\udc4f\ud835\udc56conditional\ud835\udc3a\ud835\udc4a\ud835\udc4dg^{pbi}(F|W,Z)<g^{pbi}(G|W,Z)italic_g start_POSTSUPERSCRIPT italic_p italic_b italic_i end_POSTSUPERSCRIPT ( italic_F | italic_W , italic_Z ) < italic_g start_POSTSUPERSCRIPT italic_p italic_b italic_i end_POSTSUPERSCRIPT ( italic_G | italic_W , italic_Z ) holds true, meaning the fitness of individual G\ud835\udc3aGitalic_G is worse than that of F\ud835\udc39Fitalic_F, implying the new individual G\ud835\udc3aGitalic_G cannot be accepted, and the algorithm struggles to escape local optima. As discussed in the Preliminaries chapter, the value of |kG|subscript\ud835\udc58\ud835\udc3a|k_{G}|| italic_k start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT | is generally very small, thus for most weight vectors W\ud835\udc4aWitalic_W, the respective assumptions are easily satisfied. In conclusion, the probability of the algorithm escaping LO is very small.For MOEA/D employing the M-TCH method, this chapter will analyze the possibility of the algorithm accepting a new individual G\ud835\udc3aGitalic_G based on Figure\u00a02b. Specifically, given the point W=[W1,W2]\ud835\udc4asubscript\ud835\udc4a1subscript\ud835\udc4a2W=[W_{1},W_{2}]italic_W = [ italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ], we only need to compare the magnitude of gm\u2062t\u2062c\u2062h\u2062(F|W,Z)superscript\ud835\udc54\ud835\udc5a\ud835\udc61\ud835\udc50\u210econditional\ud835\udc39\ud835\udc4a\ud835\udc4dg^{mtch}(F|W,Z)italic_g start_POSTSUPERSCRIPT italic_m italic_t italic_c italic_h end_POSTSUPERSCRIPT ( italic_F | italic_W , italic_Z ) and gm\u2062t\u2062c\u2062h\u2062(G|W,Z)superscript\ud835\udc54\ud835\udc5a\ud835\udc61\ud835\udc50\u210econditional\ud835\udc3a\ud835\udc4a\ud835\udc4dg^{mtch}(G|W,Z)italic_g start_POSTSUPERSCRIPT italic_m italic_t italic_c italic_h end_POSTSUPERSCRIPT ( italic_G | italic_W , italic_Z ), where gm\u2062t\u2062c\u2062hsuperscript\ud835\udc54\ud835\udc5a\ud835\udc61\ud835\udc50\u210eg^{mtch}italic_g start_POSTSUPERSCRIPT italic_m italic_t italic_c italic_h end_POSTSUPERSCRIPT is the fitness indicator within the M-TCH method, with its expression found in a certain formula\u00a03. The analysis will be conducted under two scenarios: (1) kO\u2062W>kZ\u2062Fsubscript\ud835\udc58\ud835\udc42\ud835\udc4asubscript\ud835\udc58\ud835\udc4d\ud835\udc39k_{OW}>k_{ZF}italic_k start_POSTSUBSCRIPT italic_O italic_W end_POSTSUBSCRIPT > italic_k start_POSTSUBSCRIPT italic_Z italic_F end_POSTSUBSCRIPT, and (2) kO\u2062W\u2264kZ\u2062Fsubscript\ud835\udc58\ud835\udc42\ud835\udc4asubscript\ud835\udc58\ud835\udc4d\ud835\udc39k_{OW}\\leq k_{ZF}italic_k start_POSTSUBSCRIPT italic_O italic_W end_POSTSUBSCRIPT \u2264 italic_k start_POSTSUBSCRIPT italic_Z italic_F end_POSTSUBSCRIPT. Additionally, we define the angle between O\u2062W\u2192\u2192\ud835\udc42\ud835\udc4a\\overrightarrow{OW}over\u2192 start_ARG italic_O italic_W end_ARG and O\u2062f1\ud835\udc42subscript\ud835\udc531Of_{1}italic_O italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT as \u03b1\ud835\udefc\\alphaitalic_\u03b1.First, we discuss the scenario where kO\u2062W>kZ\u2062Fsubscript\ud835\udc58\ud835\udc42\ud835\udc4asubscript\ud835\udc58\ud835\udc4d\ud835\udc39k_{OW}>k_{ZF}italic_k start_POSTSUBSCRIPT italic_O italic_W end_POSTSUBSCRIPT > italic_k start_POSTSUBSCRIPT italic_Z italic_F end_POSTSUBSCRIPT. We propose and prove the following theorem:Given kO\u2062W>kZ\u2062Fsubscript\ud835\udc58\ud835\udc42\ud835\udc4asubscript\ud835\udc58\ud835\udc4d\ud835\udc39k_{OW}>k_{ZF}italic_k start_POSTSUBSCRIPT italic_O italic_W end_POSTSUBSCRIPT > italic_k start_POSTSUBSCRIPT italic_Z italic_F end_POSTSUBSCRIPT, thenThe objective function coordinate plane for this scenario is illustrated in Figure\u00a05. Clearly, by the geometric meaning of the M-TCH method,\u220eNext, we discuss the scenario where kO\u2062W\u2264kZ\u2062Fsubscript\ud835\udc58\ud835\udc42\ud835\udc4asubscript\ud835\udc58\ud835\udc4d\ud835\udc39k_{OW}\\leq k_{ZF}italic_k start_POSTSUBSCRIPT italic_O italic_W end_POSTSUBSCRIPT \u2264 italic_k start_POSTSUBSCRIPT italic_Z italic_F end_POSTSUBSCRIPT. We propose and prove the following theorem:Given kO\u2062W\u2264kZ\u2062Fsubscript\ud835\udc58\ud835\udc42\ud835\udc4asubscript\ud835\udc58\ud835\udc4d\ud835\udc39k_{OW}\\leq k_{ZF}italic_k start_POSTSUBSCRIPT italic_O italic_W end_POSTSUBSCRIPT \u2264 italic_k start_POSTSUBSCRIPT italic_Z italic_F end_POSTSUBSCRIPT, ifthenThe objective function coordinate plane under this condition is illustrated in Figure\u00a06, where A\ud835\udc34Aitalic_A is a point on the PF, and A\u2062Z\u27c2O\u2062f1perpendicular-to\ud835\udc34\ud835\udc4d\ud835\udc42subscript\ud835\udc531AZ\\perp Of_{1}italic_A italic_Z \u27c2 italic_O italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. Clearly, by the geometric meaning of the M-TCH method,Therefore,\u220eRevisiting Theorem\u00a03 and Theorem\u00a04, as long as the respective assumptions are satisfied, gm\u2062t\u2062c\u2062h\u2062(F|W,Z)<gm\u2062t\u2062c\u2062h\u2062(G|W,Z)superscript\ud835\udc54\ud835\udc5a\ud835\udc61\ud835\udc50\u210econditional\ud835\udc39\ud835\udc4a\ud835\udc4dsuperscript\ud835\udc54\ud835\udc5a\ud835\udc61\ud835\udc50\u210econditional\ud835\udc3a\ud835\udc4a\ud835\udc4dg^{mtch}(F|W,Z)<g^{mtch}(G|W,Z)italic_g start_POSTSUPERSCRIPT italic_m italic_t italic_c italic_h end_POSTSUPERSCRIPT ( italic_F | italic_W , italic_Z ) < italic_g start_POSTSUPERSCRIPT italic_m italic_t italic_c italic_h end_POSTSUPERSCRIPT ( italic_G | italic_W , italic_Z ) holds true, meaning the fitness of individual G\ud835\udc3aGitalic_G is worse than that of F\ud835\udc39Fitalic_F, implying the new individual G\ud835\udc3aGitalic_G cannot be accepted, and the algorithm struggles to escape LO. As discussed in the Preliminaries chapter, the value of |kG|subscript\ud835\udc58\ud835\udc3a|k_{G}|| italic_k start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT | is generally very small, thus as long as the value of \u03b1\ud835\udefc\\alphaitalic_\u03b1 is not equal to 00, the respective assumptions are easily satisfied. Therefore, the probability of the algorithm escaping LO is very small.As you can see in Figure\u00a07, when MOEA/D-variants are applied to MOPs of non-convex PF, there is a high likelihood that they will degenerate into local search algorithms. This limits them to converging only to certain local optima on the PF, severely impacting the diversity of the population. Even recent algorithms (MOEADUR[12] published in 2022) developed in the last couple of years suffer from the same issue (see Figure\u00a07).\nPrevious strategies have shown, as elaborated in Section III, that the conventional practice of choosing reference points might direct the MOEA/D variants towards local optima in scenarios involving concave challenges. The investigation reveals that the selection of reference points is crucial for these algorithms when addressing complex MOPs.The detailed study presented in Section V-C indicates that substituting the reference point with the actual ideal point markedly improves the algorithm\u2019s population diversity and convergence, particularly in complex MOPs. Nevertheless, identifying the true ideal points in real-world MOPs is impractical, highlighting the necessity for a viable reference point selection method. Herein, we present a novel approach for determining the reference point during the algorithmic process, designated as NormW. This approach not only mitigates the issue of MOEA/D variants getting trapped in local optima but also maintains the algorithm\u2019s convergence speed on par with other methodologies. The ensuing section elaborates on our proposed technique.As derived from the discussion on Review of Prior Approaches II-C, current literature lacks a reference point selection method that effectively circumvents the local optima entrapment issue. The normW method proposed herein addresses this concern adeptly. To integrate the normW method into the MOEA/D variants algorithmic framework, one needs to substitute lines 8 and 13 in the MOEA/D pseudocode with the subsequent Algorithms 2 and 3.Subsequent sections will elucidate on the novel Zwsubscript\ud835\udc4d\ud835\udc64Z_{w}italic_Z start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT, as detailed in Pseudocode 3, and the application of the Gaussian Distribution in Pseudocode 2.The rationale for Zwsubscript\ud835\udc4d\ud835\udc64Z_{w}italic_Z start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT is detailed in Equation 17. Specifically, at each stage of evolution, the smallest objective values, denoted as Zmsubscript\ud835\udc4d\ud835\udc5aZ_{m}italic_Z start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT, are utilized to construct a quarter-circle (or for higher dimensions, a segment of a sphere) centered around the origin. The point where this shape intersects with the currently applied weight vector Wisubscript\ud835\udc4a\ud835\udc56W_{i}italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT yields the specific Zwsubscript\ud835\udc4d\ud835\udc64Z_{w}italic_Z start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT for that evolutionary iteration, labeled as ZWisubscript\ud835\udc4dsubscript\ud835\udc4a\ud835\udc56Z_{W_{i}}italic_Z start_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT.As illustrated in Figure 9, Zwsubscript\ud835\udc4d\ud835\udc64Z_{w}italic_Z start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT demonstrates how it is harmonized with evenly spaced weight vectors, facilitating a wide-ranging set of solutions.Such synchronization between Zwsubscript\ud835\udc4d\ud835\udc64Z_{w}italic_Z start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT and weight vectors plays a pivotal role in preserving solution diversity throughout the population\u2019s evolution, tackling the vital challenge of maintaining variation among generations.A Gaussian distribution is defined for the variable Y\ud835\udc4cYitalic_Y with mean \u03bc\ud835\udf07\\muitalic_\u03bc and standard deviation \u03c3\ud835\udf0e\\sigmaitalic_\u03c3, where Y\ud835\udc4cYitalic_Y signifies the count of FE, also known as the generation number, as shown in Equation 18. The choice of setting \u03c3=\u03bc/5\ud835\udf0e\ud835\udf075\\sigma=\\mu/5italic_\u03c3 = italic_\u03bc / 5 stems from Equation 19, which posits that the likelihood p\u2062(Y=y)\ud835\udc5d\ud835\udc4c\ud835\udc66p(Y=y)italic_p ( italic_Y = italic_y ) approximates 1 for y\ud835\udc66yitalic_y within the interval [0, maxFE], ensuring the algorithm cycles from 0 through maxFE.\nwhere \u03a6\u2062(x)\u03a6\ud835\udc65\\Phi(x)roman_\u03a6 ( italic_x ) represents the cumulative distribution function (CDF) for the standard normal distribution.The variable Y=FE\ud835\udc4cFEY=\\text{FE}italic_Y = FE\u2019s CDF is articulated in Equation 20. The probability that reference point Z\ud835\udc4dZitalic_Z is selected as either Z0subscript\ud835\udc4d0Z_{0}italic_Z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, Zmsubscript\ud835\udc4d\ud835\udc5aZ_{m}italic_Z start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT, or Zwsubscript\ud835\udc4d\ud835\udc64Z_{w}italic_Z start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT during the algorithm\u2019s evolution is depicted in Equation 21.\nFigure 10 illustrates the CDF of selecting various reference points, where the blue curve indicates the likelihood of opting for Z0subscript\ud835\udc4d0Z_{0}italic_Z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT within the first half of maxFE, the red curve represents the chance of selecting Zmsubscript\ud835\udc4d\ud835\udc5aZ_{m}italic_Z start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT within the latter half, and the orange curve denotes the probability of choosing Zwsubscript\ud835\udc4d\ud835\udc64Z_{w}italic_Z start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT.The segmented choice mechanism adopted by the normW method prioritizes Zwsubscript\ud835\udc4d\ud835\udc64Z_{w}italic_Z start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT in the algorithm\u2019s initial phase to preserve population diversity and leans towards Zmsubscript\ud835\udc4d\ud835\udc5aZ_{m}italic_Z start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT in the concluding phase to assure convergence. Consequently, the normW technique achieves a stable equilibrium between population diversity and convergence. Subsequently, a geometric analysis will corroborate that the normW approach prevents MOEA/D-based algorithms from devolving into mere local search mechanisms when addressing complex MOPs.Based on the RF selection strategy proposed in this paper, this chapter will analyze the capability of MOEA/D to escape local optima under the PBI and M-TCH methods, respectively. We still base our analysis on the model in Figure\u00a02b and assess the possibility of the algorithm accepting a new individual G\ud835\udc3aGitalic_G.First, the analysis results under the PBI method are presented as follows:IfthenThe objective function coordinate plane under this scenario is illustrated in Figure\u00a011, where A\u2062F\u27c2O\u2062Wperpendicular-to\ud835\udc34\ud835\udc39\ud835\udc42\ud835\udc4aAF\\perp OWitalic_A italic_F \u27c2 italic_O italic_W, C\u2062G\u2225O\u2062Wconditional\ud835\udc36\ud835\udc3a\ud835\udc42\ud835\udc4aCG\\parallel OWitalic_C italic_G \u2225 italic_O italic_W, and A\u2062F\u2225B\u2062Gconditional\ud835\udc34\ud835\udc39\ud835\udc35\ud835\udc3aAF\\parallel BGitalic_A italic_F \u2225 italic_B italic_G. Clearly, by the geometric meaning of the PBI method,\u220eRevisiting Theorem5, as long as the corresponding assumption is satisfied, gp\u2062b\u2062i\u2062(G|W,Z)<gp\u2062b\u2062i\u2062(F|W,Z)superscript\ud835\udc54\ud835\udc5d\ud835\udc4f\ud835\udc56conditional\ud835\udc3a\ud835\udc4a\ud835\udc4dsuperscript\ud835\udc54\ud835\udc5d\ud835\udc4f\ud835\udc56conditional\ud835\udc39\ud835\udc4a\ud835\udc4dg^{pbi}(G|W,Z)<g^{pbi}(F|W,Z)italic_g start_POSTSUPERSCRIPT italic_p italic_b italic_i end_POSTSUPERSCRIPT ( italic_G | italic_W , italic_Z ) < italic_g start_POSTSUPERSCRIPT italic_p italic_b italic_i end_POSTSUPERSCRIPT ( italic_F | italic_W , italic_Z ) holds true, meaning the fitness of individual G\ud835\udc3aGitalic_G is superior to that of F\ud835\udc39Fitalic_F, implying the new individual G\ud835\udc3aGitalic_G can be accepted, allowing the algorithm to escape LO. Given that the default value of \u03b8\ud835\udf03\\thetaitalic_\u03b8 is 5\u00a0[1], so for most weight vectors, the corresponding assumption is easy to be satisfied. In conclusion, the probability of the algorithms escaping LO is very high.Next, the analysis results under the M-TCH method are presented as follows:IfthenThe objective function coordinate plane under this scenario is illustrated in Figure\u00a012. Clearly, by the geometric meaning of the M-TCH method,\n\u220eRevisiting Theorem\u00a06, as long as the corresponding assumption is satisfied, gm\u2062t\u2062c\u2062h\u2062(G|W,Z)<gm\u2062t\u2062c\u2062h\u2062(F|W,Z)superscript\ud835\udc54\ud835\udc5a\ud835\udc61\ud835\udc50\u210econditional\ud835\udc3a\ud835\udc4a\ud835\udc4dsuperscript\ud835\udc54\ud835\udc5a\ud835\udc61\ud835\udc50\u210econditional\ud835\udc39\ud835\udc4a\ud835\udc4dg^{mtch}(G|W,Z)<g^{mtch}(F|W,Z)italic_g start_POSTSUPERSCRIPT italic_m italic_t italic_c italic_h end_POSTSUPERSCRIPT ( italic_G | italic_W , italic_Z ) < italic_g start_POSTSUPERSCRIPT italic_m italic_t italic_c italic_h end_POSTSUPERSCRIPT ( italic_F | italic_W , italic_Z ) holds true, meaning the fitness of individual G\ud835\udc3aGitalic_G is superior to that of F\ud835\udc39Fitalic_F, implying the new individual G\ud835\udc3aGitalic_G can be accepted, allowing the algorithm to escape LO. Notably, when ZWsubscript\ud835\udc4d\ud835\udc4aZ_{W}italic_Z start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT is located below the intersection point of O\u2062G\ud835\udc42\ud835\udc3aOGitalic_O italic_G and the quarter circle, the assumption is guaranteed to be satisfied, hence the probability of the algorithm escaping LO is significant.In this section, we first demonstrate the significance of reference points on solution diversity in evolutionary algorithms through ablation studies. We selected algorithms published between 2014 and 2022, which are available on the PlatEMO platform (see Table 2). All these algorithms employ reference points during their execution. By setting the reference points directly to the true ideal point and comparing them with the original algorithms, the results indicates the method of choosing the reference points is the fundemental reason for MOEA/D to get trapped in local optima.For conventional algorithms that address multi-objective optimization problems (MOPs), the commonly adopted benchmark test problems are DTLZ and WFG. As shown in Table II, among the 14 algorithms we selected, 13 of them utilize either DTLZ or WFG. Since the main focus of this paper is the impact of the reference point on the diversity of the evolutionary algorithms, in addition to the DTLZ and WFG categories of test problems, we have included a series of IMOP [31] test problems, which can effectively distinguish the diversity performance of different algorithms.As depicted in Table I , we have selected a total of 16 benchmark test problems. Here, \u2019N\u2019 represents the population size, \u2019M\u2019 denotes the dimension of the objective vector, \u2019D\u2019 refers to the dimension of the decision vector, and \u2019MaxFE\u2019 indicates the maximum number of function evaluations. One evaluation process refers to obtaining the objective function value of a decision variable, then using a fitness function, which is mentioned in II-B, such as the decomposition method or the nondominated sorting method, also called PR [28], to determine the fitness value. This is done to decide whether to retain this decision variable or not.Over the past decade, algorithms based on the MOEA/D framework have emerged in abundance. Here, we have selected a total of 13 algorithms from the years 2014 to 2022, along with the MOEA/D algorithm, making it a total of 14, as shown in Table 2.\nOut of the 14 algorithms we examined, 13 utilize the MOEA/D framework, but RVEAiGNG [32] does not. We included RVEAiGNG to demonstrate that when an algorithm employs a reference point, the method of selection can impact the diversity of its solutions. Among these 14 algorithms, 13 incorporate a reference point, whereas MOEADM2M does not. We picked MOEADM2M for its ability to maintain diversity within a population. Previously, it had been shown to increase diversity in comparison to MOEAD-based methods. In our study, we were able to achieve greater diversity than MOEADM2M by merely adjusting the reference point of the MOEA/D algorithm to the true ideal point. This finding emphasizes that the choice of reference point is a fundamental factor in the reduction of population diversity.As indicated in Table 2, the fitness functions of these 14 algorithms encompass decomposition methods, PK, as well as new fitness functions proposed by certain algorithms, such as the Pareto adaptive scalarizing Methods introduced by MOEADPaS[8].Below, we will conduct a ablation study using these 14 algorithms. And the parameter settings for these algorithms are all set to the default values on the PlatEMO platform[33].Previous research has been limited to enhancing the MOEA/D framework\u2019s algorithms for complex MOPs by improving the reference point, weight vectors, or decomposition methods. No studies have explicitly demonstrated the reasons why MOEA/D framework algorithms are sensitive to complex MOPs and tend to converge to local solutions on these issues. This paper, through the ablation study , identifies that the method of choosing the reference point is one of the fundamental reasons.An Ablation Study[26] in machine learning involves progressively removing parts of a model to understand their contribution to its performance. Here, we use the ablation study to compare the performance of the MOEA/D model before and after the removal of the reference point.It\u2019s important to clarify that by \u2019removal\u2019 here, we mean the elimination of the method of choosing the reference point, using the true ideal point directly in MOEA/D-based algorithms. The advantage of this approach is that it can directly and effectively reflect the impact of the reference point selection method on the MOEA/D-based algorithms. In this paper, specifically, the most common method of selecting the reference point , which means min method, made these algorithms more likely to converge to local solutions and find it challenging to escape from them. And this directly results in an inability to obtain a well-distributed set of Pareto solutions, causing the algorithm to fail in maintaining population diversity. In the context of solving MOPs, this is a catastrophic characteristic. Thus it is one of the fundamental reasons why algorithms based on the MOEA/D framework struggle with complex MOPs.Next, we will approach this from two categories of experimental results. The first category consists of 13 algorithms compared with their own versions that applied the true ideal point. The second category involves MOEADM2M compared with the MOEA/D algorithm that applied the true ideal point.As you can see from Table III, all algorithms but MOEADM2M underwent an ablation study, comparing their original versions to those where the reference point was directly set to the true ideal point, referred to *-ideal (* standing for the specific algorithm). Across 16 benchmark test problems, results for HV and IGD metrics were derived. It\u2019s evident that in terms of the HV metric, which evaluates population diversity, all 13 algorithms showed significant enhancements. Additionally, the convergence metric, IGD, also observed improvements. For instance, the MOEADDYTS algorithm, released in 2020, when compared to its MOEADDYTS-ideal version, was outperformed in 12 of the 16 test problems for HV and in 11 problems for IGD.Table IV illustrates the specific test problems from the set of 16, where the original algorithms outperform the modified ones in terms of HV and IGD metrics. For instance, for the MOEAD algorithm, it only outperforms the modified version on the WFG4 problem. This is attributed to the fact that the WFG4 problem[34] is characterized by non-separability, multimodality, and non-convexity. As such, the number of evaluations required to converge to the PF is higher than other test problems. For simplicity, our study standardized the evaluation numbers for WFG problems at thirty thousand. This count may not suffice for some algorithms to reach a converged state on the WFG4 problem, leading to scenarios where the original algorithm outperforms the modified one. This also indicates that the convergence speed of algorithms using the true ideal point can be affected.\nAs per the \u201dNo Free Lunch\u201d theorem[35], in the realm of algorithms, if there\u2019s a significant enhancement in population diversity on complicated MOPs, a slight compromise on convergence speed is acceptable. This reason applies to the WFG problems of CMOEAD, MOEADDCWV, and MOEADUR. Regarding the MOEADURAW algorithm[10] for the IMOP5 and IMOP8 problems, the issues arise since both problems have discontinuous Pareto Fronts (PFs). The MOEADURW algorithm employs weight adaptation based on population sparsity and calculates the weight vector of the newly constructed subproblem using the reference point. The discontinuous PFs disrupt the algorithm\u2019s judgment on population sparsity. Additionally, when the method for selecting the reference point changes, the calculated weight vector deviates more from the original algorithm, affecting the convergence metric IGD for both problems.\nAn intriguing observation is that the final algorithm, RVEAiGNG[32], is not based on MOEA/D, which employs reference vector adaptation for evolutionary optimization. Instead, it incorporates reference points within its algorithmic process. Insights from the ablation study of RVEAiGNG suggest that the selection method of reference points is pivotal for evolutionary algorithms to tackle complicated MOPs. An inappropriate approach can deteriorate the algorithm\u2019s global convergence capabilities.Next, we will delve into a detailed comparison of the results between the MOEADM2M and MOEAD-ideal algorithms.As previously mentioned, the introduction of MOEADM2M aims to maintain population diversity, as evidenced by its superior HV metric when compared to other algorithms[4]. In its originating paper, it was contrasted with an algorithm based on the MOEA/D framework (MOEADD) to highlight MOEADM2M\u2019s significant enhancement in population diversity. In this section, we continue to compare MOEADM2M with two algorithms based on the MOEA/D framework (MOEAD and MOEAD-ideal). The only modification in MOEA/D-ideal is setting the reference point to the true ideal point in the MOEA/D algorithm. As demonstrated in Tables III and IV, out of the 16 benchmark test problems, only one test problems yielded a scenario where MOEADM2M outperformed MOEAD-ideal. Thus, from these comparative results, we can conclude that the selection method for the reference point is one of the fundamental reasons for the decline in population diversity.A more detailed result is provided in Table 5. The first column presents a comparison of the HV metrics for MOEADM2M, MOEAD, and MOEAD-ideal across 16 test problems, while the second column contrasts their IGD metrics. It\u2019s evident that MOEADM2M only outperforms MOEAD-ideal on the IMOP1 problem. For the IMOP1 problem, we ran both the MOEADM2M and MzOEAD-ideal algorithms thirty times each. The distribution graph corresponding to the median of the population is illustrated in Figure 13b, where the black curve represents the PF. The metrics for HV and IGD, corresponding to their medians, are showcased in Figure 13. As can be observed, the population distribution of MOEADM2M is more uniform, while MOEAD-ideal tends to concentrate around the center of the PF. This is because MOEADM2M solves multi-objective subproblems with each subproblem having a dedicated subpopulation that consistently receives computational effort during the search. It employs a strategy that sacrifices convergence speed to enhance population diversity. As seen in Figure 13, MOEADM2M\u2019s HV converges after about 10,000 evaluations, while MOEAD-ideal\u2019s does so after roughly 3,000. MOEADM2M\u2019s IGD converges around 15,000 evaluations, compared to 5,000 for MOEAD-ideal. This clearly underscores MOEADM2M\u2019s emphasis on trading off convergence speed for population diversity. Additionally, due to the decomposition method limitations in MOEA/D[31], it tends to concentrate populations in the central regions for convex problems, affecting overall distribution (HV) and convergence (IGD). Hence, we observe the phenomenon where MOEADM2M outperforms MOEAD-ideal on convex IMOP1 problem.\nFrom our discussion in the ablation study sectionV-C, it\u2019s clear that the commonly used min method can lead MOEA/D-based algorithms to degrade from a global search nature to a local search, especially when addressing complex MOPs like concave problems. To address this, we utilize the normW method introduced in the methodology chapterIV, demonstrating through empirical evidence that this method can effectively resolve the issue. The experiments in this section are twofold:First, we compare MOEA/D-based algorithms employing the min method against those using the normW method. From this comparison, it\u2019s evident that the normW method, relative to the min method, exhibits improvements in both diversity and convergence of the population when solving complex MOPs.Secondly,we compare the MOEAD algorithm that employs the DRP method, a latest approach for reference point selection[16], with the MOEAD algorithm that utilizes the normW method. The aim here is to emphasize that our proposed method outperforms the latest method, demonstrating its potential to enhance both population diversity and convergence in MOEA/D-based algorithms.In all the algorithms we\u2019ve chosen, the most prevalent method, min, has been uniformly used for selecting the reference point. In this subsection, our primary focus is to compare the effects on population diversity and convergence when the chosen algorithms employ either the min or normW selection methods.\nIn this stage, the benchmark remains consistent with 16 test problems, and all experimental settings are kept unchanged, as shown in Table 1. There were originally 14 selected algorithms, as depicted in Table 2. However, since the normW method employs uniformly distributed weight vectors, we excluded algorithms related to adaptive weight vectors during the experimentation. This includes MOEADAWA, MOEADURAW, MOEADDCWV, and RVEAiGNG. For such algorithms, our recommended improvement step is to first use the generated uniformly distributed weight vectors combined with the normW method to determine the reference point before acquiring the adaptive weight vectors. For instance, the algorithm MOEADUR, which relates to adaptive weight vectors, yielded results as shown in Table VI after the above modification. We also removed certain algorithms that use two types of reference points in their procedures, which include MOEADDU and MOEADPaS.\nAs illustrated in Table 6, when the remaining algorithms switched their reference point selection method to normW, they are denoted as *-normW (where * stands for the original algorithm), and the results were then compared with those of the original algorithms. Notably, regardless of whether we consider the HV or IGD metrics, all seven selected algorithms exhibited varying degrees of improvement across the 16 test problems. For instance, MOEADM2M trailed behind MOEAD-normW in the HV metric on 13 test problems and in the IGD metric on 11 test problems. As discussed in the ablation study section V-C, MOEADM2M was primarily designed to retain population diversity. Our proposed normW method, applied only to the most basic MOEA/D algorithm, can achieve commendable results even when compared to MOEADM2M. This underscores the efficacy of our approach and highlights the pivotal role the choice of reference point plays in preserving population diversity for algorithms based on the MOEA/D framework.To the best of our knowledge regarding research on the selection of reference points, the DRP method[16] is the most widely applied and represents the most recent findings. In this section, we apply both the DRP method and our proposed normW method as selection strategies of reference points to the MOEAD algorithm and compare their performances across sixteen test problems. The results are presented in Table VII. When evaluating convergence using the IGD metric, the normW method outperforms the DRP method in 11 of the test problems. In terms of population diversity, as measured by the HV metric, the normW method surpasses the DRP method in 10 test problems.Considering our primary focus is on complex MOPs, we have selected IMOP2 and DTLZ4\u2014both highly concave problems\u2014for a comparative study of the populations. As seen in Figure14a, for DTLZ4, the DRP method still frequently converges to local solutions. And in contrast, the normW method converges to a set of evenly distributed solutions. Similarly, as depicted in Figure 14b for IMOP2, the population obtained using the DRP method is not as uniformly distributed as that of the normW method. This demonstrates the capability of the normW method to handle complex MOPs effectively.Regarding the WFG problems, as can be seen from Table VII, the DRP method outperforms the normW method in terms of average HV on WFG2, WFG3, and WFG4. Figure 15 depicts the HV obtained by the algorithms over 30,000 evaluation iterations. As clearly illustrated in Figure 15, the most prominent difference between the two is that, after convergence, the MOEAD-normW algorithm tends to stabilize and shows a gradual increase. In contrast, the MOEAD-DRP algorithm not only exhibits significant fluctuations after convergence but also occasionally demonstrates a declining trend, as evidenced in Figures 15b and 15c. This suggests that, although MOEAD-DRP might surpass MOEAD-normW in terms of average values, MOEAD-normW generally yields a more stable population in the latter stages of the algorithm. Due to the linear relationship between the number of evaluations and the reference point formula in the DRP method [16], it continues to fluctuate and decline even after convergence. We can infer from this that if the number of evaluations exceeds 30,000, say 50,000, the HV of MOEAD-normW would gradually surpass that of the MOEAD-DRP.\nFor the WFG3 problem, neither method converges fully to the PF within 50,000 evaluations, as depicted in Figure 8, which makes it challenging to determine which one is superior post-convergence. However, the DRP method appears to have the edge over the normW method before convergence. In summary, across the 16 test problems, the normW method significantly outperforms the DRP method, especially when the WFG problems converge. Thus, we have grounds to believe that substituting the reference point selection method in most MOEA/D-based algorithms with the normW method can enhance their performance to varying extents. This reaffirms the pivotal role of reference point selection in MOEA/D-based algorithms addressing complex MOPs.Based on the analyses and experiments conducted, we can confidently assert that the traditional method of selecting the reference point is one of the fundamental reasons for the performance degradation of MOEA/D-based algorithms when addressing complex MOPs. To tackle this issue, we introduced the normW method. We utilized a normal distribution to determine the probability of selecting different reference points at various stages. Innovatively, we designed a reference point, termed Z\u2062w\ud835\udc4d\ud835\udc64Zwitalic_Z italic_w, that allows the algorithm to escape local solutions. By leveraging the strengths of both approaches, we achieved significant improvements in population diversity and convergence compared to previous methods.For future research, we aim to explore the influence of the reference point on algorithmic performance, not restricting ourselves solely to MOEA/D-based algorithms. In this paper, although we selected only one multi-objective evolutionary algorithm not based on the MOEA/D framework (namely RVEAiGNG), the experimental results for the benchmark test problems improved in both the ablation study and when applying the ANormW method. This indicates that the reference point plays an indispensable role in the global search capability of evolutionary algorithms.",
    "21": "Reinforcement Learning (RL) from Human Preference-based feedback is a popular paradigm for fine-tuning generative models, which has produced impressive models such as GPT-4 and Claude3 Opus. This framework often consists of two steps: learning a reward model from an offline preference dataset followed by running online RL to optimize the learned reward model. In this work, leveraging the idea of reset, we propose a new RLHF algorithm with provable guarantees. Motivated by the fact that offline preference dataset provides informative states (i.e., data that is preferred by the labelers), our new algorithm, Dataset Reset Policy Optimization (DR-PO), integrates the existing offline preference dataset into the online policy training procedure via dataset reset: it directly resets the policy optimizer to the states in the offline dataset, instead of always starting from the initial state distribution. In theory, we show that DR-PO learns to perform at least as good as any policy that is covered by the offline dataset under general function approximation with finite sample complexity. In experiments, we demonstrate that on both the TL;DR summarization and the Anthropic Helpful Harmful (HH) dataset, the generation from DR-PO is better than that from Proximal Policy Optimization (PPO) and Direction Preference Optimization (DPO), under the metric of GPT4 win-rate. Code for this work can be found at https://github.com/Cornell-RL/drpo.Reinforcement learning aims at maximizing a cumulative reward function. However, specifying a reward function in practice can be challenging (Wirth et\u00a0al.,, 2017). Reinforcement Learning with Human Feedback (RLHF) has become an effective approach when a reward function does not exist (Christiano et\u00a0al.,, 2017). Operating under a setting where human labelers provide preference-based feedback (e.g., ranking of generations from an RL agent), RLHF learns a reward model and then optimizes the reward model via RL techniques. RLHF has found applications across various domains, including games (MacGlashan et\u00a0al.,, 2017; Christiano et\u00a0al.,, 2017; Warnell et\u00a0al.,, 2018), large language models (LLMs) (Ziegler et\u00a0al.,, 2019; Stiennon et\u00a0al.,, 2020; Wu et\u00a0al.,, 2021; Nakano et\u00a0al.,, 2021; Ouyang et\u00a0al.,, 2022; Glaese et\u00a0al.,, 2022; Bai et\u00a0al., 2022a, ; Ramamurthy et\u00a0al.,, 2022; Liu et\u00a0al.,, 2023), and robot learning (Brown et\u00a0al.,, 2019; Shin et\u00a0al.,, 2023).\nRLHF typically consists of the following two steps: (1) fitting a reward model using a pre-collected offline preference-based dataset (often generated from some pre-trained models and labeled by humans), (2) and learn a policy via online RL (e.g., Proximal Policy Optimization (Schulman et\u00a0al.,, 2017)) to optimize the learned reward model. These two steps are often done separately in the sense that once the reward model is learned, step (2) only optimizes the reward model without ever using the offline preference dataset. Is there any benefit of re-using the offline data during the procedure of optimizing the reward model via online RL? Prior work on hybrid RL (Song et\u00a0al.,, 2022; Ball et\u00a0al.,, 2023) demonstrated that combining offline data and online data can often significantly boost learning efficiency. Can we achieve a similar boost in learning efficiency for RLHF?Towards answering this, we propose an algorithm called Dataset Reset Policy Optimization (DR-PO), operating under the assumption of being able to reset, i.e., we can go back to any state and start policy optimization and data collection from that point (as opposed to reseting to initial states). While being able to reset is certainly an assumption, it is naturally satisfied when using RL to fine-tune generative models like language models and diffusion models (Lee et\u00a0al.,, 2023). This is because the underlying Markov transitions are simple, known, and deterministic. For instance, when using RL to optimize text generation, resetting to a state is equivalent to feeding a partial sentence (together with the initial prompt) to the transformer-based policy. Our algorithm, DR-PO, is a hybrid RL approach that integrates offline data into an online RL procedure: when collecting online data, DR-PO resets the policy optimizer to the states in the offline dataset for exploration. Algorithmically, DR-PO is simple: it iteratively collects a batch of online data by resetting the policy to states in the offline data, performs policy rollouts, and optimizes the policy using the online batch via policy optimization techniques such as Natural Policy Gradient (NPG) (Kakade,, 2001) or Actor-critic methods (e.g., PPO (Schulman et\u00a0al.,, 2017)).While DR-PO is as simple to implement as most of the existing policy optimization algorithms, we demonstrate that DR-PO achieves strong theoretical guarantees under natural assumptions. Specifically, when optimizing a reward model learned from an offline preference dataset, DR-PO is capable of learning a policy that is at least as good as any policy which is covered by the offline data in terms of maximizing the ground truth rewards, and DR-PO achieves this result under general function approximation with finite sample complexity. DR-PO is also computationally tractable since it only requires supervised learning style oracles such as a Maximum Likelihood Estimation (MLE) oracle (for fitting reward models) and a Least Squares Regression oracle (for learning value functions). Thus DR-PO advances the status of the theoretical work on RLHF (see more detailed discussion in Section\u00a02). Empirically, we test our approach on two standard RLHF datasets: TL;DR summarization (Stiennon et\u00a0al.,, 2020) and Anthropic HH. In TL;DR summarization, we demonstrate that the summaries generated by DR-PO outperform those from PPO and DPO (Rafailov et\u00a0al.,, 2023) in terms of GPT4 win-rate. We also show that when transferring the policies trained on TL;DR to the CNN/DailyMail news articles in a zero-shot manner, policies trained via DR-PO again generate summaries that outperform those from PPO and DPO, indicating that dataset reset does not make DR-PO overfit. Finally, we test how DR-PO scales on Anthropic HH (Bai et\u00a0al., 2022b, ) across three different model scales and show that DR-PO scales just as well as PPO while still outperforming baselines.Our key contributions can be summarized as follows.We propose to use the idea of dataset reset to integrate offline data into online RLHF. Reset is a property that comes for free when optimizing generative models using RL. By leveraging dataset reset, our new algorithm DR-PO achieves strong performance guarantees and offers significant benefits in terms of computation tractability over prior theoretical RLHF works.When instantiating PPO as a policy optimizer in DR-PO, we show that our approach can outperform strong baselines PPO and DPO over two standard RLHF benchmarks: TL;DR summarization and Anthropic HH. DR-PO achieves superior empirical performance over PPO without introducing any additional computation or memory overhead to PPO.The theoretical investigation on online RLHF started in bandit setting with the notion of dueling bandits (Yue et\u00a0al.,, 2012; Zoghi et\u00a0al.,, 2014; Dud\u00edk et\u00a0al.,, 2015), which aims at identifying the optimal arm with human preference feedback over action pairs. Extending this discussion to tabular MDPs, Novoseller et\u00a0al., (2020) proposes a dueling posterior sampling algorithm that requires computing and sampling from the posterior of the model dynamics and reward function, leading to potential computational inefficiency. Another PAC RLHF algorithm for tabular MDPs is presented by Xu et\u00a0al., (2020). However, this method involves computing complicated bonus terms to guide exploration. Additionally, Pacchiano et\u00a0al., (2021); Chen et\u00a0al., (2022) have designed online RLHF algorithms with provable guarantees by updating a confidence set of the policies iteratively, which, unfortunately, are not practically feasible either. In a more recent study, Zhan et\u00a0al., 2023b  tackles the problem of reward-free RLHF. Nevertheless, their algorithm introduces a series of non-convex optimization problems which are challenging to solve. Notably, these works either only focus on tabular MDPs Novoseller et\u00a0al., (2020); Xu et\u00a0al., (2020); Pacchiano et\u00a0al., (2021) or rely on specialized function approximation such as linear parametrization (Pacchiano et\u00a0al.,, 2021; Zhan et\u00a0al., 2023b, ) and function classes with small Eluder dimension (Chen et\u00a0al.,, 2022; Wu and Sun,, 2023), which further restricts their application in practice. In contrast, we focus on the setting where preference-labeled data is only available offline, which is more consistent with the settings considered in applications of fine-tuning language models. Also by using the idea of dataset reset, our algorithm works with function approximation that is much more general than the above prior works.The study on theoretical offline RLHF is more limited. Li et\u00a0al., (2023) focuses on learning the reward from a human\u2019s behavior in dynamic discrete choice models rather than from human preference feedback, and thus, the setting is different. Zhu et\u00a0al., 2023a  studies PAC algorithms for linear models and Zhan et\u00a0al., 2023a  extends the analysis to general function approximation. However, both of their algorithms are not computationally efficient because they rely on constructing a confidence set for the reward function and solving a constrained maximin problem.Tiapkin et\u00a0al., (2023) studied the setting where high-quality expert demonstrations exist. They use behavior cloning to train a policy using expert demonstrations and then run an Upper-confidence-bound style algorithm to optimize a reward function under a KL regularization to the behavior-cloned policy. They show that for tabular and linear MDP, the expert demonstrations reduce the sample complexity of online RL. We consider preference-based offline datasets, which may not necessarily come from a high-quality expert, and function approximation that is significantly more general than linear and tabular functions. Note that UCB based algorithms can quickly become computationally intractable beyond tabular and linear settings (e.g., Jiang et\u00a0al., (2016); Du et\u00a0al., (2021)). Our algorithm uses the idea of dataset reset for exploration and does not involve any optimism-based exploration strategy, making it computationally tractable even when dealing with general function approximation. We think that the key idea of dataset reset can also be used in the setting from Tiapkin et\u00a0al., (2023) to make their algorithm extend beyond the tabular and linear MDP settings.This work continues the recent literature of RLHF algorithms that perform online RL (Zhu et\u00a0al., 2023b, ; Wu et\u00a0al.,, 2023; Chang et\u00a0al.,, 2023) to finetune large generative models. There have also been efforts to build on top of DPO (Rafailov et\u00a0al.,, 2023) with algorithms such as IPO (Azar et\u00a0al.,, 2011) and KTO (Contextual.ai,, 2023). In this paper, our work is complementary to many of these efforts in augmenting RL through the incorporation of dataset resets in online generation. Ideas from this work could directly be applied to existing online RLHF algorithms such as P3O (Wu et\u00a0al.,, 2023) and APA (Zhu et\u00a0al., 2023b, ). Given the recent work (Yuan et\u00a0al.,, 2024) in incorporating online generations to improve DPO, an offline RLHF method, the idea of dataset resets could also be relevant in this space of hybrid RLHF methods.The idea of reset is not new in RL (Kakade,, 2003; Bagnell,, 2004; Nair et\u00a0al.,, 2018; Salimans and Chen,, 2018; Yin et\u00a0al.,, 2022; Uchendu et\u00a0al.,, 2023; Silver et\u00a0al.,, 2016; Agarwal et\u00a0al.,, 2019; Daum\u00e9\u00a0III and Marcu,, 2005; Daum\u00e9 et\u00a0al.,, 2009). When resetting is available, it helps address exploration and credit assignment problems. In this work, we show that resetting to an offline dataset helps in RLHF. The key challenge in RLHF is that the reward model is learned purely from offline data which may not have a global coverage to the entire state space. Our algorithm incorporates KL regularization to ensure the learned policies do not deviate too much from the offline data so that we do not over-optimize the learned reward model (e.g., reward hacking). While the idea of KL-regularization was also used in prior empirical RLHF works (e.g.,Stiennon et\u00a0al., (2020); Bai et\u00a0al., 2022a ), we show that by combining the two key ideas, KL regularization and dataset reset, our algorithm achieves strong performance in both theory and practice. We also demonstrate the efficacy of our approach in the application of fine-tuning language models.In this paper we consider an episodic time-inhomogeneous Markov Decision Process (MDP) \u2133\u2133\\mathcal{M}caligraphic_M with state space \ud835\udcae={\ud835\udcaeh}h=1H\ud835\udcaesuperscriptsubscriptsubscript\ud835\udcae\u210e\u210e1\ud835\udc3b\\mathcal{S}=\\{\\mathcal{S}_{h}\\}_{h=1}^{H}caligraphic_S = { caligraphic_S start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_h = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT, action space \ud835\udc9c\ud835\udc9c\\mathcal{A}caligraphic_A and horizon H\ud835\udc3bHitalic_H. Here \ud835\udcaehsubscript\ud835\udcae\u210e\\mathcal{S}_{h}caligraphic_S start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT is the subspace of all states at step h\u210ehitalic_h. We suppose the states incorporate the information of the current step and thus {\ud835\udcaeh}h=1Hsuperscriptsubscriptsubscript\ud835\udcae\u210e\u210e1\ud835\udc3b\\{\\mathcal{S}_{h}\\}_{h=1}^{H}{ caligraphic_S start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_h = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT are mutually disjoint. We assume that every episode begins at the same state s1subscript\ud835\udc601s_{1}italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and ends at the dummy state sH+1subscript\ud835\udc60\ud835\udc3b1s_{H+1}italic_s start_POSTSUBSCRIPT italic_H + 1 end_POSTSUBSCRIPT, but our analysis can be extended to a random starting state easily. In each episode, at step h\u2208[H]\u210edelimited-[]\ud835\udc3bh\\in[H]italic_h \u2208 [ italic_H ], the agent observes the current shsubscript\ud835\udc60\u210es_{h}italic_s start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT and executes an action ahsubscript\ud835\udc4e\u210ea_{h}italic_a start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT. Then the environment generates a reward r\u22c6\u2062(sh,ah)superscript\ud835\udc5f\u22c6subscript\ud835\udc60\u210esubscript\ud835\udc4e\u210er^{\\star}(s_{h},a_{h})italic_r start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT ( italic_s start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) (which can be unobservable to the agent), and transits to a new state sh+1subscript\ud835\udc60\u210e1s_{h+1}italic_s start_POSTSUBSCRIPT italic_h + 1 end_POSTSUBSCRIPT, which is sampled from the transition probability P(\u22c5|sh,ah)P(\\cdot|s_{h},a_{h})italic_P ( \u22c5 | italic_s start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ). Here we suppose the reward function r\u22c6:\ud835\udcae\u00d7\ud835\udc9c\u21a6[0,1]:superscript\ud835\udc5f\u22c6maps-to\ud835\udcae\ud835\udc9c01r^{\\star}:\\mathcal{S}\\times\\mathcal{A}\\mapsto[0,1]italic_r start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT : caligraphic_S \u00d7 caligraphic_A \u21a6 [ 0 , 1 ] is bounded, and for any possible trajectory \u03c4=(sh,ah)h=1H\ud835\udf0fsuperscriptsubscriptsubscript\ud835\udc60\u210esubscript\ud835\udc4e\u210e\u210e1\ud835\udc3b\\tau=(s_{h},a_{h})_{h=1}^{H}italic_\u03c4 = ( italic_s start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_h = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT, we have \u2211h=1Hr\u22c6\u2062(sh,ah)\u2264rmaxsuperscriptsubscript\u210e1\ud835\udc3bsuperscript\ud835\udc5f\u22c6subscript\ud835\udc60\u210esubscript\ud835\udc4e\u210esubscript\ud835\udc5fmax\\sum_{h=1}^{H}r^{\\star}(s_{h},a_{h})\\leq r_{\\mathrm{max}}\u2211 start_POSTSUBSCRIPT italic_h = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT italic_r start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT ( italic_s start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) \u2264 italic_r start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT. Note that when the reward is sparse, rmaxsubscript\ud835\udc5fmaxr_{\\mathrm{max}}italic_r start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT can be much smaller than H\ud835\udc3bHitalic_H.A policy \u03c0:\ud835\udcae\u2192\u0394\ud835\udc9c:\ud835\udf0b\u2192\ud835\udcaesubscript\u0394\ud835\udc9c\\pi:\\mathcal{S}\\to\\Delta_{\\mathcal{A}}italic_\u03c0 : caligraphic_S \u2192 roman_\u0394 start_POSTSUBSCRIPT caligraphic_A end_POSTSUBSCRIPT specifies the action selection probability of the agent conditioned on the current state. Given a policy \u03c0\ud835\udf0b\\piitalic_\u03c0, we define its state-action visitation measure as dh\u03c0\u2062(s,a)=\u2119\u03c0\u2062(sh=s,ah=a)subscriptsuperscript\ud835\udc51\ud835\udf0b\u210e\ud835\udc60\ud835\udc4esuperscript\u2119\ud835\udf0bformulae-sequencesubscript\ud835\udc60\u210e\ud835\udc60subscript\ud835\udc4e\u210e\ud835\udc4ed^{\\pi}_{h}(s,a)=\\mathbb{P}^{\\pi}(s_{h}=s,a_{h}=a)italic_d start_POSTSUPERSCRIPT italic_\u03c0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ( italic_s , italic_a ) = blackboard_P start_POSTSUPERSCRIPT italic_\u03c0 end_POSTSUPERSCRIPT ( italic_s start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT = italic_s , italic_a start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT = italic_a ) for all s\u2208\ud835\udcaeh,a\u2208\ud835\udc9c,h\u2208[H]formulae-sequence\ud835\udc60subscript\ud835\udcae\u210eformulae-sequence\ud835\udc4e\ud835\udc9c\u210edelimited-[]\ud835\udc3bs\\in\\mathcal{S}_{h},a\\in\\mathcal{A},h\\in[H]italic_s \u2208 caligraphic_S start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_a \u2208 caligraphic_A , italic_h \u2208 [ italic_H ] where \u2119\u03c0\u2062(\u22c5)superscript\u2119\ud835\udf0b\u22c5\\mathbb{P}^{\\pi}(\\cdot)blackboard_P start_POSTSUPERSCRIPT italic_\u03c0 end_POSTSUPERSCRIPT ( \u22c5 ) denotes the distribution of the trajectory when executing policy \u03c0\ud835\udf0b\\piitalic_\u03c0. We will also use dh\u03c0\u2062(s)=\u2211a\u2208\ud835\udc9cdh\u03c0\u2062(s,a)subscriptsuperscript\ud835\udc51\ud835\udf0b\u210e\ud835\udc60subscript\ud835\udc4e\ud835\udc9csubscriptsuperscript\ud835\udc51\ud835\udf0b\u210e\ud835\udc60\ud835\udc4ed^{\\pi}_{h}(s)=\\sum_{a\\in\\mathcal{A}}d^{\\pi}_{h}(s,a)italic_d start_POSTSUPERSCRIPT italic_\u03c0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ( italic_s ) = \u2211 start_POSTSUBSCRIPT italic_a \u2208 caligraphic_A end_POSTSUBSCRIPT italic_d start_POSTSUPERSCRIPT italic_\u03c0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ( italic_s , italic_a ) to denote the state visitation measure and d\u03c0\u2062(\u03c4)superscript\ud835\udc51\ud835\udf0b\ud835\udf0fd^{\\pi}(\\tau)italic_d start_POSTSUPERSCRIPT italic_\u03c0 end_POSTSUPERSCRIPT ( italic_\u03c4 ) to denote the distribution of the trajectory under policy \u03c0\ud835\udf0b\\piitalic_\u03c0. We can further define the associated value functions and Q functions of policy \u03c0\ud835\udf0b\\piitalic_\u03c0 and reward function r\ud835\udc5fritalic_r as V\u03c0,r\u2062(s)=\ud835\udd3c\u03c0\u2062[\u2211t=hHr\u2062(st,at)\u2223sh=s],Q\u03c0,r\u2062(s,a)=\ud835\udd3c\u03c0\u2062[\u2211t=hHr\u2062(st,at)\u2223sh=s,ah=a]formulae-sequencesuperscript\ud835\udc49\ud835\udf0b\ud835\udc5f\ud835\udc60subscript\ud835\udd3c\ud835\udf0bdelimited-[]conditionalsuperscriptsubscript\ud835\udc61\u210e\ud835\udc3b\ud835\udc5fsubscript\ud835\udc60\ud835\udc61subscript\ud835\udc4e\ud835\udc61subscript\ud835\udc60\u210e\ud835\udc60superscript\ud835\udc44\ud835\udf0b\ud835\udc5f\ud835\udc60\ud835\udc4esubscript\ud835\udd3c\ud835\udf0bdelimited-[]formulae-sequenceconditionalsuperscriptsubscript\ud835\udc61\u210e\ud835\udc3b\ud835\udc5fsubscript\ud835\udc60\ud835\udc61subscript\ud835\udc4e\ud835\udc61subscript\ud835\udc60\u210e\ud835\udc60subscript\ud835\udc4e\u210e\ud835\udc4eV^{\\pi,r}(s)=\\mathbb{E}_{\\pi}[\\sum_{t=h}^{H}r(s_{t},a_{t})\\mid s_{h}=s],Q^{\\pi%\n,r}(s,a)=\\mathbb{E}_{\\pi}[\\sum_{t=h}^{H}r(s_{t},a_{t})\\mid s_{h}=s,a_{h}=a]italic_V start_POSTSUPERSCRIPT italic_\u03c0 , italic_r end_POSTSUPERSCRIPT ( italic_s ) = blackboard_E start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT [ \u2211 start_POSTSUBSCRIPT italic_t = italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT italic_r ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) \u2223 italic_s start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT = italic_s ] , italic_Q start_POSTSUPERSCRIPT italic_\u03c0 , italic_r end_POSTSUPERSCRIPT ( italic_s , italic_a ) = blackboard_E start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT [ \u2211 start_POSTSUBSCRIPT italic_t = italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT italic_r ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) \u2223 italic_s start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT = italic_s , italic_a start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT = italic_a ] for all h\u2208[H],s\u2208\ud835\udcaeh,a\u2208\ud835\udc9cformulae-sequence\u210edelimited-[]\ud835\udc3bformulae-sequence\ud835\udc60subscript\ud835\udcae\u210e\ud835\udc4e\ud835\udc9ch\\in[H],s\\in\\mathcal{S}_{h},a\\in\\mathcal{A}italic_h \u2208 [ italic_H ] , italic_s \u2208 caligraphic_S start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_a \u2208 caligraphic_A.111For notation simplicity, we drop the usual subscript h\u210ehitalic_h in value functions, as we have assumed state s\ud835\udc60sitalic_s contains the information of time step h\u210ehitalic_h.  They characterize the expected cumulative reward under policy \u03c0\ud835\udf0b\\piitalic_\u03c0 starting from a state or a state-action pair.We aim to find an \u03f5italic-\u03f5\\epsilonitalic_\u03f5-optimal policy \u03c0^^\ud835\udf0b\\widehat{\\pi}over^ start_ARG italic_\u03c0 end_ARG with respect to the true reward r\u22c6superscript\ud835\udc5f\u22c6r^{\\star}italic_r start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT and a target policy \u03c0\u22c6superscript\ud835\udf0b\u22c6\\pi^{\\star}italic_\u03c0 start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT which we denote as some high-quality policy (\u03c0\u22c6superscript\ud835\udf0b\u22c6\\pi^{\\star}italic_\u03c0 start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT is not necessarily the globally optimal policy), i.e., V\u03c0\u22c6,r\u22c6\u2062(s1)\u2212V\u03c0^,r\u22c6\u2062(s1)\u2264\u03f5superscript\ud835\udc49superscript\ud835\udf0b\u22c6superscript\ud835\udc5f\u22c6subscript\ud835\udc601superscript\ud835\udc49^\ud835\udf0bsuperscript\ud835\udc5f\u22c6subscript\ud835\udc601italic-\u03f5V^{\\pi^{\\star},r^{\\star}}(s_{1})-V^{\\widehat{\\pi},r^{\\star}}(s_{1})\\leq\\epsilonitalic_V start_POSTSUPERSCRIPT italic_\u03c0 start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT , italic_r start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT ( italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_V start_POSTSUPERSCRIPT over^ start_ARG italic_\u03c0 end_ARG , italic_r start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT ( italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) \u2264 italic_\u03f5. Particularly, we would only utilize common oracles such as Maximum Likelihood Estimator (MLE) and Least Squares Regression (LSR). We also want our algorithms to be able to leverage general function classes beyond linear functions.We consider the setting where the true reward r\u22c6superscript\ud835\udc5f\u22c6r^{\\star}italic_r start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT is unobservable. Instead, we have access to an offline trajectory-pair dataset \ud835\udc9fR={(\u03c4m0,\u03c4m1,om)m=1M}subscript\ud835\udc9fRsuperscriptsubscriptsubscriptsuperscript\ud835\udf0f0\ud835\udc5asubscriptsuperscript\ud835\udf0f1\ud835\udc5asubscript\ud835\udc5c\ud835\udc5a\ud835\udc5a1\ud835\udc40\\mathcal{D}_{\\mathrm{R}}=\\{(\\tau^{0}_{m},\\tau^{1}_{m},o_{m})_{m=1}^{M}\\}caligraphic_D start_POSTSUBSCRIPT roman_R end_POSTSUBSCRIPT = { ( italic_\u03c4 start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT , italic_\u03c4 start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT , italic_o start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_m = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT } labeled with human preference, where the trajectories \u03c4m0subscriptsuperscript\ud835\udf0f0\ud835\udc5a\\tau^{0}_{m}italic_\u03c4 start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT and \u03c4m1subscriptsuperscript\ud835\udf0f1\ud835\udc5a\\tau^{1}_{m}italic_\u03c4 start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT are i.i.d. sampled from some pre-trained policy \u03c0SFTsuperscript\ud835\udf0bSFT\\pi^{\\mathrm{SFT}}italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT (e.g., in NLP tasks, this can be the instruction fine-tuned policy, which is also called supervised fine-tuned (SFT) policy). In this work, we do not explicitly consider the learning procedure of \u03c0SFTsuperscript\ud835\udf0bSFT\\pi^{\\mathrm{SFT}}italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT, and we assume it is given to us.\nHere om\u2208{0,1}subscript\ud835\udc5c\ud835\udc5a01o_{m}\\in\\{0,1\\}italic_o start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT \u2208 { 0 , 1 } characterizes the human preference over the trajectory pairs (\u03c4m0,\u03c4m1)subscriptsuperscript\ud835\udf0f0\ud835\udc5asubscriptsuperscript\ud835\udf0f1\ud835\udc5a(\\tau^{0}_{m},\\tau^{1}_{m})( italic_\u03c4 start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT , italic_\u03c4 start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) and we suppose the human preference is modeled by a monotonically increasing link function \u03a6\u03a6\\Phiroman_\u03a6:where we use r\u22c6\u2062(\u03c4)superscript\ud835\udc5f\u22c6\ud835\udf0fr^{\\star}(\\tau)italic_r start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT ( italic_\u03c4 ) to denote \u2211h=1Hr\u22c6\u2062(sh,ah)superscriptsubscript\u210e1\ud835\udc3bsuperscript\ud835\udc5f\u22c6subscript\ud835\udc60\u210esubscript\ud835\udc4e\u210e\\sum_{h=1}^{H}r^{\\star}(s_{h},a_{h})\u2211 start_POSTSUBSCRIPT italic_h = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT italic_r start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT ( italic_s start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) for any trajectory \u03c4=(sh,ah)h=1H\ud835\udf0fsuperscriptsubscriptsubscript\ud835\udc60\u210esubscript\ud835\udc4e\u210e\u210e1\ud835\udc3b\\tau=(s_{h},a_{h})_{h=1}^{H}italic_\u03c4 = ( italic_s start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_h = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT. A widely-used model is the Bradley-Terry-Luce (BTL) model (Bradley and Terry,, 1952) where the link function is chosen to be the sigmoid function \u03c3\u2062(x)=1/{1+exp\u2061(\u2212x)}\ud835\udf0e\ud835\udc6511\ud835\udc65\\sigma(x)=1/\\{1+\\exp(-x)\\}italic_\u03c3 ( italic_x ) = 1 / { 1 + roman_exp ( - italic_x ) }. We will use \u03ba=1infx\u2208[\u2212rmax,rmax]\u03a6\u2032\u2062(x)\ud835\udf051subscriptinfimum\ud835\udc65subscript\ud835\udc5fmaxsubscript\ud835\udc5fmaxsuperscript\u03a6\u2032\ud835\udc65\\kappa=\\frac{1}{\\inf_{x\\in[-r_{\\mathrm{max}},r_{\\mathrm{max}}]}\\Phi^{\\prime}(x)}italic_\u03ba = divide start_ARG 1 end_ARG start_ARG roman_inf start_POSTSUBSCRIPT italic_x \u2208 [ - italic_r start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT ] end_POSTSUBSCRIPT roman_\u03a6 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_x ) end_ARG to measure the non-linearity of the link function \u03a6\u03a6\\Phiroman_\u03a6, which in turn reflects the hardness of learning the reward model from the human preference. Given \ud835\udc9fRsubscript\ud835\udc9fR\\mathcal{D}_{\\mathrm{R}}caligraphic_D start_POSTSUBSCRIPT roman_R end_POSTSUBSCRIPT, we can learn a reward model r^^\ud835\udc5f\\widehat{r}over^ start_ARG italic_r end_ARG using MLE:With the BTL model, the above NLL becomeswhich is a loss function that has been used in many prior RLHF works(Christiano et\u00a0al.,, 2017; Stiennon et\u00a0al.,, 2020). We also assume that we have an unlabeled dataset \ud835\udc9fTR={\u03c4n}n=1Nsubscript\ud835\udc9fTRsuperscriptsubscriptsubscript\ud835\udf0f\ud835\udc5b\ud835\udc5b1\ud835\udc41\\mathcal{D}_{\\mathrm{TR}}=\\{\\tau_{n}\\}_{n=1}^{N}caligraphic_D start_POSTSUBSCRIPT roman_TR end_POSTSUBSCRIPT = { italic_\u03c4 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT where \u03c4nsubscript\ud835\udf0f\ud835\udc5b\\tau_{n}italic_\u03c4 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT is i.i.d. sampled from \u03c0SFTsuperscript\ud835\udf0bSFT\\pi^{\\mathrm{SFT}}italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT. Note that \ud835\udc9fTRsubscript\ud835\udc9fTR\\mathcal{D}_{\\mathrm{TR}}caligraphic_D start_POSTSUBSCRIPT roman_TR end_POSTSUBSCRIPT is unlabeled, so it potentially can be much larger than the human-labeled dataset \ud835\udc9fRsubscript\ud835\udc9fR\\mathcal{D}_{\\mathrm{R}}caligraphic_D start_POSTSUBSCRIPT roman_R end_POSTSUBSCRIPT.We consider the setting where we can reset the system. More formally, given any state shsubscript\ud835\udc60\u210es_{h}italic_s start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT at time step h\u210ehitalic_h, we can reset the RL agent directly to shsubscript\ud835\udc60\u210es_{h}italic_s start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT and rollout a policy \u03c0\ud835\udf0b\\piitalic_\u03c0. While this is certainly an assumption, it is satisfied in many important applications, e.g., fine-tuning generative models such as LLMs (Ouyang et\u00a0al.,, 2022; Ramamurthy et\u00a0al.,, 2022; Chang et\u00a0al.,, 2023) and Diffusion models (Lee et\u00a0al.,, 2023) with RL.\nIn text generation, a state shsubscript\ud835\udc60\u210es_{h}italic_s start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT typically means a partial sentence. Resetting from this state would then mean that we feed the partial sentence shsubscript\ud835\udc60\u210es_{h}italic_s start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT to a transformer based policy and have it generate new tokens one by one starting from the given partial sentence. We emphasize that in the RL literature, prior works (e.g., PPO and many RL theoretical works (Agarwal et\u00a0al.,, 2021; Azar et\u00a0al.,, 2017; Jin et\u00a0al.,, 2020; Zhan et\u00a0al.,, 2022)) typically do not assume the ability to reset \u2013 they often assume the agent has to always start from some initial states. However, when reset is available, it is often a game changer, in both theory (Yin et\u00a0al.,, 2022) and in practice (e.g., AlphaGo (Silver et\u00a0al.,, 2016)).We present a meta-algorithm here to provide the details of how we leverage the idea of dataset reset to collect online batch data. We abstract away the policy optimization oracle here to emphasize the novelty of our interaction with the environment for online data collection via dataset reset.\nOnce the online batch data is collected, we feed it to a policy optimization oracle, e.g., PG, NPG, Actor-critic methods, or a PPO-style update 222Here we mean the specific actor-critic style policy optimization formulation where clipping is used to ensure small policy update, and critic is learned via GAE, on a given online batch data (Schulman et\u00a0al.,, 2017)..Algorithm\u00a01 summarizes the key idea of dataset reset in DR-PO. The key difference between DR-PO and a more standard policy optimizer is that in DR-PO, for each episode, the policy collects online trajectories via resetting to a state randomly sampled from some trajectory in the offline dataset \ud835\udc9fTRsubscript\ud835\udc9fnormal-TR\\mathcal{D}_{\\mathrm{TR}}caligraphic_D start_POSTSUBSCRIPT roman_TR end_POSTSUBSCRIPT. In other words, we do not rollout the policy \u03c0\ud835\udf0b\\piitalic_\u03c0 from the initial state s1subscript\ud835\udc601s_{1}italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT as typically done in standard policy optimization algorithms like PG.\nThe online data collection procedure collects a batch of online trajectories \ud835\udc9fo\u2062nsubscript\ud835\udc9f\ud835\udc5c\ud835\udc5b\\mathcal{D}_{on}caligraphic_D start_POSTSUBSCRIPT italic_o italic_n end_POSTSUBSCRIPT. Note for each online trajectory, we record each state-action pair\u2019s reward measured under the learned reward model r^^\ud835\udc5f\\widehat{r}over^ start_ARG italic_r end_ARG, and also the log ratio of \u03c0tsuperscript\ud835\udf0b\ud835\udc61\\pi^{t}italic_\u03c0 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT and \u03c0SFTsuperscript\ud835\udf0bSFT\\pi^{\\mathrm{SFT}}italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT which serves as an empirical estimate of the policy KL divergence, i.e., \ud835\uddaa\ud835\uddab(\u03c0t(sh\u2032)||\u03c0SFT(sh\u2032))\\mathsf{KL}(\\pi^{t}(s_{h^{\\prime}})||\\pi^{\\mathrm{SFT}}(s_{h^{\\prime}}))sansserif_KL ( italic_\u03c0 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_s start_POSTSUBSCRIPT italic_h start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ) | | italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT ( italic_s start_POSTSUBSCRIPT italic_h start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ) ). Such a KL divergence term can be optionally used as a reward penalty to ensure the learned policies do not deviate too far from \u03c0SFTsuperscript\ud835\udf0bSFT\\pi^{\\mathrm{SFT}}italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT so that the reward model r^^\ud835\udc5f\\widehat{r}over^ start_ARG italic_r end_ARG stays as a good approximation of the true reward r\u22c6superscript\ud835\udc5f\u22c6r^{\\star}italic_r start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT under learned policies\u2019 trajectory distributions. We use this KL penalty both in theory and in practice.Once the online data is collected, we feed it to a policy optimization oracle PO for a policy update. A PO oracle can be a PG, NPG, or PPO style update. To be more specific, for a PPO style update procedure, we use \ud835\udc9fo\u2062nsubscript\ud835\udc9f\ud835\udc5c\ud835\udc5b\\mathcal{D}_{on}caligraphic_D start_POSTSUBSCRIPT italic_o italic_n end_POSTSUBSCRIPT to fit a critic for advantage estimation A^\u2062(s,a)^\ud835\udc34\ud835\udc60\ud835\udc4e\\widehat{A}(s,a)over^ start_ARG italic_A end_ARG ( italic_s , italic_a )333when using KL penalty, this advantage function measures the advantage under KL regularized reward \u2014 r^\u2212\u03bb\u2062\ud835\uddaa\ud835\uddab^\ud835\udc5f\ud835\udf06\ud835\uddaa\ud835\uddab\\widehat{r}-\\lambda\\mathsf{KL}over^ start_ARG italic_r end_ARG - italic_\u03bb sansserif_KL with \u03bb\u2208\u211d+\ud835\udf06superscript\u211d\\lambda\\in\\mathbb{R}^{+}italic_\u03bb \u2208 blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT as coefficient for the KL penalty.  (e.g., via generalized advantage estimation used in PPO), and then update the policy on \ud835\udc9fo\u2062nsubscript\ud835\udc9f\ud835\udc5c\ud835\udc5b\\mathcal{D}_{on}caligraphic_D start_POSTSUBSCRIPT italic_o italic_n end_POSTSUBSCRIPT with the clipping trick: \u03c0t+1\u21d0arg\u2061max\u03c0\u2062\u2211s,a\u2208\ud835\udc9fo\u2062nClip\u2062(\u03c0\u2062(a|s)\u03c0t\u2062(a|s))\u2062A^\u2062(s,a)\u21d0superscript\ud835\udf0b\ud835\udc611subscript\ud835\udf0bsubscript\ud835\udc60\ud835\udc4esubscript\ud835\udc9f\ud835\udc5c\ud835\udc5bClip\ud835\udf0bconditional\ud835\udc4e\ud835\udc60subscript\ud835\udf0b\ud835\udc61conditional\ud835\udc4e\ud835\udc60^\ud835\udc34\ud835\udc60\ud835\udc4e\\pi^{t+1}\\Leftarrow\\arg\\max_{\\pi}\\sum_{s,a\\in\\mathcal{D}_{on}}\\mathrm{Clip}%\n\\left(\\frac{\\pi(a|s)}{\\pi_{t}(a|s)}\\right)\\widehat{A}(s,a)italic_\u03c0 start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT \u21d0 roman_arg roman_max start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT \u2211 start_POSTSUBSCRIPT italic_s , italic_a \u2208 caligraphic_D start_POSTSUBSCRIPT italic_o italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_Clip ( divide start_ARG italic_\u03c0 ( italic_a | italic_s ) end_ARG start_ARG italic_\u03c0 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_a | italic_s ) end_ARG ) over^ start_ARG italic_A end_ARG ( italic_s , italic_a ). This is the policy update that we use in our experiments. In our theory, we use NPG as the PO oracle. While PPO and NPG are different when it comes to exact implementation, PPO can be understood as a heuristic that approximates NPG for the purpose of being more scalable for large-scale optimization (e.g., the clipping trick induced by PPO is approximately trying to ensure that the new policy does not deviate too much from the old one \u2013 a key property that NPG methods advocated for (Kakade,, 2001; Kakade and Langford,, 2002; Bagnell and Schneider,, 2003; Schulman et\u00a0al.,, 2015)).Implementation wise, with PPO as a PO oracle, given a standard PPO implementation, all we need to do is to feed the policy optimization and GAE oracles in PPO using the online batch of data collected in our way, i.e., \ud835\udc9fo\u2062nsubscript\ud835\udc9f\ud835\udc5c\ud835\udc5b\\mathcal{D}_{on}caligraphic_D start_POSTSUBSCRIPT italic_o italic_n end_POSTSUBSCRIPT collected via dataset reset. Our experiments on two RLHF datasets show that hyperparameters that work well for PPO also work for DR-PO.In this section, we analyze the DR-PO (Alg\u00a01) by instantiating the policy optimization oracle PO to be a Natural Policy Gradient (NPG) oracle. For completeness, we describe PO in Algorithm\u00a02, which in high level consists of policy evaluation via least square regression, and then policy update via Mirror Descent style procedure. We leave the detailed full description of the algorithm in Appendix\u00a0A.In Alg.\u00a02, we use the online data to fit a Q\ud835\udc44Qitalic_Q function estimate of the current policy \u03c0tsuperscript\ud835\udf0b\ud835\udc61\\pi^{t}italic_\u03c0 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT. Once we learn the critic, we perform policy update via running KL-based Mirror Descent. Note that this step has a closed-form expression for \u03c0t+1superscript\ud835\udf0b\ud835\udc611\\pi^{t+1}italic_\u03c0 start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT:Note that the KL penalty to \u03c0SFTsuperscript\ud835\udf0bSFT\\pi^{\\mathrm{SFT}}italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT in the policy update procedure is important to ensure that \u03c0t+1superscript\ud835\udf0b\ud835\udc611\\pi^{t+1}italic_\u03c0 start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT does not deviate too much from \u03c0SFTsuperscript\ud835\udf0bSFT\\pi^{\\mathrm{SFT}}italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT. Also this type of updates ensures that the support of \u03c0t(\u22c5|s)\\pi^{t}(\\cdot|s)italic_\u03c0 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( \u22c5 | italic_s ) is always a subset of the support of \u03c0SFT\u2062(s)superscript\ud835\udf0bSFT\ud835\udc60\\pi^{\\mathrm{SFT}}(s)italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT ( italic_s ) for all state s\ud835\udc60sitalic_s.Though we mainly focus on the settings where we can reset, when resetting is not possible (e.g., real robotics applications), we can implement the reset by a roll-in and roll-out procedure since we have access to \u03c0SFTsuperscript\ud835\udf0bSFT\\pi^{\\mathrm{SFT}}italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT: we roll-in \u03c0SFTsuperscript\ud835\udf0bSFT\\pi^{\\mathrm{SFT}}italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT to some shsubscript\ud835\udc60\u210es_{h}italic_s start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT, and then continue by rolling out our policy that is being optimized. This procedure is closely related to the PPO++ algorithm proposed in Chang et\u00a0al., (2023), where the authors empirically demonstrated that it outperforms vanilla PPO on some RLHF benchmarks (but no detailed theoretical investigation). When resetting is available, by directly resetting to the offline data generated by \u03c0SFTsuperscript\ud835\udf0bSFT\\pi^{\\mathrm{SFT}}italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT, we further reduce computation.Now we introduce the required assumptions in our analysis.We first assume that the reward function class and Q\ud835\udc44Qitalic_Q function class are realizable and bounded:Suppose that we have r\u22c6\u2208\u211bsuperscript\ud835\udc5f\u22c6\u211br^{\\star}\\in\\mathcal{R}italic_r start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT \u2208 caligraphic_R. In addition, assume that 0\u2264r\u2062(\u03c4)\u2264rmax0\ud835\udc5f\ud835\udf0fsubscript\ud835\udc5fmax0\\leq r(\\tau)\\leq r_{\\mathrm{max}}0 \u2264 italic_r ( italic_\u03c4 ) \u2264 italic_r start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT for all r\u2208\u211b\ud835\udc5f\u211br\\in\\mathcal{R}italic_r \u2208 caligraphic_R and trajectory \u03c4\ud835\udf0f\\tauitalic_\u03c4.Suppose that we have Q\u03c0t,r^\u2208\u2131superscript\ud835\udc44superscript\ud835\udf0b\ud835\udc61^\ud835\udc5f\u2131Q^{\\pi^{t},\\widehat{r}}\\in\\mathcal{F}italic_Q start_POSTSUPERSCRIPT italic_\u03c0 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , over^ start_ARG italic_r end_ARG end_POSTSUPERSCRIPT \u2208 caligraphic_F for all t\u2208[T]\ud835\udc61delimited-[]\ud835\udc47t\\in[T]italic_t \u2208 [ italic_T ]. In addition, assume that 0\u2264f\u2062(s,a)\u2264rmax0\ud835\udc53\ud835\udc60\ud835\udc4esubscript\ud835\udc5fmax0\\leq f(s,a)\\leq r_{\\mathrm{max}}0 \u2264 italic_f ( italic_s , italic_a ) \u2264 italic_r start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT for all f\u2208\u2131,s\u2208\ud835\udcae,a\u2208\ud835\udc9cformulae-sequence\ud835\udc53\u2131formulae-sequence\ud835\udc60\ud835\udcae\ud835\udc4e\ud835\udc9cf\\in\\mathcal{F},s\\in\\mathcal{S},a\\in\\mathcal{A}italic_f \u2208 caligraphic_F , italic_s \u2208 caligraphic_S , italic_a \u2208 caligraphic_A.Realizability is a standard assumption used in the theoretical analysis of supervised learning. It is possible to extend our analysis to the setting where model-misspecification exists, and we leave this extension as a future work.Then we assume that \u03c0SFTsuperscript\ud835\udf0bSFT\\pi^{\\mathrm{SFT}}italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT can cover the comparator policy \u03c0\u22c6superscript\ud835\udf0b\u22c6\\pi^{\\star}italic_\u03c0 start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT. In addition, we know the learned policy \u03c0^^\ud835\udf0b\\widehat{\\pi}over^ start_ARG italic_\u03c0 end_ARG is close to \u03c0SFTsuperscript\ud835\udf0bSFT\\pi^{\\mathrm{SFT}}italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT in terms of Kl divergence due to the regularizer \ud835\uddaa\ud835\uddab(\u22c5\u2225\u03c0SFT)\\mathsf{KL}(\\cdot\\|\\pi^{\\mathrm{SFT}})sansserif_KL ( \u22c5 \u2225 italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT ) in the mirror descent step. Thus, to deal with distribution shift, we also assume \u03c0SFTsuperscript\ud835\udf0bSFT\\pi^{\\mathrm{SFT}}italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT can cover the policies which are close to itself:Suppose that we have for any B\ud835\uddaa\ud835\uddab\u22650subscript\ud835\udc35\ud835\uddaa\ud835\uddab0B_{\\mathsf{KL}}\\geq 0italic_B start_POSTSUBSCRIPT sansserif_KL end_POSTSUBSCRIPT \u2265 0:where \u0398\u2062(\u03c0SFT,B\ud835\uddaa\ud835\uddab):={\u03c0:\ud835\uddaa\ud835\uddab\u2062(\u03c0\u2062(s)\u2225\u03c0SFT\u2062(s))\u2264B\ud835\uddaa\ud835\uddab,\u2200s\u2208\ud835\udcae}assign\u0398superscript\ud835\udf0bSFTsubscript\ud835\udc35\ud835\uddaa\ud835\uddabconditional-set\ud835\udf0bformulae-sequence\ud835\uddaa\ud835\uddabconditional\ud835\udf0b\ud835\udc60superscript\ud835\udf0bSFT\ud835\udc60subscript\ud835\udc35\ud835\uddaa\ud835\uddabfor-all\ud835\udc60\ud835\udcae\\Theta(\\pi^{\\mathrm{SFT}},B_{\\mathsf{KL}}):=\\{\\pi:\\mathsf{KL}(\\pi(s)\\|\\pi^{%\n\\mathrm{SFT}}(s))\\leq B_{\\mathsf{KL}},\\forall s\\in\\mathcal{S}\\}roman_\u0398 ( italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT , italic_B start_POSTSUBSCRIPT sansserif_KL end_POSTSUBSCRIPT ) := { italic_\u03c0 : sansserif_KL ( italic_\u03c0 ( italic_s ) \u2225 italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT ( italic_s ) ) \u2264 italic_B start_POSTSUBSCRIPT sansserif_KL end_POSTSUBSCRIPT , \u2200 italic_s \u2208 caligraphic_S }.Note that in Assumption\u00a05.4 we need \u03c0SFTsuperscript\ud835\udf0bSFT\\pi^{\\mathrm{SFT}}italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT to cover \u03c0\u22c6superscript\ud835\udf0b\u22c6\\pi^{\\star}italic_\u03c0 start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT, both trajectory-wise and state-action-wise. In particular, we always have C\ud835\uddb2\ud835\uddb3\u2264C\ud835\uddb3\ud835\uddb1subscript\ud835\udc36\ud835\uddb2\ud835\uddb3subscript\ud835\udc36\ud835\uddb3\ud835\uddb1C_{\\mathsf{ST}}\\leq C_{\\mathsf{TR}}italic_C start_POSTSUBSCRIPT sansserif_ST end_POSTSUBSCRIPT \u2264 italic_C start_POSTSUBSCRIPT sansserif_TR end_POSTSUBSCRIPT. Assuming trajectory-wise covering is necessary in RLHF because the human feedback is also trajectory-wise, as shown by the lower bounds in Zhan et\u00a0al., 2023a . Intuitively, if the offline data only covers low performance policies\u2019 traces, then the learned reward model cannot guarantee to recognize trajectories from a high performance policy during test time (because it has never seen such things in training).We can indeed relax Assumption\u00a05.4 by leveraging the information in \u211b\u211b\\mathcal{R}caligraphic_R and \u2131\u2131\\mathcal{F}caligraphic_F, as shown in the discussion in Appendix\u00a0B.Note that we have C\ud835\uddb2\ud835\udda5\ud835\uddb3\u2062(B\ud835\uddaa\ud835\uddab)<\u221esubscript\ud835\udc36\ud835\uddb2\ud835\udda5\ud835\uddb3subscript\ud835\udc35\ud835\uddaa\ud835\uddabC_{\\mathsf{SFT}}(B_{\\mathsf{KL}})<\\inftyitalic_C start_POSTSUBSCRIPT sansserif_SFT end_POSTSUBSCRIPT ( italic_B start_POSTSUBSCRIPT sansserif_KL end_POSTSUBSCRIPT ) < \u221e for all B\ud835\uddaa\ud835\uddab<\u221esubscript\ud835\udc35\ud835\uddaa\ud835\uddabB_{\\mathsf{KL}}<\\inftyitalic_B start_POSTSUBSCRIPT sansserif_KL end_POSTSUBSCRIPT < \u221e naturally because \u03c0\u2208\u0398\u2062(\u03c0SFT,B\ud835\uddaa\ud835\uddab)\ud835\udf0b\u0398superscript\ud835\udf0bSFTsubscript\ud835\udc35\ud835\uddaa\ud835\uddab\\pi\\in\\Theta(\\pi^{\\mathrm{SFT}},B_{\\mathsf{KL}})italic_\u03c0 \u2208 roman_\u0398 ( italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT , italic_B start_POSTSUBSCRIPT sansserif_KL end_POSTSUBSCRIPT ) has bounded KL diveregnce with respect to \u03c0SFTsuperscript\ud835\udf0bSFT\\pi^{\\mathrm{SFT}}italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT.Under the above assumptions, we have the following theorem to characterize the suboptimality of \u03c0^^\ud835\udf0b\\widehat{\\pi}over^ start_ARG italic_\u03c0 end_ARG returned by Algorithm\u00a03. Recall that \u03ba=1infx\u2208[\u2212rmax,rmax]\u03a6\u2032\u2062(x)\ud835\udf051subscriptinfimum\ud835\udc65subscript\ud835\udc5fmaxsubscript\ud835\udc5fmaxsuperscript\u03a6\u2032\ud835\udc65\\kappa=\\frac{1}{\\inf_{x\\in[-r_{\\mathrm{max}},r_{\\mathrm{max}}]}\\Phi^{\\prime}(x)}italic_\u03ba = divide start_ARG 1 end_ARG start_ARG roman_inf start_POSTSUBSCRIPT italic_x \u2208 [ - italic_r start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT ] end_POSTSUBSCRIPT roman_\u03a6 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_x ) end_ARG measures the non-linearity of the link function \u03a6\u03a6\\Phiroman_\u03a6.Suppose Assumption\u00a05.2,5.3,5.4 hold. For any \u03b4\u2208(0,1]\ud835\udeff01\\delta\\in(0,1]italic_\u03b4 \u2208 ( 0 , 1 ], letand set \u03b7=1T\u2062rmax2\ud835\udf021\ud835\udc47superscriptsubscript\ud835\udc5fnormal-max2\\eta=\\sqrt{\\frac{1}{Tr_{\\mathrm{max}}^{2}}}italic_\u03b7 = square-root start_ARG divide start_ARG 1 end_ARG start_ARG italic_T italic_r start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG end_ARG, then with probability at least 1\u2212\u03b41\ud835\udeff1-\\delta1 - italic_\u03b4, we have Algorithm\u00a01 with NPG update (Algorithm\u00a02) returns a policy \u03c0^normal-^\ud835\udf0b\\widehat{\\pi}over^ start_ARG italic_\u03c0 end_ARG which satisfiesTheorem\u00a05.7 indicates that the suboptimality of \u03c0^^\ud835\udf0b\\widehat{\\pi}over^ start_ARG italic_\u03c0 end_ARG scales with 1M1\ud835\udc40\\frac{1}{M}divide start_ARG 1 end_ARG start_ARG italic_M end_ARG and 1N1\ud835\udc41\\frac{1}{N}divide start_ARG 1 end_ARG start_ARG italic_N end_ARG polynomially. More specifically, term (1) in Equation\u00a02 measures the estimation error of the reward, (2) is the Q function estimation error and (3) is the optimization error of NPG. We can see that there exists a tradeoff between the estimation error and optimization error. With increasing T\ud835\udc47Titalic_T and decreasing \u03bb\ud835\udf06\\lambdaitalic_\u03bb, the optimization error (3) will decrease while the distirbution shift coefficient C\ud835\uddb2\ud835\udda5\ud835\uddb3subscript\ud835\udc36\ud835\uddb2\ud835\udda5\ud835\uddb3C_{\\mathsf{SFT}}italic_C start_POSTSUBSCRIPT sansserif_SFT end_POSTSUBSCRIPT will become larger, leading to amplified estimation error. In particular, from Theorem\u00a05.7, we can obtain the following sample complexity of DR-PO by setting T\ud835\udc47Titalic_T and \u03bb\ud835\udf06\\lambdaitalic_\u03bb appropriately:\nSuppose Assumption\u00a05.2,5.3,5.4 hold and setthen if we havewe have with probability at least 1\u2212\u03b41\ud835\udeff1-\\delta1 - italic_\u03b4 that Algorithm\u00a01 with NPG update (Algorithm\u00a02) returns a policy \u03c0^normal-^\ud835\udf0b\\widehat{\\pi}over^ start_ARG italic_\u03c0 end_ARG which satisfiesTheorem\u00a05.7 and Corollary\u00a05.8 indicate that DR-PO with NPG update is capable of finding an \u03f5italic-\u03f5\\epsilonitalic_\u03f5-optimal policy with polynomial sample complexity, i.e., \ud835\udcaa~\u2062(1/\u03f52)~\ud835\udcaa1superscriptitalic-\u03f52\\widetilde{\\mathcal{O}}(1/\\epsilon^{2})over~ start_ARG caligraphic_O end_ARG ( 1 / italic_\u03f5 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) labeled trajectory pairs and unlabeled trajectories. Algorithmically, our algorithm does not require pessimism and is model-free, which is much easier and more practical than the pessimistic model-based algorithm proposed in Zhan et\u00a0al., 2023a .In Theorem\u00a05.7 and Corollary\u00a05.8 we assume \u211b\u211b\\mathcal{R}caligraphic_R and \u2131\u2131\\mathcal{F}caligraphic_F are finite, but our results can be extended to infinite classes directly by replacing |\u211b|\u2062(|\u2131|)\u211b\u2131|\\mathcal{R}|(|\\mathcal{F}|)| caligraphic_R | ( | caligraphic_F | ) with their covering numbers.We empirically evaluate DR-PO\u2019s ability to learn from dataset resets. First, we test how well DR-PO is able to both efficiently optimize the reward score as well as minimize the KL-divergence with the reference policy. We also test the generation quality of our resulting policies in terms of Rouge (Lin,, 2004) and win rate (Rafailov et\u00a0al.,, 2023) against human references measured by GPT4 (Achiam et\u00a0al.,, 2023). Next, we conduct an ablation study, incrementally relaxing the the proportion of dataset resets in our online data collection to study how sensitive DR-PO is to this hyperparameter. We investigate DR-PO\u2019s performance when transferring to another summarization task such as CNN/DailyMail (See et\u00a0al.,, 2017). Finally, we conduct a scaling experiment on Anthropic HH by varying model sizes ranging from 1B to 7B. We find that collecting online generations with dataset resets results in a policy with a better tradeoff between reward optimization and KL-divergence, leading to improved generations over baseline RL algorithms, PPO (Schulman et\u00a0al.,, 2017) and Direct Preference Optimizaion (DPO) (Rafailov et\u00a0al.,, 2023).We evaluated DR-PO on the TL;DR summarization dataset used in Stiennon et\u00a0al., (2020)444Dataset can be obtained from https://github.com/openai/summarize-from-feedback and tested scaling performance on the Anthropic Helpful Harmful (HH) task (Bai et\u00a0al., 2022b, ). For TL;DR, a model is trained to generate summaries of online Reddit posts guided by human preference data. The task consists of two datasets: one with human reference summaries and another with preference data. Following the standards set by both Stiennon et\u00a0al., (2020) and Rafailov et\u00a0al., (2023), we train our reward models and DPO baseline on the preference dataset while performing online RL (for PPO and DR-PO) on the human reference dataset. We set the maximum context length to be 512 and the maximum generation length to be 53, ensuring that it is possible to generate all references in the dataset. For Anthropic HH, the model is asked to respond to a dialogue sequence in a helpful, harmless manner. We follow much of design choices from TRLx555https://github.com/CarperAI/trlx for dataset processing, context length, and generation length. For more details about the dataset, please see Appendix\u00a0DTo test the performance of DR-PO against our baselines we evaluate each method by its tradeoff between reward model score and KL-divergence with the reference policy, testing the effectiveness of the algorithm in optimizing the regularized RLHF objective. Furthermore, we compute the Rouge score and GPT4 win rate to evaluate the generation quality of our resulting policies. Note for our win rate calculation, we report the win rate of a randomly sampled subset (10%) of the test set for a total of 600 samples. Please see Section\u00a0D.3 for the prompt used to query GPT4 as well as an example response. When evaluating the on CNN/DailyMail we make use of the constructed preference dataset from Stiennon et\u00a0al., (2020) and for training a supervised finetuned model, we use HuggingFace\u2019s dataset version 2.0.0666https://huggingface.co/datasets/cnn_dailymail.We instantiate DR-PO by using PPO style policy optimization (Schulman et\u00a0al.,, 2017) as the policy optimizer (PO in Algorithm\u00a01). First for TL;DR, we maintain the same pretrained LLM and supervised finetuned model for all of our experiments. For supervised finetuning, we trained a Pythia 2.8B777HuggingFace Model Card: EleutherAI/pythia-2.8b-deduped (Biderman et\u00a0al.,, 2023) parameter model for 1 epoch over the dataset with human references as labels. Similarly for the reward model, we trained a Pythia 2.8B parameter model for 1 epoch over the preference labeled dataset. Then, for DPO, PPO, and DR-PO, we trained our policy and critic with low rank adapters (LoRA) (Hu et\u00a0al.,, 2022) on top of our supervised finetuned (SFT) model and our reward model (RM) respectively. Finally for our scaling experiments for Anthropic HH, we trained Pythia 125M, 1B, and 6.9B parameter models for 1 epoch over the HH dataset for both SFT and RM training. Please see Appendix\u00a0D for details and Section\u00a0D.2 for pseudocode to implement resets.Table\u00a01 compares DR-PO against PPO, DPO, and supervised finetuning. The KL-regularized reward optimization broadly used in RLHF as well as analyzed in Section\u00a05 balances reward exploitation and deviation from a reference policy. When computing the KL-divergence, we use our SFT policy as our reference policy for all our methods. Notably, DR-PO scores a higher RM value over the test set over all baselines with a slightly larger KL discrepancy than PPO. We also see that with GPT4 win rate, DR-PO achieves the highest preference over human references showcasing the benefit of learning from resets. Figure\u00a01 plots a more detailed frontier of the reward and KL tradeoff for DR-PO and PPO. We generate this plot by binning the test scores according to KL. We see that for most KL values, DR-PO is able to achieve a higher score than PPO.Next, we investigate how sensitive DR-PO is to the amount of dataset resets done during online generation. We define \u03b2\ud835\udefd\\betaitalic_\u03b2 as the proportion of generations in a given online batch of generations with dataset resets. More specifically, our main results are with \u03b2=1.0\ud835\udefd1.0\\beta=1.0italic_\u03b2 = 1.0 which translates to all generations during online training of DR-PO starting from a randomly sampled reset from the human references. Note that a \u03b2\ud835\udefd\\betaitalic_\u03b2 value of 0 recovers the baseline PPO (e.g., all generations start from initial prompts). Table\u00a02 shows the expected RM score, KL, and win rate of DR-PO as we increase the mixing proportion from 0% (PPO) to 100% (DR-PO) after 2 epochs of training. Notably, even with a small amount of dataset resets DR-PO is able to learn higher scoring generations with a lower KL than PPO. Moreover, we see that DR-PO with any amount of reference resets leads to higher win rates than PPO. Figure\u00a02 plots the RM score/KL-divergence frontier of our learned policies on the test set. Note that DR-PO is robust to the amount of dataset resets in optimizing the regularized RLHF objective. Finally, supporting our analysis from Section\u00a05, DR-PO generally performs better the more online data we gather from resets with a 100% reset proportion performing the best.Finally, we investigate DR-PO\u2019s ability to do zero-shot transfer to another summarization task, ensuring that learning a policy by reseting from human references does not diminish the generalization observed with PPO in Stiennon et\u00a0al., (2020). Specifically, we investigate whether leveraging human references on TL;DR has the unintended consequence of overfitting to the specific dataset rather than learning more generally to summarize. For our baselines, we test the zero-shot capabilities of both PPO and DPO as well as report the performance of a supervised finetuned policy on CNN/DailyMail using the same base model, Pythia 2.8B. Table\u00a03 demonstrates DR-PO\u2019s zero-shot capabilities, being the only policy to outperform a supervised finetuned model on all metrics. Therefore, we see that learning from resets not only improves RLHF on the training task but also the zero-shot transfer performance to another summarization task.Figure\u00a03 shows DR-PO\u2019s performance across different model scales on Anthropic HH task. Specifically we tested three model sizes: 125M, 1B, and 6.9B. We specifically trained on the Pythia models (Biderman et\u00a0al.,, 2023) using TRLx888https://github.com/CarperAI/trlx. We kept the decoding to be the same across all methods here with a sampling temperature of 0.01 as Rafailov et\u00a0al., (2023) showed that DPO performed best with greedier sampling. We see that both SFT and DPO showed similar scaling performance gains with PPO and DR-PO scaling better from 1B to 6.9B parameters. Figure\u00a03 shows that DR-PO has similar scaling improvements as PPO, but performs strictly better and produces generations that are more preferred than those from all of our baselines.We present DR-PO, a provably efficient algorithm that exploits a generative model\u2019s ability to reset from offline data to enhance RLHF from preference-based feedback. Both in theory and in practice, we demonstrate the effectiveness of incorporating dataset resets into online RL. While in our experiments we specifically demonstrate dataset resets on a PPO style policy optimizer, the idea of dataset reset is both general and simple to implement into any online data collection component of other RL algorithms. We leave it to exciting future work to test the full capabilities of dataset resets in other RLHF methods.Wen Sun acknowledges funding from NSF IIS-2154711, NSF CAREER 2339395, and Cornell Infosys Collaboration. Jonathan Chang is supported by LinkedIn under the LinkedIn-Cornell Grant. Kiante Brantley is supported by NSF under grant No. 2127309 to the Computing Research Association for the CIFellows Project.First we relax the single-policy concentrability in Assumption\u00a05.4 to the following assumptions.Suppose that we have:Suppose that we have for all t\u2208[T]\ud835\udc61delimited-[]\ud835\udc47t\\in[T]italic_t \u2208 [ italic_T ]:Suppose that we have:Suppose that we have for any B\ud835\uddaa\ud835\uddab\u22650subscript\ud835\udc35\ud835\uddaa\ud835\uddab0B_{\\mathsf{KL}}\\geq 0italic_B start_POSTSUBSCRIPT sansserif_KL end_POSTSUBSCRIPT \u2265 0:Note that from Cauchy-Schwartz inequality we have the following proposition:We haveFirst from Cauchy-Schwartz inequality, we haveTherefore we haveThe bound for Cssubscript\ud835\udc36\ud835\udc60C_{s}italic_C start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT follows the same arguments.Similarly, we have:Therefore we haveNote that we haveOn the other hand, we knowTherefore, we haveFor CKLsubscript\ud835\udc36KLC_{\\mathrm{KL}}italic_C start_POSTSUBSCRIPT roman_KL end_POSTSUBSCRIPT, we have\u220eWith Proposition\u00a0B.5, we only need to prove the following theorem to validate Theorem\u00a05.7:Suppose Assumption\u00a05.2,5.3,B.1,B.2,B.4 hold. Then with probability at least 1\u2212\u03b41\ud835\udeff1-\\delta1 - italic_\u03b4, we have Algorithm\u00a01 with NPG update (Algorithm\u00a02) returns a policy \u03c0^normal-^\ud835\udf0b\\widehat{\\pi}over^ start_ARG italic_\u03c0 end_ARG which satisfieswhereIn this section we provide the proof of Theorem\u00a0B.6. Our proof consists of three steps: we first quantify the estimation error of the Q function incurred by LSR oracles \u2013 this step only involves standard supervised learning analysis, then study the performance guarantee of NPG, and lastly investigate how to deal with the reward uncertainty and obtain the final suboptimality gap.We have the following lemma to bound the estimation error |Q^\u2062(s,a)\u2212Q\u03c0t,r^\u2062(s,a)|^\ud835\udc44\ud835\udc60\ud835\udc4esuperscript\ud835\udc44superscript\ud835\udf0b\ud835\udc61^\ud835\udc5f\ud835\udc60\ud835\udc4e\\left|\\widehat{Q}(s,a)-Q^{\\pi^{t},\\widehat{r}}(s,a)\\right|| over^ start_ARG italic_Q end_ARG ( italic_s , italic_a ) - italic_Q start_POSTSUPERSCRIPT italic_\u03c0 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , over^ start_ARG italic_r end_ARG end_POSTSUPERSCRIPT ( italic_s , italic_a ) |:Fix any \u03b41\u2208(0,1]subscript\ud835\udeff101\\delta_{1}\\in(0,1]italic_\u03b4 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2208 ( 0 , 1 ]. With Assumption\u00a05.3, we have with probability at least 1\u2212\u03b411subscript\ud835\udeff11-\\delta_{1}1 - italic_\u03b4 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT that for all t\u2208[T]\ud835\udc61delimited-[]\ud835\udc47t\\in[T]italic_t \u2208 [ italic_T ],where \u03c0\u00af\u2208{\u03c0t,\u03c0\u22c6}normal-\u00af\ud835\udf0bsuperscript\ud835\udf0b\ud835\udc61superscript\ud835\udf0bnormal-\u22c6\\overline{\\pi}\\in\\{\\pi^{t},\\pi^{\\star}\\}over\u00af start_ARG italic_\u03c0 end_ARG \u2208 { italic_\u03c0 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , italic_\u03c0 start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT }.From the guarantee of least squares (Lemma\u00a0C.1 in Appendix\u00a0C), fix t\u2208[T]\ud835\udc61delimited-[]\ud835\udc47t\\in[T]italic_t \u2208 [ italic_T ], we have with probability at least 1\u2212\u03b411subscript\ud835\udeff11-\\delta_{1}1 - italic_\u03b4 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT that,Take union bound over t\u2208[T]\ud835\udc61delimited-[]\ud835\udc47t\\in[T]italic_t \u2208 [ italic_T ] and we have for all t\u2208[T]\ud835\udc61delimited-[]\ud835\udc47t\\in[T]italic_t \u2208 [ italic_T ] that\u220eIn the following discussion we use f\u2062(s)\ud835\udc53\ud835\udc60f(s)italic_f ( italic_s ) to denote the vector f\u2062(s,\u22c5)\ud835\udc53\ud835\udc60\u22c5f(s,\\cdot)italic_f ( italic_s , \u22c5 ) for all functions f\ud835\udc53fitalic_f. We have the following lemma which indicates that NPG is able to find a near optimal policy with respect to the estimated reward r^^\ud835\udc5f\\widehat{r}over^ start_ARG italic_r end_ARG (recall that \u03f5evalsubscriptitalic-\u03f5eval\\epsilon_{\\mathrm{eval}}italic_\u03f5 start_POSTSUBSCRIPT roman_eval end_POSTSUBSCRIPT is defined in Lemma\u00a0B.7):Denote the event in Lemma\u00a0B.7 by \u21301subscript\u21301\\mathcal{E}_{1}caligraphic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. Then conditioned on \u21301subscript\u21301\\mathcal{E}_{1}caligraphic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, with Assumption\u00a05.3 and B.4, we haveIn the following proof we use g\u2062(\u03c0\u2062(s))\ud835\udc54\ud835\udf0b\ud835\udc60g(\\pi(s))italic_g ( italic_\u03c0 ( italic_s ) ) to denote \ud835\uddaa\ud835\uddab\u2062(\u03c0\u2062(s)\u2225\u03c0SFT\u2062(s))\ud835\uddaa\ud835\uddabconditional\ud835\udf0b\ud835\udc60superscript\ud835\udf0bSFT\ud835\udc60\\mathsf{KL}(\\pi(s)\\|\\pi^{\\mathrm{SFT}}(s))sansserif_KL ( italic_\u03c0 ( italic_s ) \u2225 italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT ( italic_s ) ) for any policy \u03c0\ud835\udf0b\\piitalic_\u03c0. First note that from the update rule in line 8 of Algorithm\u00a01, due to first order optimality, we know for all distribution p\u2208\u0394\u2062(\ud835\udc9c)\ud835\udc5d\u0394\ud835\udc9cp\\in\\Delta(\\mathcal{A})italic_p \u2208 roman_\u0394 ( caligraphic_A ) and all t\u2208[T],s\u2208\ud835\udcaeformulae-sequence\ud835\udc61delimited-[]\ud835\udc47\ud835\udc60\ud835\udcaet\\in[T],s\\in\\mathcal{S}italic_t \u2208 [ italic_T ] , italic_s \u2208 caligraphic_S that:This implies that for all t\u2208[T],s\u2208\ud835\udcaeformulae-sequence\ud835\udc61delimited-[]\ud835\udc47\ud835\udc60\ud835\udcaet\\in[T],s\\in\\mathcal{S}italic_t \u2208 [ italic_T ] , italic_s \u2208 caligraphic_S, we havewhere the last step is due to Equation\u00a0(3). Now we bound the term (1)(2)(3) respectively.Note that the KL divergence is indeed the Bregman divergence induced by g\ud835\udc54gitalic_g, therefore the following three point lemma holds true:For any distributions p1\u2062(s),p2\u2062(s),p3\u2062(s)\u2208\u0394\u2062(\ud835\udc9c)subscript\ud835\udc5d1\ud835\udc60subscript\ud835\udc5d2\ud835\udc60subscript\ud835\udc5d3\ud835\udc60normal-\u0394\ud835\udc9cp_{1}(s),p_{2}(s),p_{3}(s)\\in\\Delta(\\mathcal{A})italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_s ) , italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_s ) , italic_p start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ( italic_s ) \u2208 roman_\u0394 ( caligraphic_A ) ,we haveFrom definition of g\ud835\udc54gitalic_g, we know \u2207g\u2062(p\u2062(s))=log\u2061p\u2062(s)\u2212log\u2061\u03c0SFT\u2062(s)+\ud835\udfcf\u2207\ud835\udc54\ud835\udc5d\ud835\udc60\ud835\udc5d\ud835\udc60superscript\ud835\udf0bSFT\ud835\udc601\\nabla g(p(s))=\\log p(s)-\\log\\pi^{\\mathrm{SFT}}(s)+\\bm{1}\u2207 italic_g ( italic_p ( italic_s ) ) = roman_log italic_p ( italic_s ) - roman_log italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT ( italic_s ) + bold_1. This implies thatSubstitute the definition of KL divergence and we can prove the lemma.\n\u220eFrom Lemma\u00a0B.9, we can rewrite (1) as follows:From Cauchy-Schwartz inequality, we haveSince g\ud835\udc54gitalic_g is convex, we knowThis implies thatIn summary, we have for all t\u2208[T],s\u2208\ud835\udcaeformulae-sequence\ud835\udc61delimited-[]\ud835\udc47\ud835\udc60\ud835\udcaet\\in[T],s\\in\\mathcal{S}italic_t \u2208 [ italic_T ] , italic_s \u2208 caligraphic_S thatwhere the last step is due to Pinsker\u2019s inequality.This implies thatNote that here we use the fact that we initialize the policy as \u03c01=\u03c0SFTsuperscript\ud835\udf0b1superscript\ud835\udf0bSFT\\pi^{1}=\\pi^{\\mathrm{SFT}}italic_\u03c0 start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT = italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT and thus g\u2062(\u03c01\u2062(s))=0\ud835\udc54superscript\ud835\udf0b1\ud835\udc600g(\\pi^{1}(s))=0italic_g ( italic_\u03c0 start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( italic_s ) ) = 0.\nOn the other hand, note that we have the following performance difference lemma, whose proof is deferred to Appendix\u00a0C.3:For any policy \u03c0,\u03c0\u2032\ud835\udf0bsuperscript\ud835\udf0bnormal-\u2032\\pi,\\pi^{\\prime}italic_\u03c0 , italic_\u03c0 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT and reward function r\ud835\udc5fritalic_r, we have:Now substitute Lemma\u00a0B.10 into Equation\u00a0(4), and from Lemma\u00a0B.7 we haveThis is equivalent towhich concludes our proof.\n\u220eWe also would like to bound the KL divergence between \u03c0^^\ud835\udf0b\\widehat{\\pi}over^ start_ARG italic_\u03c0 end_ARG and \u03c0SFTsuperscript\ud835\udf0bSFT\\pi^{\\mathrm{SFT}}italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT as shown in the following lemma:We have for all t\u2208[T],s\u2208\ud835\udcaeformulae-sequence\ud835\udc61delimited-[]\ud835\udc47\ud835\udc60\ud835\udcaet\\in[T],s\\in\\mathcal{S}italic_t \u2208 [ italic_T ] , italic_s \u2208 caligraphic_S thatFrom the NPG update and use the fact that \u03c0t+1superscript\ud835\udf0b\ud835\udc611\\pi^{t+1}italic_\u03c0 start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT is the minimizer, we know for all t\u2208[T],s\u2208\ud835\udcaeformulae-sequence\ud835\udc61delimited-[]\ud835\udc47\ud835\udc60\ud835\udcaet\\in[T],s\\in\\mathcal{S}italic_t \u2208 [ italic_T ] , italic_s \u2208 caligraphic_S:where we utilize Assumption\u00a05.3 in the second step.\nNote that \ud835\uddaa\ud835\uddab\u2062(\u03c01\u2062(s)\u2225\u03c0SFT\u2062(s))=0\ud835\uddaa\ud835\uddabconditionalsuperscript\ud835\udf0b1\ud835\udc60superscript\ud835\udf0bSFT\ud835\udc600\\mathsf{KL}(\\pi^{1}(s)\\|\\pi^{\\mathrm{SFT}}(s))=0sansserif_KL ( italic_\u03c0 start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( italic_s ) \u2225 italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT ( italic_s ) ) = 0 since \u03c01=\u03c0SFTsuperscript\ud835\udf0b1superscript\ud835\udf0bSFT\\pi^{1}=\\pi^{\\mathrm{SFT}}italic_\u03c0 start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT = italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT. This implies that for all t\u2208[T]\ud835\udc61delimited-[]\ud835\udc47t\\in[T]italic_t \u2208 [ italic_T ]:\u220eNow we can start to prove Theorem\u00a05.7. First we haveNext we bound term (1)(2)(3) respectviely.From the guarantee of MLE (Lemma\u00a0C.2 in Appendix\u00a0C) we have with probability at least 1\u2212\u03b421subscript\ud835\udeff21-\\delta_{2}1 - italic_\u03b4 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT thatwhere c1>0subscript\ud835\udc5010c_{1}>0italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > 0 is a universal constant. Denote the event of the above inequality by \u21302subscript\u21302\\mathcal{E}_{2}caligraphic_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. Then conditioned on \u21302subscript\u21302\\mathcal{E}_{2}caligraphic_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, we have\nFrom Lemma\u00a0B.8, conditioned on \u21301subscript\u21301\\mathcal{E}_{1}caligraphic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, we haveNote that from Lemma\u00a0B.11, we have \u03c0t\u2208\u0398\u2062(\u03c0SFT,(t\u22121)\u2062rmax/\u03bb)superscript\ud835\udf0b\ud835\udc61\u0398superscript\ud835\udf0bSFT\ud835\udc611subscript\ud835\udc5fmax\ud835\udf06\\pi^{t}\\in\\Theta(\\pi^{\\mathrm{SFT}},(t-1)r_{\\mathrm{max}}/\\lambda)italic_\u03c0 start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT \u2208 roman_\u0398 ( italic_\u03c0 start_POSTSUPERSCRIPT roman_SFT end_POSTSUPERSCRIPT , ( italic_t - 1 ) italic_r start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT / italic_\u03bb ) for all t\u2208[T]\ud835\udc61delimited-[]\ud835\udc47t\\in[T]italic_t \u2208 [ italic_T ]. Therefore, following the same arguments as we have to bound term (1), we have for all t\u2208[T]\ud835\udc61delimited-[]\ud835\udc47t\\in[T]italic_t \u2208 [ italic_T ],which implies thatOverall, we have conditioned on event \u21301\u2229\u21302subscript\u21301subscript\u21302\\mathcal{E}_{1}\\cap\\mathcal{E}_{2}caligraphic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2229 caligraphic_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT,\nWe finish the proof by letting \u03b41=\u03b42=\u03b4/2subscript\ud835\udeff1subscript\ud835\udeff2\ud835\udeff2\\delta_{1}=\\delta_{2}=\\delta/2italic_\u03b4 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = italic_\u03b4 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = italic_\u03b4 / 2.Fix any R>0\ud835\udc450R>0italic_R > 0, \u03b4\u2208(0,1)\ud835\udeff01\\delta\\in(0,1)italic_\u03b4 \u2208 ( 0 , 1 ) and assume we have a class of real valued functions \u210b:\ud835\udcb3\u21a6[\u2212R,R]normal-:\u210bmaps-to\ud835\udcb3\ud835\udc45\ud835\udc45\\mathcal{H}:\\mathcal{X}\\mapsto[-R,R]caligraphic_H : caligraphic_X \u21a6 [ - italic_R , italic_R ]. Suppose we have K\ud835\udc3eKitalic_K i.i.d. samples {(xk,yk)}k=1Ksuperscriptsubscriptsubscript\ud835\udc65\ud835\udc58subscript\ud835\udc66\ud835\udc58\ud835\udc581\ud835\udc3e\\{(x_{k},y_{k})\\}_{k=1}^{K}{ ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT where xk\u223c\u03c1similar-tosubscript\ud835\udc65\ud835\udc58\ud835\udf0cx_{k}\\sim\\rhoitalic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u223c italic_\u03c1 and yksubscript\ud835\udc66\ud835\udc58y_{k}italic_y start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT is sampled via the conditional probability p(\u22c5\u2223xk)p(\\cdot\\mid x_{k})italic_p ( \u22c5 \u2223 italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ):where h*\u2208\u210bsuperscript\u210e\u210bh^{*}\\in\\mathcal{H}italic_h start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT \u2208 caligraphic_H and {\u03f5k}k=1Ksuperscriptsubscriptsubscriptitalic-\u03f5\ud835\udc58\ud835\udc581\ud835\udc3e\\{\\epsilon_{k}\\}_{k=1}^{K}{ italic_\u03f5 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT are independent random variables such that \ud835\udd3c\u2062[yk\u2223xk]=h*\u2062(xk)\ud835\udd3cdelimited-[]conditionalsubscript\ud835\udc66\ud835\udc58subscript\ud835\udc65\ud835\udc58superscript\u210esubscript\ud835\udc65\ud835\udc58\\mathbb{E}[y_{k}\\mid x_{k}]=h^{*}(x_{k})blackboard_E [ italic_y start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u2223 italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ] = italic_h start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ). Additionally, suppose that maxk\u2061|yk|\u2264Rsubscript\ud835\udc58subscript\ud835\udc66\ud835\udc58\ud835\udc45\\max_{k}|y_{k}|\\leq Rroman_max start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_y start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | \u2264 italic_R and maxx\u2061|h*\u2062(x)|\u2264Rsubscript\ud835\udc65superscript\u210e\ud835\udc65\ud835\udc45\\max_{x}|{h^{*}(x)}|\\leq Rroman_max start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT | italic_h start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_x ) | \u2264 italic_R. Then the least square solution h^\u2190arg\u2061minh\u2208\u210b\u2062\u2211k=1K(h\u2062(xk)\u2212yk)2normal-\u2190normal-^\u210esubscript\u210e\u210bsuperscriptsubscript\ud835\udc581\ud835\udc3esuperscript\u210esubscript\ud835\udc65\ud835\udc58subscript\ud835\udc66\ud835\udc582\\widehat{h}\\leftarrow\\arg\\min_{h\\in\\mathcal{H}}\\sum_{k=1}^{K}\\left(h(x_{k})-y_%\n{k}\\right)^{2}over^ start_ARG italic_h end_ARG \u2190 roman_arg roman_min start_POSTSUBSCRIPT italic_h \u2208 caligraphic_H end_POSTSUBSCRIPT \u2211 start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT ( italic_h ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) - italic_y start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT satisfies with probability at least 1\u2212\u03b41\ud835\udeff1-\\delta1 - italic_\u03b4,The proof is the same as in Song et\u00a0al., (2022) and thus is omitted here.With Assumption\u00a05.2, we have with probability at least 1\u2212\u03b41\ud835\udeff1-\\delta1 - italic_\u03b4 thatwhere c1>0subscript\ud835\udc5010c_{1}>0italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > 0 is a universal constant.The proof largely follows the proof of Theorem 1 in Zhan et\u00a0al., 2023a . Specifically, we have the following lemma from Zhan et\u00a0al., 2023a :Fix any \u03b4\u2208(0,1]\ud835\udeff01\\delta\\in(0,1]italic_\u03b4 \u2208 ( 0 , 1 ]. Then with probability at least 1\u2212\u03b41\ud835\udeff1-\\delta1 - italic_\u03b4, we have that for all reward function r\u2208\u211b\ud835\udc5f\u211br\\in\\mathcal{R}italic_r \u2208 caligraphic_R,Then from Lemma\u00a0C.3, since \u2211m=1Mlog\u2061Pr\u22c6\u2062(om|\u03c4m,0,\u03c4m,1)\u2264\u2211m=1Mlog\u2061Pr^\u2062(om|\u03c4m,0,\u03c4m,1)superscriptsubscript\ud835\udc5a1\ud835\udc40subscript\ud835\udc43superscript\ud835\udc5f\u22c6conditionalsuperscript\ud835\udc5c\ud835\udc5asuperscript\ud835\udf0f\ud835\udc5a0superscript\ud835\udf0f\ud835\udc5a1superscriptsubscript\ud835\udc5a1\ud835\udc40subscript\ud835\udc43^\ud835\udc5fconditionalsuperscript\ud835\udc5c\ud835\udc5asuperscript\ud835\udf0f\ud835\udc5a0superscript\ud835\udf0f\ud835\udc5a1\\sum_{m=1}^{M}\\log P_{r^{\\star}}(o^{m}|\\tau^{m,0},\\tau^{m,1})\\leq\\sum_{m=1}^{M%\n}\\log P_{\\widehat{r}}(o^{m}|\\tau^{m,0},\\tau^{m,1})\u2211 start_POSTSUBSCRIPT italic_m = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT roman_log italic_P start_POSTSUBSCRIPT italic_r start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_o start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT | italic_\u03c4 start_POSTSUPERSCRIPT italic_m , 0 end_POSTSUPERSCRIPT , italic_\u03c4 start_POSTSUPERSCRIPT italic_m , 1 end_POSTSUPERSCRIPT ) \u2264 \u2211 start_POSTSUBSCRIPT italic_m = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT roman_log italic_P start_POSTSUBSCRIPT over^ start_ARG italic_r end_ARG end_POSTSUBSCRIPT ( italic_o start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT | italic_\u03c4 start_POSTSUPERSCRIPT italic_m , 0 end_POSTSUPERSCRIPT , italic_\u03c4 start_POSTSUPERSCRIPT italic_m , 1 end_POSTSUPERSCRIPT ), we have with probability at least 1\u2212\u03b41\ud835\udeff1-\\delta1 - italic_\u03b4:Then under Assumption\u00a05.2, we can apply the mean value theorem between r\u22c6\u2062(\u03c41)\u2212r\u22c6\u2062(\u03c40)superscript\ud835\udc5f\u22c6subscript\ud835\udf0f1superscript\ud835\udc5f\u22c6subscript\ud835\udf0f0r^{\\star}(\\tau_{1})-r^{\\star}(\\tau_{0})italic_r start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT ( italic_\u03c4 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_r start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT ( italic_\u03c4 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) and r^\u2062(\u03c41)\u2212r^\u2062(\u03c40)^\ud835\udc5fsubscript\ud835\udf0f1^\ud835\udc5fsubscript\ud835\udf0f0\\widehat{r}(\\tau_{1})-\\widehat{r}(\\tau_{0})over^ start_ARG italic_r end_ARG ( italic_\u03c4 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - over^ start_ARG italic_r end_ARG ( italic_\u03c4 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) to (6) and ensure thatwhere \u03ba:=1infx\u2208[\u2212rmax,rmax]\u03a6\u2032\u2062(x)assign\ud835\udf051subscriptinfimum\ud835\udc65subscript\ud835\udc5fmaxsubscript\ud835\udc5fmaxsuperscript\u03a6\u2032\ud835\udc65\\kappa:=\\frac{1}{\\inf_{x\\in[-r_{\\mathrm{max}},r_{\\mathrm{max}}]}\\Phi^{\\prime}(%\nx)}italic_\u03ba := divide start_ARG 1 end_ARG start_ARG roman_inf start_POSTSUBSCRIPT italic_x \u2208 [ - italic_r start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT ] end_POSTSUBSCRIPT roman_\u03a6 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_x ) end_ARG measures the non-linearity of the link function \u03a6\u03a6\\Phiroman_\u03a6.\n\u220eWe restate and prove Lemma\u00a0B.10 as follows.For any policy \u03c0,\u03c0\u2032\ud835\udf0bsuperscript\ud835\udf0bnormal-\u2032\\pi,\\pi^{\\prime}italic_\u03c0 , italic_\u03c0 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT and reward function r\ud835\udc5fritalic_r, we have:For any two policies \u03c0,\u03c0\u2032\ud835\udf0bsuperscript\ud835\udf0b\u2032\\pi,\\pi^{\\prime}italic_\u03c0 , italic_\u03c0 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT and reward r\ud835\udc5fritalic_r, we have thatThis concludes our proof.\n\u220eWe present dataset specific details in table 4For both datasets we obtained the training data from https://github.com/openai/summarize-from-feedback.\nHere is a code snippet of the logit processor that handles dataset resets from references for a HuggingFace transformers model. \u03b2\ud835\udefd\\betaitalic_\u03b2 here represents the proportion of generations in the batch to do resets for.Note since we start with the references from the dataset, the computational requirements to generate with resets are the same as generating from the initial state distribution. For all of our experiments, we ran with the same per device batch size between PPO and DR-PO. For this work, we made use of 16 A6000 gpus with 48GB of VRAM. We used 4 gpus for each run.For winrate calculation, we used the following prompt:Here is an example of getting a one sentence explanation as to why GPT4 chose certain generations for the winrate.PromptReference (Summary B)GPT4 Explanation for Choosing DR-POPrompt 1DR-POPrompt 2DR-POWe write the relevant hyperparameters from our experiments for DPO, PPO, SFT, and DRPO in table 5.Shown in Table\u00a06, we investigate DPO\u2019s implicit learned reward accuracy to our RM\u2019s accuracy on both TL;DR and CNN/DailyMail\u2019s test sets. Furthermore, we also report the effects of LoRA on the RM and DPO performance. We see that DPO without LoRA has comparable preference accuracy on CNN/DM as our RM. Thus, we used the DPO policy without LoRA when comparing against PPO and DR-PO in Table\u00a03.",
    "22": "Large-scale multilingual Pretrained Language Models (mPLMs) yield impressive performance on cross-language tasks, yet significant performance disparities exist across different languages within the same mPLM.\nPrevious studies endeavored to narrow these disparities by supervise fine-tuning the mPLMs with multilingual data.\nHowever, obtaining labeled multilingual data is time-consuming, and fine-tuning mPLM with limited labeled multilingual data merely encapsulates the knowledge specific to the labeled data.\nTherefore, we introduce ALSACE to leverage the learned knowledge from the well-performing languages to guide under-performing ones within the same mPLM, eliminating the need for additional labeled multilingual data.\nExperiments show that ALSACE effectively mitigates language-level performance disparity across various mPLMs while showing the competitive performance on different multilingual NLU tasks, ranging from full resource to limited resource settings.\nThe code for our approach is available at https://github.com/pkunlp-icler/ALSACE.\nMitigating Language-Level Performance Disparity in mPLMs via Teacher Language Selection and Cross-lingual Self-Distillation\n\n\n\n\nHaozhe Zhao\u2217normal-\u2217{}^{\\ast}start_FLOATSUPERSCRIPT \u2217 end_FLOATSUPERSCRIPT1,212{}^{1,2}start_FLOATSUPERSCRIPT 1 , 2 end_FLOATSUPERSCRIPT,\nZefan Cai\u2217normal-\u2217{}^{\\ast}start_FLOATSUPERSCRIPT \u2217 end_FLOATSUPERSCRIPT1,212{}^{1,2}start_FLOATSUPERSCRIPT 1 , 2 end_FLOATSUPERSCRIPT,\nShuzheng Si\u2217normal-\u2217{}^{\\ast}start_FLOATSUPERSCRIPT \u2217 end_FLOATSUPERSCRIPT1,212{}^{1,2}start_FLOATSUPERSCRIPT 1 , 2 end_FLOATSUPERSCRIPT,\nLiang Chen11{}^{1}start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT,\n\nYufeng He1,212{}^{1,2}start_FLOATSUPERSCRIPT 1 , 2 end_FLOATSUPERSCRIPT,\nKaikai An1,212{}^{1,2}start_FLOATSUPERSCRIPT 1 , 2 end_FLOATSUPERSCRIPT,\nBaobao Chang\u2020normal-\u2020{}^{\\dagger}start_FLOATSUPERSCRIPT \u2020 end_FLOATSUPERSCRIPT1,313{}^{1,3}start_FLOATSUPERSCRIPT 1 , 3 end_FLOATSUPERSCRIPT\n\n11{}^{1}start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPTNational Key Laboratory for Multimedia Information Processing, Peking University\n\n22{}^{2}start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPTSchool of Software and Microelectronics, Peking University, China\n\n33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPTCollaborative Innovation Center for Language Ability, Xuzhou, 221009, China\n\n{mimazhe55360,zefncai}@gmail.com,\nsishuzheng@stu.pku.edu.cn\n\nchbb@pku.edu.cn\n\n Recently, Multilingual Pre-trained Language Models (mPLMs) have attracted significant attention \u00a0(Doddapaneni et\u00a0al., 2021).\nThese mPLMs, such as mBERT\u00a0(Devlin et\u00a0al., 2018) and mT5\u00a0Xue et\u00a0al. (2020), are pre-trained on extensive amounts of corpus across hundreds of different languages, which enables them to handle multiple languages within a single model and effectively perform cross-lingual tasks\u00a0(Lewis et\u00a0al., 2019; Zhang et\u00a0al., 2020; Stickland et\u00a0al., 2020; Mutuvi et\u00a0al., 2020; Brown et\u00a0al., 2020; Choudhury and Deshpande, 2021).However, all mPLMs share a key limitation. Due to discrepancies in the quality and quantity of pre-training corpus available for different languages, there is a noticeable performance disparity among different languages for the same mPLM, especially when comparing the performance of high-resource languages to that of low-resource languages.\nFor example, in Cross-lingual Natural Language Inference (XNLI) task\u00a0(Conneau et\u00a0al., 2018),\nhigh-resource languages such as English can achieve a performance advantage of approximately 15 points compared to low-resource languages like Swahili, even within the same mPLM.Several works have been proposed to investigate the reason for the performance disparity. Kassner and Sch\u00fctze (2019); Wallat et\u00a0al. (2021); Kassner et\u00a0al. (2021) demonstrate that mPLMs could learn language-specific knowledge from different languages\u2019 pre-training corpus, but the imbalance of the corpus for different languages leads to the knowledge disparity for different languages.\nTherefore, \u00a0Kassner et\u00a0al. (2021) suggests the observed language-level performance disparity can be attributed to the disparity of learned different languages knowledge during the pre-training stage.\nTherefore, Dong et\u00a0al. (2021); Hu et\u00a0al. (2021) attempts to narrow the knowledge disparity by involving additional supervised data in different languages to fine-tune the mPLM.\nHowever, obtaining such labeled multilingual data is time-consuming and expensive.\nMoreover, these labeled data mostly come from limited tasks and domains, which makes it hard to compensate for the large knowledge disparity during the pre-training stage, restricting the generalization performance of the low-resource languages on downstream tasks.To utilize the different knowledge across different languages within the same mPLM and mitigate the need for the labeled data, we introduce teAcher Language Selection And Cross-lingual sElf-distillation (ALSACE), which leverages the knowledge from the selected teacher languages to reduce the performance disparity among the languages.\nSpecifically, ALSACE mainly consists of two stages: Teacher Language Selection and Cross-Lingual Self-Distillation.\nFor teacher language selection, the motivation is that high-resource languages may not be ideal for probing knowledge to supervise the other languages.\nFor instance, although Persian is a relatively low-resource language, it may provide more precise answers for Kenya\u2019s cultural queries than English due to the closer linguistic proximity \u00a0(Yin et\u00a0al., 2022) between Persian and Swahili.\nDifferent from simply using the knowledge from high-resource languages (e.g., English) to improve the performance of low-resource languages (e.g., Swahili), we introduce Teacher Language Selection to choose reliable teacher languages for a specific task to supervise the student languages.\nSpecifically, we employ a majority voting strategy to generate pseudo-labels derived from the consensus of the mPLMs\u2019 predictions across different languages in the given multilingual corpus.\nThen, we utilize the average confidence score of the different languages on the generated pseudo labels as the indicator to select the teacher languages automatically.\nAs a result, we can select adaptive teachers for different tasks using the unlabeled sentences in the corpus.\nWe further propose Cross-Lingual Self-Distillation to leverage the knowledge from each selected teacher language to supervise other languages, reducing the performance disparity.\nWe further propose cross-lingual self-distillation to leverage the knowledge from each selected teachers languages to supervise other languages, reducing the performance disparity.\nIt employs a consistency loss that encourages closer alignment between the model output distributions of each reliable teacher language and other languages.\nIn this way, mPLMs can effectively mitigate the language-level performance disparity without relying on the supervised multilingual data.\nExperiments show ALSACE consistently mitigates language-level performance disparity in various mPLMs and show the competitive performance on different multilingual benchmarks, including XNLI\u00a0Conneau et\u00a0al. (2018), PAWS-X\u00a0Yang et\u00a0al. (2019) and XCOPA\u00a0(Ponti et\u00a0al., 2020).\nWe also conduct knowledge probing experiments on the GeoMLAMA\u00a0Yin et\u00a0al. (2022) as shown in Figure\u00a01, demonstrating that ALSACE effectively mitigates language-level performance disparity by addressing knowledge disparity.\nMoreover, our experiments show that ALSACE improves performance not only in low-resource languages but also in high-resource languages.\nThis finding illustrates that ALSACE enables effective knowledge transfer between different languages instead of only transferring knowledge from high-resource to low-resource languages.\nFurther analysis shows that ALSACE can transfer both general knowledge across different languages and language-specific knowledge, i.e., some specific knowledge locally shared by people speaking the specific language, which is only present in the corpus of some specific languages.\nKnowledge Disparity Leads to Language-Level Performance Disparity in mPLMs.\nThe mPLMs have shown strong capabilities in many NLP tasks including Natural Language Generation (NLG) (Si et\u00a0al., 2022a, 2024; Zhao et\u00a0al., 2023; Cai et\u00a0al., 2023; Song et\u00a0al., 2023; Li et\u00a0al., 2024; Liu et\u00a0al., 2023b) and natural language understanding (NLU) (Si et\u00a0al., 2022b, 2023; Liu et\u00a0al., 2023a; An et\u00a0al., 2023; Hu et\u00a0al., 2023).\nHowever, there is a noticeable performance disparity across different languages in the same mPLM.\nSeveral works are proposed to investigate the reason of language-level performance disparity in mPLMs.\nWallat et\u00a0al. (2021); Kassner et\u00a0al. (2021) demonstrate that mPLMs could learn different knowledge from different languages data in the pre-training corpus, but imbalanced corpus might lead to knowledge disparity for different languages.\n\u00a0Kassner et\u00a0al. (2021) suggests that the performance disparities across different languages could be attributed to the imbalanced knowledge distribution of these languages acquired during the pre-training phase.\nYin et\u00a0al. (2022) further observe that different languages within a single mPLM can retain distinct knowledge that is locally shared by the people speaking the specific language.\nTherefore, we attempt to address language-level performance disparity from the knowledge disparity perspective.\nPrevious studies have utilized cross-lingual knowledge to mitigate the language-level performance disparity.\nHe et\u00a0al. (2021) employ lightweight adapters on the mPLMs to mitigate forgetting issues.\nInfoXLM\u00a0(Chi et\u00a0al., 2021a) designs a new pre-training task with 42GB parallel data to align the representation of multiple languages.\nXLE\u00a0(Chi et\u00a0al., 2022) pre-trains mPLMs with a generator and discriminator structure on 142B tokens.\nThese methods attempt to incorporate multilingual resources to mitigate performance disparity.\nHowever, obtaining multilingual data can be time-consuming and restricts model performance on low-resource languages.\nThus,\n\u00a0Yang et\u00a0al. (2022); Nguyen and Tuan (2021) attempt to enhance mPLMs by distilling knowledge from well-learned monolingual teachers.\n\u00a0Qi et\u00a0al. (2022) learn from different cross-lingual templates using consistency loss to enforce correspondence representation among languages.\nDifferent from distilling knowledge from other monolingual models, we aim to reduce the language-level performance disparity within mPLMs.\nTo mitigate the language-level performance disparity within mPLMs, we utilize knowledge from the appropriate teacher language to supervise other languages.\nAn intuitive idea is to transfer the knowledge from high-resourced to low-resourced languages to mitigate the disparity.\nHowever, due to the different linguistic proximity between different languages, the high-resource languages may not be ideal teachers for transferring knowledge to other languages in the specific task.\nFor example, low-resourced Persian may provide more accurate responses to Kenya\u2019s cultural queries compared to high-resource English, which makes it a better teacher language for Swahili than English.\nTherefore, the proposed Teacher Language Selection aims to choose reliable teacher languages for a specific task to guide the student languages.Considering the given corpus D\ud835\udc37Ditalic_D for the specific multilingual task (e.g., Cross-lingual Natural Language Inference) that spans over T\ud835\udc47Titalic_T languages, we aim to utilize the proposed Teacher Language Selection to identify the teacher languages to mitigate language-level performance disparity efficiently.\nPrecisely, we first fine-tune the mPLMs with an English training set De\u2062nsubscript\ud835\udc37\ud835\udc52\ud835\udc5bD_{en}italic_D start_POSTSUBSCRIPT italic_e italic_n end_POSTSUBSCRIPT of the given task to obtain a better initialization.\nWe secondly utilize the mPLMs to generate the prediction y^t,isubscript^\ud835\udc66\ud835\udc61\ud835\udc56\\hat{y}_{t,i}over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_t , italic_i end_POSTSUBSCRIPT of the given instance xisubscript\ud835\udc65\ud835\udc56x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT from corpus D\ud835\udc37Ditalic_D in language t\u2208T\ud835\udc61\ud835\udc47t\\in Titalic_t \u2208 italic_T.\nThen, we employ a majority vote strategy on the predictions of different languages to generate the pseudo label yisubscript\ud835\udc66\ud835\udc56y_{i}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT of the instance xi\u2208Xsubscript\ud835\udc65\ud835\udc56\ud835\udc4bx_{i}\\in Xitalic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 italic_X, as follows:where P\u2062(y|xt,i)\ud835\udc43conditional\ud835\udc66subscript\ud835\udc65\ud835\udc61\ud835\udc56P(y\\,|\\,x_{t,i})italic_P ( italic_y | italic_x start_POSTSUBSCRIPT italic_t , italic_i end_POSTSUBSCRIPT ) denotes the predicted probability of the given mPLM on instance xt,isubscript\ud835\udc65\ud835\udc61\ud835\udc56x_{t,i}italic_x start_POSTSUBSCRIPT italic_t , italic_i end_POSTSUBSCRIPT in language t\ud835\udc61titalic_t. \ud835\udd40\ud835\udd40\\mathbb{I}blackboard_I is the indicator function, while k\ud835\udc58kitalic_k signifies the set of all possible results for the given task.\nThe generated pseudo-labels reflect the collective understanding of the provided instance across various languages.\nThus, it reduces the risk of incorrect pseudo-labeling compared to relying solely on the prediction from a single language (even a high-resource language like English).We further employ the pseudo-labels to compute the average confidence score stsubscript\ud835\udc60\ud835\udc61s_{t}italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT for each language, which allows us to assess the capabilities of different languages in the mPLM.\nThe average confidence score stsubscript\ud835\udc60\ud835\udc61s_{t}italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT indicates the level of agreement between each language and the common understanding of the mPLMs, i.e., languages with a higher average confidence score are more likely to make accurate predictions for a given instance.\nUltimately, we normalize the confidence score and use the normalized score st^^subscript\ud835\udc60\ud835\udc61\\hat{s_{t}}over^ start_ARG italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG to evaluate which languages demonstrate superior performance:\nwhere the T\ud835\udc47Titalic_T refers to the collection of all languages involved in the given multilingual task.\nWe set the threshold \u03b8\ud835\udf03\\thetaitalic_\u03b8 to be the average value of the normalized score st^^subscript\ud835\udc60\ud835\udc61\\hat{s_{t}}over^ start_ARG italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG to select the teacher languages T\ud835\udc61\ud835\udc52\ud835\udc4e\ud835\udc50\u210e\ud835\udc52\ud835\udc5fsubscript\ud835\udc47\ud835\udc61\ud835\udc52\ud835\udc4e\ud835\udc50\u210e\ud835\udc52\ud835\udc5fT_{\\textit{teacher}}italic_T start_POSTSUBSCRIPT teacher end_POSTSUBSCRIPT and student languages S\ud835\udc60\ud835\udc61\ud835\udc62\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61subscript\ud835\udc46\ud835\udc60\ud835\udc61\ud835\udc62\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61S_{\\textit{student}}italic_S start_POSTSUBSCRIPT student end_POSTSUBSCRIPT, as follows:In this way, we can automatically select appropriate teacher languages for the different multilingual tasks to mitigate language-level performance disparity efficiently.\nMoreover, we do not need any labeled multilingual data to improve the cross-lingual transfer ability of mPLMs (Chi et\u00a0al., 2022, 2021a).Having selected the appropriate teacher languages for the given multilingual task, we further introduce Cross-Lingual Self-Distillation to leverage the knowledge from each selected teacher language to supervise other languages.\nSpecifically, we construct a parallel multilingual pair set X^^\ud835\udc4b\\hat{X}over^ start_ARG italic_X end_ARG that consists of parallel sentence pairs between each two languages.\nTo reduce the disturbance caused by student languages, we exclusively employ parallel pairs of teacher-student and teacher-teacher languages as potential candidates for self-distillation.\nTherefore, the instance pair X^^\ud835\udc4b\\hat{X}over^ start_ARG italic_X end_ARG can be defined as:where T\ud835\udc61\ud835\udc52\ud835\udc4e\ud835\udc50\u210e\ud835\udc52\ud835\udc5fsubscript\ud835\udc47\ud835\udc61\ud835\udc52\ud835\udc4e\ud835\udc50\u210e\ud835\udc52\ud835\udc5fT_{\\textit{teacher}}italic_T start_POSTSUBSCRIPT teacher end_POSTSUBSCRIPT is the selected teacher languages.\nWe filter out student-student language pairs to prevent student languages from learning from each other.For the selected candidate instance pairs, we use Kullback-Leibler divergence as a consistency loss to encourage closer alignment between the prediction distributions of the reliable teacher language and the target language.\nIn this way, mPLMs can effectively transfer and distill the knowledge from the teacher language to the target language, mitigating the language-level performance disparity.\nThe final consistency loss \u2112\u2112\\mathcal{L}caligraphic_L can be formulated as follows:where KL(P||Q)\\text{KL}(\\text{P}||\\text{Q})KL ( P | | Q ) is the Kullback-Leibler divergence function. P\u2062(x^1)\ud835\udc43subscript^\ud835\udc651P(\\hat{x}_{1})italic_P ( over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) and P\u2062(x^2)\ud835\udc43subscript^\ud835\udc652P(\\hat{x}_{2})italic_P ( over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) are the prediction distributions of the given mPLM for the inputs x^1subscript^\ud835\udc651\\hat{x}_{1}over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and x^2subscript^\ud835\udc652\\hat{x}_{2}over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT in different languages, respectively.Datasets.As shown in Table 1, our experiments are conducted on various multilingual benchmarks: XNLI\u00a0Conneau et\u00a0al. (2018), PAWS-X\u00a0Yang et\u00a0al. (2019), XCOPA\u00a0(Ponti et\u00a0al., 2020) and GeoMLAMA\u00a0Yin et\u00a0al. (2022).\nExperimental Settings.\nWe follow the cross-lingual transfer setting as\u00a0Lauscher et\u00a0al. (2020), first fine-tuning the model with an English training set and directly evaluating the model on multilingual test sets.\nWe apply ALSACE\u00a0to the fine-tuned model using unlabeled multilingual inputs X\ud835\udc4bXitalic_X from T\ud835\udc47Titalic_T languages in order to address the language-level performance disparity across those languages.\nSpecifically, We firstly use data generation methods, Supergen\u00a0Meng et\u00a0al. (2022), which employ a language model to automatically generate text based on label-descriptive prompts, producing monolingual unlabeled data.\nNext, we use machine translation111The translation API from http://api.fanyi.baidu.com/ is utilized for generating multilingual parallel data. to translate generated monolingual data and create unlabeled parallel multilingual pairs.\nBy combining the data generation method and machine translation system, we establish an automated pipeline for generating unlabeled parallel corpora with minimal cost.\nBaselines. We take the XLM-Align\u00a0(Chi et\u00a0al., 2021b), XLMR-adapter256subscriptXLMR-adapter256\\text{XLMR-adapter}_{256}XLMR-adapter start_POSTSUBSCRIPT 256 end_POSTSUBSCRIPT\u00a0(He et\u00a0al., 2021), InfoXLM\u00a0(Chi et\u00a0al., 2021a), VECO\u00a0(Luo et\u00a0al., 2021), ERNIE-M\u00a0(Ouyang et\u00a0al., 2021)\nand XLE\u00a0(Chi et\u00a0al., 2022) as baselines.Details can be found in Appendix\u00a0A.1 and\u00a0A.2.Overall Performance.\n\nThe results presented in Table\u00a02 demonstrate that ALSACE\u00a0achieves the lowest cross-lingual transfer gaps across different baselines on XNLI for various mPLMs.\nALSACE\u00a0yields an improvement of up to 0.6 points, 2.05 points, and 1.88 points, respectively, in average accuracy compared with XLM-R-base, XLM-R-large, and mT5-large baselines.\nImportantly, we achieve competitive performance with state-of-the-art methods across different mPLMs while improving the cross-lingual transferability of mPLMs without introducing any extra information.\nFor example, InfoXLM\u00a0(Chi et\u00a0al., 2021a), which is also based on XLM-R, uses 42GB of multilingual parallel data for pretraining. In contrast, ALSACE\u00a0depends solely on a small volume of unlabeled parallel data (500-shot), which can be automatically generated with minimal effort and and exhibits superior cross-lingual transferability compared to other baselines.\nWhile we also utilize parallel data to enhance cross-lingual transferability, our motivation diverges:\nInstead of aligning multilingual representations through parallel data, our goal is to leverage the knowledge from teacher languages within mPLMs to supervise others.\nThe 500-shot unlabeled parallel data in ALSACE\u00a0are exclusively used to distill the knowledge of other languages in mPLMs.\nAs a result, Table\u00a02 shows performance enhancement and cross-lingual transfer gap reduction for most languages across different models.\nIn comparison to state-of-the-art methods, ALSACE\u00a0does not mandate an extensive pre-training process or a large number of parallel corpora while achieving competitive performance and minimizing the cross-lingual transfer gaps.Mitigating Languages-Level Performance Disparity. ALSACE\u00a0effectively mitigates the language-level performance disparity of mPLMs and shows consistent improvements across different mPLMs in both high-resource and low-resource languages.\nSpecifically, not only do the student languages achieve higher-than-average improvements, but teacher languages also benefit from the guidance of their peers.\nThrough self-distillation, ALSACE\u00a0facilitates cross-language knowledge transferring among both teacher and student languages. It also enables teacher languages to learn from each other.\nEven high-resource languages like French and Spanish have shown improvement across various mPLMs, which further supports this claim.\nNotably, low-resource languages such as Swahili and Urdu experience substantial gains with ALSACE\u00a0, achieving improvements of 2.7 points and 2.4 points, respectively. These gains are particularly significant considering the relatively limited knowledge stored in multilingual pretrained language models (mPLMs) for these languages compared to other languages.Compared with other baselines, ALSACE\u00a0effectively reduces language-level performance disparities in mPLMs across various languages and minimizes the cross-lingual transfer gap.\nWhile some methods have enhanced overall performance, they have exacerbated the performance discrepancies between languages.\nThey incorporated additional knowledge from the extensive parallel multilingual corpora into mPLMs. However, knowledge disparities persist and may even worsen, leading to increased cross-lingual transfer gaps.\nWe also perform ALSACE\u00a0across different tasks, such as PAWS-X and XCOPA. The result in Table\u00a06 and Table\u00a09 shows that ALSACE\u00a0reduces the languages-level performance disparity of mPLMs.\nAblation Study on Teacher Language Selection\nTo evaluate the effectiveness of Teacher Language Selection, we conduct an ablation study using XLM-R-large as backbone. We reported average performance and cross-lingual transfer gaps of different language groups in Table\u00a03. It provides strong evidence for the effectiveness of our method.Generally, the implementation of Teacher Language Selection in ALSACE significantly reduces the cross-lingual transfer gaps while improving performance across all languages, particularly for the student languages.\nIt validates that, despite the efficacy of self-distillation, selecting adaptive teacher languages is crucial for boosting overall performance. With Teacher Language Selection, student languages achieve above-average improvements in both performance and cross-lingual transferability.Specifically, when comparing ALSACE with other baselines, besides a performance improvement, there is a substantial reduction in the cross-lingual transfer gaps for all languages, particularly for student languages.\nALSACE reduces the cross-lingual transfer gaps for student languages, ranging from 0.70 to 0.98 points and between 0.47 to 0.51 points for teacher languages.Furthermore, excluding the teacher language selection diminishes the performance of student languages, limiting their ability to benefit from self-distillation.\nThis results in an average performance decrease of 0.34 points and an increase of 0.73 points in cross-lingual transfer gaps for student languages.\nALSACE still outperforms the random selection by 0.42 points in performance and reduces cross-lingual transfer gaps for student languages by 0.77 points. These comparisons underscore the importance of selecting adaptive teacher languages.Additionally, we remove some languages from distillation.\nFirst, we removed languages that exhibited weak performance from student languages. As expected, without the guidance of teacher languages, the performance of student languages remained poorly, with an observed increase in cross-lingual transfer gaps by 0.98 points. Subsequently, excluding languages with weak performance from teachers also led to a decrease in performance for both teachers and students by 0.34 and 0.63 points, respectively.\nIt underscores our hypothesis that the underperforming languages can serve as effective guidance for other languages due to closer linguistic proximity between languages.\nFurther details of the ablation study can be found in Appendix\u00a0A.6.\n\nAblation Study on Cross-lingual Self-Distillation\nTo further investigate the source of performance increase and validate the effectiveness of the self-distillation, we conducted additional experiments with self-distillation methods as baselines with the following two settings:English-Only Self-Training\u00a0Schick and Sch\u00fctze (2020): We utilize the model fine-tuned on an English training set to produce pseudo-labels for the unlabeled English data used in cross-lingual self-distillation. Then, we choose the top 50% of data with high confidence to fine-tune the model.Full-Language Self-Training: We generate pseudo-labels for translated multilingual data in all languages and select the top 50% of multilingual data with high confidence to fine-tune the model.We apply these two methods on mT5\u00a0Xue et\u00a0al. (2020) and XLM-R \u00a0(Conneau et\u00a0al., 2019) as baselines. As shown in Table\u00a04, ALSACE\u00a0outperforms all the self-distillation baselines on XNLI while improving the cross-lingual transferability of mPLMs, especially for the student languages.\nIt validates our method and indicates that ALSACE\u2019s improved performance stems from our self-distillation rather than from the incorporation of multilingual data. We also compared our method with other state-of-the-art self-distillation methods in Appendix\u00a0A.3.2.In scenarios with limited resources, where acquiring training data is extremely difficult (even for English), mitigating language-level performance disparities in mPLMs can be more challenging and crucial.\nTherefore, to further evaluate the effectiveness of ALSACE\u00a0, we performed experiments on both XNLI and PAWS-X datasets in such scenarios.\nSpecifically, to simulate a limited resource scenario for XNLI, we fine-tune the mPLMs on 128128128128-shot English labeled examples as the baseline. Similarly, for PAWS-X, we fine-tune the mPLMs on 512512512512-shot English labeled examples. Further details can be found in Appendix\u00a0A.1.\nTo minimize the impact of the unlabeled multilingual parallel data used in \u00a0ALSACE\u00a0, and thoroughly investigate the efficacy of self-distillation in\u00a0ALSACE\u00a0in limited resource situations, we also introduce two additional baselines: English-Only Self-Training(E. Self-Train) and Full-Language Self-Training(F. Self-Train).\nThe results in Table\u00a05 and Table\u00a06 despite that ALSACE\u00a0consistently improve the performance of all languages even when the training data is minimal. It underscores that ALSACE\u00a0improves model performance not by relying on the parallel corpora but by leveraging the knowledge of teacher languages gained from the mPLM pre-training stage, hence proving its robustness and efficiency in limited-resource settings.The knowledge stored within mPLMs can be categorized into language-agnostic knowledge related to general tasks such as XNLI, which are based on logic and conceptual understanding, and language-specific knowledge related to specific linguistic and cultural factors.\nIn order to evaluate the ALSACE\u00a0\u2019s ability to alleviate performance disparity by reducing knowledge disparity and thereby improving overall performance, we conducted knowledge probing in GeoMLAMA to evaluate the changes in language-specific knowledge of mPLMs. We use the accuracy of question answers grouped according to countries and languages to measure the knowledge of mPLMs.We examined the changes in language-specific knowledge gains before and after applying ALSACE\u00a0as shown in Figure\u00a02.\nResults show that ALSACE\u00a0improves the performance of mPLM on knowledge probing tasks over various languages.\nMore details can be found in Table\u00a012 in Appendix.Notably, as shown in Figure\u00a01, after applying Cross-lingual Self-Distillation, the specific knowledge of teacher languages can be transferred to other languages. It can be found out that under the guidance of teacher languages, other languages answer the geo-specific question correctly. For instance, as shown in the first sub-figure in Figure\u00a02, English leverages its US-specific knowledge for other languages, leading to overall improvements for those respective languages.\nSimilar results are observed in other sub-figures. This result strongly suggests that mPLMs capture far more knowledge than people previously believed, and language-specific knowledge remains a treasure for better alignment.Furthermore, we explore whether ALSACE\u00a0successfully enhances language-agnostic knowledge over languages. Therefore, as demonstrated in Figure\u00a03, we evaluate the numbers of the accurately answered questions on the XNLI benchmark. This improvement demonstrates that the language-agnostic knowledge across different languages in mPLMs can mutually learn from each other. Our method reinforces the shared knowledge among the languages by bridging the knowledge disparity. As a result, we ensure that the efficacy of our method relies on alleviating the knowledge disparities across languages, including language-agnostic and language-special knowledge.\nIn this paper, we present ALSACE\u00a0, a simple yet effective method to address the language-level performance disparity in mPLMs.\nALSACE\u00a0mainly consists of two stages: Teacher Language Selection and Cross-Lingual Self-Distillation.\nALSACE\u00a0leverages the knowledge learned from the teacher languages to guide other languages and further improves the overall performance and cross-lingual transferability of mPLMs.\nExperiments show that ALSACE effectively mitigates language-level performance disparity and shows competitive performance on various multilingual datasets.\nIn addition, we further analyze each part of the ALSACE\u00a0to show the strengths of our proposed model.\nOverall, ALSACE\u00a0is a promising approach to mitigating language-level performance disparity of mPLMs by utilizing self-distillation to reduce the performance disparity.\nOur work has three limitations:1) We conduct experiments on a limited number of languages compared to the total number supported by mPLMs. Additionally, we only test other methods on the base and large model sizes of mT5 and XLM-R models. Therefore, in future work, we plan to extend our research to more languages and different mPLMs in different model sizes.2) In the grand scheme of things, the languages we evaluate are relatively high-resource compared to some extremely low-resource languages such as Kaixana and Ainu. Improving our method on these extremely low-resource languages will be more exciting and meaningful. We plan to explore even more data-scarce settings in future work.3)\nWe use the cross-lingual transfer gap to measure mPLMs\u2019 cross-lingual transferability, aligning with prevailing research. However,if we reservedly enhances the performance of non-English languages while improving English greatly,\nthe model\u2019s transfer gap could still be high despite the improvement in all languages.\nHence, we advocate for the development of the metric that can better reflect the performance equity and utility in multilingual models.We extend our heartfelt gratitude to the anonymous reviewers whose dedication and insightful feedback have significantly enhanced the caliber of this paper. Their constructive critiques and valuable suggestions were instrumental in refining our work. Additionally, we are deeply appreciative of the Program Chairs and Area Chairs for their meticulous handling of our submission and for their comprehensive and invaluable feedback. Their guidance has been pivotal in elevating the quality of our research.\nThis work is supported by the National Science Foundation of China under Grant No.61936012 and 61876004.Implement Details. The unlabeled data used in ALSACE\u00a0is constructed by Supergen\u00a0Meng et\u00a0al. (2022), which uses PLM to generate text guided by label-descriptive prompts.\nWe use machine translation222The translation API from http://api.fanyi.baidu.com/ is used to generate the multilingual parallel data. to generate unlabeled parallel multilingual text pairs based on the generated text. We leverage data generation methods(Supergen) and machine translation systems to construct an automatic pipeline for generating this valuable unlabeled parallel corpus at the lowest cost.\nWe perform ALSACE\u00a0on mPLMs using 500500500500-shot unlabeled multilingual data with batch size 32323232 on each language corresponding to the tasks of XNLI, PAWS-X, and XCOPA.\nWe set the learning rate to 3\u2062e\u221283\ud835\udc5283e-83 italic_e - 8, and a dropout rate of 0.10.10.10.1.\nThe thresholds \u03b8\ud835\udf03\\thetaitalic_\u03b8 in Equation 3 are used to select the teacher languages are 0.06, 0.2, and 0.2 for XNLI, PAWS-X, and XCOPA, respectively. We set the threshold \u03b8\ud835\udf03\\thetaitalic_\u03b8 to be the average value of the language score st^^subscript\ud835\udc60\ud835\udc61\\hat{s_{t}}over^ start_ARG italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG across all languages.To evaluate the effectiveness of ALSACE\u00a0in limited resource scenarios,\nwe fine-tune the mPLMs for 100 epochs with learning-rate of 1\u2062e\u221261\ud835\udc5261e-61 italic_e - 6 on 128128128128-shot English labeled examples as the baseline.\nSimilarly, for PAWS-X, we fine-tune the mPLMs for 150 epochs with learning-rate of 1\u2062e\u221261\ud835\udc5261e-61 italic_e - 6 on 512512512512-shot English labeled examples.XLM-Align\u00a0(Chi et\u00a0al., 2021b) presents denoising word alignment as a new cross-lingual pre-training task with 310M instances. It self-labels word alignments for parallel sentences and haphazardly masks tokens in a bitext pair for mPLMs to predict.\n\nInfoXLM\u00a0(Chi et\u00a0al., 2021a) implements on the basis of mPLMs and tries to align the representation of multiple languages by introducing parallel corpora with a new pre-training task. Initializes its parameters with XLM-R and employs contrastive learning using 42GB parallel corpora to encourage encoded representations of bilingual sentence pairs to be more similar than negative examples.\n\nXLMR-adapter256subscriptXLMR-adapter256\\text{XLMR-adapter}_{256}XLMR-adapter start_POSTSUBSCRIPT 256 end_POSTSUBSCRIPT\u00a0(He et\u00a0al., 2021) employs lightweight adapter modules on the XLM-R-large and achieves significant performances on low-resource and cross-lingual tasks.\n\nERNIE-M\u00a0(Ouyang et\u00a0al., 2021) is similar to InfoXLM and XLM-Align, which is implemented on the basis of XLM-R. It integrates back-translation into the pre-training process to encourage the model to align the representation of multiple languages with parallel corpora of about 68.8GB.\n\nVECO\u00a0(Luo et\u00a0al., 2021) plug a cross-attention module into the transformer encoder to explicitly build the interdependence between languages to pretrain a variable cross-lingual language model for both NLU and NLG.\n\nXLE\u00a0(Chi et\u00a0al., 2022) use ELECTRA-style tasks for pre-training mPLMs with a generator and discriminator structure using 142B tokens.\nInfoXLM\u00a0(Chi et\u00a0al., 2021a) initializes its parameters with XLM-R and employs contrastive learning using 42GB parallel corpora to encourage encoded representations of bilingual sentence pairs to be more similar than negative examples.ERNIE-M\u00a0(Ouyang et\u00a0al., 2021) is implemented on the basis of XLM-R, and it integrates back-translation into the pre-training process to encourage the model to align the representation of multiple languages with parallel corpora of about 68.8GB.\nWhile InfoXLM and ERNIE-M are built upon the basis of XLM-R by utilizing 42GB and 68.8GB data, respectively, our method only relies on a small amount of unlabeled parallel corpora (500-shot), which can be easily constructed with minimal effort.\nDespite this minimal requirement, our approach achieves substantial enhancements compared to the baseline XLM-R model. Table\u00a07 illustrates the improvement of different methods across all languages on the XNLIdataset in comparison with the initial XLM-R-large baseline.Qi et\u00a0al. (2022) introduced PCT, a method that learns from various cross-lingual templates through a consistency loss, ensuring corresponding representations are aligned across languages.\nAs indicated in Table\u00a08, our ALSACE\u00a0surpasses PCT-XLM-R-large in performance and demonstrates superior cross-lingual transferabilities. Thanks to the teacher language selection, ALSACE\u00a0not only minimizes the performance disparities among the student languages but also enables the teacher languages to benefit from self-distillation. This approach yields improved overall performance and narrows the cross-lingual transfer gaps more effectively than PCT-XLM-R-large.We evaluate ALSACE\u00a0on the XCOPA benchmark, which is the causal commonsense reasoning benchmark across a range of typologically diverse languages, including both high and low-resource languages.\nFollowing the setting of Ponti et\u00a0al. (2020), models are either fine-tuned solely on the COPA\u00a0Roemmele et\u00a0al. (2011) training set and then evaluated on XCOPA\u2019s multilingual test sets or sequentially fine-tuned\u2014initially on the SIQA dataset\u00a0Sap et\u00a0al. (2019) followed by the COPA training set. Results in Table\u00a09 show that our method achieves substantial performance gains in most languages under various settings across different model sizes. These outcomes underscore the robustness and overall effectiveness of our method.To explore whether we need to select the teacher languages before transferring knowledge, we design an exploratory experiment on the XNLI dataset to demonstrate that selecting teacher language is necessary.\nWe measure the contribution of different ensemble strategies to model performance.\nSpecifically,\nlanguage Weighted: For predicted labels and confidence scores from different languages, we use the confidence score of each language as weights and calculate the final ensemble prediction.Best Performing Language (en): We use the results predicted by English as the final prediction.\nVoted: We give the same weight to the predicted labels for each language and get the final prediction result based on the voting result.Figure\u00a04 compares different multilingual models using different ensemble methods on the XNLI benchmark.\nVoted does not perform well due to noise from the under-performing student languages. On the other hand, by using the normalized language score P\u2062(s\u2062t)\ud835\udc43\ud835\udc60\ud835\udc61P(st)italic_P ( italic_s italic_t ) as weights for each language output in ensembling, it surpasses the performance of English, which is considered the best-performing high-resource language.\nThis noteworthy discrepancy indicates that high-resource languages may not be suitable teacher languages. Besides high-resource languages, other languages also contribute to enhancing model performance.\nFigure\u00a02 shows an experiment on GeomLAMA, demonstrating that high-resource languages may not be the most suitable for probing knowledge about a specific language condition. For instance, when addressing a query related to Chinese culture, Persian might yield a more accurate answer compared to English.\nWe conducted an ablation study to investigate the impact of teacher language selection, with detailed results provided in Figure\u00a010. A comparison of ALSACE\u2019s performance with and without including student-student pairs indicates that even though there is a performance improvement when student-student pairs are excluded, a significant performance gap remains compared to the complete ALSACE model. This is particularly evident for student languages, as detailed in Table\u00a013.\nAdditionally, when focusing on the student languages, such as Swahili and Urdu, the exclusion of student-student pairs results in comparatively diminished benefits from self-distillation.The results clearly demonstrate that while the improvements persist, the performance of the ALSACE model employing randomly selected teacher languages still needs to catch up to the full ALSACE model across nearly all languages. This finding further underscores the efficacy of the teacher language selection strategy.\nALSACE demonstrates competitive performance across various baselines, achieving notable results even with a limited amount of unlabeled parallel data. We successfully alleviated the performance disparities among different languages.\nAs for the performance disparities, while there might still exist some gaps among different languages, ALSACE effectively mitigates these disparities, especially evident in languages like Swahili (sw), Urdu (ur), and Thai (th), as showcased in the performance comparison with English (en) in Table\u00a02. This aligns with our motivation to enhance cross-lingual transferability.\nFigure\u00a02 shows the detailed experiment results on GeomLAMA\u2018\u00a0(Yin et\u00a0al., 2022), which demonstrates that ALSACE\u00a0improves the performance of mPLM on knowledge probing tasks over various languages."
}